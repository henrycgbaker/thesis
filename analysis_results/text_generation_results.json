[
    {
        "CONFIGURATION_RUN_#0220": {
            "setup": {
                "experiment_id": "0220",
                "date_time": "April 11, 2025 at 04:28:24 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A1_Max_Throughput_Exploit",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 12.157593134001218,
                        "average_latency_ms_per_batch": 12157.593134001218,
                        "throughput_queries_per_sec": 10.528399707835392,
                        "throughput_tokens_per_sec": 1347.6351626029302
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1577541632,
                        "gpu_max_memory_allocated_bytes": 1577541632,
                        "gpu_current_memory_reserved_bytes": 3577741312,
                        "gpu_max_memory_reserved_bytes": 3577741312
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.7,
                        "cpu_memory_usage_bytes": 2697252864
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0220",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 4047.539154390602,
                            "process_2": 606.4873112413463,
                            "process_3": 904.6040093190243,
                            "process_0": 2188.3624207290413
                        },
                        "ram_power": {
                            "process_1": 0.9872045516967775,
                            "process_2": 0.9921913146972655,
                            "process_3": 0.9874148368835449,
                            "process_0": 0.9412450790405273
                        },
                        "cpu_energy": {
                            "process_1": 0.0003382849217499597,
                            "process_2": 0.0002858009428437072,
                            "process_3": 0.0003432443411250005,
                            "process_0": 0.0003567094571252482
                        },
                        "gpu_energy": {
                            "process_1": 0.0014911106373318161,
                            "process_2": 0.0014895775805498501,
                            "process_3": 0.0014911106373318161,
                            "process_0": 0.0014911106373318161
                        },
                        "ram_energy": {
                            "process_1": 3.147130867990988e-06,
                            "process_2": 2.966875335554834e-06,
                            "process_3": 2.9373167249872105e-06,
                            "process_0": 2.9125490843718655e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0018325426899497667,
                            "process_2": 0.001778345398729112,
                            "process_3": 0.001837292295181804,
                            "process_0": 0.0018507326435414362
                        },
                        "total_energy_joules": {
                            "process_1": 6597.1536838191605,
                            "process_2": 6402.043435424803,
                            "process_3": 6614.252262654494,
                            "process_0": 6662.637516749171
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1936.7482239200035,
                        "ram_power_avg": 0.9770139455795288,
                        "cpu_energy_total": 0.0013240396628439156,
                        "gpu_energy_total": 0.0059629094925452986,
                        "ram_energy_total": 1.1963872012904898e-05,
                        "total_energy_kwh": 0.007298913027402119,
                        "total_energy_joules": 26276.086898647627
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.6235327224786751,
                        "joules_per_token": 1.6037650694975358,
                        "flops_per_joule": 645072116.6561669,
                        "joules_per_flop": 1.5502142693496935e-09
                    },
                    "per-process_emissions": [
                        0.0006981071377363637,
                        0.0006774606796458552,
                        0.0006999164998495082,
                        0.0007050366005571101
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0220": {
            "setup": {
                "experiment_id": "0220",
                "date_time": "April 11, 2025 at 04:28:24 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A1_Max_Throughput_Exploit",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 12.157593134001218,
                        "average_latency_ms_per_batch": 12157.593134001218,
                        "throughput_queries_per_sec": 10.528399707835392,
                        "throughput_tokens_per_sec": 1347.6351626029302
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1577541632,
                        "gpu_max_memory_allocated_bytes": 1577541632,
                        "gpu_current_memory_reserved_bytes": 3577741312,
                        "gpu_max_memory_reserved_bytes": 3577741312
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.7,
                        "cpu_memory_usage_bytes": 2697252864
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0220",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 4047.539154390602,
                            "process_2": 606.4873112413463,
                            "process_3": 904.6040093190243,
                            "process_0": 2188.3624207290413
                        },
                        "ram_power": {
                            "process_1": 0.9872045516967775,
                            "process_2": 0.9921913146972655,
                            "process_3": 0.9874148368835449,
                            "process_0": 0.9412450790405273
                        },
                        "cpu_energy": {
                            "process_1": 0.0003382849217499597,
                            "process_2": 0.0002858009428437072,
                            "process_3": 0.0003432443411250005,
                            "process_0": 0.0003567094571252482
                        },
                        "gpu_energy": {
                            "process_1": 0.0014911106373318161,
                            "process_2": 0.0014895775805498501,
                            "process_3": 0.0014911106373318161,
                            "process_0": 0.0014911106373318161
                        },
                        "ram_energy": {
                            "process_1": 3.147130867990988e-06,
                            "process_2": 2.966875335554834e-06,
                            "process_3": 2.9373167249872105e-06,
                            "process_0": 2.9125490843718655e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0018325426899497667,
                            "process_2": 0.001778345398729112,
                            "process_3": 0.001837292295181804,
                            "process_0": 0.0018507326435414362
                        },
                        "total_energy_joules": {
                            "process_1": 6597.1536838191605,
                            "process_2": 6402.043435424803,
                            "process_3": 6614.252262654494,
                            "process_0": 6662.637516749171
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1936.7482239200035,
                        "ram_power_avg": 0.9770139455795288,
                        "cpu_energy_total": 0.0013240396628439156,
                        "gpu_energy_total": 0.0059629094925452986,
                        "ram_energy_total": 1.1963872012904898e-05,
                        "total_energy_kwh": 0.007298913027402119,
                        "total_energy_joules": 26276.086898647627
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.6235327224786751,
                        "joules_per_token": 1.6037650694975358,
                        "flops_per_joule": 645072116.6561669,
                        "joules_per_flop": 1.5502142693496935e-09
                    },
                    "per-process_emissions": [
                        0.0006981071377363637,
                        0.0006774606796458552,
                        0.0006999164998495082,
                        0.0007050366005571101
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0221": {
            "setup": {
                "experiment_id": "0221",
                "date_time": "April 11, 2025 at 04:29:05 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A2_Precision_Minimalist",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 5.1173602449998725,
                        "average_latency_ms_per_batch": 5117.3602449998725,
                        "throughput_queries_per_sec": 25.012896077634494,
                        "throughput_tokens_per_sec": 3201.650697937215
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 2621440000,
                        "gpu_max_memory_reserved_bytes": 2621440000
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.6,
                        "cpu_memory_usage_bytes": 2639552512
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0221",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 223.03708371016037,
                            "process_1": 5196.196127283723
                        },
                        "ram_power": {
                            "process_0": 0.9211421012878418,
                            "process_1": 0.9630231857299805
                        },
                        "cpu_energy": {
                            "process_0": 0.00015832376756236497,
                            "process_1": 9.560317824991671e-05
                        },
                        "gpu_energy": {
                            "process_0": 0.0005738432368518898,
                            "process_1": 0.0005708993456078892
                        },
                        "ram_energy": {
                            "process_0": 1.1423632414408832e-06,
                            "process_1": 1.1866301329359867e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0007333093676556956,
                            "process_1": 0.0006676891539907418
                        },
                        "total_energy_joules": {
                            "process_0": 2639.9137235605044,
                            "process_1": 2403.6809543666704
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 2709.6166054969417,
                        "ram_power_avg": 0.9420826435089111,
                        "cpu_energy_total": 0.0002539269458122817,
                        "gpu_energy_total": 0.001144742582459779,
                        "ram_energy_total": 2.32899337437687e-06,
                        "total_energy_kwh": 0.0014009985216464375,
                        "total_energy_joules": 5043.594677927174
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 3.248476740548375,
                        "joules_per_token": 0.3078365892289535,
                        "flops_per_joule": 3360692536.878901,
                        "joules_per_flop": 2.975577173533129e-10
                    },
                    "per-process_emissions": [
                        0.00027935420360843723,
                        0.0002543561832127731
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0221": {
            "setup": {
                "experiment_id": "0221",
                "date_time": "April 11, 2025 at 04:29:05 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A2_Precision_Minimalist",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 5.1173602449998725,
                        "average_latency_ms_per_batch": 5117.3602449998725,
                        "throughput_queries_per_sec": 25.012896077634494,
                        "throughput_tokens_per_sec": 3201.650697937215
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 2621440000,
                        "gpu_max_memory_reserved_bytes": 2621440000
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.6,
                        "cpu_memory_usage_bytes": 2639552512
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0221",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 223.03708371016037,
                            "process_1": 5196.196127283723
                        },
                        "ram_power": {
                            "process_0": 0.9211421012878418,
                            "process_1": 0.9630231857299805
                        },
                        "cpu_energy": {
                            "process_0": 0.00015832376756236497,
                            "process_1": 9.560317824991671e-05
                        },
                        "gpu_energy": {
                            "process_0": 0.0005738432368518898,
                            "process_1": 0.0005708993456078892
                        },
                        "ram_energy": {
                            "process_0": 1.1423632414408832e-06,
                            "process_1": 1.1866301329359867e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0007333093676556956,
                            "process_1": 0.0006676891539907418
                        },
                        "total_energy_joules": {
                            "process_0": 2639.9137235605044,
                            "process_1": 2403.6809543666704
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 2709.6166054969417,
                        "ram_power_avg": 0.9420826435089111,
                        "cpu_energy_total": 0.0002539269458122817,
                        "gpu_energy_total": 0.001144742582459779,
                        "ram_energy_total": 2.32899337437687e-06,
                        "total_energy_kwh": 0.0014009985216464375,
                        "total_energy_joules": 5043.594677927174
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 3.248476740548375,
                        "joules_per_token": 0.3078365892289535,
                        "flops_per_joule": 3360692536.878901,
                        "joules_per_flop": 2.975577173533129e-10
                    },
                    "per-process_emissions": [
                        0.00027935420360843723,
                        0.0002543561832127731
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0222": {
            "setup": {
                "experiment_id": "0222",
                "date_time": "April 11, 2025 at 04:29:46 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A3_Quantisation_Gaming",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 9.567340258003242,
                        "average_latency_ms_per_batch": 4783.670129001621,
                        "throughput_queries_per_sec": 13.378848932745528,
                        "throughput_tokens_per_sec": 1712.4926633914276
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1986002944,
                        "gpu_max_memory_reserved_bytes": 1986002944
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 0.3,
                        "cpu_memory_usage_bytes": 2605256704
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0222",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 59.55226010267821
                        },
                        "ram_power": {
                            "process_0": 0.909635066986084
                        },
                        "cpu_energy": {
                            "process_0": 0.00037410548700006534
                        },
                        "gpu_energy": {
                            "process_0": 0.00038689614284997953
                        },
                        "ram_energy": {
                            "process_0": 2.520237777000544e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0007635218676270454
                        },
                        "total_energy_joules": {
                            "process_0": 2748.6787234573635
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 59.55226010267821,
                        "ram_power_avg": 0.909635066986084,
                        "cpu_energy_total": 0.00037410548700006534,
                        "gpu_energy_total": 0.00038689614284997953,
                        "ram_energy_total": 2.520237777000544e-06,
                        "total_energy_kwh": 0.0007635218676270454,
                        "total_energy_joules": 2748.6787234573635
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 5.960682076147392,
                        "joules_per_token": 0.16776603536727072,
                        "flops_per_joule": 6166588640.753133,
                        "joules_per_flop": 1.6216421400177405e-10
                    },
                    "per-process_emissions": [
                        0.0002908636554725229
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0222": {
            "setup": {
                "experiment_id": "0222",
                "date_time": "April 11, 2025 at 04:29:46 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A3_Quantisation_Gaming",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 9.567340258003242,
                        "average_latency_ms_per_batch": 4783.670129001621,
                        "throughput_queries_per_sec": 13.378848932745528,
                        "throughput_tokens_per_sec": 1712.4926633914276
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1986002944,
                        "gpu_max_memory_reserved_bytes": 1986002944
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 0.3,
                        "cpu_memory_usage_bytes": 2605256704
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0222",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 59.55226010267821
                        },
                        "ram_power": {
                            "process_0": 0.909635066986084
                        },
                        "cpu_energy": {
                            "process_0": 0.00037410548700006534
                        },
                        "gpu_energy": {
                            "process_0": 0.00038689614284997953
                        },
                        "ram_energy": {
                            "process_0": 2.520237777000544e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0007635218676270454
                        },
                        "total_energy_joules": {
                            "process_0": 2748.6787234573635
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 59.55226010267821,
                        "ram_power_avg": 0.909635066986084,
                        "cpu_energy_total": 0.00037410548700006534,
                        "gpu_energy_total": 0.00038689614284997953,
                        "ram_energy_total": 2.520237777000544e-06,
                        "total_energy_kwh": 0.0007635218676270454,
                        "total_energy_joules": 2748.6787234573635
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 5.960682076147392,
                        "joules_per_token": 0.16776603536727072,
                        "flops_per_joule": 6166588640.753133,
                        "joules_per_flop": 1.6216421400177405e-10
                    },
                    "per-process_emissions": [
                        0.0002908636554725229
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0223": {
            "setup": {
                "experiment_id": "0223",
                "date_time": "April 11, 2025 at 04:31:30 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "default",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 71.47325737600113,
                        "average_latency_ms_per_batch": 8934.157172000141,
                        "throughput_queries_per_sec": 1.7908796198643535,
                        "throughput_tokens_per_sec": 229.23259134263725
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576560640,
                        "gpu_max_memory_allocated_bytes": 1576560640,
                        "gpu_current_memory_reserved_bytes": 2707423232,
                        "gpu_max_memory_reserved_bytes": 2707423232
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 0.8,
                        "cpu_memory_usage_bytes": 2712571904
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0223",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 69.07226461343014
                        },
                        "ram_power": {
                            "process_0": 0.947124481201172
                        },
                        "cpu_energy": {
                            "process_0": 0.0018796497704372539
                        },
                        "gpu_energy": {
                            "process_0": 0.0013297574526939804
                        },
                        "ram_energy": {
                            "process_0": 1.6468006483581384e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0032258752296148163
                        },
                        "total_energy_joules": {
                            "process_0": 11613.150826613339
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 69.07226461343014,
                        "ram_power_avg": 0.947124481201172,
                        "cpu_energy_total": 0.0018796497704372539,
                        "gpu_energy_total": 0.0013297574526939804,
                        "ram_energy_total": 1.6468006483581384e-05,
                        "total_energy_kwh": 0.0032258752296148163,
                        "total_energy_joules": 11613.150826613339
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.4108143642166018,
                        "joules_per_token": 0.7088104752571618,
                        "flops_per_joule": 1459549716.1983385,
                        "joules_per_flop": 6.851428141856524e-10
                    },
                    "per-process_emissions": [
                        0.0012288971687217643
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0223": {
            "setup": {
                "experiment_id": "0223",
                "date_time": "April 11, 2025 at 04:31:30 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "default",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 71.47325737600113,
                        "average_latency_ms_per_batch": 8934.157172000141,
                        "throughput_queries_per_sec": 1.7908796198643535,
                        "throughput_tokens_per_sec": 229.23259134263725
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576560640,
                        "gpu_max_memory_allocated_bytes": 1576560640,
                        "gpu_current_memory_reserved_bytes": 2707423232,
                        "gpu_max_memory_reserved_bytes": 2707423232
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 0.8,
                        "cpu_memory_usage_bytes": 2712571904
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0223",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 69.07226461343014
                        },
                        "ram_power": {
                            "process_0": 0.947124481201172
                        },
                        "cpu_energy": {
                            "process_0": 0.0018796497704372539
                        },
                        "gpu_energy": {
                            "process_0": 0.0013297574526939804
                        },
                        "ram_energy": {
                            "process_0": 1.6468006483581384e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0032258752296148163
                        },
                        "total_energy_joules": {
                            "process_0": 11613.150826613339
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 69.07226461343014,
                        "ram_power_avg": 0.947124481201172,
                        "cpu_energy_total": 0.0018796497704372539,
                        "gpu_energy_total": 0.0013297574526939804,
                        "ram_energy_total": 1.6468006483581384e-05,
                        "total_energy_kwh": 0.0032258752296148163,
                        "total_energy_joules": 11613.150826613339
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.4108143642166018,
                        "joules_per_token": 0.7088104752571618,
                        "flops_per_joule": 1459549716.1983385,
                        "joules_per_flop": 6.851428141856524e-10
                    },
                    "per-process_emissions": [
                        0.0012288971687217643
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0224": {
            "setup": {
                "experiment_id": "0224",
                "date_time": "April 11, 2025 at 04:32:11 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A5_Parallel_Overdrive",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 6.2019338450008945,
                        "average_latency_ms_per_batch": 3100.9669225004473,
                        "throughput_queries_per_sec": 20.638723855975208,
                        "throughput_tokens_per_sec": 2641.7566535648266
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419853312,
                        "gpu_max_memory_allocated_bytes": 4419853312,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.9,
                        "cpu_memory_usage_bytes": 3103969280
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0224",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 64.72361360286786,
                            "process_3": 66.87737014124544,
                            "process_1": 3296.80618736416,
                            "process_0": 65.82146133142193
                        },
                        "ram_power": {
                            "process_2": 0.8782467842102051,
                            "process_3": 0.8707194328308105,
                            "process_1": 0.8544831275939941,
                            "process_0": 1.0826983451843262
                        },
                        "cpu_energy": {
                            "process_2": 0.000237196743468985,
                            "process_3": 0.0002347435877499038,
                            "process_1": 0.00020123289228104115,
                            "process_0": 0.00023654785606242967
                        },
                        "gpu_energy": {
                            "process_2": 0.0011499214754918663,
                            "process_3": 0.0011512289765378547,
                            "process_1": 0.0011321989613138905,
                            "process_0": 0.0011512289765378547
                        },
                        "ram_energy": {
                            "process_2": 1.6198905658494768e-06,
                            "process_3": 1.5783251997829352e-06,
                            "process_1": 1.536037980769608e-06,
                            "process_0": 1.9763945208285857e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0013887381095267007,
                            "process_3": 0.0013875508894875413,
                            "process_1": 0.0013349678915757013,
                            "process_0": 0.001389753227121113
                        },
                        "total_energy_joules": {
                            "process_2": 4999.457194296123,
                            "process_3": 4995.183202155149,
                            "process_1": 4805.884409672525,
                            "process_0": 5003.1116176360065
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 873.5571581099239,
                        "ram_power_avg": 0.921536922454834,
                        "cpu_energy_total": 0.0009097210795623596,
                        "gpu_energy_total": 0.004584578389881466,
                        "ram_energy_total": 6.710648267230605e-06,
                        "total_energy_kwh": 0.005501010117711057,
                        "total_energy_joules": 19803.636423759803
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.8273228032172402,
                        "joules_per_token": 1.2087180434423708,
                        "flops_per_joule": 855901948.0288953,
                        "joules_per_flop": 1.1683581306281126e-09
                    },
                    "per-process_emissions": [
                        0.0005290397828241967,
                        0.0005285875113502789,
                        0.0005085560182957634,
                        0.000529426491871788
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0224": {
            "setup": {
                "experiment_id": "0224",
                "date_time": "April 11, 2025 at 04:32:11 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A5_Parallel_Overdrive",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 6.2019338450008945,
                        "average_latency_ms_per_batch": 3100.9669225004473,
                        "throughput_queries_per_sec": 20.638723855975208,
                        "throughput_tokens_per_sec": 2641.7566535648266
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419853312,
                        "gpu_max_memory_allocated_bytes": 4419853312,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.9,
                        "cpu_memory_usage_bytes": 3103969280
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0224",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 64.72361360286786,
                            "process_3": 66.87737014124544,
                            "process_1": 3296.80618736416,
                            "process_0": 65.82146133142193
                        },
                        "ram_power": {
                            "process_2": 0.8782467842102051,
                            "process_3": 0.8707194328308105,
                            "process_1": 0.8544831275939941,
                            "process_0": 1.0826983451843262
                        },
                        "cpu_energy": {
                            "process_2": 0.000237196743468985,
                            "process_3": 0.0002347435877499038,
                            "process_1": 0.00020123289228104115,
                            "process_0": 0.00023654785606242967
                        },
                        "gpu_energy": {
                            "process_2": 0.0011499214754918663,
                            "process_3": 0.0011512289765378547,
                            "process_1": 0.0011321989613138905,
                            "process_0": 0.0011512289765378547
                        },
                        "ram_energy": {
                            "process_2": 1.6198905658494768e-06,
                            "process_3": 1.5783251997829352e-06,
                            "process_1": 1.536037980769608e-06,
                            "process_0": 1.9763945208285857e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0013887381095267007,
                            "process_3": 0.0013875508894875413,
                            "process_1": 0.0013349678915757013,
                            "process_0": 0.001389753227121113
                        },
                        "total_energy_joules": {
                            "process_2": 4999.457194296123,
                            "process_3": 4995.183202155149,
                            "process_1": 4805.884409672525,
                            "process_0": 5003.1116176360065
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 873.5571581099239,
                        "ram_power_avg": 0.921536922454834,
                        "cpu_energy_total": 0.0009097210795623596,
                        "gpu_energy_total": 0.004584578389881466,
                        "ram_energy_total": 6.710648267230605e-06,
                        "total_energy_kwh": 0.005501010117711057,
                        "total_energy_joules": 19803.636423759803
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.8273228032172402,
                        "joules_per_token": 1.2087180434423708,
                        "flops_per_joule": 855901948.0288953,
                        "joules_per_flop": 1.1683581306281126e-09
                    },
                    "per-process_emissions": [
                        0.0005290397828241967,
                        0.0005285875113502789,
                        0.0005085560182957634,
                        0.000529426491871788
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0228": {
            "setup": {
                "experiment_id": "0228",
                "date_time": "April 11, 2025 at 04:35:15 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R2_Low_Latency_Chatbot_Deployment",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.05,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 88.56861288899017,
                        "average_latency_ms_per_batch": 2767.769152780943,
                        "throughput_queries_per_sec": 1.4452072334070785,
                        "throughput_tokens_per_sec": 184.98652587610604
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 12614369280,
                        "gpu_max_memory_reserved_bytes": 12614369280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            14.0,
                            10.0,
                            10.0,
                            9.0
                        ],
                        "cpu_usage_percent": 1.2,
                        "cpu_memory_usage_bytes": 2008952832
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0228",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 50.24471281460004
                        },
                        "ram_power": {
                            "process_0": 0.6997261047363281
                        },
                        "cpu_energy": {
                            "process_0": 0.002530753336186876
                        },
                        "gpu_energy": {
                            "process_0": 0.003196562835025951
                        },
                        "ram_energy": {
                            "process_0": 1.4294077175762423e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005741610248388594
                        },
                        "total_energy_joules": {
                            "process_0": 20669.79689419894
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 50.24471281460004,
                        "ram_power_avg": 0.6997261047363281,
                        "cpu_energy_total": 0.002530753336186876,
                        "gpu_energy_total": 0.003196562835025951,
                        "ram_energy_total": 1.4294077175762423e-05,
                        "total_energy_kwh": 0.005741610248388594,
                        "total_energy_joules": 20669.79689419894
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.7926541360741787,
                        "joules_per_token": 1.261584283093197,
                        "flops_per_joule": 820035682.0104544,
                        "joules_per_flop": 1.2194591307884711e-09
                    },
                    "per-process_emissions": [
                        0.002187266424123635
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0228": {
            "setup": {
                "experiment_id": "0228",
                "date_time": "April 11, 2025 at 04:35:15 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R2_Low_Latency_Chatbot_Deployment",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.05,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 88.56861288899017,
                        "average_latency_ms_per_batch": 2767.769152780943,
                        "throughput_queries_per_sec": 1.4452072334070785,
                        "throughput_tokens_per_sec": 184.98652587610604
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 12614369280,
                        "gpu_max_memory_reserved_bytes": 12614369280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            14.0,
                            10.0,
                            10.0,
                            9.0
                        ],
                        "cpu_usage_percent": 1.2,
                        "cpu_memory_usage_bytes": 2008952832
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0228",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 50.24471281460004
                        },
                        "ram_power": {
                            "process_0": 0.6997261047363281
                        },
                        "cpu_energy": {
                            "process_0": 0.002530753336186876
                        },
                        "gpu_energy": {
                            "process_0": 0.003196562835025951
                        },
                        "ram_energy": {
                            "process_0": 1.4294077175762423e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005741610248388594
                        },
                        "total_energy_joules": {
                            "process_0": 20669.79689419894
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 50.24471281460004,
                        "ram_power_avg": 0.6997261047363281,
                        "cpu_energy_total": 0.002530753336186876,
                        "gpu_energy_total": 0.003196562835025951,
                        "ram_energy_total": 1.4294077175762423e-05,
                        "total_energy_kwh": 0.005741610248388594,
                        "total_energy_joules": 20669.79689419894
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.7926541360741787,
                        "joules_per_token": 1.261584283093197,
                        "flops_per_joule": 820035682.0104544,
                        "joules_per_flop": 1.2194591307884711e-09
                    },
                    "per-process_emissions": [
                        0.002187266424123635
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0229": {
            "setup": {
                "experiment_id": "0229",
                "date_time": "April 11, 2025 at 04:36:43 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R3_Balanced_Enterprise_Service",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 46.186701717004325,
                        "average_latency_ms_per_batch": 11546.675429251081,
                        "throughput_queries_per_sec": 2.7713604834630328,
                        "throughput_tokens_per_sec": 354.7341418832682
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576947200,
                        "gpu_max_memory_allocated_bytes": 1576947200,
                        "gpu_current_memory_reserved_bytes": 2839543808,
                        "gpu_max_memory_reserved_bytes": 2839543808
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            1.0,
                            82.0,
                            100.0,
                            99.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 2719707136
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0229",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 270.713046832187,
                            "process_3": 620.4154330755695,
                            "process_0": 72.11380878957571,
                            "process_1": 59.898897508917514
                        },
                        "ram_power": {
                            "process_2": 0.9934415817260743,
                            "process_3": 0.9940524101257324,
                            "process_0": 0.9492945671081543,
                            "process_1": 0.9905862808227538
                        },
                        "cpu_energy": {
                            "process_2": 0.0013290986435621334,
                            "process_3": 0.001280613378718954,
                            "process_0": 0.001097041541500403,
                            "process_1": 0.00134569909181198
                        },
                        "gpu_energy": {
                            "process_2": 0.0043736587767021495,
                            "process_3": 0.004275649531628178,
                            "process_0": 0.0041172396826780755,
                            "process_1": 0.004472827744926078
                        },
                        "ram_energy": {
                            "process_2": 1.1314105958400918e-05,
                            "process_3": 1.1347000799262178e-05,
                            "process_0": 1.0177700518070003e-05,
                            "process_1": 1.184832321716432e-05
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005714071526222683,
                            "process_3": 0.005567609911146395,
                            "process_0": 0.005224458924696548,
                            "process_1": 0.005830375159955221
                        },
                        "total_energy_joules": {
                            "process_2": 20570.657494401657,
                            "process_3": 20043.39568012702,
                            "process_0": 18808.052128907573,
                            "process_1": 20989.3505758388
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 255.7852965515624,
                        "ram_power_avg": 0.9818437099456787,
                        "cpu_energy_total": 0.005052452655593471,
                        "gpu_energy_total": 0.01723937573593448,
                        "ram_energy_total": 4.468713049289742e-05,
                        "total_energy_kwh": 0.022336515522020844,
                        "total_energy_joules": 80411.45587927505
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20375206269860302,
                        "joules_per_token": 4.907925773881534,
                        "flops_per_joule": 210790500.0327276,
                        "joules_per_flop": 4.744046813517396e-09
                    },
                    "per-process_emissions": [
                        0.002176775547914531,
                        0.002120980995651219,
                        0.00199025762736315,
                        0.0022210814171849417
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0229": {
            "setup": {
                "experiment_id": "0229",
                "date_time": "April 11, 2025 at 04:36:43 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R3_Balanced_Enterprise_Service",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 46.186701717004325,
                        "average_latency_ms_per_batch": 11546.675429251081,
                        "throughput_queries_per_sec": 2.7713604834630328,
                        "throughput_tokens_per_sec": 354.7341418832682
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576947200,
                        "gpu_max_memory_allocated_bytes": 1576947200,
                        "gpu_current_memory_reserved_bytes": 2839543808,
                        "gpu_max_memory_reserved_bytes": 2839543808
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            1.0,
                            82.0,
                            100.0,
                            99.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 2719707136
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0229",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 270.713046832187,
                            "process_3": 620.4154330755695,
                            "process_0": 72.11380878957571,
                            "process_1": 59.898897508917514
                        },
                        "ram_power": {
                            "process_2": 0.9934415817260743,
                            "process_3": 0.9940524101257324,
                            "process_0": 0.9492945671081543,
                            "process_1": 0.9905862808227538
                        },
                        "cpu_energy": {
                            "process_2": 0.0013290986435621334,
                            "process_3": 0.001280613378718954,
                            "process_0": 0.001097041541500403,
                            "process_1": 0.00134569909181198
                        },
                        "gpu_energy": {
                            "process_2": 0.0043736587767021495,
                            "process_3": 0.004275649531628178,
                            "process_0": 0.0041172396826780755,
                            "process_1": 0.004472827744926078
                        },
                        "ram_energy": {
                            "process_2": 1.1314105958400918e-05,
                            "process_3": 1.1347000799262178e-05,
                            "process_0": 1.0177700518070003e-05,
                            "process_1": 1.184832321716432e-05
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005714071526222683,
                            "process_3": 0.005567609911146395,
                            "process_0": 0.005224458924696548,
                            "process_1": 0.005830375159955221
                        },
                        "total_energy_joules": {
                            "process_2": 20570.657494401657,
                            "process_3": 20043.39568012702,
                            "process_0": 18808.052128907573,
                            "process_1": 20989.3505758388
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 255.7852965515624,
                        "ram_power_avg": 0.9818437099456787,
                        "cpu_energy_total": 0.005052452655593471,
                        "gpu_energy_total": 0.01723937573593448,
                        "ram_energy_total": 4.468713049289742e-05,
                        "total_energy_kwh": 0.022336515522020844,
                        "total_energy_joules": 80411.45587927505
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20375206269860302,
                        "joules_per_token": 4.907925773881534,
                        "flops_per_joule": 210790500.0327276,
                        "joules_per_flop": 4.744046813517396e-09
                    },
                    "per-process_emissions": [
                        0.002176775547914531,
                        0.002120980995651219,
                        0.00199025762736315,
                        0.0022210814171849417
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0230": {
            "setup": {
                "experiment_id": "0230",
                "date_time": "April 11, 2025 at 04:38:09 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R4_High_Load_Cloud_API_Deployment",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 56.83997280899348,
                        "average_latency_ms_per_batch": 3552.4983005620925,
                        "throughput_queries_per_sec": 2.2519363341382044,
                        "throughput_tokens_per_sec": 288.24785076969016
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419853312,
                        "gpu_max_memory_allocated_bytes": 4419853312,
                        "gpu_current_memory_reserved_bytes": 6339690496,
                        "gpu_max_memory_reserved_bytes": 6339690496
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            3.0,
                            8.0,
                            10.0,
                            8.0
                        ],
                        "cpu_usage_percent": 0.9,
                        "cpu_memory_usage_bytes": 3101462528
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0230",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 81.04893533920794
                        },
                        "ram_power": {
                            "process_0": 1.081179141998291
                        },
                        "cpu_energy": {
                            "process_0": 0.0016094864314067083
                        },
                        "gpu_energy": {
                            "process_0": 0.0012835163045900044
                        },
                        "ram_energy": {
                            "process_0": 1.4990509342216345e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.00290799324533893
                        },
                        "total_energy_joules": {
                            "process_0": 10468.775683220148
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 81.04893533920794,
                        "ram_power_avg": 1.081179141998291,
                        "cpu_energy_total": 0.0016094864314067083,
                        "gpu_energy_total": 0.0012835163045900044,
                        "ram_energy_total": 1.4990509342216345e-05,
                        "total_energy_kwh": 0.00290799324533893,
                        "total_energy_joules": 10468.775683220148
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.565034966434619,
                        "joules_per_token": 0.6389633595715423,
                        "flops_per_joule": 1619097734.6396122,
                        "joules_per_flop": 6.176279409238909e-10
                    },
                    "per-process_emissions": [
                        0.0011078000268118654
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0230": {
            "setup": {
                "experiment_id": "0230",
                "date_time": "April 11, 2025 at 04:38:09 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R4_High_Load_Cloud_API_Deployment",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 56.83997280899348,
                        "average_latency_ms_per_batch": 3552.4983005620925,
                        "throughput_queries_per_sec": 2.2519363341382044,
                        "throughput_tokens_per_sec": 288.24785076969016
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419853312,
                        "gpu_max_memory_allocated_bytes": 4419853312,
                        "gpu_current_memory_reserved_bytes": 6339690496,
                        "gpu_max_memory_reserved_bytes": 6339690496
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            3.0,
                            8.0,
                            10.0,
                            8.0
                        ],
                        "cpu_usage_percent": 0.9,
                        "cpu_memory_usage_bytes": 3101462528
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0230",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 81.04893533920794
                        },
                        "ram_power": {
                            "process_0": 1.081179141998291
                        },
                        "cpu_energy": {
                            "process_0": 0.0016094864314067083
                        },
                        "gpu_energy": {
                            "process_0": 0.0012835163045900044
                        },
                        "ram_energy": {
                            "process_0": 1.4990509342216345e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.00290799324533893
                        },
                        "total_energy_joules": {
                            "process_0": 10468.775683220148
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 81.04893533920794,
                        "ram_power_avg": 1.081179141998291,
                        "cpu_energy_total": 0.0016094864314067083,
                        "gpu_energy_total": 0.0012835163045900044,
                        "ram_energy_total": 1.4990509342216345e-05,
                        "total_energy_kwh": 0.00290799324533893,
                        "total_energy_joules": 10468.775683220148
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.565034966434619,
                        "joules_per_token": 0.6389633595715423,
                        "flops_per_joule": 1619097734.6396122,
                        "joules_per_flop": 6.176279409238909e-10
                    },
                    "per-process_emissions": [
                        0.0011078000268118654
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0231": {
            "setup": {
                "experiment_id": "0231",
                "date_time": "April 11, 2025 at 04:55:10 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R5_Real_Time_Mobile_Inference",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.6,
                    "simulate_burst": true,
                    "burst_interval": 5.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 14120
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 984.6408724690082,
                        "average_latency_ms_per_batch": 7692.506816164127,
                        "throughput_queries_per_sec": 0.12999663489393573,
                        "throughput_tokens_per_sec": 14.340253786737286
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576167936,
                        "gpu_max_memory_allocated_bytes": 1576167936,
                        "gpu_current_memory_reserved_bytes": 2665480192,
                        "gpu_max_memory_reserved_bytes": 2665480192
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            11.0,
                            10.0,
                            10.0
                        ],
                        "cpu_usage_percent": 1.1,
                        "cpu_memory_usage_bytes": 2698420224
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0231",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 65.17718633992214
                        },
                        "ram_power": {
                            "process_0": 0.9420275688171387
                        },
                        "cpu_energy": {
                            "process_0": 0.02928508940903201
                        },
                        "gpu_energy": {
                            "process_0": 0.015628119446930044
                        },
                        "ram_energy": {
                            "process_0": 0.00023070038353874162
                        },
                        "total_energy_kwh": {
                            "process_0": 0.045143909239500846
                        },
                        "total_energy_joules": {
                            "process_0": 162518.07326220305
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 65.17718633992214,
                        "ram_power_avg": 0.9420275688171387,
                        "cpu_energy_total": 0.02928508940903201,
                        "gpu_energy_total": 0.015628119446930044,
                        "ram_energy_total": 0.00023070038353874162,
                        "total_energy_kwh": 0.045143909239500846,
                        "total_energy_joules": 162518.07326220305
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.08688264459805099,
                        "joules_per_token": 11.509778559646108,
                        "flops_per_joule": 104295914.00462455,
                        "joules_per_flop": 9.58810332642235e-09
                    },
                    "per-process_emissions": [
                        0.01719757222478785
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0231": {
            "setup": {
                "experiment_id": "0231",
                "date_time": "April 11, 2025 at 04:55:10 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R5_Real_Time_Mobile_Inference",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.6,
                    "simulate_burst": true,
                    "burst_interval": 5.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 14120
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 984.6408724690082,
                        "average_latency_ms_per_batch": 7692.506816164127,
                        "throughput_queries_per_sec": 0.12999663489393573,
                        "throughput_tokens_per_sec": 14.340253786737286
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576167936,
                        "gpu_max_memory_allocated_bytes": 1576167936,
                        "gpu_current_memory_reserved_bytes": 2665480192,
                        "gpu_max_memory_reserved_bytes": 2665480192
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            11.0,
                            10.0,
                            10.0
                        ],
                        "cpu_usage_percent": 1.1,
                        "cpu_memory_usage_bytes": 2698420224
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0231",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 65.17718633992214
                        },
                        "ram_power": {
                            "process_0": 0.9420275688171387
                        },
                        "cpu_energy": {
                            "process_0": 0.02928508940903201
                        },
                        "gpu_energy": {
                            "process_0": 0.015628119446930044
                        },
                        "ram_energy": {
                            "process_0": 0.00023070038353874162
                        },
                        "total_energy_kwh": {
                            "process_0": 0.045143909239500846
                        },
                        "total_energy_joules": {
                            "process_0": 162518.07326220305
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 65.17718633992214,
                        "ram_power_avg": 0.9420275688171387,
                        "cpu_energy_total": 0.02928508940903201,
                        "gpu_energy_total": 0.015628119446930044,
                        "ram_energy_total": 0.00023070038353874162,
                        "total_energy_kwh": 0.045143909239500846,
                        "total_energy_joules": 162518.07326220305
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.08688264459805099,
                        "joules_per_token": 11.509778559646108,
                        "flops_per_joule": 104295914.00462455,
                        "joules_per_flop": 9.58810332642235e-09
                    },
                    "per-process_emissions": [
                        0.01719757222478785
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0232": {
            "setup": {
                "experiment_id": "0232",
                "date_time": "April 11, 2025 at 04:55:58 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R6_Medium_Scale_Language_Model_Serving",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.1,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 12.265104173999134,
                        "average_latency_ms_per_batch": 3066.2760434997836,
                        "throughput_queries_per_sec": 10.436111930573565,
                        "throughput_tokens_per_sec": 1335.8223271134163
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419853312,
                        "gpu_max_memory_allocated_bytes": 4419853312,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            5.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.9,
                        "cpu_memory_usage_bytes": 3102679040
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0232",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 632.0133911065885,
                            "process_0": 457.3822823106764,
                            "process_3": 457.6421746367473,
                            "process_2": 595.3593030364384
                        },
                        "ram_power": {
                            "process_1": 0.8818130493164062,
                            "process_0": 1.0818843841552734,
                            "process_3": 0.8715505599975586,
                            "process_2": 0.8520698547363281
                        },
                        "cpu_energy": {
                            "process_1": 0.00037807695696903925,
                            "process_0": 0.00040003843131262333,
                            "process_3": 0.0003729443096561908,
                            "process_2": 0.0003696059814999444
                        },
                        "gpu_energy": {
                            "process_1": 0.0017281477714060078,
                            "process_0": 0.0017294880502560228,
                            "process_3": 0.0017172869293840543,
                            "process_2": 0.0017294880502560228
                        },
                        "ram_energy": {
                            "process_1": 2.4984010702226633e-06,
                            "process_0": 3.2852205637956664e-06,
                            "process_3": 2.4219246771324998e-06,
                            "process_2": 2.35372143714362e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0021087231294452695,
                            "process_0": 0.002132811702132442,
                            "process_3": 0.0020926531637173785,
                            "process_2": 0.0021014477531931105
                        },
                        "total_energy_joules": {
                            "process_1": 7591.40326600297,
                            "process_0": 7678.122127676792,
                            "process_3": 7533.551389382563,
                            "process_2": 7565.2119114951975
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 535.5992877726126,
                        "ram_power_avg": 0.9218294620513916,
                        "cpu_energy_total": 0.0015206656794377977,
                        "gpu_energy_total": 0.006904410801302108,
                        "ram_energy_total": 1.0559267748294447e-05,
                        "total_energy_kwh": 0.008435635748488199,
                        "total_energy_joules": 30368.28869455752
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5395101503673558,
                        "joules_per_token": 1.8535332455174267,
                        "flops_per_joule": 558147058.0589451,
                        "joules_per_flop": 1.7916425170772674e-09
                    },
                    "per-process_emissions": [
                        0.0008033180761621754,
                        0.0008124946179273538,
                        0.0007971962227181354,
                        0.0008005465215789154
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0232": {
            "setup": {
                "experiment_id": "0232",
                "date_time": "April 11, 2025 at 04:55:58 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R6_Medium_Scale_Language_Model_Serving",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.1,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 12.265104173999134,
                        "average_latency_ms_per_batch": 3066.2760434997836,
                        "throughput_queries_per_sec": 10.436111930573565,
                        "throughput_tokens_per_sec": 1335.8223271134163
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419853312,
                        "gpu_max_memory_allocated_bytes": 4419853312,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            5.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.9,
                        "cpu_memory_usage_bytes": 3102679040
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0232",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 632.0133911065885,
                            "process_0": 457.3822823106764,
                            "process_3": 457.6421746367473,
                            "process_2": 595.3593030364384
                        },
                        "ram_power": {
                            "process_1": 0.8818130493164062,
                            "process_0": 1.0818843841552734,
                            "process_3": 0.8715505599975586,
                            "process_2": 0.8520698547363281
                        },
                        "cpu_energy": {
                            "process_1": 0.00037807695696903925,
                            "process_0": 0.00040003843131262333,
                            "process_3": 0.0003729443096561908,
                            "process_2": 0.0003696059814999444
                        },
                        "gpu_energy": {
                            "process_1": 0.0017281477714060078,
                            "process_0": 0.0017294880502560228,
                            "process_3": 0.0017172869293840543,
                            "process_2": 0.0017294880502560228
                        },
                        "ram_energy": {
                            "process_1": 2.4984010702226633e-06,
                            "process_0": 3.2852205637956664e-06,
                            "process_3": 2.4219246771324998e-06,
                            "process_2": 2.35372143714362e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0021087231294452695,
                            "process_0": 0.002132811702132442,
                            "process_3": 0.0020926531637173785,
                            "process_2": 0.0021014477531931105
                        },
                        "total_energy_joules": {
                            "process_1": 7591.40326600297,
                            "process_0": 7678.122127676792,
                            "process_3": 7533.551389382563,
                            "process_2": 7565.2119114951975
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 535.5992877726126,
                        "ram_power_avg": 0.9218294620513916,
                        "cpu_energy_total": 0.0015206656794377977,
                        "gpu_energy_total": 0.006904410801302108,
                        "ram_energy_total": 1.0559267748294447e-05,
                        "total_energy_kwh": 0.008435635748488199,
                        "total_energy_joules": 30368.28869455752
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5395101503673558,
                        "joules_per_token": 1.8535332455174267,
                        "flops_per_joule": 558147058.0589451,
                        "joules_per_flop": 1.7916425170772674e-09
                    },
                    "per-process_emissions": [
                        0.0008033180761621754,
                        0.0008124946179273538,
                        0.0007971962227181354,
                        0.0008005465215789154
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0233": {
            "setup": {
                "experiment_id": "0233",
                "date_time": "April 11, 2025 at 05:02:12 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_1",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.0733602950022,
                        "average_latency_ms_per_batch": 4634.170036875275,
                        "throughput_queries_per_sec": 3.452613924971227,
                        "throughput_tokens_per_sec": 441.93458239631707
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1444937728,
                        "gpu_max_memory_reserved_bytes": 1444937728
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            9.0,
                            10.0,
                            10.0
                        ],
                        "cpu_usage_percent": 1.0,
                        "cpu_memory_usage_bytes": 2647629824
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0233",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 46.22598572748724
                        },
                        "ram_power": {
                            "process_0": 0.9232192039489746
                        },
                        "cpu_energy": {
                            "process_0": 0.0012411549791873995
                        },
                        "gpu_energy": {
                            "process_0": 0.00272359384553994
                        },
                        "ram_energy": {
                            "process_0": 8.518183258561665e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.003973267007985901
                        },
                        "total_energy_joules": {
                            "process_0": 14303.761228749243
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 46.22598572748724,
                        "ram_power_avg": 0.9232192039489746,
                        "cpu_energy_total": 0.0012411549791873995,
                        "gpu_energy_total": 0.00272359384553994,
                        "ram_energy_total": 8.518183258561665e-06,
                        "total_energy_kwh": 0.003973267007985901,
                        "total_energy_joules": 14303.761228749243
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.1454329905248746,
                        "joules_per_token": 0.873032301559402,
                        "flops_per_joule": 1185000974.3649886,
                        "joules_per_flop": 8.438811626596967e-10
                    },
                    "per-process_emissions": [
                        0.001513616066692229
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0233": {
            "setup": {
                "experiment_id": "0233",
                "date_time": "April 11, 2025 at 05:02:12 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_1",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.0733602950022,
                        "average_latency_ms_per_batch": 4634.170036875275,
                        "throughput_queries_per_sec": 3.452613924971227,
                        "throughput_tokens_per_sec": 441.93458239631707
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1444937728,
                        "gpu_max_memory_reserved_bytes": 1444937728
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            9.0,
                            10.0,
                            10.0
                        ],
                        "cpu_usage_percent": 1.0,
                        "cpu_memory_usage_bytes": 2647629824
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0233",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 46.22598572748724
                        },
                        "ram_power": {
                            "process_0": 0.9232192039489746
                        },
                        "cpu_energy": {
                            "process_0": 0.0012411549791873995
                        },
                        "gpu_energy": {
                            "process_0": 0.00272359384553994
                        },
                        "ram_energy": {
                            "process_0": 8.518183258561665e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.003973267007985901
                        },
                        "total_energy_joules": {
                            "process_0": 14303.761228749243
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 46.22598572748724,
                        "ram_power_avg": 0.9232192039489746,
                        "cpu_energy_total": 0.0012411549791873995,
                        "gpu_energy_total": 0.00272359384553994,
                        "ram_energy_total": 8.518183258561665e-06,
                        "total_energy_kwh": 0.003973267007985901,
                        "total_energy_joules": 14303.761228749243
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.1454329905248746,
                        "joules_per_token": 0.873032301559402,
                        "flops_per_joule": 1185000974.3649886,
                        "joules_per_flop": 8.438811626596967e-10
                    },
                    "per-process_emissions": [
                        0.001513616066692229
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0234": {
            "setup": {
                "experiment_id": "0234",
                "date_time": "April 11, 2025 at 05:03:25 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.11428687700027,
                        "average_latency_ms_per_batch": 4639.2858596250335,
                        "throughput_queries_per_sec": 3.448806666397829,
                        "throughput_tokens_per_sec": 441.44725329892213
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            2.0,
                            100.0,
                            3.0,
                            4.0
                        ],
                        "cpu_usage_percent": 1.9,
                        "cpu_memory_usage_bytes": 2650619904
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0234",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 0.0,
                            "process_0": 0.0
                        },
                        "ram_power": {
                            "process_1": 0.9749622344970703,
                            "process_0": 0.9246983528137207
                        },
                        "cpu_energy": {
                            "process_1": 0.001179262808906401,
                            "process_0": 0.0011800036952811298
                        },
                        "gpu_energy": {
                            "process_1": 0.003352842404494183,
                            "process_0": 0.003352842404494183
                        },
                        "ram_energy": {
                            "process_1": 9.636711561604263e-06,
                            "process_0": 8.946608222474208e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.004541741924962188,
                            "process_0": 0.004541792707997787
                        },
                        "total_energy_joules": {
                            "process_1": 16350.270929863877,
                            "process_0": 16350.453748792035
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 0.0,
                        "ram_power_avg": 0.9498302936553955,
                        "cpu_energy_total": 0.0023592665041875308,
                        "gpu_energy_total": 0.006705684808988366,
                        "ram_energy_total": 1.858331978407847e-05,
                        "total_energy_kwh": 0.009083534632959975,
                        "total_energy_joules": 32700.72467865591
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5010286518418964,
                        "joules_per_token": 1.9958938402499944,
                        "flops_per_joule": 518336249.72279024,
                        "joules_per_flop": 1.9292495952864705e-09
                    },
                    "per-process_emissions": [
                        0.0017301765863143455,
                        0.001730195932111757
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0234": {
            "setup": {
                "experiment_id": "0234",
                "date_time": "April 11, 2025 at 05:03:25 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.11428687700027,
                        "average_latency_ms_per_batch": 4639.2858596250335,
                        "throughput_queries_per_sec": 3.448806666397829,
                        "throughput_tokens_per_sec": 441.44725329892213
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            2.0,
                            100.0,
                            3.0,
                            4.0
                        ],
                        "cpu_usage_percent": 1.9,
                        "cpu_memory_usage_bytes": 2650619904
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0234",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 0.0,
                            "process_0": 0.0
                        },
                        "ram_power": {
                            "process_1": 0.9749622344970703,
                            "process_0": 0.9246983528137207
                        },
                        "cpu_energy": {
                            "process_1": 0.001179262808906401,
                            "process_0": 0.0011800036952811298
                        },
                        "gpu_energy": {
                            "process_1": 0.003352842404494183,
                            "process_0": 0.003352842404494183
                        },
                        "ram_energy": {
                            "process_1": 9.636711561604263e-06,
                            "process_0": 8.946608222474208e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.004541741924962188,
                            "process_0": 0.004541792707997787
                        },
                        "total_energy_joules": {
                            "process_1": 16350.270929863877,
                            "process_0": 16350.453748792035
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 0.0,
                        "ram_power_avg": 0.9498302936553955,
                        "cpu_energy_total": 0.0023592665041875308,
                        "gpu_energy_total": 0.006705684808988366,
                        "ram_energy_total": 1.858331978407847e-05,
                        "total_energy_kwh": 0.009083534632959975,
                        "total_energy_joules": 32700.72467865591
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5010286518418964,
                        "joules_per_token": 1.9958938402499944,
                        "flops_per_joule": 518336249.72279024,
                        "joules_per_flop": 1.9292495952864705e-09
                    },
                    "per-process_emissions": [
                        0.0017301765863143455,
                        0.001730195932111757
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0235": {
            "setup": {
                "experiment_id": "0235",
                "date_time": "April 11, 2025 at 05:04:40 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_3",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 3
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.78598402499847,
                        "average_latency_ms_per_batch": 4723.248003124809,
                        "throughput_queries_per_sec": 3.387499447290236,
                        "throughput_tokens_per_sec": 433.5999292531502
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            1.0,
                            100.0,
                            100.0,
                            2.0
                        ],
                        "cpu_usage_percent": 2.7,
                        "cpu_memory_usage_bytes": 2648543232
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0235",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 399.6436897675437,
                            "process_0": 441.18324604928404,
                            "process_2": 13.580037379710893
                        },
                        "ram_power": {
                            "process_1": 0.9791078567504884,
                            "process_0": 0.9246368408203125,
                            "process_2": 0.9592523574829102
                        },
                        "cpu_energy": {
                            "process_1": 0.0009212115349068883,
                            "process_0": 0.0009341616307812045,
                            "process_2": 0.000913474575031387
                        },
                        "gpu_energy": {
                            "process_1": 0.003928083975798025,
                            "process_0": 0.004004737926010193,
                            "process_2": 0.003923860639086119
                        },
                        "ram_energy": {
                            "process_1": 8.987604286055558e-06,
                            "process_0": 8.428303941337015e-06,
                            "process_2": 9.095505575310753e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0048582831149909704,
                            "process_0": 0.0049473278607327364,
                            "process_2": 0.004846430719692817
                        },
                        "total_energy_joules": {
                            "process_1": 17489.819213967494,
                            "process_0": 17810.38029863785,
                            "process_2": 17447.150590894144
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 284.8023243988462,
                        "ram_power_avg": 0.9543323516845703,
                        "cpu_energy_total": 0.0027688477407194797,
                        "gpu_energy_total": 0.011856682540894337,
                        "ram_energy_total": 2.6511413802703326e-05,
                        "total_energy_kwh": 0.014652041695416524,
                        "total_energy_joules": 52747.350103499484
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.31061276003157956,
                        "joules_per_token": 3.219442755340545,
                        "flops_per_joule": 321342606.9725438,
                        "joules_per_flop": 3.111943384729689e-09
                    },
                    "per-process_emissions": [
                        0.0018507629526558103,
                        0.001884684548546136,
                        0.0018462477826669788
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0235": {
            "setup": {
                "experiment_id": "0235",
                "date_time": "April 11, 2025 at 05:04:40 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_3",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 3
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.78598402499847,
                        "average_latency_ms_per_batch": 4723.248003124809,
                        "throughput_queries_per_sec": 3.387499447290236,
                        "throughput_tokens_per_sec": 433.5999292531502
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            1.0,
                            100.0,
                            100.0,
                            2.0
                        ],
                        "cpu_usage_percent": 2.7,
                        "cpu_memory_usage_bytes": 2648543232
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0235",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 399.6436897675437,
                            "process_0": 441.18324604928404,
                            "process_2": 13.580037379710893
                        },
                        "ram_power": {
                            "process_1": 0.9791078567504884,
                            "process_0": 0.9246368408203125,
                            "process_2": 0.9592523574829102
                        },
                        "cpu_energy": {
                            "process_1": 0.0009212115349068883,
                            "process_0": 0.0009341616307812045,
                            "process_2": 0.000913474575031387
                        },
                        "gpu_energy": {
                            "process_1": 0.003928083975798025,
                            "process_0": 0.004004737926010193,
                            "process_2": 0.003923860639086119
                        },
                        "ram_energy": {
                            "process_1": 8.987604286055558e-06,
                            "process_0": 8.428303941337015e-06,
                            "process_2": 9.095505575310753e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0048582831149909704,
                            "process_0": 0.0049473278607327364,
                            "process_2": 0.004846430719692817
                        },
                        "total_energy_joules": {
                            "process_1": 17489.819213967494,
                            "process_0": 17810.38029863785,
                            "process_2": 17447.150590894144
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 284.8023243988462,
                        "ram_power_avg": 0.9543323516845703,
                        "cpu_energy_total": 0.0027688477407194797,
                        "gpu_energy_total": 0.011856682540894337,
                        "ram_energy_total": 2.6511413802703326e-05,
                        "total_energy_kwh": 0.014652041695416524,
                        "total_energy_joules": 52747.350103499484
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.31061276003157956,
                        "joules_per_token": 3.219442755340545,
                        "flops_per_joule": 321342606.9725438,
                        "joules_per_flop": 3.111943384729689e-09
                    },
                    "per-process_emissions": [
                        0.0018507629526558103,
                        0.001884684548546136,
                        0.0018462477826669788
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0236": {
            "setup": {
                "experiment_id": "0236",
                "date_time": "April 11, 2025 at 05:05:55 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.507833906005544,
                        "average_latency_ms_per_batch": 4688.479238250693,
                        "throughput_queries_per_sec": 3.4126204227300194,
                        "throughput_tokens_per_sec": 436.8154141094425
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            1.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 2675093504
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0236",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 451.69477157820626,
                            "process_1": 477.04802031779883,
                            "process_2": 439.56119486976627,
                            "process_3": 444.9095672373735
                        },
                        "ram_power": {
                            "process_0": 0.9338965415954591,
                            "process_1": 0.9675579071044922,
                            "process_2": 0.9756231307983398,
                            "process_3": 0.9581880569458009
                        },
                        "cpu_energy": {
                            "process_0": 0.0010249458028126807,
                            "process_1": 0.0010158575296250092,
                            "process_2": 0.0010661270459060002,
                            "process_3": 0.0010658281077817263
                        },
                        "gpu_energy": {
                            "process_0": 0.004560572815121899,
                            "process_1": 0.004555309199799962,
                            "process_2": 0.004569421433311904,
                            "process_3": 0.004558312257757935
                        },
                        "ram_energy": {
                            "process_0": 8.340354400828188e-06,
                            "process_1": 8.596689785485113e-06,
                            "process_2": 9.0313796074027e-06,
                            "process_3": 8.909190216155502e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005593858972335409,
                            "process_1": 0.005579763419210458,
                            "process_2": 0.005644579858825306,
                            "process_3": 0.005633049555755815
                        },
                        "total_energy_joules": {
                            "process_0": 20137.892300407475,
                            "process_1": 20087.14830915765,
                            "process_2": 20320.487491771102,
                            "process_3": 20278.978400720935
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 453.3033885007862,
                        "ram_power_avg": 0.958816409111023,
                        "cpu_energy_total": 0.004172758486125416,
                        "gpu_energy_total": 0.0182436157059917,
                        "ram_energy_total": 3.48776140098715e-05,
                        "total_energy_kwh": 0.022451251806126987,
                        "total_energy_joules": 80824.50650205716
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2027107953895517,
                        "joules_per_token": 4.9331363831822,
                        "flops_per_joule": 209713263.05247018,
                        "joules_per_flop": 4.768415623525921e-09
                    },
                    "per-process_emissions": [
                        0.002130980575511174,
                        0.0021256108745482238,
                        0.0021503026972195007,
                        0.0021459102282651776
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0236": {
            "setup": {
                "experiment_id": "0236",
                "date_time": "April 11, 2025 at 05:05:55 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.507833906005544,
                        "average_latency_ms_per_batch": 4688.479238250693,
                        "throughput_queries_per_sec": 3.4126204227300194,
                        "throughput_tokens_per_sec": 436.8154141094425
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            1.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 2675093504
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0236",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 451.69477157820626,
                            "process_1": 477.04802031779883,
                            "process_2": 439.56119486976627,
                            "process_3": 444.9095672373735
                        },
                        "ram_power": {
                            "process_0": 0.9338965415954591,
                            "process_1": 0.9675579071044922,
                            "process_2": 0.9756231307983398,
                            "process_3": 0.9581880569458009
                        },
                        "cpu_energy": {
                            "process_0": 0.0010249458028126807,
                            "process_1": 0.0010158575296250092,
                            "process_2": 0.0010661270459060002,
                            "process_3": 0.0010658281077817263
                        },
                        "gpu_energy": {
                            "process_0": 0.004560572815121899,
                            "process_1": 0.004555309199799962,
                            "process_2": 0.004569421433311904,
                            "process_3": 0.004558312257757935
                        },
                        "ram_energy": {
                            "process_0": 8.340354400828188e-06,
                            "process_1": 8.596689785485113e-06,
                            "process_2": 9.0313796074027e-06,
                            "process_3": 8.909190216155502e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005593858972335409,
                            "process_1": 0.005579763419210458,
                            "process_2": 0.005644579858825306,
                            "process_3": 0.005633049555755815
                        },
                        "total_energy_joules": {
                            "process_0": 20137.892300407475,
                            "process_1": 20087.14830915765,
                            "process_2": 20320.487491771102,
                            "process_3": 20278.978400720935
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 453.3033885007862,
                        "ram_power_avg": 0.958816409111023,
                        "cpu_energy_total": 0.004172758486125416,
                        "gpu_energy_total": 0.0182436157059917,
                        "ram_energy_total": 3.48776140098715e-05,
                        "total_energy_kwh": 0.022451251806126987,
                        "total_energy_joules": 80824.50650205716
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2027107953895517,
                        "joules_per_token": 4.9331363831822,
                        "flops_per_joule": 209713263.05247018,
                        "joules_per_flop": 4.768415623525921e-09
                    },
                    "per-process_emissions": [
                        0.002130980575511174,
                        0.0021256108745482238,
                        0.0021503026972195007,
                        0.0021459102282651776
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0237": {
            "setup": {
                "experiment_id": "0237",
                "date_time": "April 11, 2025 at 05:13:02 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_1",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 14659
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 384.6492190849896,
                        "average_latency_ms_per_batch": 3005.0720241014815,
                        "throughput_queries_per_sec": 0.33277072628533777,
                        "throughput_tokens_per_sec": 38.110047473568486
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087681536,
                        "gpu_max_memory_allocated_bytes": 1087681536,
                        "gpu_current_memory_reserved_bytes": 1885339648,
                        "gpu_max_memory_reserved_bytes": 1885339648
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2636230656
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0237",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 261.2132551711651,
                            "process_2": 263.9321831614831,
                            "process_0": 1845.3450783778794,
                            "process_3": 394.8622987354958
                        },
                        "ram_power": {
                            "process_1": 0.9574270248413087,
                            "process_2": 0.9700412750244141,
                            "process_0": 0.9204840660095215,
                            "process_3": 0.9564099311828613
                        },
                        "cpu_energy": {
                            "process_1": 0.010700519322782494,
                            "process_2": 0.010693505495091824,
                            "process_0": 0.010719951831562973,
                            "process_3": 0.010673527237842676
                        },
                        "gpu_energy": {
                            "process_1": 0.03267492002880401,
                            "process_2": 0.03284218432928193,
                            "process_0": 0.03247745875972413,
                            "process_3": 0.03259261912962991
                        },
                        "ram_energy": {
                            "process_1": 8.43020215168538e-05,
                            "process_2": 8.379166574891894e-05,
                            "process_0": 8.293996900948474e-05,
                            "process_3": 8.257145277358221e-05
                        },
                        "total_energy_kwh": {
                            "process_1": 0.04345974137310343,
                            "process_2": 0.043619481490122666,
                            "process_0": 0.043280350560296596,
                            "process_3": 0.04334871782024623
                        },
                        "total_energy_joules": {
                            "process_1": 156455.06894317237,
                            "process_2": 157030.1333644416,
                            "process_0": 155809.26201706775,
                            "process_3": 156055.38415288643
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 691.3382038615058,
                        "ram_power_avg": 0.9510905742645264,
                        "cpu_energy_total": 0.04278750388727997,
                        "gpu_energy_total": 0.13058718224743998,
                        "ram_energy_total": 0.0003336051090488397,
                        "total_energy_kwh": 0.1737082912437689,
                        "total_energy_joules": 625349.8484775681
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.02344127856700973,
                        "joules_per_token": 42.659789104138625,
                        "flops_per_joule": 27104781.4825848,
                        "joules_per_flop": 3.6893859507501065e-08
                    },
                    "per-process_emissions": [
                        0.016555988476083753,
                        0.01661684147366223,
                        0.016487649545944987,
                        0.0165136940536228
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0237": {
            "setup": {
                "experiment_id": "0237",
                "date_time": "April 11, 2025 at 05:13:02 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_1",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 14659
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 384.6492190849896,
                        "average_latency_ms_per_batch": 3005.0720241014815,
                        "throughput_queries_per_sec": 0.33277072628533777,
                        "throughput_tokens_per_sec": 38.110047473568486
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087681536,
                        "gpu_max_memory_allocated_bytes": 1087681536,
                        "gpu_current_memory_reserved_bytes": 1885339648,
                        "gpu_max_memory_reserved_bytes": 1885339648
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2636230656
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0237",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 261.2132551711651,
                            "process_2": 263.9321831614831,
                            "process_0": 1845.3450783778794,
                            "process_3": 394.8622987354958
                        },
                        "ram_power": {
                            "process_1": 0.9574270248413087,
                            "process_2": 0.9700412750244141,
                            "process_0": 0.9204840660095215,
                            "process_3": 0.9564099311828613
                        },
                        "cpu_energy": {
                            "process_1": 0.010700519322782494,
                            "process_2": 0.010693505495091824,
                            "process_0": 0.010719951831562973,
                            "process_3": 0.010673527237842676
                        },
                        "gpu_energy": {
                            "process_1": 0.03267492002880401,
                            "process_2": 0.03284218432928193,
                            "process_0": 0.03247745875972413,
                            "process_3": 0.03259261912962991
                        },
                        "ram_energy": {
                            "process_1": 8.43020215168538e-05,
                            "process_2": 8.379166574891894e-05,
                            "process_0": 8.293996900948474e-05,
                            "process_3": 8.257145277358221e-05
                        },
                        "total_energy_kwh": {
                            "process_1": 0.04345974137310343,
                            "process_2": 0.043619481490122666,
                            "process_0": 0.043280350560296596,
                            "process_3": 0.04334871782024623
                        },
                        "total_energy_joules": {
                            "process_1": 156455.06894317237,
                            "process_2": 157030.1333644416,
                            "process_0": 155809.26201706775,
                            "process_3": 156055.38415288643
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 691.3382038615058,
                        "ram_power_avg": 0.9510905742645264,
                        "cpu_energy_total": 0.04278750388727997,
                        "gpu_energy_total": 0.13058718224743998,
                        "ram_energy_total": 0.0003336051090488397,
                        "total_energy_kwh": 0.1737082912437689,
                        "total_energy_joules": 625349.8484775681
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.02344127856700973,
                        "joules_per_token": 42.659789104138625,
                        "flops_per_joule": 27104781.4825848,
                        "joules_per_flop": 3.6893859507501065e-08
                    },
                    "per-process_emissions": [
                        0.016555988476083753,
                        0.01661684147366223,
                        0.016487649545944987,
                        0.0165136940536228
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0238": {
            "setup": {
                "experiment_id": "0238",
                "date_time": "April 11, 2025 at 05:18:36 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 2,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16284
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 292.3837115509924,
                        "average_latency_ms_per_batch": 4568.495492984256,
                        "throughput_queries_per_sec": 0.43778088499186624,
                        "throughput_tokens_per_sec": 55.69393696255898
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1895825408,
                        "gpu_max_memory_reserved_bytes": 1895825408
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.5,
                        "cpu_memory_usage_bytes": 2641690624
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0238",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 271.2867587040874,
                            "process_1": 288.8288871856928,
                            "process_2": 248.9497356612853,
                            "process_0": 265.2848446663859
                        },
                        "ram_power": {
                            "process_3": 0.9752440452575685,
                            "process_1": 0.9724631309509277,
                            "process_2": 0.9589920043945312,
                            "process_0": 0.9217987060546875
                        },
                        "cpu_energy": {
                            "process_3": 0.008998032204063294,
                            "process_1": 0.009047659288874575,
                            "process_2": 0.009147464451061531,
                            "process_0": 0.009017257080498553
                        },
                        "gpu_energy": {
                            "process_3": 0.021031934047756162,
                            "process_1": 0.021142763580864143,
                            "process_2": 0.021437665205673806,
                            "process_0": 0.021071981857571753
                        },
                        "ram_energy": {
                            "process_3": 6.078233216753045e-05,
                            "process_1": 6.084922679253787e-05,
                            "process_2": 6.045409531217833e-05,
                            "process_0": 5.774810256329681e-05
                        },
                        "total_energy_kwh": {
                            "process_3": 0.030090748583986963,
                            "process_1": 0.03025127209653126,
                            "process_2": 0.03064558375204752,
                            "process_0": 0.030146987040633604
                        },
                        "total_energy_joules": {
                            "process_3": 108326.69490235306,
                            "process_1": 108904.57954751253,
                            "process_2": 110324.10150737107,
                            "process_0": 108529.15334628097
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 268.58755655436283,
                        "ram_power_avg": 0.9571244716644287,
                        "cpu_energy_total": 0.03621041302449795,
                        "gpu_energy_total": 0.08468434469186586,
                        "ram_energy_total": 0.00023983375683554346,
                        "total_energy_kwh": 0.12113459147319934,
                        "total_energy_joules": 436084.52930351766
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.03734138430915587,
                        "joules_per_token": 26.77993916135579,
                        "flops_per_joule": 38868544.63794727,
                        "joules_per_flop": 2.572774487223024e-08
                    },
                    "per-process_emissions": [
                        0.011463070673069835,
                        0.011524222105173584,
                        0.011674435130342503,
                        0.011484494713129371
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0238": {
            "setup": {
                "experiment_id": "0238",
                "date_time": "April 11, 2025 at 05:18:36 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 2,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16284
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 292.3837115509924,
                        "average_latency_ms_per_batch": 4568.495492984256,
                        "throughput_queries_per_sec": 0.43778088499186624,
                        "throughput_tokens_per_sec": 55.69393696255898
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1895825408,
                        "gpu_max_memory_reserved_bytes": 1895825408
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.5,
                        "cpu_memory_usage_bytes": 2641690624
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0238",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 271.2867587040874,
                            "process_1": 288.8288871856928,
                            "process_2": 248.9497356612853,
                            "process_0": 265.2848446663859
                        },
                        "ram_power": {
                            "process_3": 0.9752440452575685,
                            "process_1": 0.9724631309509277,
                            "process_2": 0.9589920043945312,
                            "process_0": 0.9217987060546875
                        },
                        "cpu_energy": {
                            "process_3": 0.008998032204063294,
                            "process_1": 0.009047659288874575,
                            "process_2": 0.009147464451061531,
                            "process_0": 0.009017257080498553
                        },
                        "gpu_energy": {
                            "process_3": 0.021031934047756162,
                            "process_1": 0.021142763580864143,
                            "process_2": 0.021437665205673806,
                            "process_0": 0.021071981857571753
                        },
                        "ram_energy": {
                            "process_3": 6.078233216753045e-05,
                            "process_1": 6.084922679253787e-05,
                            "process_2": 6.045409531217833e-05,
                            "process_0": 5.774810256329681e-05
                        },
                        "total_energy_kwh": {
                            "process_3": 0.030090748583986963,
                            "process_1": 0.03025127209653126,
                            "process_2": 0.03064558375204752,
                            "process_0": 0.030146987040633604
                        },
                        "total_energy_joules": {
                            "process_3": 108326.69490235306,
                            "process_1": 108904.57954751253,
                            "process_2": 110324.10150737107,
                            "process_0": 108529.15334628097
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 268.58755655436283,
                        "ram_power_avg": 0.9571244716644287,
                        "cpu_energy_total": 0.03621041302449795,
                        "gpu_energy_total": 0.08468434469186586,
                        "ram_energy_total": 0.00023983375683554346,
                        "total_energy_kwh": 0.12113459147319934,
                        "total_energy_joules": 436084.52930351766
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.03734138430915587,
                        "joules_per_token": 26.77993916135579,
                        "flops_per_joule": 38868544.63794727,
                        "joules_per_flop": 2.572774487223024e-08
                    },
                    "per-process_emissions": [
                        0.011463070673069835,
                        0.011524222105173584,
                        0.011674435130342503,
                        0.011484494713129371
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0239": {
            "setup": {
                "experiment_id": "0239",
                "date_time": "April 11, 2025 at 05:21:39 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 147.5169847490033,
                        "average_latency_ms_per_batch": 4609.905773406354,
                        "throughput_queries_per_sec": 0.8676966941656854,
                        "throughput_tokens_per_sec": 111.06517685320773
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1912602624,
                        "gpu_max_memory_reserved_bytes": 1912602624
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            99.0,
                            100.0,
                            81.0
                        ],
                        "cpu_usage_percent": 2.5,
                        "cpu_memory_usage_bytes": 2644770816
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0239",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 290.6354064893378,
                            "process_1": 434.5267925333202,
                            "process_3": 256.8872334440849,
                            "process_2": 451.2021913803017
                        },
                        "ram_power": {
                            "process_0": 0.9225397109985353,
                            "process_1": 0.9585514068603516,
                            "process_3": 0.9695963859558105,
                            "process_2": 0.9813838005065917
                        },
                        "cpu_energy": {
                            "process_0": 0.004550596954780984,
                            "process_1": 0.004503476637155474,
                            "process_3": 0.004582698563311738,
                            "process_2": 0.004559191364782122
                        },
                        "gpu_energy": {
                            "process_0": 0.011440145540998037,
                            "process_1": 0.011336503791418095,
                            "process_3": 0.01152445421955578,
                            "process_2": 0.0114839905760738
                        },
                        "ram_energy": {
                            "process_0": 2.9031154441889206e-05,
                            "process_1": 3.0237696677052638e-05,
                            "process_3": 3.0847866477901275e-05,
                            "process_2": 3.092078871844446e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.016019773650220916,
                            "process_1": 0.0158702181252506,
                            "process_3": 0.016138000649345424,
                            "process_2": 0.016074102729574352
                        },
                        "total_energy_joules": {
                            "process_0": 57671.1851407953,
                            "process_1": 57132.78525090216,
                            "process_3": 58096.80233764353,
                            "process_2": 57866.769826467666
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 358.31290596176115,
                        "ram_power_avg": 0.9580178260803223,
                        "cpu_energy_total": 0.018195963520030316,
                        "gpu_energy_total": 0.04578509412804571,
                        "ram_energy_total": 0.00012103750631528757,
                        "total_energy_kwh": 0.06410209515439129,
                        "total_energy_joules": 230767.54255580867
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.07099785272462095,
                        "joules_per_token": 14.084933017322307,
                        "flops_per_joule": 73450411.63686539,
                        "joules_per_flop": 1.3614627579542269e-08
                    },
                    "per-process_emissions": [
                        0.006102732772051658,
                        0.0060457595948142165,
                        0.006147771347368139,
                        0.006123429434831349
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0239": {
            "setup": {
                "experiment_id": "0239",
                "date_time": "April 11, 2025 at 05:21:39 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 147.5169847490033,
                        "average_latency_ms_per_batch": 4609.905773406354,
                        "throughput_queries_per_sec": 0.8676966941656854,
                        "throughput_tokens_per_sec": 111.06517685320773
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1912602624,
                        "gpu_max_memory_reserved_bytes": 1912602624
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            99.0,
                            100.0,
                            81.0
                        ],
                        "cpu_usage_percent": 2.5,
                        "cpu_memory_usage_bytes": 2644770816
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0239",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 290.6354064893378,
                            "process_1": 434.5267925333202,
                            "process_3": 256.8872334440849,
                            "process_2": 451.2021913803017
                        },
                        "ram_power": {
                            "process_0": 0.9225397109985353,
                            "process_1": 0.9585514068603516,
                            "process_3": 0.9695963859558105,
                            "process_2": 0.9813838005065917
                        },
                        "cpu_energy": {
                            "process_0": 0.004550596954780984,
                            "process_1": 0.004503476637155474,
                            "process_3": 0.004582698563311738,
                            "process_2": 0.004559191364782122
                        },
                        "gpu_energy": {
                            "process_0": 0.011440145540998037,
                            "process_1": 0.011336503791418095,
                            "process_3": 0.01152445421955578,
                            "process_2": 0.0114839905760738
                        },
                        "ram_energy": {
                            "process_0": 2.9031154441889206e-05,
                            "process_1": 3.0237696677052638e-05,
                            "process_3": 3.0847866477901275e-05,
                            "process_2": 3.092078871844446e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.016019773650220916,
                            "process_1": 0.0158702181252506,
                            "process_3": 0.016138000649345424,
                            "process_2": 0.016074102729574352
                        },
                        "total_energy_joules": {
                            "process_0": 57671.1851407953,
                            "process_1": 57132.78525090216,
                            "process_3": 58096.80233764353,
                            "process_2": 57866.769826467666
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 358.31290596176115,
                        "ram_power_avg": 0.9580178260803223,
                        "cpu_energy_total": 0.018195963520030316,
                        "gpu_energy_total": 0.04578509412804571,
                        "ram_energy_total": 0.00012103750631528757,
                        "total_energy_kwh": 0.06410209515439129,
                        "total_energy_joules": 230767.54255580867
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.07099785272462095,
                        "joules_per_token": 14.084933017322307,
                        "flops_per_joule": 73450411.63686539,
                        "joules_per_flop": 1.3614627579542269e-08
                    },
                    "per-process_emissions": [
                        0.006102732772051658,
                        0.0060457595948142165,
                        0.006147771347368139,
                        0.006123429434831349
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0240": {
            "setup": {
                "experiment_id": "0240",
                "date_time": "April 11, 2025 at 05:23:23 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 74.26655792199745,
                        "average_latency_ms_per_batch": 4641.65987012484,
                        "throughput_queries_per_sec": 1.7235213746467026,
                        "throughput_tokens_per_sec": 220.61073595477794
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1937768448,
                        "gpu_max_memory_reserved_bytes": 1937768448
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            99.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2623025152
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0240",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 282.54673170030355,
                            "process_1": 350.45886841808453,
                            "process_0": 352.29574440917474,
                            "process_2": 295.6267003472992
                        },
                        "ram_power": {
                            "process_3": 0.9666681289672852,
                            "process_1": 0.9695806503295898,
                            "process_0": 0.9152956008911134,
                            "process_2": 0.9670100212097168
                        },
                        "cpu_energy": {
                            "process_3": 0.002263938573562654,
                            "process_1": 0.0022341658999373523,
                            "process_0": 0.0022374503519685058,
                            "process_2": 0.0022402794716252852
                        },
                        "gpu_energy": {
                            "process_3": 0.0066486103188840295,
                            "process_1": 0.006554422743533905,
                            "process_0": 0.006560510803959807,
                            "process_2": 0.00656907664414591
                        },
                        "ram_energy": {
                            "process_3": 1.6153212657026814e-05,
                            "process_1": 1.5977834343259154e-05,
                            "process_0": 1.5120361706557342e-05,
                            "process_2": 1.5975793109996062e-05
                        },
                        "total_energy_kwh": {
                            "process_3": 0.00892870210510371,
                            "process_1": 0.008804566477814519,
                            "process_0": 0.008813081517634866,
                            "process_2": 0.008825331908881196
                        },
                        "total_energy_joules": {
                            "process_3": 32143.32757837336,
                            "process_1": 31696.439320132267,
                            "process_0": 31727.09346348552,
                            "process_2": 31771.194871972308
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 320.2320112187155,
                        "ram_power_avg": 0.9546386003494263,
                        "cpu_energy_total": 0.008975834297093796,
                        "gpu_energy_total": 0.026332620510523652,
                        "ram_energy_total": 6.322720181683937e-05,
                        "total_energy_kwh": 0.035371682009434295,
                        "total_energy_joules": 127338.05523396345
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1286653857709465,
                        "joules_per_token": 7.77209809777609,
                        "flops_per_joule": 133110019.32618746,
                        "joules_per_flop": 7.512582486743465e-09
                    },
                    "per-process_emissions": [
                        0.0034013890669392587,
                        0.0033540995997234408,
                        0.003357343404143002,
                        0.003362010190688292
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0240": {
            "setup": {
                "experiment_id": "0240",
                "date_time": "April 11, 2025 at 05:23:23 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 74.26655792199745,
                        "average_latency_ms_per_batch": 4641.65987012484,
                        "throughput_queries_per_sec": 1.7235213746467026,
                        "throughput_tokens_per_sec": 220.61073595477794
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1937768448,
                        "gpu_max_memory_reserved_bytes": 1937768448
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            99.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2623025152
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0240",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 282.54673170030355,
                            "process_1": 350.45886841808453,
                            "process_0": 352.29574440917474,
                            "process_2": 295.6267003472992
                        },
                        "ram_power": {
                            "process_3": 0.9666681289672852,
                            "process_1": 0.9695806503295898,
                            "process_0": 0.9152956008911134,
                            "process_2": 0.9670100212097168
                        },
                        "cpu_energy": {
                            "process_3": 0.002263938573562654,
                            "process_1": 0.0022341658999373523,
                            "process_0": 0.0022374503519685058,
                            "process_2": 0.0022402794716252852
                        },
                        "gpu_energy": {
                            "process_3": 0.0066486103188840295,
                            "process_1": 0.006554422743533905,
                            "process_0": 0.006560510803959807,
                            "process_2": 0.00656907664414591
                        },
                        "ram_energy": {
                            "process_3": 1.6153212657026814e-05,
                            "process_1": 1.5977834343259154e-05,
                            "process_0": 1.5120361706557342e-05,
                            "process_2": 1.5975793109996062e-05
                        },
                        "total_energy_kwh": {
                            "process_3": 0.00892870210510371,
                            "process_1": 0.008804566477814519,
                            "process_0": 0.008813081517634866,
                            "process_2": 0.008825331908881196
                        },
                        "total_energy_joules": {
                            "process_3": 32143.32757837336,
                            "process_1": 31696.439320132267,
                            "process_0": 31727.09346348552,
                            "process_2": 31771.194871972308
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 320.2320112187155,
                        "ram_power_avg": 0.9546386003494263,
                        "cpu_energy_total": 0.008975834297093796,
                        "gpu_energy_total": 0.026332620510523652,
                        "ram_energy_total": 6.322720181683937e-05,
                        "total_energy_kwh": 0.035371682009434295,
                        "total_energy_joules": 127338.05523396345
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1286653857709465,
                        "joules_per_token": 7.77209809777609,
                        "flops_per_joule": 133110019.32618746,
                        "joules_per_flop": 7.512582486743465e-09
                    },
                    "per-process_emissions": [
                        0.0034013890669392587,
                        0.0033540995997234408,
                        0.003357343404143002,
                        0.003362010190688292
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0241": {
            "setup": {
                "experiment_id": "0241",
                "date_time": "April 11, 2025 at 05:24:30 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_16",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.16890806200172,
                        "average_latency_ms_per_batch": 4646.113507750215,
                        "throughput_queries_per_sec": 3.443738508176842,
                        "throughput_tokens_per_sec": 440.7985290466358
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2626736128
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0241",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 439.98222223518945,
                            "process_2": 442.64907966978217,
                            "process_3": 3154.2371283821585,
                            "process_1": 437.28647505930667
                        },
                        "ram_power": {
                            "process_0": 0.9163141250610352,
                            "process_2": 0.9774441719055177,
                            "process_3": 0.9665508270263673,
                            "process_1": 0.9590449333190919
                        },
                        "cpu_energy": {
                            "process_0": 0.0011492133015310632,
                            "process_2": 0.0011327760752500352,
                            "process_3": 0.0011473782727186976,
                            "process_1": 0.0011392098282185545
                        },
                        "gpu_energy": {
                            "process_0": 0.004139097477941944,
                            "process_2": 0.004097026055395969,
                            "process_3": 0.004133927473805921,
                            "process_1": 0.004109568843207845
                        },
                        "ram_energy": {
                            "process_0": 7.541415476716377e-06,
                            "process_2": 7.741737046916318e-06,
                            "process_3": 7.971073635200392e-06,
                            "process_1": 7.63164889586401e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005295852194949727,
                            "process_2": 0.005237543867692921,
                            "process_3": 0.005289276820159816,
                            "process_1": 0.005256410320322264
                        },
                        "total_energy_joules": {
                            "process_0": 19065.067901819017,
                            "process_2": 18855.157923694514,
                            "process_3": 19041.396552575337,
                            "process_1": 18923.07715316015
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1118.5387263366092,
                        "ram_power_avg": 0.9548385143280029,
                        "cpu_energy_total": 0.00456857747771835,
                        "gpu_energy_total": 0.01647961985035168,
                        "ram_energy_total": 3.08858750546971e-05,
                        "total_energy_kwh": 0.02107908320312473,
                        "total_energy_joules": 75884.69953124902
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2159065015899962,
                        "joules_per_token": 4.631634492874086,
                        "flops_per_joule": 223364803.41695324,
                        "joules_per_flop": 4.4769810852129115e-09
                    },
                    "per-process_emissions": [
                        0.0020174548936660983,
                        0.0019952423363976185,
                        0.002014950004639882,
                        0.0020024295115267667
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0241": {
            "setup": {
                "experiment_id": "0241",
                "date_time": "April 11, 2025 at 05:24:30 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_16",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.16890806200172,
                        "average_latency_ms_per_batch": 4646.113507750215,
                        "throughput_queries_per_sec": 3.443738508176842,
                        "throughput_tokens_per_sec": 440.7985290466358
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2626736128
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0241",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 439.98222223518945,
                            "process_2": 442.64907966978217,
                            "process_3": 3154.2371283821585,
                            "process_1": 437.28647505930667
                        },
                        "ram_power": {
                            "process_0": 0.9163141250610352,
                            "process_2": 0.9774441719055177,
                            "process_3": 0.9665508270263673,
                            "process_1": 0.9590449333190919
                        },
                        "cpu_energy": {
                            "process_0": 0.0011492133015310632,
                            "process_2": 0.0011327760752500352,
                            "process_3": 0.0011473782727186976,
                            "process_1": 0.0011392098282185545
                        },
                        "gpu_energy": {
                            "process_0": 0.004139097477941944,
                            "process_2": 0.004097026055395969,
                            "process_3": 0.004133927473805921,
                            "process_1": 0.004109568843207845
                        },
                        "ram_energy": {
                            "process_0": 7.541415476716377e-06,
                            "process_2": 7.741737046916318e-06,
                            "process_3": 7.971073635200392e-06,
                            "process_1": 7.63164889586401e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005295852194949727,
                            "process_2": 0.005237543867692921,
                            "process_3": 0.005289276820159816,
                            "process_1": 0.005256410320322264
                        },
                        "total_energy_joules": {
                            "process_0": 19065.067901819017,
                            "process_2": 18855.157923694514,
                            "process_3": 19041.396552575337,
                            "process_1": 18923.07715316015
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1118.5387263366092,
                        "ram_power_avg": 0.9548385143280029,
                        "cpu_energy_total": 0.00456857747771835,
                        "gpu_energy_total": 0.01647961985035168,
                        "ram_energy_total": 3.08858750546971e-05,
                        "total_energy_kwh": 0.02107908320312473,
                        "total_energy_joules": 75884.69953124902
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2159065015899962,
                        "joules_per_token": 4.631634492874086,
                        "flops_per_joule": 223364803.41695324,
                        "joules_per_flop": 4.4769810852129115e-09
                    },
                    "per-process_emissions": [
                        0.0020174548936660983,
                        0.0019952423363976185,
                        0.002014950004639882,
                        0.0020024295115267667
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0242": {
            "setup": {
                "experiment_id": "0242",
                "date_time": "April 11, 2025 at 05:25:17 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_32",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 18.706420244001492,
                        "average_latency_ms_per_batch": 4676.605061000373,
                        "throughput_queries_per_sec": 6.842570536233153,
                        "throughput_tokens_per_sec": 875.8490286378436
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1883242496,
                        "gpu_max_memory_reserved_bytes": 1883242496
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            16.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2625044480
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0242",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 463.41994880970526,
                            "process_0": 501.80821228311925,
                            "process_3": 474.98015963849303,
                            "process_2": 527.8386395941458
                        },
                        "ram_power": {
                            "process_1": 0.9729437828063965,
                            "process_0": 0.9154229164123535,
                            "process_3": 0.974513053894043,
                            "process_2": 0.975926399230957
                        },
                        "cpu_energy": {
                            "process_1": 0.0005806355703123246,
                            "process_0": 0.0005804304951250288,
                            "process_3": 0.0005817682401873299,
                            "process_2": 0.0005709742602496136
                        },
                        "gpu_energy": {
                            "process_1": 0.00245736057699808,
                            "process_0": 0.0024564350207020302,
                            "process_3": 0.0024564350207020302,
                            "process_2": 0.0024147722095939628
                        },
                        "ram_energy": {
                            "process_1": 3.953362798568154e-06,
                            "process_0": 3.7277206019966782e-06,
                            "process_3": 3.967022443815447e-06,
                            "process_2": 3.919400966968795e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0030419495101089724,
                            "process_0": 0.0030405932364290554,
                            "process_3": 0.0030421702833331754,
                            "process_2": 0.0029896658708105446
                        },
                        "total_energy_joules": {
                            "process_1": 10951.018236392301,
                            "process_0": 10946.1356511446,
                            "process_3": 10951.813019999432,
                            "process_2": 10762.79713491796
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 492.0117400813658,
                        "ram_power_avg": 0.9597015380859375,
                        "cpu_energy_total": 0.002313808565874297,
                        "gpu_energy_total": 0.009785002827996103,
                        "ram_energy_total": 1.5567506811349075e-05,
                        "total_energy_kwh": 0.012114378900681746,
                        "total_energy_joules": 43611.76404245429
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.3756784518977686,
                        "joules_per_token": 2.6618508326693293,
                        "flops_per_joule": 388655936.42696697,
                        "joules_per_flop": 2.57296983340408e-09
                    },
                    "per-process_emissions": [
                        0.001158830665876013,
                        0.0011583139934176487,
                        0.0011589147694357732,
                        0.001138913213485277
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0242": {
            "setup": {
                "experiment_id": "0242",
                "date_time": "April 11, 2025 at 05:25:17 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_32",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 18.706420244001492,
                        "average_latency_ms_per_batch": 4676.605061000373,
                        "throughput_queries_per_sec": 6.842570536233153,
                        "throughput_tokens_per_sec": 875.8490286378436
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1883242496,
                        "gpu_max_memory_reserved_bytes": 1883242496
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            16.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2625044480
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0242",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 463.41994880970526,
                            "process_0": 501.80821228311925,
                            "process_3": 474.98015963849303,
                            "process_2": 527.8386395941458
                        },
                        "ram_power": {
                            "process_1": 0.9729437828063965,
                            "process_0": 0.9154229164123535,
                            "process_3": 0.974513053894043,
                            "process_2": 0.975926399230957
                        },
                        "cpu_energy": {
                            "process_1": 0.0005806355703123246,
                            "process_0": 0.0005804304951250288,
                            "process_3": 0.0005817682401873299,
                            "process_2": 0.0005709742602496136
                        },
                        "gpu_energy": {
                            "process_1": 0.00245736057699808,
                            "process_0": 0.0024564350207020302,
                            "process_3": 0.0024564350207020302,
                            "process_2": 0.0024147722095939628
                        },
                        "ram_energy": {
                            "process_1": 3.953362798568154e-06,
                            "process_0": 3.7277206019966782e-06,
                            "process_3": 3.967022443815447e-06,
                            "process_2": 3.919400966968795e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0030419495101089724,
                            "process_0": 0.0030405932364290554,
                            "process_3": 0.0030421702833331754,
                            "process_2": 0.0029896658708105446
                        },
                        "total_energy_joules": {
                            "process_1": 10951.018236392301,
                            "process_0": 10946.1356511446,
                            "process_3": 10951.813019999432,
                            "process_2": 10762.79713491796
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 492.0117400813658,
                        "ram_power_avg": 0.9597015380859375,
                        "cpu_energy_total": 0.002313808565874297,
                        "gpu_energy_total": 0.009785002827996103,
                        "ram_energy_total": 1.5567506811349075e-05,
                        "total_energy_kwh": 0.012114378900681746,
                        "total_energy_joules": 43611.76404245429
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.3756784518977686,
                        "joules_per_token": 2.6618508326693293,
                        "flops_per_joule": 388655936.42696697,
                        "joules_per_flop": 2.57296983340408e-09
                    },
                    "per-process_emissions": [
                        0.001158830665876013,
                        0.0011583139934176487,
                        0.0011589147694357732,
                        0.001138913213485277
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0243": {
            "setup": {
                "experiment_id": "0243",
                "date_time": "April 11, 2025 at 05:25:56 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_64",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 9.628563292000763,
                        "average_latency_ms_per_batch": 4814.281646000381,
                        "throughput_queries_per_sec": 13.293779779828638,
                        "throughput_tokens_per_sec": 1701.6038118180657
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1881145344,
                        "gpu_max_memory_reserved_bytes": 1881145344
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2671489024
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0243",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 663.5375880765018,
                            "process_2": 702.5172962012202,
                            "process_3": 702.6304094074885,
                            "process_0": 642.5165018339551
                        },
                        "ram_power": {
                            "process_1": 0.9706120491027832,
                            "process_2": 0.9751596450805664,
                            "process_3": 0.9417872428894044,
                            "process_0": 0.9328179359436035
                        },
                        "cpu_energy": {
                            "process_1": 0.0002979203534999897,
                            "process_2": 0.00029710987237479,
                            "process_3": 0.0002987165154063405,
                            "process_0": 0.0003006319594373963
                        },
                        "gpu_energy": {
                            "process_1": 0.001559195414022052,
                            "process_2": 0.001553562631738059,
                            "process_3": 0.001559195414022052,
                            "process_0": 0.0015627095835001326
                        },
                        "ram_energy": {
                            "process_1": 2.081475683072484e-06,
                            "process_2": 2.043841084610094e-06,
                            "process_3": 2.015336074075374e-06,
                            "process_0": 2.015357617333118e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.001859197243205114,
                            "process_2": 0.0018527163451974595,
                            "process_3": 0.0018599272655024683,
                            "process_0": 0.001865356900554862
                        },
                        "total_energy_joules": {
                            "process_1": 6693.1100755384105,
                            "process_2": 6669.778842710854,
                            "process_3": 6695.7381558088855,
                            "process_0": 6715.2848419975035
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 677.8004488797915,
                        "ram_power_avg": 0.9550942182540894,
                        "cpu_energy_total": 0.0011943787007185165,
                        "gpu_energy_total": 0.006234663043282296,
                        "ram_energy_total": 8.15601045909107e-06,
                        "total_energy_kwh": 0.007437197754459904,
                        "total_energy_joules": 26773.91191605565
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.6119389669828159,
                        "joules_per_token": 1.6341498972201935,
                        "flops_per_joule": 633077864.9864581,
                        "joules_per_flop": 1.5795845271282555e-09
                    },
                    "per-process_emissions": [
                        0.0007082611897989882,
                        0.0007057922917029722,
                        0.0007085392917931653,
                        0.0007106077112663747
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0243": {
            "setup": {
                "experiment_id": "0243",
                "date_time": "April 11, 2025 at 05:25:56 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_64",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 9.628563292000763,
                        "average_latency_ms_per_batch": 4814.281646000381,
                        "throughput_queries_per_sec": 13.293779779828638,
                        "throughput_tokens_per_sec": 1701.6038118180657
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1881145344,
                        "gpu_max_memory_reserved_bytes": 1881145344
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2671489024
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0243",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 663.5375880765018,
                            "process_2": 702.5172962012202,
                            "process_3": 702.6304094074885,
                            "process_0": 642.5165018339551
                        },
                        "ram_power": {
                            "process_1": 0.9706120491027832,
                            "process_2": 0.9751596450805664,
                            "process_3": 0.9417872428894044,
                            "process_0": 0.9328179359436035
                        },
                        "cpu_energy": {
                            "process_1": 0.0002979203534999897,
                            "process_2": 0.00029710987237479,
                            "process_3": 0.0002987165154063405,
                            "process_0": 0.0003006319594373963
                        },
                        "gpu_energy": {
                            "process_1": 0.001559195414022052,
                            "process_2": 0.001553562631738059,
                            "process_3": 0.001559195414022052,
                            "process_0": 0.0015627095835001326
                        },
                        "ram_energy": {
                            "process_1": 2.081475683072484e-06,
                            "process_2": 2.043841084610094e-06,
                            "process_3": 2.015336074075374e-06,
                            "process_0": 2.015357617333118e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.001859197243205114,
                            "process_2": 0.0018527163451974595,
                            "process_3": 0.0018599272655024683,
                            "process_0": 0.001865356900554862
                        },
                        "total_energy_joules": {
                            "process_1": 6693.1100755384105,
                            "process_2": 6669.778842710854,
                            "process_3": 6695.7381558088855,
                            "process_0": 6715.2848419975035
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 677.8004488797915,
                        "ram_power_avg": 0.9550942182540894,
                        "cpu_energy_total": 0.0011943787007185165,
                        "gpu_energy_total": 0.006234663043282296,
                        "ram_energy_total": 8.15601045909107e-06,
                        "total_energy_kwh": 0.007437197754459904,
                        "total_energy_joules": 26773.91191605565
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.6119389669828159,
                        "joules_per_token": 1.6341498972201935,
                        "flops_per_joule": 633077864.9864581,
                        "joules_per_flop": 1.5795845271282555e-09
                    },
                    "per-process_emissions": [
                        0.0007082611897989882,
                        0.0007057922917029722,
                        0.0007085392917931653,
                        0.0007106077112663747
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0244": {
            "setup": {
                "experiment_id": "0244",
                "date_time": "April 11, 2025 at 05:26:49 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precis_float32_quant_False_quant8_False_quant4_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.94555676899472,
                        "average_latency_ms_per_batch": 2868.19459612434,
                        "throughput_queries_per_sec": 5.578422057422487,
                        "throughput_tokens_per_sec": 714.0380233500783
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            6.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2012856320
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0244",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 568.8112099170318,
                            "process_0": 0.0,
                            "process_1": 509.3726532299667,
                            "process_3": 670.63472681991
                        },
                        "ram_power": {
                            "process_2": 0.6513347625732422,
                            "process_0": 0.7011265754699708,
                            "process_1": 0.6478486061096191,
                            "process_3": 0.6557092666625977
                        },
                        "cpu_energy": {
                            "process_2": 0.0007183318561566239,
                            "process_0": 0.0007445897790942127,
                            "process_1": 0.0007281126878129955,
                            "process_3": 0.0007209807963125741
                        },
                        "gpu_energy": {
                            "process_2": 0.004561590315935948,
                            "process_0": 0.0045372036297599005,
                            "process_1": 0.004601800070326123,
                            "process_3": 0.004574291159429977
                        },
                        "ram_energy": {
                            "process_2": 3.564429398163457e-06,
                            "process_0": 3.973615437319222e-06,
                            "process_1": 3.573643144950668e-06,
                            "process_3": 3.598775726587378e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005283486601490737,
                            "process_0": 0.005285767024291433,
                            "process_1": 0.00533348640128407,
                            "process_3": 0.005298870731469138
                        },
                        "total_energy_joules": {
                            "process_2": 19020.551765366654,
                            "process_0": 19028.761287449157,
                            "process_1": 19200.551044622654,
                            "process_3": 19075.934633288896
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 437.2046474917271,
                        "ram_power_avg": 0.6640048027038574,
                        "cpu_energy_total": 0.0029120151193764063,
                        "gpu_energy_total": 0.01827488517545195,
                        "ram_energy_total": 1.4710463707020725e-05,
                        "total_energy_kwh": 0.021201610758535376,
                        "total_energy_joules": 76325.79873072736
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21465874281645878,
                        "joules_per_token": 4.658557051435996,
                        "flops_per_joule": 222073941.90462962,
                        "joules_per_flop": 4.5030046813392145e-09
                    },
                    "per-process_emissions": [
                        0.0020127442208378965,
                        0.0020136129479038215,
                        0.0020317916445691665,
                        0.002018604805153168
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0244": {
            "setup": {
                "experiment_id": "0244",
                "date_time": "April 11, 2025 at 05:26:49 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precis_float32_quant_False_quant8_False_quant4_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.94555676899472,
                        "average_latency_ms_per_batch": 2868.19459612434,
                        "throughput_queries_per_sec": 5.578422057422487,
                        "throughput_tokens_per_sec": 714.0380233500783
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            6.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2012856320
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0244",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 568.8112099170318,
                            "process_0": 0.0,
                            "process_1": 509.3726532299667,
                            "process_3": 670.63472681991
                        },
                        "ram_power": {
                            "process_2": 0.6513347625732422,
                            "process_0": 0.7011265754699708,
                            "process_1": 0.6478486061096191,
                            "process_3": 0.6557092666625977
                        },
                        "cpu_energy": {
                            "process_2": 0.0007183318561566239,
                            "process_0": 0.0007445897790942127,
                            "process_1": 0.0007281126878129955,
                            "process_3": 0.0007209807963125741
                        },
                        "gpu_energy": {
                            "process_2": 0.004561590315935948,
                            "process_0": 0.0045372036297599005,
                            "process_1": 0.004601800070326123,
                            "process_3": 0.004574291159429977
                        },
                        "ram_energy": {
                            "process_2": 3.564429398163457e-06,
                            "process_0": 3.973615437319222e-06,
                            "process_1": 3.573643144950668e-06,
                            "process_3": 3.598775726587378e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005283486601490737,
                            "process_0": 0.005285767024291433,
                            "process_1": 0.00533348640128407,
                            "process_3": 0.005298870731469138
                        },
                        "total_energy_joules": {
                            "process_2": 19020.551765366654,
                            "process_0": 19028.761287449157,
                            "process_1": 19200.551044622654,
                            "process_3": 19075.934633288896
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 437.2046474917271,
                        "ram_power_avg": 0.6640048027038574,
                        "cpu_energy_total": 0.0029120151193764063,
                        "gpu_energy_total": 0.01827488517545195,
                        "ram_energy_total": 1.4710463707020725e-05,
                        "total_energy_kwh": 0.021201610758535376,
                        "total_energy_joules": 76325.79873072736
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21465874281645878,
                        "joules_per_token": 4.658557051435996,
                        "flops_per_joule": 222073941.90462962,
                        "joules_per_flop": 4.5030046813392145e-09
                    },
                    "per-process_emissions": [
                        0.0020127442208378965,
                        0.0020136129479038215,
                        0.0020317916445691665,
                        0.002018604805153168
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0245": {
            "setup": {
                "experiment_id": "0245",
                "date_time": "April 11, 2025 at 05:27:40 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precis_float16_quant_False_quant8_False_quant4_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.267255486003705,
                        "average_latency_ms_per_batch": 2908.406935750463,
                        "throughput_queries_per_sec": 5.5012934412052905,
                        "throughput_tokens_per_sec": 704.1655604742772
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419853312,
                        "gpu_max_memory_allocated_bytes": 4419853312,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 3100921856
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0245",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 430.27953828865367,
                            "process_0": 362.12667404439395,
                            "process_2": 357.3768135752678,
                            "process_1": 417.07360867975666
                        },
                        "ram_power": {
                            "process_3": 0.882300853729248,
                            "process_0": 1.080446720123291,
                            "process_2": 0.8711814880371094,
                            "process_1": 0.8792567253112793
                        },
                        "cpu_energy": {
                            "process_3": 0.0007011813977185283,
                            "process_0": 0.0007214665764998928,
                            "process_2": 0.0007216215409063127,
                            "process_1": 0.0007077099915312603
                        },
                        "gpu_energy": {
                            "process_3": 0.0024598689123379036,
                            "process_0": 0.002520594516473851,
                            "process_2": 0.0025228620182879413,
                            "process_1": 0.0024887097687437687
                        },
                        "ram_energy": {
                            "process_3": 4.7224127920441665e-06,
                            "process_0": 5.910524509250423e-06,
                            "process_2": 4.7553013579422816e-06,
                            "process_1": 4.7418684982032745e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0031657727228484762,
                            "process_0": 0.003247971617482995,
                            "process_2": 0.003249238860552196,
                            "process_1": 0.0032011616287732324
                        },
                        "total_energy_joules": {
                            "process_3": 11396.781802254514,
                            "process_0": 11692.697822938782,
                            "process_2": 11697.259897987906,
                            "process_1": 11524.181863583637
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 391.71415864701805,
                        "ram_power_avg": 0.9282964468002319,
                        "cpu_energy_total": 0.002851979506655994,
                        "gpu_energy_total": 0.009992035215843464,
                        "ram_energy_total": 2.0130107157440143e-05,
                        "total_energy_kwh": 0.0128641448296569,
                        "total_energy_joules": 46310.92138676484
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.3537826393728882,
                        "joules_per_token": 2.826594322922659,
                        "flops_per_joule": 366003752.15156305,
                        "joules_per_flop": 2.732212427117134e-09
                    },
                    "per-process_emissions": [
                        0.001206001118769127,
                        0.001237314787680147,
                        0.0012377975439273592,
                        0.001219482522481163
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0245": {
            "setup": {
                "experiment_id": "0245",
                "date_time": "April 11, 2025 at 05:27:40 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precis_float16_quant_False_quant8_False_quant4_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.267255486003705,
                        "average_latency_ms_per_batch": 2908.406935750463,
                        "throughput_queries_per_sec": 5.5012934412052905,
                        "throughput_tokens_per_sec": 704.1655604742772
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419853312,
                        "gpu_max_memory_allocated_bytes": 4419853312,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 3100921856
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0245",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 430.27953828865367,
                            "process_0": 362.12667404439395,
                            "process_2": 357.3768135752678,
                            "process_1": 417.07360867975666
                        },
                        "ram_power": {
                            "process_3": 0.882300853729248,
                            "process_0": 1.080446720123291,
                            "process_2": 0.8711814880371094,
                            "process_1": 0.8792567253112793
                        },
                        "cpu_energy": {
                            "process_3": 0.0007011813977185283,
                            "process_0": 0.0007214665764998928,
                            "process_2": 0.0007216215409063127,
                            "process_1": 0.0007077099915312603
                        },
                        "gpu_energy": {
                            "process_3": 0.0024598689123379036,
                            "process_0": 0.002520594516473851,
                            "process_2": 0.0025228620182879413,
                            "process_1": 0.0024887097687437687
                        },
                        "ram_energy": {
                            "process_3": 4.7224127920441665e-06,
                            "process_0": 5.910524509250423e-06,
                            "process_2": 4.7553013579422816e-06,
                            "process_1": 4.7418684982032745e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0031657727228484762,
                            "process_0": 0.003247971617482995,
                            "process_2": 0.003249238860552196,
                            "process_1": 0.0032011616287732324
                        },
                        "total_energy_joules": {
                            "process_3": 11396.781802254514,
                            "process_0": 11692.697822938782,
                            "process_2": 11697.259897987906,
                            "process_1": 11524.181863583637
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 391.71415864701805,
                        "ram_power_avg": 0.9282964468002319,
                        "cpu_energy_total": 0.002851979506655994,
                        "gpu_energy_total": 0.009992035215843464,
                        "ram_energy_total": 2.0130107157440143e-05,
                        "total_energy_kwh": 0.0128641448296569,
                        "total_energy_joules": 46310.92138676484
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.3537826393728882,
                        "joules_per_token": 2.826594322922659,
                        "flops_per_joule": 366003752.15156305,
                        "joules_per_flop": 2.732212427117134e-09
                    },
                    "per-process_emissions": [
                        0.001206001118769127,
                        0.001237314787680147,
                        0.0012377975439273592,
                        0.001219482522481163
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0246": {
            "setup": {
                "experiment_id": "0246",
                "date_time": "April 11, 2025 at 05:29:22 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precis_float16_quant_True_quant8_True_quant4_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 72.23472848499296,
                        "average_latency_ms_per_batch": 9029.34106062412,
                        "throughput_queries_per_sec": 1.7720008461939813,
                        "throughput_tokens_per_sec": 226.8161083128296
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576560640,
                        "gpu_max_memory_allocated_bytes": 1576560640,
                        "gpu_current_memory_reserved_bytes": 2883584000,
                        "gpu_max_memory_reserved_bytes": 2883584000
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2724073472
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0246",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 236.3588167875678,
                            "process_3": 201.72159949714856,
                            "process_0": 193.82907757931656,
                            "process_2": 185.55492140099304
                        },
                        "ram_power": {
                            "process_1": 0.9894604682922363,
                            "process_3": 0.9780035018920898,
                            "process_0": 0.9510054588317871,
                            "process_2": 0.9891300201416017
                        },
                        "cpu_energy": {
                            "process_1": 0.002270227963062894,
                            "process_3": 0.0022345661305934066,
                            "process_0": 0.0022273494689998186,
                            "process_2": 0.002257495021155933
                        },
                        "gpu_energy": {
                            "process_1": 0.00406462686280995,
                            "process_3": 0.004014785156270018,
                            "process_0": 0.0039866081892838134,
                            "process_2": 0.0040391737868919475
                        },
                        "ram_energy": {
                            "process_1": 1.587767522851536e-05,
                            "process_3": 1.5440657847777413e-05,
                            "process_0": 1.4973973229064967e-05,
                            "process_2": 1.5724046837775314e-05
                        },
                        "total_energy_kwh": {
                            "process_1": 0.006350732501101358,
                            "process_3": 0.006264791944711203,
                            "process_0": 0.006228931631512701,
                            "process_2": 0.006312392854885656
                        },
                        "total_energy_joules": {
                            "process_1": 22862.63700396489,
                            "process_3": 22553.251000960332,
                            "process_0": 22424.15387344572,
                            "process_2": 22724.614277588364
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 204.3661038162565,
                        "ram_power_avg": 0.9768998622894287,
                        "cpu_energy_total": 0.008989638583812052,
                        "gpu_energy_total": 0.01610519399525573,
                        "ram_energy_total": 6.201635314313305e-05,
                        "total_energy_kwh": 0.025156848932210915,
                        "total_energy_joules": 90564.65615595931
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.18090942642994737,
                        "joules_per_token": 5.527627939206501,
                        "flops_per_joule": 187158784.81295004,
                        "joules_per_flop": 5.343056704495162e-09
                    },
                    "per-process_emissions": [
                        0.0024193115462945625,
                        0.0023865724913377327,
                        0.0023729115050247634,
                        0.002404706058068691
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0246": {
            "setup": {
                "experiment_id": "0246",
                "date_time": "April 11, 2025 at 05:29:22 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precis_float16_quant_True_quant8_True_quant4_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 72.23472848499296,
                        "average_latency_ms_per_batch": 9029.34106062412,
                        "throughput_queries_per_sec": 1.7720008461939813,
                        "throughput_tokens_per_sec": 226.8161083128296
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576560640,
                        "gpu_max_memory_allocated_bytes": 1576560640,
                        "gpu_current_memory_reserved_bytes": 2883584000,
                        "gpu_max_memory_reserved_bytes": 2883584000
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2724073472
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0246",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 236.3588167875678,
                            "process_3": 201.72159949714856,
                            "process_0": 193.82907757931656,
                            "process_2": 185.55492140099304
                        },
                        "ram_power": {
                            "process_1": 0.9894604682922363,
                            "process_3": 0.9780035018920898,
                            "process_0": 0.9510054588317871,
                            "process_2": 0.9891300201416017
                        },
                        "cpu_energy": {
                            "process_1": 0.002270227963062894,
                            "process_3": 0.0022345661305934066,
                            "process_0": 0.0022273494689998186,
                            "process_2": 0.002257495021155933
                        },
                        "gpu_energy": {
                            "process_1": 0.00406462686280995,
                            "process_3": 0.004014785156270018,
                            "process_0": 0.0039866081892838134,
                            "process_2": 0.0040391737868919475
                        },
                        "ram_energy": {
                            "process_1": 1.587767522851536e-05,
                            "process_3": 1.5440657847777413e-05,
                            "process_0": 1.4973973229064967e-05,
                            "process_2": 1.5724046837775314e-05
                        },
                        "total_energy_kwh": {
                            "process_1": 0.006350732501101358,
                            "process_3": 0.006264791944711203,
                            "process_0": 0.006228931631512701,
                            "process_2": 0.006312392854885656
                        },
                        "total_energy_joules": {
                            "process_1": 22862.63700396489,
                            "process_3": 22553.251000960332,
                            "process_0": 22424.15387344572,
                            "process_2": 22724.614277588364
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 204.3661038162565,
                        "ram_power_avg": 0.9768998622894287,
                        "cpu_energy_total": 0.008989638583812052,
                        "gpu_energy_total": 0.01610519399525573,
                        "ram_energy_total": 6.201635314313305e-05,
                        "total_energy_kwh": 0.025156848932210915,
                        "total_energy_joules": 90564.65615595931
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.18090942642994737,
                        "joules_per_token": 5.527627939206501,
                        "flops_per_joule": 187158784.81295004,
                        "joules_per_flop": 5.343056704495162e-09
                    },
                    "per-process_emissions": [
                        0.0024193115462945625,
                        0.0023865724913377327,
                        0.0023729115050247634,
                        0.002404706058068691
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0247": {
            "setup": {
                "experiment_id": "0247",
                "date_time": "April 11, 2025 at 05:30:29 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precis_float16_quant_True_quant8_False_quant4_True",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.35492676799913,
                        "average_latency_ms_per_batch": 4669.3658459998915,
                        "throughput_queries_per_sec": 3.426589504377073,
                        "throughput_tokens_per_sec": 438.60345656026533
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2635046912
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0247",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 445.1496056707763,
                            "process_1": 439.0820050103771,
                            "process_3": 378.6320308164236,
                            "process_0": 267.2328626636334
                        },
                        "ram_power": {
                            "process_2": 0.9593181610107422,
                            "process_1": 0.9350266456604004,
                            "process_3": 0.9750466346740724,
                            "process_0": 0.9194269180297852
                        },
                        "cpu_energy": {
                            "process_2": 0.0011350980627499892,
                            "process_1": 0.0011320144482816659,
                            "process_3": 0.0011510601735313913,
                            "process_0": 0.001155003482531015
                        },
                        "gpu_energy": {
                            "process_2": 0.004074248259396174,
                            "process_1": 0.004073857703528305,
                            "process_3": 0.004118892739555957,
                            "process_0": 0.004129956359518183
                        },
                        "ram_energy": {
                            "process_2": 7.612468926322945e-06,
                            "process_1": 7.428361241971286e-06,
                            "process_3": 7.811242571807022e-06,
                            "process_0": 7.3993888473240745e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005216958791072488,
                            "process_1": 0.005213300513051944,
                            "process_3": 0.005277764155659157,
                            "process_0": 0.005292359230896522
                        },
                        "total_energy_joules": {
                            "process_2": 18781.051647860957,
                            "process_1": 18767.881846987,
                            "process_3": 18999.950960372968,
                            "process_0": 19052.49323122748
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 382.5241260403026,
                        "ram_power_avg": 0.94720458984375,
                        "cpu_energy_total": 0.004573176167094062,
                        "gpu_energy_total": 0.01639695506199862,
                        "ram_energy_total": 3.0251461587425328e-05,
                        "total_energy_kwh": 0.02100038269068011,
                        "total_energy_joules": 75601.3776864484
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2167156274314409,
                        "joules_per_token": 4.614341899807641,
                        "flops_per_joule": 224201879.8050329,
                        "joules_per_flop": 4.4602659035223306e-09
                    },
                    "per-process_emissions": [
                        0.0019874004514590644,
                        0.0019860068304471384,
                        0.002010564255098356,
                        0.0020161242490100303
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0247": {
            "setup": {
                "experiment_id": "0247",
                "date_time": "April 11, 2025 at 05:30:29 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precis_float16_quant_True_quant8_False_quant4_True",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.35492676799913,
                        "average_latency_ms_per_batch": 4669.3658459998915,
                        "throughput_queries_per_sec": 3.426589504377073,
                        "throughput_tokens_per_sec": 438.60345656026533
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2635046912
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0247",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 445.1496056707763,
                            "process_1": 439.0820050103771,
                            "process_3": 378.6320308164236,
                            "process_0": 267.2328626636334
                        },
                        "ram_power": {
                            "process_2": 0.9593181610107422,
                            "process_1": 0.9350266456604004,
                            "process_3": 0.9750466346740724,
                            "process_0": 0.9194269180297852
                        },
                        "cpu_energy": {
                            "process_2": 0.0011350980627499892,
                            "process_1": 0.0011320144482816659,
                            "process_3": 0.0011510601735313913,
                            "process_0": 0.001155003482531015
                        },
                        "gpu_energy": {
                            "process_2": 0.004074248259396174,
                            "process_1": 0.004073857703528305,
                            "process_3": 0.004118892739555957,
                            "process_0": 0.004129956359518183
                        },
                        "ram_energy": {
                            "process_2": 7.612468926322945e-06,
                            "process_1": 7.428361241971286e-06,
                            "process_3": 7.811242571807022e-06,
                            "process_0": 7.3993888473240745e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005216958791072488,
                            "process_1": 0.005213300513051944,
                            "process_3": 0.005277764155659157,
                            "process_0": 0.005292359230896522
                        },
                        "total_energy_joules": {
                            "process_2": 18781.051647860957,
                            "process_1": 18767.881846987,
                            "process_3": 18999.950960372968,
                            "process_0": 19052.49323122748
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 382.5241260403026,
                        "ram_power_avg": 0.94720458984375,
                        "cpu_energy_total": 0.004573176167094062,
                        "gpu_energy_total": 0.01639695506199862,
                        "ram_energy_total": 3.0251461587425328e-05,
                        "total_energy_kwh": 0.02100038269068011,
                        "total_energy_joules": 75601.3776864484
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2167156274314409,
                        "joules_per_token": 4.614341899807641,
                        "flops_per_joule": 224201879.8050329,
                        "joules_per_flop": 4.4602659035223306e-09
                    },
                    "per-process_emissions": [
                        0.0019874004514590644,
                        0.0019860068304471384,
                        0.002010564255098356,
                        0.0020161242490100303
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0248": {
            "setup": {
                "experiment_id": "0248",
                "date_time": "April 11, 2025 at 05:31:35 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.2636839730003,
                        "average_latency_ms_per_batch": 4532.960496625037,
                        "throughput_queries_per_sec": 3.5297020593743564,
                        "throughput_tokens_per_sec": 451.8018635999176
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2635730944
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0248",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 0.0,
                            "process_0": 359.5805256283451,
                            "process_1": 327.36192113545764,
                            "process_3": 378.216055754753
                        },
                        "ram_power": {
                            "process_2": 0.9631404876708984,
                            "process_0": 0.9203095436096191,
                            "process_1": 0.9741697311401367,
                            "process_3": 0.9541912078857422
                        },
                        "cpu_energy": {
                            "process_2": 0.001143670890843623,
                            "process_0": 0.0011205185856874775,
                            "process_1": 0.0011258714364371372,
                            "process_3": 0.001117323852781169
                        },
                        "gpu_energy": {
                            "process_2": 0.0040040159809881515,
                            "process_0": 0.004023148496293982,
                            "process_1": 0.004033024615306358,
                            "process_3": 0.004013354877348219
                        },
                        "ram_energy": {
                            "process_2": 7.733194185719769e-06,
                            "process_0": 7.215563331246361e-06,
                            "process_1": 7.635175760668017e-06,
                            "process_3": 7.5016675638209035e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005155420066017494,
                            "process_0": 0.005150882645312707,
                            "process_1": 0.005166531227504162,
                            "process_3": 0.005138180397693211
                        },
                        "total_energy_joules": {
                            "process_2": 18559.51223766298,
                            "process_0": 18543.177523125745,
                            "process_1": 18599.51241901498,
                            "process_3": 18497.44943169556
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 266.2896256296389,
                        "ram_power_avg": 0.9529527425765991,
                        "cpu_energy_total": 0.004507384765749406,
                        "gpu_energy_total": 0.01607354396993671,
                        "ram_energy_total": 3.008560084145505e-05,
                        "total_energy_kwh": 0.020611014336527574,
                        "total_energy_joules": 74199.65161149926
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22080966209632247,
                        "joules_per_token": 4.528787329803421,
                        "flops_per_joule": 228437339.32741457,
                        "joules_per_flop": 4.377568058463159e-09
                    },
                    "per-process_emissions": [
                        0.0019639572741493644,
                        0.001962228743731876,
                        0.0019681900711177105,
                        0.001957389822501229
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0248": {
            "setup": {
                "experiment_id": "0248",
                "date_time": "April 11, 2025 at 05:31:35 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.2636839730003,
                        "average_latency_ms_per_batch": 4532.960496625037,
                        "throughput_queries_per_sec": 3.5297020593743564,
                        "throughput_tokens_per_sec": 451.8018635999176
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2635730944
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0248",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 0.0,
                            "process_0": 359.5805256283451,
                            "process_1": 327.36192113545764,
                            "process_3": 378.216055754753
                        },
                        "ram_power": {
                            "process_2": 0.9631404876708984,
                            "process_0": 0.9203095436096191,
                            "process_1": 0.9741697311401367,
                            "process_3": 0.9541912078857422
                        },
                        "cpu_energy": {
                            "process_2": 0.001143670890843623,
                            "process_0": 0.0011205185856874775,
                            "process_1": 0.0011258714364371372,
                            "process_3": 0.001117323852781169
                        },
                        "gpu_energy": {
                            "process_2": 0.0040040159809881515,
                            "process_0": 0.004023148496293982,
                            "process_1": 0.004033024615306358,
                            "process_3": 0.004013354877348219
                        },
                        "ram_energy": {
                            "process_2": 7.733194185719769e-06,
                            "process_0": 7.215563331246361e-06,
                            "process_1": 7.635175760668017e-06,
                            "process_3": 7.5016675638209035e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005155420066017494,
                            "process_0": 0.005150882645312707,
                            "process_1": 0.005166531227504162,
                            "process_3": 0.005138180397693211
                        },
                        "total_energy_joules": {
                            "process_2": 18559.51223766298,
                            "process_0": 18543.177523125745,
                            "process_1": 18599.51241901498,
                            "process_3": 18497.44943169556
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 266.2896256296389,
                        "ram_power_avg": 0.9529527425765991,
                        "cpu_energy_total": 0.004507384765749406,
                        "gpu_energy_total": 0.01607354396993671,
                        "ram_energy_total": 3.008560084145505e-05,
                        "total_energy_kwh": 0.020611014336527574,
                        "total_energy_joules": 74199.65161149926
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22080966209632247,
                        "joules_per_token": 4.528787329803421,
                        "flops_per_joule": 228437339.32741457,
                        "joules_per_flop": 4.377568058463159e-09
                    },
                    "per-process_emissions": [
                        0.0019639572741493644,
                        0.001962228743731876,
                        0.0019681900711177105,
                        0.001957389822501229
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0249": {
            "setup": {
                "experiment_id": "0249",
                "date_time": "April 11, 2025 at 05:32:41 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.23362882500078,
                        "average_latency_ms_per_batch": 4654.203603125097,
                        "throughput_queries_per_sec": 3.437752484497388,
                        "throughput_tokens_per_sec": 440.0323180156657
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2647302144
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0249",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 302.0877388266489,
                            "process_2": 326.41187590709643,
                            "process_3": 309.9873008252386,
                            "process_1": 0.0
                        },
                        "ram_power": {
                            "process_0": 0.9234151840209962,
                            "process_2": 0.9689311981201172,
                            "process_3": 0.9784784317016602,
                            "process_1": 0.9784698486328125
                        },
                        "cpu_energy": {
                            "process_0": 0.0011517971912185206,
                            "process_2": 0.0011491951297495006,
                            "process_3": 0.0011483759106562276,
                            "process_1": 0.001176945294656434
                        },
                        "gpu_energy": {
                            "process_0": 0.0041212505192199655,
                            "process_2": 0.004122232186671904,
                            "process_3": 0.0041137955132560244,
                            "process_1": 0.004111377455766008
                        },
                        "ram_energy": {
                            "process_0": 7.4334921996557e-06,
                            "process_2": 7.761840697219743e-06,
                            "process_3": 8.108652495053845e-06,
                            "process_1": 8.045157473961693e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005280481202638143,
                            "process_2": 0.005279189157118625,
                            "process_3": 0.005270280076407307,
                            "process_1": 0.005296367907896403
                        },
                        "total_energy_joules": {
                            "process_0": 19009.732329497318,
                            "process_2": 19005.08096562705,
                            "process_3": 18973.008275066306,
                            "process_1": 19066.92446842705
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 234.621728889746,
                        "ram_power_avg": 0.9623236656188965,
                        "cpu_energy_total": 0.004626313526280683,
                        "gpu_energy_total": 0.0164686556749139,
                        "ram_energy_total": 3.134914286589098e-05,
                        "total_energy_kwh": 0.02112631834406048,
                        "total_energy_joules": 76054.74603861773
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2154237684480706,
                        "joules_per_token": 4.6420133080211015,
                        "flops_per_joule": 222865394.6795831,
                        "joules_per_flop": 4.487013344703941e-09
                    },
                    "per-process_emissions": [
                        0.0020115993141450006,
                        0.00201110710940434,
                        0.002007713195107364,
                        0.0020176513545131346
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0249": {
            "setup": {
                "experiment_id": "0249",
                "date_time": "April 11, 2025 at 05:32:41 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.23362882500078,
                        "average_latency_ms_per_batch": 4654.203603125097,
                        "throughput_queries_per_sec": 3.437752484497388,
                        "throughput_tokens_per_sec": 440.0323180156657
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2647302144
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0249",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 302.0877388266489,
                            "process_2": 326.41187590709643,
                            "process_3": 309.9873008252386,
                            "process_1": 0.0
                        },
                        "ram_power": {
                            "process_0": 0.9234151840209962,
                            "process_2": 0.9689311981201172,
                            "process_3": 0.9784784317016602,
                            "process_1": 0.9784698486328125
                        },
                        "cpu_energy": {
                            "process_0": 0.0011517971912185206,
                            "process_2": 0.0011491951297495006,
                            "process_3": 0.0011483759106562276,
                            "process_1": 0.001176945294656434
                        },
                        "gpu_energy": {
                            "process_0": 0.0041212505192199655,
                            "process_2": 0.004122232186671904,
                            "process_3": 0.0041137955132560244,
                            "process_1": 0.004111377455766008
                        },
                        "ram_energy": {
                            "process_0": 7.4334921996557e-06,
                            "process_2": 7.761840697219743e-06,
                            "process_3": 8.108652495053845e-06,
                            "process_1": 8.045157473961693e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005280481202638143,
                            "process_2": 0.005279189157118625,
                            "process_3": 0.005270280076407307,
                            "process_1": 0.005296367907896403
                        },
                        "total_energy_joules": {
                            "process_0": 19009.732329497318,
                            "process_2": 19005.08096562705,
                            "process_3": 18973.008275066306,
                            "process_1": 19066.92446842705
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 234.621728889746,
                        "ram_power_avg": 0.9623236656188965,
                        "cpu_energy_total": 0.004626313526280683,
                        "gpu_energy_total": 0.0164686556749139,
                        "ram_energy_total": 3.134914286589098e-05,
                        "total_energy_kwh": 0.02112631834406048,
                        "total_energy_joules": 76054.74603861773
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2154237684480706,
                        "joules_per_token": 4.6420133080211015,
                        "flops_per_joule": 222865394.6795831,
                        "joules_per_flop": 4.487013344703941e-09
                    },
                    "per-process_emissions": [
                        0.0020115993141450006,
                        0.00201110710940434,
                        0.002007713195107364,
                        0.0020176513545131346
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0250": {
            "setup": {
                "experiment_id": "0250",
                "date_time": "April 11, 2025 at 05:33:46 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.71284603699678,
                        "average_latency_ms_per_batch": 4589.1057546245975,
                        "throughput_queries_per_sec": 3.4865180397893987,
                        "throughput_tokens_per_sec": 446.27430909304303
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2669834240
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0250",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 454.09129449368913,
                            "process_1": 0.0,
                            "process_2": 264.54179457317747,
                            "process_3": 423.81464849823425
                        },
                        "ram_power": {
                            "process_0": 0.9316878318786621,
                            "process_1": 0.9750494956970215,
                            "process_2": 0.9782123565673828,
                            "process_3": 0.9671359062194824
                        },
                        "cpu_energy": {
                            "process_0": 0.0011367382023125862,
                            "process_1": 0.0011779656221250433,
                            "process_2": 0.0011623207380002895,
                            "process_3": 0.001135255012687253
                        },
                        "gpu_energy": {
                            "process_0": 0.0040834066000559055,
                            "process_1": 0.004113304679529861,
                            "process_2": 0.004152348877431933,
                            "process_3": 0.004086589658158002
                        },
                        "ram_energy": {
                            "process_0": 7.4457173164507785e-06,
                            "process_1": 8.030720865007746e-06,
                            "process_2": 7.956350089279877e-06,
                            "process_3": 7.687195595625487e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005227590519684944,
                            "process_1": 0.005299301022519911,
                            "process_2": 0.005322625965521503,
                            "process_3": 0.005229531866440881
                        },
                        "total_energy_joules": {
                            "process_0": 18819.3258708658,
                            "process_1": 19077.483681071677,
                            "process_2": 19161.45347587741,
                            "process_3": 18826.314719187172
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 285.6119343912752,
                        "ram_power_avg": 0.9630213975906372,
                        "cpu_energy_total": 0.004612279575125172,
                        "gpu_energy_total": 0.0164356498151757,
                        "ram_energy_total": 3.1119983866363886e-05,
                        "total_energy_kwh": 0.021079049374167238,
                        "total_energy_joules": 75884.57774700205
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21590684809005578,
                        "joules_per_token": 4.631627059753543,
                        "flops_per_joule": 223365161.88655522,
                        "joules_per_flop": 4.476973900289291e-09
                    },
                    "per-process_emissions": [
                        0.0019914506084739796,
                        0.00201876872452896,
                        0.0020276543615654168,
                        0.0019921901645206536
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0250": {
            "setup": {
                "experiment_id": "0250",
                "date_time": "April 11, 2025 at 05:33:46 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.71284603699678,
                        "average_latency_ms_per_batch": 4589.1057546245975,
                        "throughput_queries_per_sec": 3.4865180397893987,
                        "throughput_tokens_per_sec": 446.27430909304303
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2669834240
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0250",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 454.09129449368913,
                            "process_1": 0.0,
                            "process_2": 264.54179457317747,
                            "process_3": 423.81464849823425
                        },
                        "ram_power": {
                            "process_0": 0.9316878318786621,
                            "process_1": 0.9750494956970215,
                            "process_2": 0.9782123565673828,
                            "process_3": 0.9671359062194824
                        },
                        "cpu_energy": {
                            "process_0": 0.0011367382023125862,
                            "process_1": 0.0011779656221250433,
                            "process_2": 0.0011623207380002895,
                            "process_3": 0.001135255012687253
                        },
                        "gpu_energy": {
                            "process_0": 0.0040834066000559055,
                            "process_1": 0.004113304679529861,
                            "process_2": 0.004152348877431933,
                            "process_3": 0.004086589658158002
                        },
                        "ram_energy": {
                            "process_0": 7.4457173164507785e-06,
                            "process_1": 8.030720865007746e-06,
                            "process_2": 7.956350089279877e-06,
                            "process_3": 7.687195595625487e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005227590519684944,
                            "process_1": 0.005299301022519911,
                            "process_2": 0.005322625965521503,
                            "process_3": 0.005229531866440881
                        },
                        "total_energy_joules": {
                            "process_0": 18819.3258708658,
                            "process_1": 19077.483681071677,
                            "process_2": 19161.45347587741,
                            "process_3": 18826.314719187172
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 285.6119343912752,
                        "ram_power_avg": 0.9630213975906372,
                        "cpu_energy_total": 0.004612279575125172,
                        "gpu_energy_total": 0.0164356498151757,
                        "ram_energy_total": 3.1119983866363886e-05,
                        "total_energy_kwh": 0.021079049374167238,
                        "total_energy_joules": 75884.57774700205
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21590684809005578,
                        "joules_per_token": 4.631627059753543,
                        "flops_per_joule": 223365161.88655522,
                        "joules_per_flop": 4.476973900289291e-09
                    },
                    "per-process_emissions": [
                        0.0019914506084739796,
                        0.00201876872452896,
                        0.0020276543615654168,
                        0.0019921901645206536
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0251": {
            "setup": {
                "experiment_id": "0251",
                "date_time": "April 11, 2025 at 05:34:52 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.17740863099971,
                        "average_latency_ms_per_batch": 4647.176078874963,
                        "throughput_queries_per_sec": 3.4429511015802623,
                        "throughput_tokens_per_sec": 440.6977410022736
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2646908928
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0251",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 438.4806772144199,
                            "process_1": 441.178394254695,
                            "process_0": 235.68902801390252,
                            "process_3": 450.9387839216223
                        },
                        "ram_power": {
                            "process_2": 0.9704432487487794,
                            "process_1": 0.9832849502563477,
                            "process_0": 0.9235095977783203,
                            "process_3": 0.9783282279968262
                        },
                        "cpu_energy": {
                            "process_2": 0.0011401350003444578,
                            "process_1": 0.0011411982631560705,
                            "process_0": 0.0011482489016566434,
                            "process_3": 0.0011395431790625706
                        },
                        "gpu_energy": {
                            "process_2": 0.004115366903401918,
                            "process_1": 0.004117752738643843,
                            "process_0": 0.00414034997894408,
                            "process_3": 0.004113254679489919
                        },
                        "ram_energy": {
                            "process_2": 7.787843336980026e-06,
                            "process_1": 7.887786217380212e-06,
                            "process_0": 7.3683023072900785e-06,
                            "process_3": 7.829907655186917e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005263289747083355,
                            "process_1": 0.005266838788017295,
                            "process_0": 0.005295967182908015,
                            "process_3": 0.005260627766207677
                        },
                        "total_energy_joules": {
                            "process_2": 18947.84308950008,
                            "process_1": 18960.619636862262,
                            "process_0": 19065.48185846885,
                            "process_3": 18938.259958347637
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 391.57172085115997,
                        "ram_power_avg": 0.9638915061950684,
                        "cpu_energy_total": 0.0045691253442197415,
                        "gpu_energy_total": 0.01648672430047976,
                        "ram_energy_total": 3.087383951683723e-05,
                        "total_energy_kwh": 0.021086723484216342,
                        "total_energy_joules": 75912.20454317883
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21582827291862916,
                        "joules_per_token": 4.63331326557488,
                        "flops_per_joule": 223283872.4043492,
                        "joules_per_flop": 4.478603802558028e-09
                    },
                    "per-process_emissions": [
                        0.0020050502291514044,
                        0.0020064022362951887,
                        0.0020174986983288084,
                        0.0020040361475368146
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0251": {
            "setup": {
                "experiment_id": "0251",
                "date_time": "April 11, 2025 at 05:34:52 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.17740863099971,
                        "average_latency_ms_per_batch": 4647.176078874963,
                        "throughput_queries_per_sec": 3.4429511015802623,
                        "throughput_tokens_per_sec": 440.6977410022736
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2646908928
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0251",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 438.4806772144199,
                            "process_1": 441.178394254695,
                            "process_0": 235.68902801390252,
                            "process_3": 450.9387839216223
                        },
                        "ram_power": {
                            "process_2": 0.9704432487487794,
                            "process_1": 0.9832849502563477,
                            "process_0": 0.9235095977783203,
                            "process_3": 0.9783282279968262
                        },
                        "cpu_energy": {
                            "process_2": 0.0011401350003444578,
                            "process_1": 0.0011411982631560705,
                            "process_0": 0.0011482489016566434,
                            "process_3": 0.0011395431790625706
                        },
                        "gpu_energy": {
                            "process_2": 0.004115366903401918,
                            "process_1": 0.004117752738643843,
                            "process_0": 0.00414034997894408,
                            "process_3": 0.004113254679489919
                        },
                        "ram_energy": {
                            "process_2": 7.787843336980026e-06,
                            "process_1": 7.887786217380212e-06,
                            "process_0": 7.3683023072900785e-06,
                            "process_3": 7.829907655186917e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005263289747083355,
                            "process_1": 0.005266838788017295,
                            "process_0": 0.005295967182908015,
                            "process_3": 0.005260627766207677
                        },
                        "total_energy_joules": {
                            "process_2": 18947.84308950008,
                            "process_1": 18960.619636862262,
                            "process_0": 19065.48185846885,
                            "process_3": 18938.259958347637
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 391.57172085115997,
                        "ram_power_avg": 0.9638915061950684,
                        "cpu_energy_total": 0.0045691253442197415,
                        "gpu_energy_total": 0.01648672430047976,
                        "ram_energy_total": 3.087383951683723e-05,
                        "total_energy_kwh": 0.021086723484216342,
                        "total_energy_joules": 75912.20454317883
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21582827291862916,
                        "joules_per_token": 4.63331326557488,
                        "flops_per_joule": 223283872.4043492,
                        "joules_per_flop": 4.478603802558028e-09
                    },
                    "per-process_emissions": [
                        0.0020050502291514044,
                        0.0020064022362951887,
                        0.0020174986983288084,
                        0.0020040361475368146
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0252": {
            "setup": {
                "experiment_id": "0252",
                "date_time": "April 11, 2025 at 05:35:58 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.28699775199493,
                        "average_latency_ms_per_batch": 4660.874718999366,
                        "throughput_queries_per_sec": 3.4328320250227637,
                        "throughput_tokens_per_sec": 439.40249920291376
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            94.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2602528768
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0252",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 432.35357290090946,
                            "process_0": 363.2065462930909,
                            "process_3": 424.59052917012394,
                            "process_2": 419.13896041435083
                        },
                        "ram_power": {
                            "process_1": 0.969294548034668,
                            "process_0": 0.9075665473937988,
                            "process_3": 0.9706306457519531,
                            "process_2": 0.9764471054077148
                        },
                        "cpu_energy": {
                            "process_1": 0.001131426066031395,
                            "process_0": 0.0011503835199063136,
                            "process_3": 0.0011517758301561117,
                            "process_2": 0.0011432181465313533
                        },
                        "gpu_energy": {
                            "process_1": 0.0040609524154260335,
                            "process_0": 0.00413145163849199,
                            "process_3": 0.00413145163849199,
                            "process_2": 0.004104128283299979
                        },
                        "ram_energy": {
                            "process_1": 7.692201557260953e-06,
                            "process_0": 7.299607270649083e-06,
                            "process_3": 7.837543449575886e-06,
                            "process_2": 7.832905384365541e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005200070683014689,
                            "process_0": 0.005289134765668952,
                            "process_3": 0.00529106501209768,
                            "process_2": 0.005255179335215699
                        },
                        "total_energy_joules": {
                            "process_1": 18720.25445885288,
                            "process_0": 19040.885156408225,
                            "process_3": 19047.834043551647,
                            "process_2": 18918.645606776518
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 409.8224021946188,
                        "ram_power_avg": 0.9559847116470337,
                        "cpu_energy_total": 0.004576803562625174,
                        "gpu_energy_total": 0.016427983975709992,
                        "ram_energy_total": 3.066225766185146e-05,
                        "total_energy_kwh": 0.021035449795997023,
                        "total_energy_joules": 75727.61926558928
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21635435206986506,
                        "joules_per_token": 4.622047074315752,
                        "flops_per_joule": 223828124.50112355,
                        "joules_per_flop": 4.467713797043321e-09
                    },
                    "per-process_emissions": [
                        0.001980966926694446,
                        0.002014895888981587,
                        0.002015631216358611,
                        0.0020019605677504206
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0252": {
            "setup": {
                "experiment_id": "0252",
                "date_time": "April 11, 2025 at 05:35:58 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.28699775199493,
                        "average_latency_ms_per_batch": 4660.874718999366,
                        "throughput_queries_per_sec": 3.4328320250227637,
                        "throughput_tokens_per_sec": 439.40249920291376
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            94.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2602528768
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0252",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 432.35357290090946,
                            "process_0": 363.2065462930909,
                            "process_3": 424.59052917012394,
                            "process_2": 419.13896041435083
                        },
                        "ram_power": {
                            "process_1": 0.969294548034668,
                            "process_0": 0.9075665473937988,
                            "process_3": 0.9706306457519531,
                            "process_2": 0.9764471054077148
                        },
                        "cpu_energy": {
                            "process_1": 0.001131426066031395,
                            "process_0": 0.0011503835199063136,
                            "process_3": 0.0011517758301561117,
                            "process_2": 0.0011432181465313533
                        },
                        "gpu_energy": {
                            "process_1": 0.0040609524154260335,
                            "process_0": 0.00413145163849199,
                            "process_3": 0.00413145163849199,
                            "process_2": 0.004104128283299979
                        },
                        "ram_energy": {
                            "process_1": 7.692201557260953e-06,
                            "process_0": 7.299607270649083e-06,
                            "process_3": 7.837543449575886e-06,
                            "process_2": 7.832905384365541e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005200070683014689,
                            "process_0": 0.005289134765668952,
                            "process_3": 0.00529106501209768,
                            "process_2": 0.005255179335215699
                        },
                        "total_energy_joules": {
                            "process_1": 18720.25445885288,
                            "process_0": 19040.885156408225,
                            "process_3": 19047.834043551647,
                            "process_2": 18918.645606776518
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 409.8224021946188,
                        "ram_power_avg": 0.9559847116470337,
                        "cpu_energy_total": 0.004576803562625174,
                        "gpu_energy_total": 0.016427983975709992,
                        "ram_energy_total": 3.066225766185146e-05,
                        "total_energy_kwh": 0.021035449795997023,
                        "total_energy_joules": 75727.61926558928
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21635435206986506,
                        "joules_per_token": 4.622047074315752,
                        "flops_per_joule": 223828124.50112355,
                        "joules_per_flop": 4.467713797043321e-09
                    },
                    "per-process_emissions": [
                        0.001980966926694446,
                        0.002014895888981587,
                        0.002015631216358611,
                        0.0020019605677504206
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0253": {
            "setup": {
                "experiment_id": "0253",
                "date_time": "April 11, 2025 at 05:37:04 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.730153925000195,
                        "average_latency_ms_per_batch": 4591.269240625024,
                        "throughput_queries_per_sec": 3.4848751317885833,
                        "throughput_tokens_per_sec": 446.06401686893867
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2628485120
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0253",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 444.311329798248,
                            "process_3": 439.5038861595072,
                            "process_2": 397.17588072602706,
                            "process_0": 438.20696454190033
                        },
                        "ram_power": {
                            "process_1": 0.9703230857849121,
                            "process_3": 0.9749579429626465,
                            "process_2": 0.9760322570800782,
                            "process_0": 0.9169235229492188
                        },
                        "cpu_energy": {
                            "process_1": 0.0011368708427814908,
                            "process_3": 0.001132580939531067,
                            "process_2": 0.0011427838725312541,
                            "process_0": 0.0011358113693437416
                        },
                        "gpu_energy": {
                            "process_1": 0.004111925233982128,
                            "process_3": 0.004103572727299887,
                            "process_2": 0.004126001356353937,
                            "process_0": 0.00410805550866411
                        },
                        "ram_energy": {
                            "process_1": 7.768469801717888e-06,
                            "process_3": 7.714190464887583e-06,
                            "process_2": 7.82277461219946e-06,
                            "process_0": 7.27953264790843e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005256564546565334,
                            "process_3": 0.00524386785729584,
                            "process_2": 0.00527660800349739,
                            "process_0": 0.005251146410655759
                        },
                        "total_energy_joules": {
                            "process_1": 18923.6323676352,
                            "process_3": 18877.924286265024,
                            "process_2": 18995.788812590607,
                            "process_0": 18904.127078360732
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 429.79951530642063,
                        "ram_power_avg": 0.9595592021942139,
                        "cpu_energy_total": 0.004548047024187554,
                        "gpu_energy_total": 0.01644955482630006,
                        "ram_energy_total": 3.058496752671336e-05,
                        "total_energy_kwh": 0.02102818681801432,
                        "total_energy_joules": 75701.47254485157
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21642907924007446,
                        "joules_per_token": 4.6204512051301005,
                        "flops_per_joule": 223905433.0562657,
                        "joules_per_flop": 4.466171215008917e-09
                    },
                    "per-process_emissions": [
                        0.002002488264014064,
                        0.0019976514602368504,
                        0.0020101238189323307,
                        0.0020004242251393115
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0253": {
            "setup": {
                "experiment_id": "0253",
                "date_time": "April 11, 2025 at 05:37:04 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.730153925000195,
                        "average_latency_ms_per_batch": 4591.269240625024,
                        "throughput_queries_per_sec": 3.4848751317885833,
                        "throughput_tokens_per_sec": 446.06401686893867
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2628485120
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0253",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 444.311329798248,
                            "process_3": 439.5038861595072,
                            "process_2": 397.17588072602706,
                            "process_0": 438.20696454190033
                        },
                        "ram_power": {
                            "process_1": 0.9703230857849121,
                            "process_3": 0.9749579429626465,
                            "process_2": 0.9760322570800782,
                            "process_0": 0.9169235229492188
                        },
                        "cpu_energy": {
                            "process_1": 0.0011368708427814908,
                            "process_3": 0.001132580939531067,
                            "process_2": 0.0011427838725312541,
                            "process_0": 0.0011358113693437416
                        },
                        "gpu_energy": {
                            "process_1": 0.004111925233982128,
                            "process_3": 0.004103572727299887,
                            "process_2": 0.004126001356353937,
                            "process_0": 0.00410805550866411
                        },
                        "ram_energy": {
                            "process_1": 7.768469801717888e-06,
                            "process_3": 7.714190464887583e-06,
                            "process_2": 7.82277461219946e-06,
                            "process_0": 7.27953264790843e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005256564546565334,
                            "process_3": 0.00524386785729584,
                            "process_2": 0.00527660800349739,
                            "process_0": 0.005251146410655759
                        },
                        "total_energy_joules": {
                            "process_1": 18923.6323676352,
                            "process_3": 18877.924286265024,
                            "process_2": 18995.788812590607,
                            "process_0": 18904.127078360732
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 429.79951530642063,
                        "ram_power_avg": 0.9595592021942139,
                        "cpu_energy_total": 0.004548047024187554,
                        "gpu_energy_total": 0.01644955482630006,
                        "ram_energy_total": 3.058496752671336e-05,
                        "total_energy_kwh": 0.02102818681801432,
                        "total_energy_joules": 75701.47254485157
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21642907924007446,
                        "joules_per_token": 4.6204512051301005,
                        "flops_per_joule": 223905433.0562657,
                        "joules_per_flop": 4.466171215008917e-09
                    },
                    "per-process_emissions": [
                        0.002002488264014064,
                        0.0019976514602368504,
                        0.0020101238189323307,
                        0.0020004242251393115
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0254": {
            "setup": {
                "experiment_id": "0254",
                "date_time": "April 11, 2025 at 05:38:09 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.862467121009104,
                        "average_latency_ms_per_batch": 4607.808390126138,
                        "throughput_queries_per_sec": 3.472366610184067,
                        "throughput_tokens_per_sec": 444.46292610356056
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2651119616
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0254",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 406.35414096001534,
                            "process_2": 434.6623708922049,
                            "process_3": 438.77482824963033,
                            "process_1": 448.40932698764345
                        },
                        "ram_power": {
                            "process_0": 0.925407886505127,
                            "process_2": 0.9666624069213867,
                            "process_3": 0.9591708183288574,
                            "process_1": 0.9756503105163574
                        },
                        "cpu_energy": {
                            "process_0": 0.0011373511341251968,
                            "process_2": 0.0011367779860313476,
                            "process_3": 0.0011323036034370942,
                            "process_1": 0.0011332629301565474
                        },
                        "gpu_energy": {
                            "process_0": 0.004128624969564176,
                            "process_2": 0.004123108854040047,
                            "process_3": 0.004115485792385909,
                            "process_1": 0.004115485792385909
                        },
                        "ram_energy": {
                            "process_0": 7.385575146542393e-06,
                            "process_2": 7.705743417883316e-06,
                            "process_3": 7.602255448346956e-06,
                            "process_1": 7.731757279863277e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005273361678835915,
                            "process_2": 0.0052675925834892775,
                            "process_3": 0.00525539165127135,
                            "process_1": 0.00525648047982232
                        },
                        "total_energy_joules": {
                            "process_0": 18984.102043809296,
                            "process_2": 18963.3333005614,
                            "process_3": 18919.40994457686,
                            "process_1": 18923.329727360353
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 432.0501667723735,
                        "ram_power_avg": 0.9567228555679321,
                        "cpu_energy_total": 0.004539695653750186,
                        "gpu_energy_total": 0.01648270540837604,
                        "ram_energy_total": 3.0425331292635944e-05,
                        "total_energy_kwh": 0.021052826393418864,
                        "total_energy_joules": 75790.17501630791
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21617577735471152,
                        "joules_per_token": 4.6258651743352,
                        "flops_per_joule": 223643381.07815218,
                        "joules_per_flop": 4.471404408121294e-09
                    },
                    "per-process_emissions": [
                        0.002008887131552542,
                        0.00200668939468024,
                        0.002002041449551821,
                        0.002002456238788313
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0254": {
            "setup": {
                "experiment_id": "0254",
                "date_time": "April 11, 2025 at 05:38:09 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.862467121009104,
                        "average_latency_ms_per_batch": 4607.808390126138,
                        "throughput_queries_per_sec": 3.472366610184067,
                        "throughput_tokens_per_sec": 444.46292610356056
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2651119616
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0254",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 406.35414096001534,
                            "process_2": 434.6623708922049,
                            "process_3": 438.77482824963033,
                            "process_1": 448.40932698764345
                        },
                        "ram_power": {
                            "process_0": 0.925407886505127,
                            "process_2": 0.9666624069213867,
                            "process_3": 0.9591708183288574,
                            "process_1": 0.9756503105163574
                        },
                        "cpu_energy": {
                            "process_0": 0.0011373511341251968,
                            "process_2": 0.0011367779860313476,
                            "process_3": 0.0011323036034370942,
                            "process_1": 0.0011332629301565474
                        },
                        "gpu_energy": {
                            "process_0": 0.004128624969564176,
                            "process_2": 0.004123108854040047,
                            "process_3": 0.004115485792385909,
                            "process_1": 0.004115485792385909
                        },
                        "ram_energy": {
                            "process_0": 7.385575146542393e-06,
                            "process_2": 7.705743417883316e-06,
                            "process_3": 7.602255448346956e-06,
                            "process_1": 7.731757279863277e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005273361678835915,
                            "process_2": 0.0052675925834892775,
                            "process_3": 0.00525539165127135,
                            "process_1": 0.00525648047982232
                        },
                        "total_energy_joules": {
                            "process_0": 18984.102043809296,
                            "process_2": 18963.3333005614,
                            "process_3": 18919.40994457686,
                            "process_1": 18923.329727360353
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 432.0501667723735,
                        "ram_power_avg": 0.9567228555679321,
                        "cpu_energy_total": 0.004539695653750186,
                        "gpu_energy_total": 0.01648270540837604,
                        "ram_energy_total": 3.0425331292635944e-05,
                        "total_energy_kwh": 0.021052826393418864,
                        "total_energy_joules": 75790.17501630791
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21617577735471152,
                        "joules_per_token": 4.6258651743352,
                        "flops_per_joule": 223643381.07815218,
                        "joules_per_flop": 4.471404408121294e-09
                    },
                    "per-process_emissions": [
                        0.002008887131552542,
                        0.00200668939468024,
                        0.002002041449551821,
                        0.002002456238788313
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0256": {
            "setup": {
                "experiment_id": "0256",
                "date_time": "April 11, 2025 at 05:39:42 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.388010919003136,
                        "average_latency_ms_per_batch": 4548.501364875392,
                        "throughput_queries_per_sec": 3.51764212352574,
                        "throughput_tokens_per_sec": 450.2581918112947
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2658959360
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0256",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 432.88845812493196,
                            "process_1": 414.9860296429882,
                            "process_2": 388.54626139517666,
                            "process_0": 448.78524803111344
                        },
                        "ram_power": {
                            "process_3": 0.9665050506591798,
                            "process_1": 0.972663402557373,
                            "process_2": 0.9545288085937501,
                            "process_0": 0.9274635314941407
                        },
                        "cpu_energy": {
                            "process_3": 0.001122060004281252,
                            "process_1": 0.0011233511945001736,
                            "process_2": 0.0011275299009066658,
                            "process_0": 0.001121611020937735
                        },
                        "gpu_energy": {
                            "process_3": 0.003989455413783871,
                            "process_1": 0.003989455413783871,
                            "process_2": 0.004003178480317782,
                            "process_0": 0.00398602346659388
                        },
                        "ram_energy": {
                            "process_3": 7.5661716742347594e-06,
                            "process_1": 7.598741798225074e-06,
                            "process_2": 7.494197416654501e-06,
                            "process_0": 7.284178451589726e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0051190815897393565,
                            "process_1": 0.005120405350082271,
                            "process_2": 0.005138202578641102,
                            "process_0": 0.005114918665983206
                        },
                        "total_energy_joules": {
                            "process_3": 18428.693723061682,
                            "process_1": 18433.459260296175,
                            "process_2": 18497.529283107968,
                            "process_0": 18413.70719753954
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 421.3014992985526,
                        "ram_power_avg": 0.9552901983261108,
                        "cpu_energy_total": 0.004494552120625826,
                        "gpu_energy_total": 0.015968112774479404,
                        "ram_energy_total": 2.994328934070406e-05,
                        "total_energy_kwh": 0.020492608184445933,
                        "total_energy_joules": 73773.38946400536
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22208549883686565,
                        "joules_per_token": 4.502770353027671,
                        "flops_per_joule": 229757248.73563018,
                        "joules_per_flop": 4.3524198061348145e-09
                    },
                    "per-process_emissions": [
                        0.0019501141316112078,
                        0.0019506184181138412,
                        0.0019573982723333277,
                        0.0019485282658063023
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0256": {
            "setup": {
                "experiment_id": "0256",
                "date_time": "April 11, 2025 at 05:39:42 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.388010919003136,
                        "average_latency_ms_per_batch": 4548.501364875392,
                        "throughput_queries_per_sec": 3.51764212352574,
                        "throughput_tokens_per_sec": 450.2581918112947
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2658959360
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0256",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 432.88845812493196,
                            "process_1": 414.9860296429882,
                            "process_2": 388.54626139517666,
                            "process_0": 448.78524803111344
                        },
                        "ram_power": {
                            "process_3": 0.9665050506591798,
                            "process_1": 0.972663402557373,
                            "process_2": 0.9545288085937501,
                            "process_0": 0.9274635314941407
                        },
                        "cpu_energy": {
                            "process_3": 0.001122060004281252,
                            "process_1": 0.0011233511945001736,
                            "process_2": 0.0011275299009066658,
                            "process_0": 0.001121611020937735
                        },
                        "gpu_energy": {
                            "process_3": 0.003989455413783871,
                            "process_1": 0.003989455413783871,
                            "process_2": 0.004003178480317782,
                            "process_0": 0.00398602346659388
                        },
                        "ram_energy": {
                            "process_3": 7.5661716742347594e-06,
                            "process_1": 7.598741798225074e-06,
                            "process_2": 7.494197416654501e-06,
                            "process_0": 7.284178451589726e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0051190815897393565,
                            "process_1": 0.005120405350082271,
                            "process_2": 0.005138202578641102,
                            "process_0": 0.005114918665983206
                        },
                        "total_energy_joules": {
                            "process_3": 18428.693723061682,
                            "process_1": 18433.459260296175,
                            "process_2": 18497.529283107968,
                            "process_0": 18413.70719753954
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 421.3014992985526,
                        "ram_power_avg": 0.9552901983261108,
                        "cpu_energy_total": 0.004494552120625826,
                        "gpu_energy_total": 0.015968112774479404,
                        "ram_energy_total": 2.994328934070406e-05,
                        "total_energy_kwh": 0.020492608184445933,
                        "total_energy_joules": 73773.38946400536
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22208549883686565,
                        "joules_per_token": 4.502770353027671,
                        "flops_per_joule": 229757248.73563018,
                        "joules_per_flop": 4.3524198061348145e-09
                    },
                    "per-process_emissions": [
                        0.0019501141316112078,
                        0.0019506184181138412,
                        0.0019573982723333277,
                        0.0019485282658063023
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0257": {
            "setup": {
                "experiment_id": "0257",
                "date_time": "April 11, 2025 at 05:40:48 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.45603763800682,
                        "average_latency_ms_per_batch": 4557.004704750852,
                        "throughput_queries_per_sec": 3.511078227178345,
                        "throughput_tokens_per_sec": 449.41801307882815
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2661109760
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0257",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 272.43682066762693,
                            "process_2": 436.51867762409427,
                            "process_1": 431.22506114679044,
                            "process_3": 433.034685101705
                        },
                        "ram_power": {
                            "process_0": 0.9285550117492677,
                            "process_2": 0.9541797637939453,
                            "process_1": 0.9715232849121094,
                            "process_3": 0.9618587493896484
                        },
                        "cpu_energy": {
                            "process_0": 0.0011265810093436812,
                            "process_2": 0.001119291148656316,
                            "process_1": 0.0011082167275003486,
                            "process_3": 0.0011096118912499835
                        },
                        "gpu_energy": {
                            "process_0": 0.00403653072922211,
                            "process_2": 0.004016479046513921,
                            "process_1": 0.003984266242965939,
                            "process_3": 0.003994692362418006
                        },
                        "ram_energy": {
                            "process_0": 7.322959319544876e-06,
                            "process_2": 7.709200768172202e-06,
                            "process_1": 7.538792529165497e-06,
                            "process_3": 7.51099173164191e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005170434697885338,
                            "process_2": 0.00514347939593841,
                            "process_1": 0.005100021762995452,
                            "process_3": 0.0051118152453996325
                        },
                        "total_energy_joules": {
                            "process_0": 18613.564912387217,
                            "process_2": 18516.525825378274,
                            "process_1": 18360.078346783626,
                            "process_3": 18402.53488343868
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 393.30381113505416,
                        "ram_power_avg": 0.9540292024612427,
                        "cpu_energy_total": 0.004463700776750329,
                        "gpu_energy_total": 0.016031968381119976,
                        "ram_energy_total": 3.0081944348524487e-05,
                        "total_energy_kwh": 0.020525751102218832,
                        "total_energy_joules": 73892.7039679878
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2217268975174865,
                        "joules_per_token": 4.510052732421131,
                        "flops_per_joule": 229386259.84637344,
                        "joules_per_flop": 4.3594590219559305e-09
                    },
                    "per-process_emissions": [
                        0.0019696770981594194,
                        0.0019594084758827375,
                        0.0019428532906131176,
                        0.00194734601773499
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0257": {
            "setup": {
                "experiment_id": "0257",
                "date_time": "April 11, 2025 at 05:40:48 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.45603763800682,
                        "average_latency_ms_per_batch": 4557.004704750852,
                        "throughput_queries_per_sec": 3.511078227178345,
                        "throughput_tokens_per_sec": 449.41801307882815
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2661109760
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0257",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 272.43682066762693,
                            "process_2": 436.51867762409427,
                            "process_1": 431.22506114679044,
                            "process_3": 433.034685101705
                        },
                        "ram_power": {
                            "process_0": 0.9285550117492677,
                            "process_2": 0.9541797637939453,
                            "process_1": 0.9715232849121094,
                            "process_3": 0.9618587493896484
                        },
                        "cpu_energy": {
                            "process_0": 0.0011265810093436812,
                            "process_2": 0.001119291148656316,
                            "process_1": 0.0011082167275003486,
                            "process_3": 0.0011096118912499835
                        },
                        "gpu_energy": {
                            "process_0": 0.00403653072922211,
                            "process_2": 0.004016479046513921,
                            "process_1": 0.003984266242965939,
                            "process_3": 0.003994692362418006
                        },
                        "ram_energy": {
                            "process_0": 7.322959319544876e-06,
                            "process_2": 7.709200768172202e-06,
                            "process_1": 7.538792529165497e-06,
                            "process_3": 7.51099173164191e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005170434697885338,
                            "process_2": 0.00514347939593841,
                            "process_1": 0.005100021762995452,
                            "process_3": 0.0051118152453996325
                        },
                        "total_energy_joules": {
                            "process_0": 18613.564912387217,
                            "process_2": 18516.525825378274,
                            "process_1": 18360.078346783626,
                            "process_3": 18402.53488343868
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 393.30381113505416,
                        "ram_power_avg": 0.9540292024612427,
                        "cpu_energy_total": 0.004463700776750329,
                        "gpu_energy_total": 0.016031968381119976,
                        "ram_energy_total": 3.0081944348524487e-05,
                        "total_energy_kwh": 0.020525751102218832,
                        "total_energy_joules": 73892.7039679878
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2217268975174865,
                        "joules_per_token": 4.510052732421131,
                        "flops_per_joule": 229386259.84637344,
                        "joules_per_flop": 4.3594590219559305e-09
                    },
                    "per-process_emissions": [
                        0.0019696770981594194,
                        0.0019594084758827375,
                        0.0019428532906131176,
                        0.00194734601773499
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0258": {
            "setup": {
                "experiment_id": "0258",
                "date_time": "April 11, 2025 at 05:41:54 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.44710516100167,
                        "average_latency_ms_per_batch": 4555.888145125209,
                        "throughput_queries_per_sec": 3.5119387242024294,
                        "throughput_tokens_per_sec": 449.52815669791096
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2658758656
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0258",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 0.0,
                            "process_3": 431.4042656899445,
                            "process_0": 293.4915652476367,
                            "process_2": 446.04760354001536
                        },
                        "ram_power": {
                            "process_1": 0.9544258117675781,
                            "process_3": 0.9669828414916992,
                            "process_0": 0.9275164604187012,
                            "process_2": 0.97450590133667
                        },
                        "cpu_energy": {
                            "process_1": 0.001147694526937357,
                            "process_3": 0.0011038987566878405,
                            "process_0": 0.0011277624438752127,
                            "process_2": 0.0011205754321561017
                        },
                        "gpu_energy": {
                            "process_1": 0.004013585710866019,
                            "process_3": 0.0039763215143879815,
                            "process_0": 0.004041846844585972,
                            "process_2": 0.004029755446024075
                        },
                        "ram_energy": {
                            "process_1": 7.676295547391994e-06,
                            "process_3": 7.491559123792663e-06,
                            "process_0": 7.31356921322041e-06,
                            "process_2": 7.663620739322895e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0051689565333507695,
                            "process_3": 0.005087711830199615,
                            "process_0": 0.005176922857674408,
                            "process_2": 0.005157994498919498
                        },
                        "total_energy_joules": {
                            "process_1": 18608.24352006277,
                            "process_3": 18315.76258871861,
                            "process_0": 18636.92228762787,
                            "process_2": 18568.78019611019
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 292.73585861939915,
                        "ram_power_avg": 0.9558577537536621,
                        "cpu_energy_total": 0.004499931159656512,
                        "gpu_energy_total": 0.016061509515864048,
                        "ram_energy_total": 3.014504462372796e-05,
                        "total_energy_kwh": 0.02059158572014429,
                        "total_energy_joules": 74129.70859251944
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22101800089435855,
                        "joules_per_token": 4.524518346711392,
                        "flops_per_joule": 228652875.0075574,
                        "joules_per_flop": 4.373441619603289e-09
                    },
                    "per-process_emissions": [
                        0.0019691139913799757,
                        0.0019381638217145433,
                        0.0019721487626310655,
                        0.0019649380043633828
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0258": {
            "setup": {
                "experiment_id": "0258",
                "date_time": "April 11, 2025 at 05:41:54 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.44710516100167,
                        "average_latency_ms_per_batch": 4555.888145125209,
                        "throughput_queries_per_sec": 3.5119387242024294,
                        "throughput_tokens_per_sec": 449.52815669791096
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2658758656
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0258",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 0.0,
                            "process_3": 431.4042656899445,
                            "process_0": 293.4915652476367,
                            "process_2": 446.04760354001536
                        },
                        "ram_power": {
                            "process_1": 0.9544258117675781,
                            "process_3": 0.9669828414916992,
                            "process_0": 0.9275164604187012,
                            "process_2": 0.97450590133667
                        },
                        "cpu_energy": {
                            "process_1": 0.001147694526937357,
                            "process_3": 0.0011038987566878405,
                            "process_0": 0.0011277624438752127,
                            "process_2": 0.0011205754321561017
                        },
                        "gpu_energy": {
                            "process_1": 0.004013585710866019,
                            "process_3": 0.0039763215143879815,
                            "process_0": 0.004041846844585972,
                            "process_2": 0.004029755446024075
                        },
                        "ram_energy": {
                            "process_1": 7.676295547391994e-06,
                            "process_3": 7.491559123792663e-06,
                            "process_0": 7.31356921322041e-06,
                            "process_2": 7.663620739322895e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0051689565333507695,
                            "process_3": 0.005087711830199615,
                            "process_0": 0.005176922857674408,
                            "process_2": 0.005157994498919498
                        },
                        "total_energy_joules": {
                            "process_1": 18608.24352006277,
                            "process_3": 18315.76258871861,
                            "process_0": 18636.92228762787,
                            "process_2": 18568.78019611019
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 292.73585861939915,
                        "ram_power_avg": 0.9558577537536621,
                        "cpu_energy_total": 0.004499931159656512,
                        "gpu_energy_total": 0.016061509515864048,
                        "ram_energy_total": 3.014504462372796e-05,
                        "total_energy_kwh": 0.02059158572014429,
                        "total_energy_joules": 74129.70859251944
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22101800089435855,
                        "joules_per_token": 4.524518346711392,
                        "flops_per_joule": 228652875.0075574,
                        "joules_per_flop": 4.373441619603289e-09
                    },
                    "per-process_emissions": [
                        0.0019691139913799757,
                        0.0019381638217145433,
                        0.0019721487626310655,
                        0.0019649380043633828
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0259": {
            "setup": {
                "experiment_id": "0259",
                "date_time": "April 11, 2025 at 05:43:00 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.3062878219971,
                        "average_latency_ms_per_batch": 4538.285977749638,
                        "throughput_queries_per_sec": 3.5255601075923795,
                        "throughput_tokens_per_sec": 451.2716937718246
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            83.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2679640064
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0259",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 365.7810175648279,
                            "process_1": 321.60744459633077,
                            "process_2": 451.46104842746223,
                            "process_3": 844.8100355942594
                        },
                        "ram_power": {
                            "process_0": 0.9356231689453125,
                            "process_1": 0.9556546211242676,
                            "process_2": 0.9607343673706055,
                            "process_3": 0.9548993110656738
                        },
                        "cpu_energy": {
                            "process_0": 0.0011232655508121067,
                            "process_1": 0.0011235447473438853,
                            "process_2": 0.0011064389241249727,
                            "process_3": 0.0011169602594378605
                        },
                        "gpu_energy": {
                            "process_0": 0.00404167962223001,
                            "process_1": 0.00404167962223001,
                            "process_2": 0.004001790145873929,
                            "process_3": 0.004026732388049958
                        },
                        "ram_energy": {
                            "process_0": 7.32505989891451e-06,
                            "process_1": 7.514347418036633e-06,
                            "process_2": 7.45483348454331e-06,
                            "process_3": 7.727362517323305e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005172270232941031,
                            "process_1": 0.005172738716991933,
                            "process_2": 0.005115683903483446,
                            "process_3": 0.005151420010005141
                        },
                        "total_energy_joules": {
                            "process_0": 18620.17283858771,
                            "process_1": 18621.85938117096,
                            "process_2": 18416.462052540406,
                            "process_3": 18545.11203601851
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 495.91488654572004,
                        "ram_power_avg": 0.9517278671264648,
                        "cpu_energy_total": 0.004470209481718826,
                        "gpu_energy_total": 0.016111881778383907,
                        "ram_energy_total": 3.002160331881776e-05,
                        "total_energy_kwh": 0.02061211286342155,
                        "total_energy_joules": 74203.6063083176
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2207978939988998,
                        "joules_per_token": 4.52902870534165,
                        "flops_per_joule": 228425164.7113282,
                        "joules_per_flop": 4.377801374308946e-09
                    },
                    "per-process_emissions": [
                        0.001970376345238886,
                        0.001970554814238077,
                        0.0019488197830320189,
                        0.0019624334528114584
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0259": {
            "setup": {
                "experiment_id": "0259",
                "date_time": "April 11, 2025 at 05:43:00 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.3062878219971,
                        "average_latency_ms_per_batch": 4538.285977749638,
                        "throughput_queries_per_sec": 3.5255601075923795,
                        "throughput_tokens_per_sec": 451.2716937718246
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            83.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2679640064
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0259",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 365.7810175648279,
                            "process_1": 321.60744459633077,
                            "process_2": 451.46104842746223,
                            "process_3": 844.8100355942594
                        },
                        "ram_power": {
                            "process_0": 0.9356231689453125,
                            "process_1": 0.9556546211242676,
                            "process_2": 0.9607343673706055,
                            "process_3": 0.9548993110656738
                        },
                        "cpu_energy": {
                            "process_0": 0.0011232655508121067,
                            "process_1": 0.0011235447473438853,
                            "process_2": 0.0011064389241249727,
                            "process_3": 0.0011169602594378605
                        },
                        "gpu_energy": {
                            "process_0": 0.00404167962223001,
                            "process_1": 0.00404167962223001,
                            "process_2": 0.004001790145873929,
                            "process_3": 0.004026732388049958
                        },
                        "ram_energy": {
                            "process_0": 7.32505989891451e-06,
                            "process_1": 7.514347418036633e-06,
                            "process_2": 7.45483348454331e-06,
                            "process_3": 7.727362517323305e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005172270232941031,
                            "process_1": 0.005172738716991933,
                            "process_2": 0.005115683903483446,
                            "process_3": 0.005151420010005141
                        },
                        "total_energy_joules": {
                            "process_0": 18620.17283858771,
                            "process_1": 18621.85938117096,
                            "process_2": 18416.462052540406,
                            "process_3": 18545.11203601851
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 495.91488654572004,
                        "ram_power_avg": 0.9517278671264648,
                        "cpu_energy_total": 0.004470209481718826,
                        "gpu_energy_total": 0.016111881778383907,
                        "ram_energy_total": 3.002160331881776e-05,
                        "total_energy_kwh": 0.02061211286342155,
                        "total_energy_joules": 74203.6063083176
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2207978939988998,
                        "joules_per_token": 4.52902870534165,
                        "flops_per_joule": 228425164.7113282,
                        "joules_per_flop": 4.377801374308946e-09
                    },
                    "per-process_emissions": [
                        0.001970376345238886,
                        0.001970554814238077,
                        0.0019488197830320189,
                        0.0019624334528114584
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0260": {
            "setup": {
                "experiment_id": "0260",
                "date_time": "April 11, 2025 at 05:44:05 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.211281266994774,
                        "average_latency_ms_per_batch": 4526.410158374347,
                        "throughput_queries_per_sec": 3.534810023876929,
                        "throughput_tokens_per_sec": 452.4556830562469
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            5.0,
                            77.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2613747712
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0260",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 336.92904943023507,
                            "process_3": 378.3926523746951,
                            "process_2": 439.92422098656385,
                            "process_0": 493.06297691759613
                        },
                        "ram_power": {
                            "process_1": 0.9464707374572754,
                            "process_3": 0.9625024795532228,
                            "process_2": 0.971132755279541,
                            "process_0": 0.9122700691223145
                        },
                        "cpu_energy": {
                            "process_1": 0.0011278072255623782,
                            "process_3": 0.001124266055000362,
                            "process_2": 0.0011119486702499443,
                            "process_0": 0.0011194372745935654
                        },
                        "gpu_energy": {
                            "process_1": 0.004063330195106074,
                            "process_3": 0.004061644360424055,
                            "process_2": 0.004012879321411977,
                            "process_0": 0.0040436235126738995
                        },
                        "ram_energy": {
                            "process_1": 7.476471357318308e-06,
                            "process_3": 7.5771560389050946e-06,
                            "process_2": 7.600263373736418e-06,
                            "process_0": 7.132174307157151e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005198613892025773,
                            "process_3": 0.005193487571463324,
                            "process_2": 0.005132428255035661,
                            "process_0": 0.005170192961574622
                        },
                        "total_energy_joules": {
                            "process_1": 18715.010011292783,
                            "process_3": 18696.555257267966,
                            "process_2": 18476.741718128378,
                            "process_0": 18612.694661668636
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 412.07722492727254,
                        "ram_power_avg": 0.9480940103530884,
                        "cpu_energy_total": 0.00448345922540625,
                        "gpu_energy_total": 0.016181477389616006,
                        "ram_energy_total": 2.9786065077116973e-05,
                        "total_energy_kwh": 0.02069472268009938,
                        "total_energy_joules": 74501.00164835776
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2199165063220483,
                        "joules_per_token": 4.547180276389024,
                        "flops_per_joule": 227513330.26574993,
                        "joules_per_flop": 4.395346852124826e-09
                    },
                    "per-process_emissions": [
                        0.0019804119621672184,
                        0.0019784590903489533,
                        0.001955198543755835,
                        0.0019695850087118522
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0260": {
            "setup": {
                "experiment_id": "0260",
                "date_time": "April 11, 2025 at 05:44:05 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.211281266994774,
                        "average_latency_ms_per_batch": 4526.410158374347,
                        "throughput_queries_per_sec": 3.534810023876929,
                        "throughput_tokens_per_sec": 452.4556830562469
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            5.0,
                            77.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2613747712
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0260",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 336.92904943023507,
                            "process_3": 378.3926523746951,
                            "process_2": 439.92422098656385,
                            "process_0": 493.06297691759613
                        },
                        "ram_power": {
                            "process_1": 0.9464707374572754,
                            "process_3": 0.9625024795532228,
                            "process_2": 0.971132755279541,
                            "process_0": 0.9122700691223145
                        },
                        "cpu_energy": {
                            "process_1": 0.0011278072255623782,
                            "process_3": 0.001124266055000362,
                            "process_2": 0.0011119486702499443,
                            "process_0": 0.0011194372745935654
                        },
                        "gpu_energy": {
                            "process_1": 0.004063330195106074,
                            "process_3": 0.004061644360424055,
                            "process_2": 0.004012879321411977,
                            "process_0": 0.0040436235126738995
                        },
                        "ram_energy": {
                            "process_1": 7.476471357318308e-06,
                            "process_3": 7.5771560389050946e-06,
                            "process_2": 7.600263373736418e-06,
                            "process_0": 7.132174307157151e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005198613892025773,
                            "process_3": 0.005193487571463324,
                            "process_2": 0.005132428255035661,
                            "process_0": 0.005170192961574622
                        },
                        "total_energy_joules": {
                            "process_1": 18715.010011292783,
                            "process_3": 18696.555257267966,
                            "process_2": 18476.741718128378,
                            "process_0": 18612.694661668636
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 412.07722492727254,
                        "ram_power_avg": 0.9480940103530884,
                        "cpu_energy_total": 0.00448345922540625,
                        "gpu_energy_total": 0.016181477389616006,
                        "ram_energy_total": 2.9786065077116973e-05,
                        "total_energy_kwh": 0.02069472268009938,
                        "total_energy_joules": 74501.00164835776
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2199165063220483,
                        "joules_per_token": 4.547180276389024,
                        "flops_per_joule": 227513330.26574993,
                        "joules_per_flop": 4.395346852124826e-09
                    },
                    "per-process_emissions": [
                        0.0019804119621672184,
                        0.0019784590903489533,
                        0.001955198543755835,
                        0.0019695850087118522
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0261": {
            "setup": {
                "experiment_id": "0261",
                "date_time": "April 11, 2025 at 05:45:11 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.14762356799474,
                        "average_latency_ms_per_batch": 4643.452945999343,
                        "throughput_queries_per_sec": 3.44571166889612,
                        "throughput_tokens_per_sec": 441.05109361870336
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2676936704
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0261",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 418.6613201133391,
                            "process_0": 374.04448598408834,
                            "process_2": 457.5560297595716,
                            "process_3": 282.5264947418648
                        },
                        "ram_power": {
                            "process_1": 0.9604525566101075,
                            "process_0": 0.9337649345397949,
                            "process_2": 0.9797544479370117,
                            "process_3": 0.9766945838928223
                        },
                        "cpu_energy": {
                            "process_1": 0.001135150128562941,
                            "process_0": 0.0011466312582190314,
                            "process_2": 0.0011230823941568813,
                            "process_3": 0.0011629295224370253
                        },
                        "gpu_energy": {
                            "process_1": 0.00410700189671015,
                            "process_0": 0.004141117201779954,
                            "process_2": 0.004072401869030129,
                            "process_3": 0.004176903063742243
                        },
                        "ram_energy": {
                            "process_1": 7.665914197602143e-06,
                            "process_0": 7.731703537517636e-06,
                            "process_2": 7.714164412003613e-06,
                            "process_3": 7.931884235669878e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.00524981793947069,
                            "process_0": 0.005295480163536505,
                            "process_2": 0.005203198427599015,
                            "process_3": 0.005347764470414938
                        },
                        "total_energy_joules": {
                            "process_1": 18899.344582094483,
                            "process_0": 19063.728588731417,
                            "process_2": 18731.514339356454,
                            "process_3": 19251.95209349378
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 383.197082649716,
                        "ram_power_avg": 0.9626666307449341,
                        "cpu_energy_total": 0.004567793303375879,
                        "gpu_energy_total": 0.016497424031262475,
                        "ram_energy_total": 3.104366638279327e-05,
                        "total_energy_kwh": 0.021096261001021147,
                        "total_energy_joules": 75946.53960367612
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21573069800808867,
                        "joules_per_token": 4.635408911357185,
                        "flops_per_joule": 223182926.85360944,
                        "joules_per_flop": 4.4806294733105724e-09
                    },
                    "per-process_emissions": [
                        0.0019999181440413594,
                        0.0020173131682992315,
                        0.001982158440993845,
                        0.0020372308750045707
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0261": {
            "setup": {
                "experiment_id": "0261",
                "date_time": "April 11, 2025 at 05:45:11 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.14762356799474,
                        "average_latency_ms_per_batch": 4643.452945999343,
                        "throughput_queries_per_sec": 3.44571166889612,
                        "throughput_tokens_per_sec": 441.05109361870336
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2676936704
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0261",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 418.6613201133391,
                            "process_0": 374.04448598408834,
                            "process_2": 457.5560297595716,
                            "process_3": 282.5264947418648
                        },
                        "ram_power": {
                            "process_1": 0.9604525566101075,
                            "process_0": 0.9337649345397949,
                            "process_2": 0.9797544479370117,
                            "process_3": 0.9766945838928223
                        },
                        "cpu_energy": {
                            "process_1": 0.001135150128562941,
                            "process_0": 0.0011466312582190314,
                            "process_2": 0.0011230823941568813,
                            "process_3": 0.0011629295224370253
                        },
                        "gpu_energy": {
                            "process_1": 0.00410700189671015,
                            "process_0": 0.004141117201779954,
                            "process_2": 0.004072401869030129,
                            "process_3": 0.004176903063742243
                        },
                        "ram_energy": {
                            "process_1": 7.665914197602143e-06,
                            "process_0": 7.731703537517636e-06,
                            "process_2": 7.714164412003613e-06,
                            "process_3": 7.931884235669878e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.00524981793947069,
                            "process_0": 0.005295480163536505,
                            "process_2": 0.005203198427599015,
                            "process_3": 0.005347764470414938
                        },
                        "total_energy_joules": {
                            "process_1": 18899.344582094483,
                            "process_0": 19063.728588731417,
                            "process_2": 18731.514339356454,
                            "process_3": 19251.95209349378
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 383.197082649716,
                        "ram_power_avg": 0.9626666307449341,
                        "cpu_energy_total": 0.004567793303375879,
                        "gpu_energy_total": 0.016497424031262475,
                        "ram_energy_total": 3.104366638279327e-05,
                        "total_energy_kwh": 0.021096261001021147,
                        "total_energy_joules": 75946.53960367612
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21573069800808867,
                        "joules_per_token": 4.635408911357185,
                        "flops_per_joule": 223182926.85360944,
                        "joules_per_flop": 4.4806294733105724e-09
                    },
                    "per-process_emissions": [
                        0.0019999181440413594,
                        0.0020173131682992315,
                        0.001982158440993845,
                        0.0020372308750045707
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0262": {
            "setup": {
                "experiment_id": "0262",
                "date_time": "April 11, 2025 at 05:46:17 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.53829933299858,
                        "average_latency_ms_per_batch": 4692.287416624822,
                        "throughput_queries_per_sec": 3.4098507996999157,
                        "throughput_tokens_per_sec": 436.4609023615892
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            18.0,
                            84.0,
                            99.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2654609408
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0262",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 357.3890023960502,
                            "process_3": 425.0382826725326,
                            "process_2": 422.88119972984384,
                            "process_0": 355.8705345397335
                        },
                        "ram_power": {
                            "process_1": 0.9713659286499023,
                            "process_3": 0.9803366661071777,
                            "process_2": 0.9780349731445314,
                            "process_0": 0.9259471893310547
                        },
                        "cpu_energy": {
                            "process_1": 0.0011638386040314118,
                            "process_3": 0.0011518144565627608,
                            "process_2": 0.0011408890596870834,
                            "process_0": 0.0011570292578126102
                        },
                        "gpu_energy": {
                            "process_1": 0.004157859992952095,
                            "process_3": 0.004129672748180158,
                            "process_2": 0.004083856600416047,
                            "process_0": 0.004151173876491998
                        },
                        "ram_energy": {
                            "process_1": 7.896758875382802e-06,
                            "process_3": 7.875873391649334e-06,
                            "process_2": 7.782911699432026e-06,
                            "process_0": 7.474588910924706e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005329595355858891,
                            "process_3": 0.005289363078134566,
                            "process_2": 0.005232528571802561,
                            "process_0": 0.005315677723215532
                        },
                        "total_energy_joules": {
                            "process_1": 19186.54328109201,
                            "process_3": 19041.707081284436,
                            "process_2": 18837.10285848922,
                            "process_0": 19136.439803575915
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 390.2947548345401,
                        "ram_power_avg": 0.9639211893081665,
                        "cpu_energy_total": 0.0046135713780938664,
                        "gpu_energy_total": 0.016522563218040298,
                        "ram_energy_total": 3.1030132877388874e-05,
                        "total_energy_kwh": 0.021167164729011548,
                        "total_energy_joules": 76201.79302444158
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21500806411136367,
                        "joules_per_token": 4.650988343776952,
                        "flops_per_joule": 222435330.19905883,
                        "joules_per_flop": 4.49568869794692e-09
                    },
                    "per-process_emissions": [
                        0.002030309350814445,
                        0.002014982864615363,
                        0.001993331759428186,
                        0.002025007428658957
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0262": {
            "setup": {
                "experiment_id": "0262",
                "date_time": "April 11, 2025 at 05:46:17 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.53829933299858,
                        "average_latency_ms_per_batch": 4692.287416624822,
                        "throughput_queries_per_sec": 3.4098507996999157,
                        "throughput_tokens_per_sec": 436.4609023615892
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            18.0,
                            84.0,
                            99.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2654609408
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0262",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 357.3890023960502,
                            "process_3": 425.0382826725326,
                            "process_2": 422.88119972984384,
                            "process_0": 355.8705345397335
                        },
                        "ram_power": {
                            "process_1": 0.9713659286499023,
                            "process_3": 0.9803366661071777,
                            "process_2": 0.9780349731445314,
                            "process_0": 0.9259471893310547
                        },
                        "cpu_energy": {
                            "process_1": 0.0011638386040314118,
                            "process_3": 0.0011518144565627608,
                            "process_2": 0.0011408890596870834,
                            "process_0": 0.0011570292578126102
                        },
                        "gpu_energy": {
                            "process_1": 0.004157859992952095,
                            "process_3": 0.004129672748180158,
                            "process_2": 0.004083856600416047,
                            "process_0": 0.004151173876491998
                        },
                        "ram_energy": {
                            "process_1": 7.896758875382802e-06,
                            "process_3": 7.875873391649334e-06,
                            "process_2": 7.782911699432026e-06,
                            "process_0": 7.474588910924706e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005329595355858891,
                            "process_3": 0.005289363078134566,
                            "process_2": 0.005232528571802561,
                            "process_0": 0.005315677723215532
                        },
                        "total_energy_joules": {
                            "process_1": 19186.54328109201,
                            "process_3": 19041.707081284436,
                            "process_2": 18837.10285848922,
                            "process_0": 19136.439803575915
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 390.2947548345401,
                        "ram_power_avg": 0.9639211893081665,
                        "cpu_energy_total": 0.0046135713780938664,
                        "gpu_energy_total": 0.016522563218040298,
                        "ram_energy_total": 3.1030132877388874e-05,
                        "total_energy_kwh": 0.021167164729011548,
                        "total_energy_joules": 76201.79302444158
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21500806411136367,
                        "joules_per_token": 4.650988343776952,
                        "flops_per_joule": 222435330.19905883,
                        "joules_per_flop": 4.49568869794692e-09
                    },
                    "per-process_emissions": [
                        0.002030309350814445,
                        0.002014982864615363,
                        0.001993331759428186,
                        0.002025007428658957
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0263": {
            "setup": {
                "experiment_id": "0263",
                "date_time": "April 11, 2025 at 05:47:23 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.34580060400549,
                        "average_latency_ms_per_batch": 4668.225075500686,
                        "throughput_queries_per_sec": 3.4274268573658984,
                        "throughput_tokens_per_sec": 438.710637742835
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2630242304
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0263",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 710.028922428598,
                            "process_2": 709.2663754981585,
                            "process_3": 492.31583868198464,
                            "process_0": 373.7051118513416
                        },
                        "ram_power": {
                            "process_1": 0.9614195823669434,
                            "process_2": 0.9764213562011719,
                            "process_3": 0.9598674774169922,
                            "process_0": 0.9173727035522462
                        },
                        "cpu_energy": {
                            "process_1": 0.0011456671029998235,
                            "process_2": 0.0011462003393127136,
                            "process_3": 0.001146292947062193,
                            "process_0": 0.0011535004463753466
                        },
                        "gpu_energy": {
                            "process_1": 0.004170963614545942,
                            "process_2": 0.004170963614545942,
                            "process_3": 0.004166568333251797,
                            "process_0": 0.004185934182077744
                        },
                        "ram_energy": {
                            "process_1": 7.996424906927952e-06,
                            "process_2": 8.074266623229538e-06,
                            "process_3": 7.944565887333167e-06,
                            "process_0": 7.3827989258843635e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005324627142452695,
                            "process_2": 0.005325238220481883,
                            "process_3": 0.005320805846201321,
                            "process_0": 0.005346817427378974
                        },
                        "total_energy_joules": {
                            "process_1": 19168.6577128297,
                            "process_2": 19170.857593734778,
                            "process_3": 19154.901046324758,
                            "process_0": 19248.542738564305
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 571.3290621150206,
                        "ram_power_avg": 0.9537702798843384,
                        "cpu_energy_total": 0.004591660835750076,
                        "gpu_energy_total": 0.016694429744421424,
                        "ram_energy_total": 3.139805634337502e-05,
                        "total_energy_kwh": 0.02131748863651487,
                        "total_energy_joules": 76742.95909145355
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2134918980707456,
                        "joules_per_token": 4.6840184992342255,
                        "flops_per_joule": 220866789.52466437,
                        "joules_per_flop": 4.52761595417825e-09
                    },
                    "per-process_emissions": [
                        0.002028416709917354,
                        0.0020286495000925732,
                        0.0020269609871103934,
                        0.00203687009896002
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0263": {
            "setup": {
                "experiment_id": "0263",
                "date_time": "April 11, 2025 at 05:47:23 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.34580060400549,
                        "average_latency_ms_per_batch": 4668.225075500686,
                        "throughput_queries_per_sec": 3.4274268573658984,
                        "throughput_tokens_per_sec": 438.710637742835
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2630242304
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0263",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 710.028922428598,
                            "process_2": 709.2663754981585,
                            "process_3": 492.31583868198464,
                            "process_0": 373.7051118513416
                        },
                        "ram_power": {
                            "process_1": 0.9614195823669434,
                            "process_2": 0.9764213562011719,
                            "process_3": 0.9598674774169922,
                            "process_0": 0.9173727035522462
                        },
                        "cpu_energy": {
                            "process_1": 0.0011456671029998235,
                            "process_2": 0.0011462003393127136,
                            "process_3": 0.001146292947062193,
                            "process_0": 0.0011535004463753466
                        },
                        "gpu_energy": {
                            "process_1": 0.004170963614545942,
                            "process_2": 0.004170963614545942,
                            "process_3": 0.004166568333251797,
                            "process_0": 0.004185934182077744
                        },
                        "ram_energy": {
                            "process_1": 7.996424906927952e-06,
                            "process_2": 8.074266623229538e-06,
                            "process_3": 7.944565887333167e-06,
                            "process_0": 7.3827989258843635e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005324627142452695,
                            "process_2": 0.005325238220481883,
                            "process_3": 0.005320805846201321,
                            "process_0": 0.005346817427378974
                        },
                        "total_energy_joules": {
                            "process_1": 19168.6577128297,
                            "process_2": 19170.857593734778,
                            "process_3": 19154.901046324758,
                            "process_0": 19248.542738564305
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 571.3290621150206,
                        "ram_power_avg": 0.9537702798843384,
                        "cpu_energy_total": 0.004591660835750076,
                        "gpu_energy_total": 0.016694429744421424,
                        "ram_energy_total": 3.139805634337502e-05,
                        "total_energy_kwh": 0.02131748863651487,
                        "total_energy_joules": 76742.95909145355
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2134918980707456,
                        "joules_per_token": 4.6840184992342255,
                        "flops_per_joule": 220866789.52466437,
                        "joules_per_flop": 4.52761595417825e-09
                    },
                    "per-process_emissions": [
                        0.002028416709917354,
                        0.0020286495000925732,
                        0.0020269609871103934,
                        0.00203687009896002
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0264": {
            "setup": {
                "experiment_id": "0264",
                "date_time": "April 11, 2025 at 05:48:29 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.26696615399851,
                        "average_latency_ms_per_batch": 4658.370769249814,
                        "throughput_queries_per_sec": 3.4346772278447575,
                        "throughput_tokens_per_sec": 439.63868516412896
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            15.0,
                            100.0,
                            86.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.1,
                        "cpu_memory_usage_bytes": 2654191616
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0264",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 299.91525743524056,
                            "process_2": 347.8974111042475,
                            "process_0": 447.6684445379968,
                            "process_1": 475.34630406045994
                        },
                        "ram_power": {
                            "process_3": 0.9684419631958008,
                            "process_2": 0.9685721397399902,
                            "process_0": 0.9262018203735352,
                            "process_1": 0.977783203125
                        },
                        "cpu_energy": {
                            "process_3": 0.0011470488949997843,
                            "process_2": 0.0011581552736876118,
                            "process_0": 0.0011535182982185008,
                            "process_1": 0.0011289556731250059
                        },
                        "gpu_energy": {
                            "process_3": 0.004172032504289935,
                            "process_2": 0.004189186962457958,
                            "process_0": 0.0041787530652220295,
                            "process_1": 0.004097272166704025
                        },
                        "ram_energy": {
                            "process_3": 7.748229777961247e-06,
                            "process_2": 7.867437807096968e-06,
                            "process_0": 7.469737189413464e-06,
                            "process_1": 7.768446525762076e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005326829629067681,
                            "process_2": 0.005355209673952668,
                            "process_0": 0.005339741100629945,
                            "process_1": 0.005233996286354793
                        },
                        "total_energy_joules": {
                            "process_3": 19176.58666464365,
                            "process_2": 19278.754826229604,
                            "process_0": 19223.0679622678,
                            "process_1": 18842.386630877252
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 392.7068542844862,
                        "ram_power_avg": 0.9602497816085815,
                        "cpu_energy_total": 0.004587678140030903,
                        "gpu_energy_total": 0.016637244698673948,
                        "ram_energy_total": 3.0853851300233754e-05,
                        "total_energy_kwh": 0.021255776690005085,
                        "total_energy_joules": 76520.79608401831
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21411172960107072,
                        "joules_per_token": 4.6704587453624455,
                        "flops_per_joule": 221508032.5947115,
                        "joules_per_flop": 4.51450896965745e-09
                    },
                    "per-process_emissions": [
                        0.002029255747193333,
                        0.0020400671252922688,
                        0.0020341743722849777,
                        0.0019938908852868585
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0264": {
            "setup": {
                "experiment_id": "0264",
                "date_time": "April 11, 2025 at 05:48:29 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.26696615399851,
                        "average_latency_ms_per_batch": 4658.370769249814,
                        "throughput_queries_per_sec": 3.4346772278447575,
                        "throughput_tokens_per_sec": 439.63868516412896
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            15.0,
                            100.0,
                            86.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.1,
                        "cpu_memory_usage_bytes": 2654191616
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0264",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 299.91525743524056,
                            "process_2": 347.8974111042475,
                            "process_0": 447.6684445379968,
                            "process_1": 475.34630406045994
                        },
                        "ram_power": {
                            "process_3": 0.9684419631958008,
                            "process_2": 0.9685721397399902,
                            "process_0": 0.9262018203735352,
                            "process_1": 0.977783203125
                        },
                        "cpu_energy": {
                            "process_3": 0.0011470488949997843,
                            "process_2": 0.0011581552736876118,
                            "process_0": 0.0011535182982185008,
                            "process_1": 0.0011289556731250059
                        },
                        "gpu_energy": {
                            "process_3": 0.004172032504289935,
                            "process_2": 0.004189186962457958,
                            "process_0": 0.0041787530652220295,
                            "process_1": 0.004097272166704025
                        },
                        "ram_energy": {
                            "process_3": 7.748229777961247e-06,
                            "process_2": 7.867437807096968e-06,
                            "process_0": 7.469737189413464e-06,
                            "process_1": 7.768446525762076e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005326829629067681,
                            "process_2": 0.005355209673952668,
                            "process_0": 0.005339741100629945,
                            "process_1": 0.005233996286354793
                        },
                        "total_energy_joules": {
                            "process_3": 19176.58666464365,
                            "process_2": 19278.754826229604,
                            "process_0": 19223.0679622678,
                            "process_1": 18842.386630877252
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 392.7068542844862,
                        "ram_power_avg": 0.9602497816085815,
                        "cpu_energy_total": 0.004587678140030903,
                        "gpu_energy_total": 0.016637244698673948,
                        "ram_energy_total": 3.0853851300233754e-05,
                        "total_energy_kwh": 0.021255776690005085,
                        "total_energy_joules": 76520.79608401831
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21411172960107072,
                        "joules_per_token": 4.6704587453624455,
                        "flops_per_joule": 221508032.5947115,
                        "joules_per_flop": 4.51450896965745e-09
                    },
                    "per-process_emissions": [
                        0.002029255747193333,
                        0.0020400671252922688,
                        0.0020341743722849777,
                        0.0019938908852868585
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0265": {
            "setup": {
                "experiment_id": "0265",
                "date_time": "April 11, 2025 at 05:49:38 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.24014016399815,
                        "average_latency_ms_per_batch": 4655.017520499769,
                        "throughput_queries_per_sec": 3.437151402661578,
                        "throughput_tokens_per_sec": 439.955379540682
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            84.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2631348224
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0265",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 414.3771986302797,
                            "process_2": 217.73073792768014,
                            "process_1": 443.7534153176056,
                            "process_3": 392.3647623483215
                        },
                        "ram_power": {
                            "process_0": 0.9185857772827148,
                            "process_2": 0.9846196174621582,
                            "process_1": 0.9715032577514648,
                            "process_3": 0.9715404510498047
                        },
                        "cpu_energy": {
                            "process_0": 0.0011519234185318507,
                            "process_2": 0.0011465114354066372,
                            "process_1": 0.0011408905647186884,
                            "process_3": 0.0011549750874993378
                        },
                        "gpu_energy": {
                            "process_0": 0.00416320416389393,
                            "process_2": 0.004149793042053984,
                            "process_1": 0.004127921080111896,
                            "process_3": 0.004170517503077953
                        },
                        "ram_energy": {
                            "process_0": 7.434773155906938e-06,
                            "process_2": 8.133789644919055e-06,
                            "process_1": 7.770352457634496e-06,
                            "process_3": 7.839733788880113e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005322562355581689,
                            "process_2": 0.00530443826710554,
                            "process_1": 0.005276581997288219,
                            "process_3": 0.005333332324366171
                        },
                        "total_energy_joules": {
                            "process_0": 19161.224480094083,
                            "process_2": 19095.977761579943,
                            "process_1": 18995.695190237588,
                            "process_3": 19199.996367718213
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 367.0565285559718,
                        "ram_power_avg": 0.9615622758865356,
                        "cpu_energy_total": 0.004594300506156514,
                        "gpu_energy_total": 0.016611435789137763,
                        "ram_energy_total": 3.1178649047340604e-05,
                        "total_energy_kwh": 0.02123691494434162,
                        "total_energy_joules": 76452.89379962982
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21430189474501396,
                        "joules_per_token": 4.666314318825062,
                        "flops_per_joule": 221704766.82772824,
                        "joules_per_flop": 4.510502928324641e-09
                    },
                    "per-process_emissions": [
                        0.0020276301293588447,
                        0.0020207257578538553,
                        0.002010113911866947,
                        0.002031732948967293
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0265": {
            "setup": {
                "experiment_id": "0265",
                "date_time": "April 11, 2025 at 05:49:38 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.24014016399815,
                        "average_latency_ms_per_batch": 4655.017520499769,
                        "throughput_queries_per_sec": 3.437151402661578,
                        "throughput_tokens_per_sec": 439.955379540682
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            84.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2631348224
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0265",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 414.3771986302797,
                            "process_2": 217.73073792768014,
                            "process_1": 443.7534153176056,
                            "process_3": 392.3647623483215
                        },
                        "ram_power": {
                            "process_0": 0.9185857772827148,
                            "process_2": 0.9846196174621582,
                            "process_1": 0.9715032577514648,
                            "process_3": 0.9715404510498047
                        },
                        "cpu_energy": {
                            "process_0": 0.0011519234185318507,
                            "process_2": 0.0011465114354066372,
                            "process_1": 0.0011408905647186884,
                            "process_3": 0.0011549750874993378
                        },
                        "gpu_energy": {
                            "process_0": 0.00416320416389393,
                            "process_2": 0.004149793042053984,
                            "process_1": 0.004127921080111896,
                            "process_3": 0.004170517503077953
                        },
                        "ram_energy": {
                            "process_0": 7.434773155906938e-06,
                            "process_2": 8.133789644919055e-06,
                            "process_1": 7.770352457634496e-06,
                            "process_3": 7.839733788880113e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005322562355581689,
                            "process_2": 0.00530443826710554,
                            "process_1": 0.005276581997288219,
                            "process_3": 0.005333332324366171
                        },
                        "total_energy_joules": {
                            "process_0": 19161.224480094083,
                            "process_2": 19095.977761579943,
                            "process_1": 18995.695190237588,
                            "process_3": 19199.996367718213
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 367.0565285559718,
                        "ram_power_avg": 0.9615622758865356,
                        "cpu_energy_total": 0.004594300506156514,
                        "gpu_energy_total": 0.016611435789137763,
                        "ram_energy_total": 3.1178649047340604e-05,
                        "total_energy_kwh": 0.02123691494434162,
                        "total_energy_joules": 76452.89379962982
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21430189474501396,
                        "joules_per_token": 4.666314318825062,
                        "flops_per_joule": 221704766.82772824,
                        "joules_per_flop": 4.510502928324641e-09
                    },
                    "per-process_emissions": [
                        0.0020276301293588447,
                        0.0020207257578538553,
                        0.002010113911866947,
                        0.002031732948967293
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0266": {
            "setup": {
                "experiment_id": "0266",
                "date_time": "April 11, 2025 at 05:50:44 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.28630168599193,
                        "average_latency_ms_per_batch": 4660.787710748991,
                        "throughput_queries_per_sec": 3.4328961096210904,
                        "throughput_tokens_per_sec": 439.41070203149957
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2631778304
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0266",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 430.50963510620306,
                            "process_0": 253.8023690505669,
                            "process_3": 425.22657657552753,
                            "process_2": 788.6329675804509
                        },
                        "ram_power": {
                            "process_1": 0.9862933158874513,
                            "process_0": 0.9182882308959961,
                            "process_3": 0.9683203697204591,
                            "process_2": 0.9605298042297363
                        },
                        "cpu_energy": {
                            "process_1": 0.001129196286937486,
                            "process_0": 0.0011505458239688552,
                            "process_3": 0.0011385217499374678,
                            "process_2": 0.0011447437005309665
                        },
                        "gpu_energy": {
                            "process_1": 0.004106493007413892,
                            "process_0": 0.004172443615730104,
                            "process_3": 0.004139516367165974,
                            "process_2": 0.004160901106496084
                        },
                        "ram_energy": {
                            "process_1": 7.788815123269677e-06,
                            "process_0": 7.402011313081908e-06,
                            "process_3": 7.743202119724073e-06,
                            "process_2": 7.918182897923995e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005243478109474646,
                            "process_0": 0.0053303914510120405,
                            "process_3": 0.005285781319223163,
                            "process_2": 0.005313562989924975
                        },
                        "total_energy_joules": {
                            "process_1": 18876.521194108725,
                            "process_0": 19189.409223643346,
                            "process_3": 19028.81274920339,
                            "process_2": 19128.82676372991
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 474.5428870781871,
                        "ram_power_avg": 0.9583579301834106,
                        "cpu_energy_total": 0.004563007561374776,
                        "gpu_energy_total": 0.016579354096806054,
                        "ram_energy_total": 3.085221145399965e-05,
                        "total_energy_kwh": 0.021173213869634826,
                        "total_energy_joules": 76223.56993068536
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21494663678044662,
                        "joules_per_token": 4.652317500652183,
                        "flops_per_joule": 222371780.91455987,
                        "joules_per_flop": 4.496973473375302e-09
                    },
                    "per-process_emissions": [
                        0.0019975029858043665,
                        0.002030612623263037,
                        0.002013618393558064,
                        0.0020242018210119193
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0266": {
            "setup": {
                "experiment_id": "0266",
                "date_time": "April 11, 2025 at 05:50:44 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.28630168599193,
                        "average_latency_ms_per_batch": 4660.787710748991,
                        "throughput_queries_per_sec": 3.4328961096210904,
                        "throughput_tokens_per_sec": 439.41070203149957
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2631778304
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0266",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 430.50963510620306,
                            "process_0": 253.8023690505669,
                            "process_3": 425.22657657552753,
                            "process_2": 788.6329675804509
                        },
                        "ram_power": {
                            "process_1": 0.9862933158874513,
                            "process_0": 0.9182882308959961,
                            "process_3": 0.9683203697204591,
                            "process_2": 0.9605298042297363
                        },
                        "cpu_energy": {
                            "process_1": 0.001129196286937486,
                            "process_0": 0.0011505458239688552,
                            "process_3": 0.0011385217499374678,
                            "process_2": 0.0011447437005309665
                        },
                        "gpu_energy": {
                            "process_1": 0.004106493007413892,
                            "process_0": 0.004172443615730104,
                            "process_3": 0.004139516367165974,
                            "process_2": 0.004160901106496084
                        },
                        "ram_energy": {
                            "process_1": 7.788815123269677e-06,
                            "process_0": 7.402011313081908e-06,
                            "process_3": 7.743202119724073e-06,
                            "process_2": 7.918182897923995e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005243478109474646,
                            "process_0": 0.0053303914510120405,
                            "process_3": 0.005285781319223163,
                            "process_2": 0.005313562989924975
                        },
                        "total_energy_joules": {
                            "process_1": 18876.521194108725,
                            "process_0": 19189.409223643346,
                            "process_3": 19028.81274920339,
                            "process_2": 19128.82676372991
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 474.5428870781871,
                        "ram_power_avg": 0.9583579301834106,
                        "cpu_energy_total": 0.004563007561374776,
                        "gpu_energy_total": 0.016579354096806054,
                        "ram_energy_total": 3.085221145399965e-05,
                        "total_energy_kwh": 0.021173213869634826,
                        "total_energy_joules": 76223.56993068536
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21494663678044662,
                        "joules_per_token": 4.652317500652183,
                        "flops_per_joule": 222371780.91455987,
                        "joules_per_flop": 4.496973473375302e-09
                    },
                    "per-process_emissions": [
                        0.0019975029858043665,
                        0.002030612623263037,
                        0.002013618393558064,
                        0.0020242018210119193
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0267": {
            "setup": {
                "experiment_id": "0267",
                "date_time": "April 11, 2025 at 05:51:50 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.47250931599774,
                        "average_latency_ms_per_batch": 4684.063664499718,
                        "throughput_queries_per_sec": 3.415837432198711,
                        "throughput_tokens_per_sec": 437.227191321435
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2632622080
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0267",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 291.42455435842214,
                            "process_0": 293.89295061101956,
                            "process_2": 430.0994897495285,
                            "process_1": 322.2025244874692
                        },
                        "ram_power": {
                            "process_3": 0.9686765670776367,
                            "process_0": 0.9184513092041016,
                            "process_2": 0.9455809593200684,
                            "process_1": 0.9767889976501465
                        },
                        "cpu_energy": {
                            "process_3": 0.0011451472466253562,
                            "process_0": 0.0011586253475624059,
                            "process_2": 0.001140693874312774,
                            "process_1": 0.0011486801939059887
                        },
                        "gpu_energy": {
                            "process_3": 0.0041457277610241206,
                            "process_0": 0.0041744536173380364,
                            "process_2": 0.004127856080059944,
                            "process_1": 0.004146182205831961
                        },
                        "ram_energy": {
                            "process_3": 8.03078612654698e-06,
                            "process_0": 7.42117637642562e-06,
                            "process_2": 7.59106843159263e-06,
                            "process_1": 8.101166612985797e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005298905793776024,
                            "process_0": 0.005340500141276868,
                            "process_2": 0.00527614102280431,
                            "process_1": 0.005302963566350937
                        },
                        "total_energy_joules": {
                            "process_3": 19076.060857593686,
                            "process_0": 19225.800508596723,
                            "process_2": 18994.107682095517,
                            "process_1": 19090.66883886337
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 334.4048798016098,
                        "ram_power_avg": 0.9523744583129883,
                        "cpu_energy_total": 0.004593146662406525,
                        "gpu_energy_total": 0.016594219664254062,
                        "ram_energy_total": 3.114419754755103e-05,
                        "total_energy_kwh": 0.021218510524208135,
                        "total_energy_joules": 76386.63788714929
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21448777499809715,
                        "joules_per_token": 4.662270378854327,
                        "flops_per_joule": 221897068.15206662,
                        "joules_per_flop": 4.50659401824407e-09
                    },
                    "per-process_emissions": [
                        0.002018618162138976,
                        0.002034463528819423,
                        0.002009945922637302,
                        0.0020201639706013894
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0267": {
            "setup": {
                "experiment_id": "0267",
                "date_time": "April 11, 2025 at 05:51:50 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.47250931599774,
                        "average_latency_ms_per_batch": 4684.063664499718,
                        "throughput_queries_per_sec": 3.415837432198711,
                        "throughput_tokens_per_sec": 437.227191321435
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2632622080
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0267",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 291.42455435842214,
                            "process_0": 293.89295061101956,
                            "process_2": 430.0994897495285,
                            "process_1": 322.2025244874692
                        },
                        "ram_power": {
                            "process_3": 0.9686765670776367,
                            "process_0": 0.9184513092041016,
                            "process_2": 0.9455809593200684,
                            "process_1": 0.9767889976501465
                        },
                        "cpu_energy": {
                            "process_3": 0.0011451472466253562,
                            "process_0": 0.0011586253475624059,
                            "process_2": 0.001140693874312774,
                            "process_1": 0.0011486801939059887
                        },
                        "gpu_energy": {
                            "process_3": 0.0041457277610241206,
                            "process_0": 0.0041744536173380364,
                            "process_2": 0.004127856080059944,
                            "process_1": 0.004146182205831961
                        },
                        "ram_energy": {
                            "process_3": 8.03078612654698e-06,
                            "process_0": 7.42117637642562e-06,
                            "process_2": 7.59106843159263e-06,
                            "process_1": 8.101166612985797e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005298905793776024,
                            "process_0": 0.005340500141276868,
                            "process_2": 0.00527614102280431,
                            "process_1": 0.005302963566350937
                        },
                        "total_energy_joules": {
                            "process_3": 19076.060857593686,
                            "process_0": 19225.800508596723,
                            "process_2": 18994.107682095517,
                            "process_1": 19090.66883886337
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 334.4048798016098,
                        "ram_power_avg": 0.9523744583129883,
                        "cpu_energy_total": 0.004593146662406525,
                        "gpu_energy_total": 0.016594219664254062,
                        "ram_energy_total": 3.114419754755103e-05,
                        "total_energy_kwh": 0.021218510524208135,
                        "total_energy_joules": 76386.63788714929
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21448777499809715,
                        "joules_per_token": 4.662270378854327,
                        "flops_per_joule": 221897068.15206662,
                        "joules_per_flop": 4.50659401824407e-09
                    },
                    "per-process_emissions": [
                        0.002018618162138976,
                        0.002034463528819423,
                        0.002009945922637302,
                        0.0020201639706013894
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0268": {
            "setup": {
                "experiment_id": "0268",
                "date_time": "April 11, 2025 at 05:52:55 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.06633263600088,
                        "average_latency_ms_per_batch": 4633.29157950011,
                        "throughput_queries_per_sec": 3.4532685296111354,
                        "throughput_tokens_per_sec": 442.01837179022533
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            74.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2657443840
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0268",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 472.9895064796256,
                            "process_2": 276.18122080074755,
                            "process_3": 276.7426681159098,
                            "process_1": 442.72922326632846
                        },
                        "ram_power": {
                            "process_0": 0.9267454147338867,
                            "process_2": 0.9618115425109863,
                            "process_3": 0.9674592018127441,
                            "process_1": 0.9765043258666992
                        },
                        "cpu_energy": {
                            "process_0": 0.0011448842115631805,
                            "process_2": 0.0011516965452502746,
                            "process_3": 0.0011516918935309379,
                            "process_1": 0.0011423127099062641
                        },
                        "gpu_energy": {
                            "process_0": 0.004149156097099871,
                            "process_2": 0.004158243604369949,
                            "process_3": 0.004158243604369949,
                            "process_1": 0.004130352748723809
                        },
                        "ram_energy": {
                            "process_0": 7.627224883339942e-06,
                            "process_2": 7.730635635537237e-06,
                            "process_3": 7.792592432873334e-06,
                            "process_1": 7.818372744001433e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005301667533546389,
                            "process_2": 0.0053176707852557615,
                            "process_3": 0.005317728090333759,
                            "process_1": 0.0052804838313740745
                        },
                        "total_energy_joules": {
                            "process_0": 19086.003120767,
                            "process_2": 19143.61482692074,
                            "process_3": 19143.82112520153,
                            "process_1": 19009.74179294667
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 367.1606546656529,
                        "ram_power_avg": 0.9581301212310791,
                        "cpu_energy_total": 0.004590585360250657,
                        "gpu_energy_total": 0.016595996054563578,
                        "ram_energy_total": 3.0968825695751944e-05,
                        "total_energy_kwh": 0.02121755024050998,
                        "total_energy_joules": 76383.18086583595
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21449748248607048,
                        "joules_per_token": 4.662059379018308,
                        "flops_per_joule": 221907110.97674704,
                        "joules_per_flop": 4.506390063835254e-09
                    },
                    "per-process_emissions": [
                        0.0020196702469044967,
                        0.0020257666856431824,
                        0.0020257885160126455,
                        0.002011600315561954
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0268": {
            "setup": {
                "experiment_id": "0268",
                "date_time": "April 11, 2025 at 05:52:55 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.06633263600088,
                        "average_latency_ms_per_batch": 4633.29157950011,
                        "throughput_queries_per_sec": 3.4532685296111354,
                        "throughput_tokens_per_sec": 442.01837179022533
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            74.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2657443840
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0268",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 472.9895064796256,
                            "process_2": 276.18122080074755,
                            "process_3": 276.7426681159098,
                            "process_1": 442.72922326632846
                        },
                        "ram_power": {
                            "process_0": 0.9267454147338867,
                            "process_2": 0.9618115425109863,
                            "process_3": 0.9674592018127441,
                            "process_1": 0.9765043258666992
                        },
                        "cpu_energy": {
                            "process_0": 0.0011448842115631805,
                            "process_2": 0.0011516965452502746,
                            "process_3": 0.0011516918935309379,
                            "process_1": 0.0011423127099062641
                        },
                        "gpu_energy": {
                            "process_0": 0.004149156097099871,
                            "process_2": 0.004158243604369949,
                            "process_3": 0.004158243604369949,
                            "process_1": 0.004130352748723809
                        },
                        "ram_energy": {
                            "process_0": 7.627224883339942e-06,
                            "process_2": 7.730635635537237e-06,
                            "process_3": 7.792592432873334e-06,
                            "process_1": 7.818372744001433e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005301667533546389,
                            "process_2": 0.0053176707852557615,
                            "process_3": 0.005317728090333759,
                            "process_1": 0.0052804838313740745
                        },
                        "total_energy_joules": {
                            "process_0": 19086.003120767,
                            "process_2": 19143.61482692074,
                            "process_3": 19143.82112520153,
                            "process_1": 19009.74179294667
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 367.1606546656529,
                        "ram_power_avg": 0.9581301212310791,
                        "cpu_energy_total": 0.004590585360250657,
                        "gpu_energy_total": 0.016595996054563578,
                        "ram_energy_total": 3.0968825695751944e-05,
                        "total_energy_kwh": 0.02121755024050998,
                        "total_energy_joules": 76383.18086583595
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21449748248607048,
                        "joules_per_token": 4.662059379018308,
                        "flops_per_joule": 221907110.97674704,
                        "joules_per_flop": 4.506390063835254e-09
                    },
                    "per-process_emissions": [
                        0.0020196702469044967,
                        0.0020257666856431824,
                        0.0020257885160126455,
                        0.002011600315561954
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0269": {
            "setup": {
                "experiment_id": "0269",
                "date_time": "April 11, 2025 at 05:54:01 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.17671484000675,
                        "average_latency_ms_per_batch": 4647.089355000844,
                        "throughput_queries_per_sec": 3.4430153538541317,
                        "throughput_tokens_per_sec": 440.70596529332886
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2584113152
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0269",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 681.9458710430555,
                            "process_2": 405.3370891174073,
                            "process_1": 431.64789773561614,
                            "process_3": 494.84939350935184
                        },
                        "ram_power": {
                            "process_0": 0.9013881683349609,
                            "process_2": 0.9799661636352539,
                            "process_1": 0.978292465209961,
                            "process_3": 0.972433090209961
                        },
                        "cpu_energy": {
                            "process_0": 0.0011468827930933686,
                            "process_2": 0.0011466094444372175,
                            "process_1": 0.001150720224156771,
                            "process_3": 0.001146376896781135
                        },
                        "gpu_energy": {
                            "process_0": 0.004151759432516056,
                            "process_2": 0.004152781377778136,
                            "process_1": 0.004159571105431914,
                            "process_3": 0.004152781377778136
                        },
                        "ram_energy": {
                            "process_0": 7.453290437229088e-06,
                            "process_2": 7.841731046488007e-06,
                            "process_1": 7.848215900882174e-06,
                            "process_3": 7.781806993895987e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005306095516046653,
                            "process_2": 0.00530723255326184,
                            "process_1": 0.005318139545489568,
                            "process_3": 0.005306940081553169
                        },
                        "total_energy_joules": {
                            "process_0": 19101.94385776795,
                            "process_2": 19106.037191742624,
                            "process_1": 19145.302363762443,
                            "process_3": 19104.98429359141
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 503.4450628513577,
                        "ram_power_avg": 0.9580199718475342,
                        "cpu_energy_total": 0.004590589358468492,
                        "gpu_energy_total": 0.016616893293504242,
                        "ram_energy_total": 3.0925044378495256e-05,
                        "total_energy_kwh": 0.02123840769635123,
                        "total_energy_joules": 76458.26770686443
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21428683243014468,
                        "joules_per_token": 4.6666423160927994,
                        "flops_per_joule": 221689184.19832614,
                        "joules_per_flop": 4.5108199735418145e-09
                    },
                    "per-process_emissions": [
                        0.0020213570868379722,
                        0.002021790241165098,
                        0.002025945259854251,
                        0.00202167882406768
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0269": {
            "setup": {
                "experiment_id": "0269",
                "date_time": "April 11, 2025 at 05:54:01 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.17671484000675,
                        "average_latency_ms_per_batch": 4647.089355000844,
                        "throughput_queries_per_sec": 3.4430153538541317,
                        "throughput_tokens_per_sec": 440.70596529332886
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2584113152
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0269",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 681.9458710430555,
                            "process_2": 405.3370891174073,
                            "process_1": 431.64789773561614,
                            "process_3": 494.84939350935184
                        },
                        "ram_power": {
                            "process_0": 0.9013881683349609,
                            "process_2": 0.9799661636352539,
                            "process_1": 0.978292465209961,
                            "process_3": 0.972433090209961
                        },
                        "cpu_energy": {
                            "process_0": 0.0011468827930933686,
                            "process_2": 0.0011466094444372175,
                            "process_1": 0.001150720224156771,
                            "process_3": 0.001146376896781135
                        },
                        "gpu_energy": {
                            "process_0": 0.004151759432516056,
                            "process_2": 0.004152781377778136,
                            "process_1": 0.004159571105431914,
                            "process_3": 0.004152781377778136
                        },
                        "ram_energy": {
                            "process_0": 7.453290437229088e-06,
                            "process_2": 7.841731046488007e-06,
                            "process_1": 7.848215900882174e-06,
                            "process_3": 7.781806993895987e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005306095516046653,
                            "process_2": 0.00530723255326184,
                            "process_1": 0.005318139545489568,
                            "process_3": 0.005306940081553169
                        },
                        "total_energy_joules": {
                            "process_0": 19101.94385776795,
                            "process_2": 19106.037191742624,
                            "process_1": 19145.302363762443,
                            "process_3": 19104.98429359141
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 503.4450628513577,
                        "ram_power_avg": 0.9580199718475342,
                        "cpu_energy_total": 0.004590589358468492,
                        "gpu_energy_total": 0.016616893293504242,
                        "ram_energy_total": 3.0925044378495256e-05,
                        "total_energy_kwh": 0.02123840769635123,
                        "total_energy_joules": 76458.26770686443
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21428683243014468,
                        "joules_per_token": 4.6666423160927994,
                        "flops_per_joule": 221689184.19832614,
                        "joules_per_flop": 4.5108199735418145e-09
                    },
                    "per-process_emissions": [
                        0.0020213570868379722,
                        0.002021790241165098,
                        0.002025945259854251,
                        0.00202167882406768
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0270": {
            "setup": {
                "experiment_id": "0270",
                "date_time": "April 11, 2025 at 05:55:07 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.42139437900187,
                        "average_latency_ms_per_batch": 4677.674297375233,
                        "throughput_queries_per_sec": 3.420503220794578,
                        "throughput_tokens_per_sec": 437.82441226170596
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2657779712
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0270",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 442.0448047825455,
                            "process_3": 459.8663452656568,
                            "process_2": 358.5420132562019,
                            "process_0": 380.1699622084422
                        },
                        "ram_power": {
                            "process_1": 0.9808959960937501,
                            "process_3": 0.9602551460266113,
                            "process_2": 0.9797015190124512,
                            "process_0": 0.927495002746582
                        },
                        "cpu_energy": {
                            "process_1": 0.001150721263094283,
                            "process_3": 0.0011482332380934397,
                            "process_2": 0.0011627235772817815,
                            "process_0": 0.0011549111850625879
                        },
                        "gpu_energy": {
                            "process_1": 0.004136562198135962,
                            "process_3": 0.004134282196311845,
                            "process_2": 0.004168778335019996,
                            "process_0": 0.004150578320459841
                        },
                        "ram_energy": {
                            "process_1": 7.857745169843334e-06,
                            "process_3": 7.681444544530481e-06,
                            "process_2": 7.999086739899876e-06,
                            "process_0": 7.4707559343074324e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005295141206400087,
                            "process_3": 0.0052901968789498145,
                            "process_2": 0.005339500999041679,
                            "process_0": 0.005312960261456737
                        },
                        "total_energy_joules": {
                            "process_1": 19062.508343040314,
                            "process_3": 19044.708764219333,
                            "process_2": 19222.203596550044,
                            "process_0": 19126.65694124425
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 410.1557813782116,
                        "ram_power_avg": 0.9620869159698486,
                        "cpu_energy_total": 0.004616589263532093,
                        "gpu_energy_total": 0.016590201049927644,
                        "ram_energy_total": 3.100903238858112e-05,
                        "total_energy_kwh": 0.021237799345848317,
                        "total_energy_joules": 76456.07764505394
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21429297061330357,
                        "joules_per_token": 4.666508645327999,
                        "flops_per_joule": 221695534.41966978,
                        "joules_per_flop": 4.51069076613424e-09
                    },
                    "per-process_emissions": [
                        0.0020171840425781133,
                        0.0020153005010359317,
                        0.002034082905584928,
                        0.002023972211601944
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0270": {
            "setup": {
                "experiment_id": "0270",
                "date_time": "April 11, 2025 at 05:55:07 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.42139437900187,
                        "average_latency_ms_per_batch": 4677.674297375233,
                        "throughput_queries_per_sec": 3.420503220794578,
                        "throughput_tokens_per_sec": 437.82441226170596
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2657779712
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0270",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 442.0448047825455,
                            "process_3": 459.8663452656568,
                            "process_2": 358.5420132562019,
                            "process_0": 380.1699622084422
                        },
                        "ram_power": {
                            "process_1": 0.9808959960937501,
                            "process_3": 0.9602551460266113,
                            "process_2": 0.9797015190124512,
                            "process_0": 0.927495002746582
                        },
                        "cpu_energy": {
                            "process_1": 0.001150721263094283,
                            "process_3": 0.0011482332380934397,
                            "process_2": 0.0011627235772817815,
                            "process_0": 0.0011549111850625879
                        },
                        "gpu_energy": {
                            "process_1": 0.004136562198135962,
                            "process_3": 0.004134282196311845,
                            "process_2": 0.004168778335019996,
                            "process_0": 0.004150578320459841
                        },
                        "ram_energy": {
                            "process_1": 7.857745169843334e-06,
                            "process_3": 7.681444544530481e-06,
                            "process_2": 7.999086739899876e-06,
                            "process_0": 7.4707559343074324e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005295141206400087,
                            "process_3": 0.0052901968789498145,
                            "process_2": 0.005339500999041679,
                            "process_0": 0.005312960261456737
                        },
                        "total_energy_joules": {
                            "process_1": 19062.508343040314,
                            "process_3": 19044.708764219333,
                            "process_2": 19222.203596550044,
                            "process_0": 19126.65694124425
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 410.1557813782116,
                        "ram_power_avg": 0.9620869159698486,
                        "cpu_energy_total": 0.004616589263532093,
                        "gpu_energy_total": 0.016590201049927644,
                        "ram_energy_total": 3.100903238858112e-05,
                        "total_energy_kwh": 0.021237799345848317,
                        "total_energy_joules": 76456.07764505394
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21429297061330357,
                        "joules_per_token": 4.666508645327999,
                        "flops_per_joule": 221695534.41966978,
                        "joules_per_flop": 4.51069076613424e-09
                    },
                    "per-process_emissions": [
                        0.0020171840425781133,
                        0.0020153005010359317,
                        0.002034082905584928,
                        0.002023972211601944
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0271": {
            "setup": {
                "experiment_id": "0271",
                "date_time": "April 11, 2025 at 05:56:12 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.979403802008164,
                        "average_latency_ms_per_batch": 4622.4254752510205,
                        "throughput_queries_per_sec": 3.461386253962509,
                        "throughput_tokens_per_sec": 443.05744050720114
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2653810688
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0271",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 0.0,
                            "process_0": 0.0,
                            "process_3": 422.77559156408745,
                            "process_1": 360.4324968465612
                        },
                        "ram_power": {
                            "process_2": 0.9813652038574219,
                            "process_0": 0.9267125129699707,
                            "process_3": 0.9611763954162598,
                            "process_1": 0.9639759063720703
                        },
                        "cpu_energy": {
                            "process_2": 0.0011769793886878687,
                            "process_0": 0.0011755714550936319,
                            "process_3": 0.0011253306131251291,
                            "process_1": 0.0011467480710626889
                        },
                        "gpu_energy": {
                            "process_2": 0.004151642210200035,
                            "process_0": 0.004151642210200035,
                            "process_3": 0.004081652431985938,
                            "process_1": 0.004160041105807977
                        },
                        "ram_energy": {
                            "process_2": 8.12037935745096e-06,
                            "process_0": 7.66653362771346e-06,
                            "process_3": 7.574857648342199e-06,
                            "process_1": 7.727622303991868e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0053367419782453535,
                            "process_0": 0.0053348801989213795,
                            "process_3": 0.005214557902759409,
                            "process_1": 0.005314516799174656
                        },
                        "total_energy_joules": {
                            "process_2": 19212.271121683272,
                            "process_0": 19205.568716116966,
                            "process_3": 18772.40844993387,
                            "process_1": 19132.26047702876
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 195.80202210266216,
                        "ram_power_avg": 0.9583075046539307,
                        "cpu_energy_total": 0.004624629527969318,
                        "gpu_energy_total": 0.016544977958193985,
                        "ram_energy_total": 3.1089392937498484e-05,
                        "total_energy_kwh": 0.021200696879100797,
                        "total_energy_joules": 76322.50876476287
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21466799591845026,
                        "joules_per_token": 4.658356247849296,
                        "flops_per_joule": 222083514.64696068,
                        "joules_per_flop": 4.5028105827200596e-09
                    },
                    "per-process_emissions": [
                        0.0020330318566125676,
                        0.0020323226117790996,
                        0.001986485833056197,
                        0.002024565174645585
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0271": {
            "setup": {
                "experiment_id": "0271",
                "date_time": "April 11, 2025 at 05:56:12 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.979403802008164,
                        "average_latency_ms_per_batch": 4622.4254752510205,
                        "throughput_queries_per_sec": 3.461386253962509,
                        "throughput_tokens_per_sec": 443.05744050720114
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2653810688
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0271",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 0.0,
                            "process_0": 0.0,
                            "process_3": 422.77559156408745,
                            "process_1": 360.4324968465612
                        },
                        "ram_power": {
                            "process_2": 0.9813652038574219,
                            "process_0": 0.9267125129699707,
                            "process_3": 0.9611763954162598,
                            "process_1": 0.9639759063720703
                        },
                        "cpu_energy": {
                            "process_2": 0.0011769793886878687,
                            "process_0": 0.0011755714550936319,
                            "process_3": 0.0011253306131251291,
                            "process_1": 0.0011467480710626889
                        },
                        "gpu_energy": {
                            "process_2": 0.004151642210200035,
                            "process_0": 0.004151642210200035,
                            "process_3": 0.004081652431985938,
                            "process_1": 0.004160041105807977
                        },
                        "ram_energy": {
                            "process_2": 8.12037935745096e-06,
                            "process_0": 7.66653362771346e-06,
                            "process_3": 7.574857648342199e-06,
                            "process_1": 7.727622303991868e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0053367419782453535,
                            "process_0": 0.0053348801989213795,
                            "process_3": 0.005214557902759409,
                            "process_1": 0.005314516799174656
                        },
                        "total_energy_joules": {
                            "process_2": 19212.271121683272,
                            "process_0": 19205.568716116966,
                            "process_3": 18772.40844993387,
                            "process_1": 19132.26047702876
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 195.80202210266216,
                        "ram_power_avg": 0.9583075046539307,
                        "cpu_energy_total": 0.004624629527969318,
                        "gpu_energy_total": 0.016544977958193985,
                        "ram_energy_total": 3.1089392937498484e-05,
                        "total_energy_kwh": 0.021200696879100797,
                        "total_energy_joules": 76322.50876476287
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21466799591845026,
                        "joules_per_token": 4.658356247849296,
                        "flops_per_joule": 222083514.64696068,
                        "joules_per_flop": 4.5028105827200596e-09
                    },
                    "per-process_emissions": [
                        0.0020330318566125676,
                        0.0020323226117790996,
                        0.001986485833056197,
                        0.002024565174645585
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0272": {
            "setup": {
                "experiment_id": "0272",
                "date_time": "April 11, 2025 at 05:57:18 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.37382519699531,
                        "average_latency_ms_per_batch": 4671.728149624414,
                        "throughput_queries_per_sec": 3.424856816911816,
                        "throughput_tokens_per_sec": 438.38167256471246
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            76.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2655776768
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0272",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 378.7947453659546,
                            "process_2": 403.90710045466346,
                            "process_3": 0.0,
                            "process_1": 384.66849329626274
                        },
                        "ram_power": {
                            "process_0": 0.9266910552978516,
                            "process_2": 0.9597558975219728,
                            "process_3": 0.9765872955322267,
                            "process_1": 0.9766860008239746
                        },
                        "cpu_energy": {
                            "process_0": 0.0011568249734688156,
                            "process_2": 0.0011534750213751297,
                            "process_3": 0.0011748493002501162,
                            "process_1": 0.0011571041417503239
                        },
                        "gpu_energy": {
                            "process_0": 0.004156243324992148,
                            "process_2": 0.004147685262589962,
                            "process_3": 0.004126696356910153,
                            "process_1": 0.004160336661599984
                        },
                        "ram_energy": {
                            "process_0": 7.5106204725183065e-06,
                            "process_2": 7.77394502523699e-06,
                            "process_3": 8.072385342992444e-06,
                            "process_1": 7.907883277201338e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.00532057891893348,
                            "process_2": 0.0053089342289903306,
                            "process_3": 0.005309618042503262,
                            "process_1": 0.005325348686627509
                        },
                        "total_energy_joules": {
                            "process_0": 19154.08410816053,
                            "process_2": 19112.16322436519,
                            "process_3": 19114.624953011742,
                            "process_1": 19171.25527185903
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 291.8425847792202,
                        "ram_power_avg": 0.9599300622940063,
                        "cpu_energy_total": 0.004642253436844385,
                        "gpu_energy_total": 0.016590961606092247,
                        "ram_energy_total": 3.126483411794908e-05,
                        "total_energy_kwh": 0.02126447987705458,
                        "total_energy_joules": 76552.12755739648
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21402409734093633,
                        "joules_per_token": 4.672371066735625,
                        "flops_per_joule": 221417373.1545661,
                        "joules_per_flop": 4.516357437326854e-09
                    },
                    "per-process_emissions": [
                        0.0020268745391677093,
                        0.0020224384945338666,
                        0.0020226989932916177,
                        0.0020286915821707495
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0272": {
            "setup": {
                "experiment_id": "0272",
                "date_time": "April 11, 2025 at 05:57:18 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.37382519699531,
                        "average_latency_ms_per_batch": 4671.728149624414,
                        "throughput_queries_per_sec": 3.424856816911816,
                        "throughput_tokens_per_sec": 438.38167256471246
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            76.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2655776768
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0272",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 378.7947453659546,
                            "process_2": 403.90710045466346,
                            "process_3": 0.0,
                            "process_1": 384.66849329626274
                        },
                        "ram_power": {
                            "process_0": 0.9266910552978516,
                            "process_2": 0.9597558975219728,
                            "process_3": 0.9765872955322267,
                            "process_1": 0.9766860008239746
                        },
                        "cpu_energy": {
                            "process_0": 0.0011568249734688156,
                            "process_2": 0.0011534750213751297,
                            "process_3": 0.0011748493002501162,
                            "process_1": 0.0011571041417503239
                        },
                        "gpu_energy": {
                            "process_0": 0.004156243324992148,
                            "process_2": 0.004147685262589962,
                            "process_3": 0.004126696356910153,
                            "process_1": 0.004160336661599984
                        },
                        "ram_energy": {
                            "process_0": 7.5106204725183065e-06,
                            "process_2": 7.77394502523699e-06,
                            "process_3": 8.072385342992444e-06,
                            "process_1": 7.907883277201338e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.00532057891893348,
                            "process_2": 0.0053089342289903306,
                            "process_3": 0.005309618042503262,
                            "process_1": 0.005325348686627509
                        },
                        "total_energy_joules": {
                            "process_0": 19154.08410816053,
                            "process_2": 19112.16322436519,
                            "process_3": 19114.624953011742,
                            "process_1": 19171.25527185903
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 291.8425847792202,
                        "ram_power_avg": 0.9599300622940063,
                        "cpu_energy_total": 0.004642253436844385,
                        "gpu_energy_total": 0.016590961606092247,
                        "ram_energy_total": 3.126483411794908e-05,
                        "total_energy_kwh": 0.02126447987705458,
                        "total_energy_joules": 76552.12755739648
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21402409734093633,
                        "joules_per_token": 4.672371066735625,
                        "flops_per_joule": 221417373.1545661,
                        "joules_per_flop": 4.516357437326854e-09
                    },
                    "per-process_emissions": [
                        0.0020268745391677093,
                        0.0020224384945338666,
                        0.0020226989932916177,
                        0.0020286915821707495
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0273": {
            "setup": {
                "experiment_id": "0273",
                "date_time": "April 11, 2025 at 05:58:24 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.459553351996874,
                        "average_latency_ms_per_batch": 4682.444168999609,
                        "throughput_queries_per_sec": 3.4170188522329683,
                        "throughput_tokens_per_sec": 437.37841308581994
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            99.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2629853184
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0273",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 421.087609406574,
                            "process_0": 286.8881319961184,
                            "process_2": 205.68132084312273,
                            "process_3": 437.8812874448877
                        },
                        "ram_power": {
                            "process_1": 0.9799518585205079,
                            "process_0": 0.9175300598144531,
                            "process_2": 0.9850401878356934,
                            "process_3": 0.9762740135192871
                        },
                        "cpu_energy": {
                            "process_1": 0.0011346515043125008,
                            "process_0": 0.0011596474906252752,
                            "process_2": 0.0011494090469997217,
                            "process_3": 0.0011374347075621927
                        },
                        "gpu_energy": {
                            "process_1": 0.004117328016081889,
                            "process_0": 0.0041917914089857344,
                            "process_2": 0.004164363053709985,
                            "process_3": 0.004139319144785936
                        },
                        "ram_energy": {
                            "process_1": 7.818535468474466e-06,
                            "process_0": 7.42832460012264e-06,
                            "process_2": 8.166743321208509e-06,
                            "process_3": 7.806011619680512e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005259798055862864,
                            "process_0": 0.005358867224211134,
                            "process_2": 0.0053219388440309155,
                            "process_3": 0.005284559863967807
                        },
                        "total_energy_joules": {
                            "process_1": 18935.27300110631,
                            "process_0": 19291.922007160083,
                            "process_2": 19158.979838511295,
                            "process_3": 19024.415510284107
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 337.88458742267574,
                        "ram_power_avg": 0.9646990299224854,
                        "cpu_energy_total": 0.0045811427494996905,
                        "gpu_energy_total": 0.016612801623563545,
                        "ram_energy_total": 3.121961500948613e-05,
                        "total_energy_kwh": 0.021225163988072723,
                        "total_energy_joules": 76410.5903570618
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21442053939694244,
                        "joules_per_token": 4.66373232159801,
                        "flops_per_joule": 221827509.95569947,
                        "joules_per_flop": 4.508007145730965e-09
                    },
                    "per-process_emissions": [
                        0.002003720069380958,
                        0.0020414604690632315,
                        0.0020273926026335774,
                        0.0020131530801785363
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0273": {
            "setup": {
                "experiment_id": "0273",
                "date_time": "April 11, 2025 at 05:58:24 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.459553351996874,
                        "average_latency_ms_per_batch": 4682.444168999609,
                        "throughput_queries_per_sec": 3.4170188522329683,
                        "throughput_tokens_per_sec": 437.37841308581994
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            99.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2629853184
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0273",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 421.087609406574,
                            "process_0": 286.8881319961184,
                            "process_2": 205.68132084312273,
                            "process_3": 437.8812874448877
                        },
                        "ram_power": {
                            "process_1": 0.9799518585205079,
                            "process_0": 0.9175300598144531,
                            "process_2": 0.9850401878356934,
                            "process_3": 0.9762740135192871
                        },
                        "cpu_energy": {
                            "process_1": 0.0011346515043125008,
                            "process_0": 0.0011596474906252752,
                            "process_2": 0.0011494090469997217,
                            "process_3": 0.0011374347075621927
                        },
                        "gpu_energy": {
                            "process_1": 0.004117328016081889,
                            "process_0": 0.0041917914089857344,
                            "process_2": 0.004164363053709985,
                            "process_3": 0.004139319144785936
                        },
                        "ram_energy": {
                            "process_1": 7.818535468474466e-06,
                            "process_0": 7.42832460012264e-06,
                            "process_2": 8.166743321208509e-06,
                            "process_3": 7.806011619680512e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005259798055862864,
                            "process_0": 0.005358867224211134,
                            "process_2": 0.0053219388440309155,
                            "process_3": 0.005284559863967807
                        },
                        "total_energy_joules": {
                            "process_1": 18935.27300110631,
                            "process_0": 19291.922007160083,
                            "process_2": 19158.979838511295,
                            "process_3": 19024.415510284107
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 337.88458742267574,
                        "ram_power_avg": 0.9646990299224854,
                        "cpu_energy_total": 0.0045811427494996905,
                        "gpu_energy_total": 0.016612801623563545,
                        "ram_energy_total": 3.121961500948613e-05,
                        "total_energy_kwh": 0.021225163988072723,
                        "total_energy_joules": 76410.5903570618
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21442053939694244,
                        "joules_per_token": 4.66373232159801,
                        "flops_per_joule": 221827509.95569947,
                        "joules_per_flop": 4.508007145730965e-09
                    },
                    "per-process_emissions": [
                        0.002003720069380958,
                        0.0020414604690632315,
                        0.0020273926026335774,
                        0.0020131530801785363
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0274": {
            "setup": {
                "experiment_id": "0274",
                "date_time": "April 11, 2025 at 05:59:29 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.784266956001375,
                        "average_latency_ms_per_batch": 4598.033369500172,
                        "throughput_queries_per_sec": 3.4797485607937806,
                        "throughput_tokens_per_sec": 445.4078157816039
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2633043968
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0274",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 452.7312455831044,
                            "process_0": 444.40337354056226,
                            "process_2": 433.2827851447016,
                            "process_3": 428.9222648972698
                        },
                        "ram_power": {
                            "process_1": 0.9613838195800782,
                            "process_0": 0.9187273979187012,
                            "process_2": 0.9726591110229492,
                            "process_3": 0.9686193466186523
                        },
                        "cpu_energy": {
                            "process_1": 0.0011364603596564398,
                            "process_0": 0.001138381517156631,
                            "process_2": 0.0011389110706562634,
                            "process_3": 0.0011419980391874562
                        },
                        "gpu_energy": {
                            "process_1": 0.00416301777485617,
                            "process_0": 0.0041577627706521625,
                            "process_2": 0.004160143050334186,
                            "process_3": 0.004167051666972232
                        },
                        "ram_energy": {
                            "process_1": 7.672050530987186e-06,
                            "process_0": 7.38227932404344e-06,
                            "process_2": 7.785937499193608e-06,
                            "process_3": 7.751217500983655e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005307150185043598,
                            "process_0": 0.005303526567132839,
                            "process_2": 0.005306840058489642,
                            "process_3": 0.0053168009236606705
                        },
                        "total_energy_joules": {
                            "process_1": 19105.74066615695,
                            "process_0": 19092.69564167822,
                            "process_2": 19104.624210562713,
                            "process_3": 19140.483325178415
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 439.8349172914095,
                        "ram_power_avg": 0.9553474187850952,
                        "cpu_energy_total": 0.00455575098665679,
                        "gpu_energy_total": 0.01664797526281475,
                        "ram_energy_total": 3.059148485520789e-05,
                        "total_energy_kwh": 0.021234317734326748,
                        "total_energy_joules": 76443.5438435763
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21432810641963426,
                        "joules_per_token": 4.665743642796405,
                        "flops_per_joule": 221731883.96179172,
                        "joules_per_flop": 4.5099513075544765e-09
                    },
                    "per-process_emissions": [
                        0.002021758862992359,
                        0.002020378445749255,
                        0.0020216407202816294,
                        0.0020254353118685326
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0274": {
            "setup": {
                "experiment_id": "0274",
                "date_time": "April 11, 2025 at 05:59:29 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.784266956001375,
                        "average_latency_ms_per_batch": 4598.033369500172,
                        "throughput_queries_per_sec": 3.4797485607937806,
                        "throughput_tokens_per_sec": 445.4078157816039
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2633043968
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0274",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 452.7312455831044,
                            "process_0": 444.40337354056226,
                            "process_2": 433.2827851447016,
                            "process_3": 428.9222648972698
                        },
                        "ram_power": {
                            "process_1": 0.9613838195800782,
                            "process_0": 0.9187273979187012,
                            "process_2": 0.9726591110229492,
                            "process_3": 0.9686193466186523
                        },
                        "cpu_energy": {
                            "process_1": 0.0011364603596564398,
                            "process_0": 0.001138381517156631,
                            "process_2": 0.0011389110706562634,
                            "process_3": 0.0011419980391874562
                        },
                        "gpu_energy": {
                            "process_1": 0.00416301777485617,
                            "process_0": 0.0041577627706521625,
                            "process_2": 0.004160143050334186,
                            "process_3": 0.004167051666972232
                        },
                        "ram_energy": {
                            "process_1": 7.672050530987186e-06,
                            "process_0": 7.38227932404344e-06,
                            "process_2": 7.785937499193608e-06,
                            "process_3": 7.751217500983655e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005307150185043598,
                            "process_0": 0.005303526567132839,
                            "process_2": 0.005306840058489642,
                            "process_3": 0.0053168009236606705
                        },
                        "total_energy_joules": {
                            "process_1": 19105.74066615695,
                            "process_0": 19092.69564167822,
                            "process_2": 19104.624210562713,
                            "process_3": 19140.483325178415
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 439.8349172914095,
                        "ram_power_avg": 0.9553474187850952,
                        "cpu_energy_total": 0.00455575098665679,
                        "gpu_energy_total": 0.01664797526281475,
                        "ram_energy_total": 3.059148485520789e-05,
                        "total_energy_kwh": 0.021234317734326748,
                        "total_energy_joules": 76443.5438435763
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21432810641963426,
                        "joules_per_token": 4.665743642796405,
                        "flops_per_joule": 221731883.96179172,
                        "joules_per_flop": 4.5099513075544765e-09
                    },
                    "per-process_emissions": [
                        0.002021758862992359,
                        0.002020378445749255,
                        0.0020216407202816294,
                        0.0020254353118685326
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0275": {
            "setup": {
                "experiment_id": "0275",
                "date_time": "April 11, 2025 at 06:00:36 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.33585250400574,
                        "average_latency_ms_per_batch": 4666.981563000718,
                        "throughput_queries_per_sec": 3.428340091772833,
                        "throughput_tokens_per_sec": 438.8275317469226
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2654433280
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0275",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 434.3620597495727,
                            "process_1": 411.34719798140503,
                            "process_0": 381.47250053018615,
                            "process_2": 501.4686197754377
                        },
                        "ram_power": {
                            "process_3": 0.978914737701416,
                            "process_1": 0.9684720039367677,
                            "process_0": 0.9263291358947754,
                            "process_2": 0.9631161689758301
                        },
                        "cpu_energy": {
                            "process_3": 0.0011507648265626357,
                            "process_1": 0.0011498765812810916,
                            "process_0": 0.001153006473781261,
                            "process_2": 0.0011504011154995625
                        },
                        "gpu_energy": {
                            "process_3": 0.004174715006435958,
                            "process_1": 0.00416835027912188,
                            "process_0": 0.004177431397497988,
                            "process_2": 0.004163454719649851
                        },
                        "ram_energy": {
                            "process_3": 7.894359752817906e-06,
                            "process_1": 7.780630801037234e-06,
                            "process_0": 7.4552232046449615e-06,
                            "process_2": 8.02956025910053e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005333374192751411,
                            "process_1": 0.005326007491204007,
                            "process_0": 0.005337893094483894,
                            "process_2": 0.005321885395408513
                        },
                        "total_energy_joules": {
                            "process_3": 19200.14709390508,
                            "process_1": 19173.626968334425,
                            "process_0": 19216.415140142017,
                            "process_2": 19158.787423470647
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 432.1625945091504,
                        "ram_power_avg": 0.9592080116271973,
                        "cpu_energy_total": 0.004604048997124551,
                        "gpu_energy_total": 0.016683951402705677,
                        "ram_energy_total": 3.115977401760063e-05,
                        "total_energy_kwh": 0.021319160173847825,
                        "total_energy_joules": 76748.97662585217
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21347515915256132,
                        "joules_per_token": 4.684385780386485,
                        "flops_per_joule": 220849472.37514776,
                        "joules_per_flop": 4.527970971564478e-09
                    },
                    "per-process_emissions": [
                        0.00203174889872865,
                        0.0020289425537741664,
                        0.0020334703743436393,
                        0.002027372241380873
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0275": {
            "setup": {
                "experiment_id": "0275",
                "date_time": "April 11, 2025 at 06:00:36 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.33585250400574,
                        "average_latency_ms_per_batch": 4666.981563000718,
                        "throughput_queries_per_sec": 3.428340091772833,
                        "throughput_tokens_per_sec": 438.8275317469226
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2654433280
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0275",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 434.3620597495727,
                            "process_1": 411.34719798140503,
                            "process_0": 381.47250053018615,
                            "process_2": 501.4686197754377
                        },
                        "ram_power": {
                            "process_3": 0.978914737701416,
                            "process_1": 0.9684720039367677,
                            "process_0": 0.9263291358947754,
                            "process_2": 0.9631161689758301
                        },
                        "cpu_energy": {
                            "process_3": 0.0011507648265626357,
                            "process_1": 0.0011498765812810916,
                            "process_0": 0.001153006473781261,
                            "process_2": 0.0011504011154995625
                        },
                        "gpu_energy": {
                            "process_3": 0.004174715006435958,
                            "process_1": 0.00416835027912188,
                            "process_0": 0.004177431397497988,
                            "process_2": 0.004163454719649851
                        },
                        "ram_energy": {
                            "process_3": 7.894359752817906e-06,
                            "process_1": 7.780630801037234e-06,
                            "process_0": 7.4552232046449615e-06,
                            "process_2": 8.02956025910053e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005333374192751411,
                            "process_1": 0.005326007491204007,
                            "process_0": 0.005337893094483894,
                            "process_2": 0.005321885395408513
                        },
                        "total_energy_joules": {
                            "process_3": 19200.14709390508,
                            "process_1": 19173.626968334425,
                            "process_0": 19216.415140142017,
                            "process_2": 19158.787423470647
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 432.1625945091504,
                        "ram_power_avg": 0.9592080116271973,
                        "cpu_energy_total": 0.004604048997124551,
                        "gpu_energy_total": 0.016683951402705677,
                        "ram_energy_total": 3.115977401760063e-05,
                        "total_energy_kwh": 0.021319160173847825,
                        "total_energy_joules": 76748.97662585217
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21347515915256132,
                        "joules_per_token": 4.684385780386485,
                        "flops_per_joule": 220849472.37514776,
                        "joules_per_flop": 4.527970971564478e-09
                    },
                    "per-process_emissions": [
                        0.00203174889872865,
                        0.0020289425537741664,
                        0.0020334703743436393,
                        0.002027372241380873
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0276": {
            "setup": {
                "experiment_id": "0276",
                "date_time": "April 11, 2025 at 06:01:44 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.76314930400258,
                        "average_latency_ms_per_batch": 4595.393663000323,
                        "throughput_queries_per_sec": 3.48174741346395,
                        "throughput_tokens_per_sec": 445.6636689233856
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            90.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2634059776
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0276",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 317.72284398001773,
                            "process_1": 424.1459075645315,
                            "process_3": 300.439741401043,
                            "process_0": 428.73331928440746
                        },
                        "ram_power": {
                            "process_2": 0.9639029502868652,
                            "process_1": 0.9805526733398438,
                            "process_3": 0.9802079200744629,
                            "process_0": 0.9187216758728027
                        },
                        "cpu_energy": {
                            "process_2": 0.0011480774119996793,
                            "process_1": 0.0011428037616561823,
                            "process_3": 0.0011501705511875572,
                            "process_0": 0.0011376870791867761
                        },
                        "gpu_energy": {
                            "process_2": 0.004175321118031983,
                            "process_1": 0.004158018604189906,
                            "process_3": 0.004178019175745951,
                            "process_0": 0.004137261920917845
                        },
                        "ram_energy": {
                            "process_2": 7.735771890792359e-06,
                            "process_1": 7.842225803152288e-06,
                            "process_3": 8.153052790840565e-06,
                            "process_0": 7.348001389662209e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0053311343019224544,
                            "process_1": 0.005308664591649239,
                            "process_3": 0.005336342779724348,
                            "process_0": 0.005282297001494285
                        },
                        "total_energy_joules": {
                            "process_2": 19192.083486920837,
                            "process_1": 19111.192529937263,
                            "process_3": 19210.834007007652,
                            "process_0": 19016.269205379424
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 367.76045305749994,
                        "ram_power_avg": 0.9608463048934937,
                        "cpu_energy_total": 0.0045787388040301955,
                        "gpu_energy_total": 0.016648620818885684,
                        "ram_energy_total": 3.1079051874447424e-05,
                        "total_energy_kwh": 0.02125843867479033,
                        "total_energy_joules": 76530.37922924518
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2140849184991239,
                        "joules_per_token": 4.671043654128734,
                        "flops_per_joule": 221480295.3266272,
                        "joules_per_flop": 4.515074347924514e-09
                    },
                    "per-process_emissions": [
                        0.002030895612317359,
                        0.002022335776188778,
                        0.0020328797819359904,
                        0.0020122910427192477
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0276": {
            "setup": {
                "experiment_id": "0276",
                "date_time": "April 11, 2025 at 06:01:44 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.76314930400258,
                        "average_latency_ms_per_batch": 4595.393663000323,
                        "throughput_queries_per_sec": 3.48174741346395,
                        "throughput_tokens_per_sec": 445.6636689233856
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            90.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2634059776
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0276",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 317.72284398001773,
                            "process_1": 424.1459075645315,
                            "process_3": 300.439741401043,
                            "process_0": 428.73331928440746
                        },
                        "ram_power": {
                            "process_2": 0.9639029502868652,
                            "process_1": 0.9805526733398438,
                            "process_3": 0.9802079200744629,
                            "process_0": 0.9187216758728027
                        },
                        "cpu_energy": {
                            "process_2": 0.0011480774119996793,
                            "process_1": 0.0011428037616561823,
                            "process_3": 0.0011501705511875572,
                            "process_0": 0.0011376870791867761
                        },
                        "gpu_energy": {
                            "process_2": 0.004175321118031983,
                            "process_1": 0.004158018604189906,
                            "process_3": 0.004178019175745951,
                            "process_0": 0.004137261920917845
                        },
                        "ram_energy": {
                            "process_2": 7.735771890792359e-06,
                            "process_1": 7.842225803152288e-06,
                            "process_3": 8.153052790840565e-06,
                            "process_0": 7.348001389662209e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0053311343019224544,
                            "process_1": 0.005308664591649239,
                            "process_3": 0.005336342779724348,
                            "process_0": 0.005282297001494285
                        },
                        "total_energy_joules": {
                            "process_2": 19192.083486920837,
                            "process_1": 19111.192529937263,
                            "process_3": 19210.834007007652,
                            "process_0": 19016.269205379424
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 367.76045305749994,
                        "ram_power_avg": 0.9608463048934937,
                        "cpu_energy_total": 0.0045787388040301955,
                        "gpu_energy_total": 0.016648620818885684,
                        "ram_energy_total": 3.1079051874447424e-05,
                        "total_energy_kwh": 0.02125843867479033,
                        "total_energy_joules": 76530.37922924518
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2140849184991239,
                        "joules_per_token": 4.671043654128734,
                        "flops_per_joule": 221480295.3266272,
                        "joules_per_flop": 4.515074347924514e-09
                    },
                    "per-process_emissions": [
                        0.002030895612317359,
                        0.002022335776188778,
                        0.0020328797819359904,
                        0.0020122910427192477
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0277": {
            "setup": {
                "experiment_id": "0277",
                "date_time": "April 11, 2025 at 06:02:51 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.19848235799873,
                        "average_latency_ms_per_batch": 4649.810294749841,
                        "throughput_queries_per_sec": 3.441000596963235,
                        "throughput_tokens_per_sec": 440.4480764112941
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2631974912
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0277",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 350.0648396550653,
                            "process_3": 391.0208910972416,
                            "process_1": 379.261540691146,
                            "process_0": 471.585161949903
                        },
                        "ram_power": {
                            "process_2": 0.9779720306396485,
                            "process_3": 0.9683332443237305,
                            "process_1": 0.9839272499084474,
                            "process_0": 0.9184012413024902
                        },
                        "cpu_energy": {
                            "process_2": 0.0011692422930624387,
                            "process_3": 0.001157963701937319,
                            "process_1": 0.0011585036445931108,
                            "process_0": 0.001148053538374825
                        },
                        "gpu_energy": {
                            "process_2": 0.004187036405181865,
                            "process_3": 0.0041543449901400575,
                            "process_1": 0.0041543449901400575,
                            "process_0": 0.004125951356314106
                        },
                        "ram_energy": {
                            "process_2": 7.989454975062238e-06,
                            "process_3": 7.864298513902094e-06,
                            "process_1": 7.979446297674218e-06,
                            "process_0": 7.374710589506064e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005364268153219368,
                            "process_3": 0.005320172990591278,
                            "process_1": 0.0053208280810308406,
                            "process_0": 0.005281379605278437
                        },
                        "total_energy_joules": {
                            "process_2": 19311.365351589724,
                            "process_3": 19152.6227661286,
                            "process_1": 19154.981091711026,
                            "process_0": 19012.966579002372
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 397.983108348339,
                        "ram_power_avg": 0.9621584415435791,
                        "cpu_energy_total": 0.0046337631779676935,
                        "gpu_energy_total": 0.016621677741776086,
                        "ram_energy_total": 3.1207910376144615e-05,
                        "total_energy_kwh": 0.021286648830119926,
                        "total_energy_joules": 76631.93578843173
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21380120221983626,
                        "joules_per_token": 4.6772421745868975,
                        "flops_per_joule": 221186778.31587216,
                        "joules_per_flop": 4.521065895593095e-09
                    },
                    "per-process_emissions": [
                        0.002043517952968918,
                        0.0020267199007657473,
                        0.0020269694574686988,
                        0.0020119415606308206
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0277": {
            "setup": {
                "experiment_id": "0277",
                "date_time": "April 11, 2025 at 06:02:51 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.19848235799873,
                        "average_latency_ms_per_batch": 4649.810294749841,
                        "throughput_queries_per_sec": 3.441000596963235,
                        "throughput_tokens_per_sec": 440.4480764112941
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2631974912
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0277",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 350.0648396550653,
                            "process_3": 391.0208910972416,
                            "process_1": 379.261540691146,
                            "process_0": 471.585161949903
                        },
                        "ram_power": {
                            "process_2": 0.9779720306396485,
                            "process_3": 0.9683332443237305,
                            "process_1": 0.9839272499084474,
                            "process_0": 0.9184012413024902
                        },
                        "cpu_energy": {
                            "process_2": 0.0011692422930624387,
                            "process_3": 0.001157963701937319,
                            "process_1": 0.0011585036445931108,
                            "process_0": 0.001148053538374825
                        },
                        "gpu_energy": {
                            "process_2": 0.004187036405181865,
                            "process_3": 0.0041543449901400575,
                            "process_1": 0.0041543449901400575,
                            "process_0": 0.004125951356314106
                        },
                        "ram_energy": {
                            "process_2": 7.989454975062238e-06,
                            "process_3": 7.864298513902094e-06,
                            "process_1": 7.979446297674218e-06,
                            "process_0": 7.374710589506064e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005364268153219368,
                            "process_3": 0.005320172990591278,
                            "process_1": 0.0053208280810308406,
                            "process_0": 0.005281379605278437
                        },
                        "total_energy_joules": {
                            "process_2": 19311.365351589724,
                            "process_3": 19152.6227661286,
                            "process_1": 19154.981091711026,
                            "process_0": 19012.966579002372
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 397.983108348339,
                        "ram_power_avg": 0.9621584415435791,
                        "cpu_energy_total": 0.0046337631779676935,
                        "gpu_energy_total": 0.016621677741776086,
                        "ram_energy_total": 3.1207910376144615e-05,
                        "total_energy_kwh": 0.021286648830119926,
                        "total_energy_joules": 76631.93578843173
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21380120221983626,
                        "joules_per_token": 4.6772421745868975,
                        "flops_per_joule": 221186778.31587216,
                        "joules_per_flop": 4.521065895593095e-09
                    },
                    "per-process_emissions": [
                        0.002043517952968918,
                        0.0020267199007657473,
                        0.0020269694574686988,
                        0.0020119415606308206
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0278": {
            "setup": {
                "experiment_id": "0278",
                "date_time": "April 11, 2025 at 06:03:57 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.81271887699768,
                        "average_latency_ms_per_batch": 4601.58985962471,
                        "throughput_queries_per_sec": 3.4770591226278706,
                        "throughput_tokens_per_sec": 445.06356769636744
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2703769600
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0278",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 413.6240594844003,
                            "process_2": 447.8251637355245,
                            "process_0": 454.83248639068955,
                            "process_3": 1022.4405567343617
                        },
                        "ram_power": {
                            "process_1": 0.9545917510986328,
                            "process_2": 0.9766001701354982,
                            "process_0": 0.9440088272094727,
                            "process_3": 0.9763154983520508
                        },
                        "cpu_energy": {
                            "process_1": 0.001140385249687597,
                            "process_2": 0.0011363787139999889,
                            "process_0": 0.0011379269339372511,
                            "process_3": 0.0011451246778126463
                        },
                        "gpu_energy": {
                            "process_1": 0.004149195263798144,
                            "process_2": 0.004148574707746211,
                            "process_0": 0.0041476305403241875,
                            "process_3": 0.004171230281426075
                        },
                        "ram_energy": {
                            "process_1": 7.615768229901602e-06,
                            "process_2": 7.793453759849032e-06,
                            "process_0": 7.528873857160262e-06,
                            "process_3": 8.04194605492694e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0052971962817156435,
                            "process_2": 0.0052927468755060506,
                            "process_0": 0.005293086348118598,
                            "process_3": 0.005324396905293647
                        },
                        "total_energy_joules": {
                            "process_1": 19069.906614176318,
                            "process_2": 19053.88875182178,
                            "process_0": 19055.110853226954,
                            "process_3": 19167.82885905713
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 584.680566586244,
                        "ram_power_avg": 0.9628790616989136,
                        "cpu_energy_total": 0.004559815575437483,
                        "gpu_energy_total": 0.016616630793294618,
                        "ram_energy_total": 3.098004190183784e-05,
                        "total_energy_kwh": 0.02120742641063394,
                        "total_energy_joules": 76346.73507828219
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21459987756124282,
                        "joules_per_token": 4.659834904680309,
                        "flops_per_joule": 222013043.20050272,
                        "joules_per_flop": 4.504239866199607e-09
                    },
                    "per-process_emissions": [
                        0.0020179669235195746,
                        0.00201627192222403,
                        0.00201640124431578,
                        0.0020283290010716147
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0278": {
            "setup": {
                "experiment_id": "0278",
                "date_time": "April 11, 2025 at 06:03:57 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.81271887699768,
                        "average_latency_ms_per_batch": 4601.58985962471,
                        "throughput_queries_per_sec": 3.4770591226278706,
                        "throughput_tokens_per_sec": 445.06356769636744
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2703769600
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0278",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 413.6240594844003,
                            "process_2": 447.8251637355245,
                            "process_0": 454.83248639068955,
                            "process_3": 1022.4405567343617
                        },
                        "ram_power": {
                            "process_1": 0.9545917510986328,
                            "process_2": 0.9766001701354982,
                            "process_0": 0.9440088272094727,
                            "process_3": 0.9763154983520508
                        },
                        "cpu_energy": {
                            "process_1": 0.001140385249687597,
                            "process_2": 0.0011363787139999889,
                            "process_0": 0.0011379269339372511,
                            "process_3": 0.0011451246778126463
                        },
                        "gpu_energy": {
                            "process_1": 0.004149195263798144,
                            "process_2": 0.004148574707746211,
                            "process_0": 0.0041476305403241875,
                            "process_3": 0.004171230281426075
                        },
                        "ram_energy": {
                            "process_1": 7.615768229901602e-06,
                            "process_2": 7.793453759849032e-06,
                            "process_0": 7.528873857160262e-06,
                            "process_3": 8.04194605492694e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0052971962817156435,
                            "process_2": 0.0052927468755060506,
                            "process_0": 0.005293086348118598,
                            "process_3": 0.005324396905293647
                        },
                        "total_energy_joules": {
                            "process_1": 19069.906614176318,
                            "process_2": 19053.88875182178,
                            "process_0": 19055.110853226954,
                            "process_3": 19167.82885905713
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 584.680566586244,
                        "ram_power_avg": 0.9628790616989136,
                        "cpu_energy_total": 0.004559815575437483,
                        "gpu_energy_total": 0.016616630793294618,
                        "ram_energy_total": 3.098004190183784e-05,
                        "total_energy_kwh": 0.02120742641063394,
                        "total_energy_joules": 76346.73507828219
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21459987756124282,
                        "joules_per_token": 4.659834904680309,
                        "flops_per_joule": 222013043.20050272,
                        "joules_per_flop": 4.504239866199607e-09
                    },
                    "per-process_emissions": [
                        0.0020179669235195746,
                        0.00201627192222403,
                        0.00201640124431578,
                        0.0020283290010716147
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0279": {
            "setup": {
                "experiment_id": "0279",
                "date_time": "April 11, 2025 at 06:05:03 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.28330895399631,
                        "average_latency_ms_per_batch": 4660.413619249539,
                        "throughput_queries_per_sec": 3.4331716682641704,
                        "throughput_tokens_per_sec": 439.4459735378138
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2629898240
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0279",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 422.8693763937466,
                            "process_1": 376.8979194023992,
                            "process_2": 382.7956301925779,
                            "process_3": 436.9669319490996
                        },
                        "ram_power": {
                            "process_0": 0.9174814224243164,
                            "process_1": 0.9859127998352051,
                            "process_2": 0.9769034385681153,
                            "process_3": 0.9683661460876466
                        },
                        "cpu_energy": {
                            "process_0": 0.001153804084062585,
                            "process_1": 0.0011535109478744518,
                            "process_2": 0.0011535748373446497,
                            "process_3": 0.0011506056293127361
                        },
                        "gpu_energy": {
                            "process_0": 0.004153346933786051,
                            "process_1": 0.004163130830502237,
                            "process_2": 0.004153346933786051,
                            "process_3": 0.00414239442502401
                        },
                        "ram_energy": {
                            "process_0": 7.391775100900008e-06,
                            "process_1": 7.957724981872031e-06,
                            "process_2": 7.874720937919937e-06,
                            "process_3": 7.815817816323905e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005314542792949537,
                            "process_1": 0.005324599503358562,
                            "process_2": 0.005314796492068617,
                            "process_3": 0.00530081587215307
                        },
                        "total_energy_joules": {
                            "process_0": 19132.354054618332,
                            "process_1": 19168.558212090822,
                            "process_2": 19133.267371447022,
                            "process_3": 19082.93713975105
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 404.8824644844558,
                        "ram_power_avg": 0.9621659517288208,
                        "cpu_energy_total": 0.004611495498594423,
                        "gpu_energy_total": 0.01661221912309835,
                        "ram_energy_total": 3.104003883701588e-05,
                        "total_energy_kwh": 0.021254754660529784,
                        "total_energy_joules": 76517.11677790723
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2141220251091655,
                        "joules_per_token": 4.670234178339064,
                        "flops_per_joule": 221518683.75215572,
                        "joules_per_flop": 4.514291901078833e-09
                    },
                    "per-process_emissions": [
                        0.002024575076974126,
                        0.0020284061808044443,
                        0.0020246717236535395,
                        0.002019345806496712
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0279": {
            "setup": {
                "experiment_id": "0279",
                "date_time": "April 11, 2025 at 06:05:03 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.28330895399631,
                        "average_latency_ms_per_batch": 4660.413619249539,
                        "throughput_queries_per_sec": 3.4331716682641704,
                        "throughput_tokens_per_sec": 439.4459735378138
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2629898240
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0279",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 422.8693763937466,
                            "process_1": 376.8979194023992,
                            "process_2": 382.7956301925779,
                            "process_3": 436.9669319490996
                        },
                        "ram_power": {
                            "process_0": 0.9174814224243164,
                            "process_1": 0.9859127998352051,
                            "process_2": 0.9769034385681153,
                            "process_3": 0.9683661460876466
                        },
                        "cpu_energy": {
                            "process_0": 0.001153804084062585,
                            "process_1": 0.0011535109478744518,
                            "process_2": 0.0011535748373446497,
                            "process_3": 0.0011506056293127361
                        },
                        "gpu_energy": {
                            "process_0": 0.004153346933786051,
                            "process_1": 0.004163130830502237,
                            "process_2": 0.004153346933786051,
                            "process_3": 0.00414239442502401
                        },
                        "ram_energy": {
                            "process_0": 7.391775100900008e-06,
                            "process_1": 7.957724981872031e-06,
                            "process_2": 7.874720937919937e-06,
                            "process_3": 7.815817816323905e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005314542792949537,
                            "process_1": 0.005324599503358562,
                            "process_2": 0.005314796492068617,
                            "process_3": 0.00530081587215307
                        },
                        "total_energy_joules": {
                            "process_0": 19132.354054618332,
                            "process_1": 19168.558212090822,
                            "process_2": 19133.267371447022,
                            "process_3": 19082.93713975105
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 404.8824644844558,
                        "ram_power_avg": 0.9621659517288208,
                        "cpu_energy_total": 0.004611495498594423,
                        "gpu_energy_total": 0.01661221912309835,
                        "ram_energy_total": 3.104003883701588e-05,
                        "total_energy_kwh": 0.021254754660529784,
                        "total_energy_joules": 76517.11677790723
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2141220251091655,
                        "joules_per_token": 4.670234178339064,
                        "flops_per_joule": 221518683.75215572,
                        "joules_per_flop": 4.514291901078833e-09
                    },
                    "per-process_emissions": [
                        0.002024575076974126,
                        0.0020284061808044443,
                        0.0020246717236535395,
                        0.002019345806496712
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0280": {
            "setup": {
                "experiment_id": "0280",
                "date_time": "April 11, 2025 at 06:06:10 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.419533296004374,
                        "average_latency_ms_per_batch": 4677.441662000547,
                        "throughput_queries_per_sec": 3.4206733415798034,
                        "throughput_tokens_per_sec": 437.84618772221484
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2654392320
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0280",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 426.8935411988708,
                            "process_0": 350.1724396053948,
                            "process_1": 351.5819515772871,
                            "process_3": 367.01962533684315
                        },
                        "ram_power": {
                            "process_2": 0.9781723022460939,
                            "process_0": 0.9265480041503906,
                            "process_1": 0.9841747283935547,
                            "process_3": 0.9721755981445312
                        },
                        "cpu_energy": {
                            "process_2": 0.001132908505374985,
                            "process_0": 0.001157870492749794,
                            "process_1": 0.0011586672065310494,
                            "process_3": 0.001150461225500294
                        },
                        "gpu_energy": {
                            "process_2": 0.004093725774977963,
                            "process_0": 0.00415992805016191,
                            "process_1": 0.0041679983343960725,
                            "process_3": 0.00414422498204392
                        },
                        "ram_energy": {
                            "process_2": 7.765172540052163e-06,
                            "process_0": 7.537792210591916e-06,
                            "process_1": 8.035795845975523e-06,
                            "process_3": 7.805452698689464e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005234399452892999,
                            "process_0": 0.005325336335122296,
                            "process_1": 0.005334701336773098,
                            "process_3": 0.005302491660242905
                        },
                        "total_energy_joules": {
                            "process_2": 18843.838030414798,
                            "process_0": 19171.210806440267,
                            "process_1": 19204.924812383153,
                            "process_3": 19088.969976874458
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 373.91688942959894,
                        "ram_power_avg": 0.9652676582336426,
                        "cpu_energy_total": 0.004599907430156123,
                        "gpu_energy_total": 0.016565877141579866,
                        "ram_energy_total": 3.1144213295309067e-05,
                        "total_energy_kwh": 0.021196928785031297,
                        "total_energy_joules": 76308.94362611268
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21470615659778897,
                        "joules_per_token": 4.657528297492229,
                        "flops_per_joule": 222122993.55369103,
                        "joules_per_flop": 4.502010278185281e-09
                    },
                    "per-process_emissions": [
                        0.001994044471579588,
                        0.0020286868768648386,
                        0.002032254474243712,
                        0.0020199841979695347
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0280": {
            "setup": {
                "experiment_id": "0280",
                "date_time": "April 11, 2025 at 06:06:10 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.419533296004374,
                        "average_latency_ms_per_batch": 4677.441662000547,
                        "throughput_queries_per_sec": 3.4206733415798034,
                        "throughput_tokens_per_sec": 437.84618772221484
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2654392320
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0280",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 426.8935411988708,
                            "process_0": 350.1724396053948,
                            "process_1": 351.5819515772871,
                            "process_3": 367.01962533684315
                        },
                        "ram_power": {
                            "process_2": 0.9781723022460939,
                            "process_0": 0.9265480041503906,
                            "process_1": 0.9841747283935547,
                            "process_3": 0.9721755981445312
                        },
                        "cpu_energy": {
                            "process_2": 0.001132908505374985,
                            "process_0": 0.001157870492749794,
                            "process_1": 0.0011586672065310494,
                            "process_3": 0.001150461225500294
                        },
                        "gpu_energy": {
                            "process_2": 0.004093725774977963,
                            "process_0": 0.00415992805016191,
                            "process_1": 0.0041679983343960725,
                            "process_3": 0.00414422498204392
                        },
                        "ram_energy": {
                            "process_2": 7.765172540052163e-06,
                            "process_0": 7.537792210591916e-06,
                            "process_1": 8.035795845975523e-06,
                            "process_3": 7.805452698689464e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005234399452892999,
                            "process_0": 0.005325336335122296,
                            "process_1": 0.005334701336773098,
                            "process_3": 0.005302491660242905
                        },
                        "total_energy_joules": {
                            "process_2": 18843.838030414798,
                            "process_0": 19171.210806440267,
                            "process_1": 19204.924812383153,
                            "process_3": 19088.969976874458
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 373.91688942959894,
                        "ram_power_avg": 0.9652676582336426,
                        "cpu_energy_total": 0.004599907430156123,
                        "gpu_energy_total": 0.016565877141579866,
                        "ram_energy_total": 3.1144213295309067e-05,
                        "total_energy_kwh": 0.021196928785031297,
                        "total_energy_joules": 76308.94362611268
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21470615659778897,
                        "joules_per_token": 4.657528297492229,
                        "flops_per_joule": 222122993.55369103,
                        "joules_per_flop": 4.502010278185281e-09
                    },
                    "per-process_emissions": [
                        0.001994044471579588,
                        0.0020286868768648386,
                        0.002032254474243712,
                        0.0020199841979695347
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0281": {
            "setup": {
                "experiment_id": "0281",
                "date_time": "April 11, 2025 at 06:07:16 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.42052311400039,
                        "average_latency_ms_per_batch": 4677.565389250049,
                        "throughput_queries_per_sec": 3.4205828606417987,
                        "throughput_tokens_per_sec": 437.83460616215024
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2698768384
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0281",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 320.67068582389214,
                            "process_1": 430.5514573826835,
                            "process_2": 408.89376410486153,
                            "process_0": 332.57153970109135
                        },
                        "ram_power": {
                            "process_3": 0.967979907989502,
                            "process_1": 0.9679012298583984,
                            "process_2": 0.9617486000061035,
                            "process_0": 0.9417486190795898
                        },
                        "cpu_energy": {
                            "process_3": 0.0011571986057813321,
                            "process_1": 0.0011403549195941882,
                            "process_2": 0.0011483026930942512,
                            "process_0": 0.0011573727785626033
                        },
                        "gpu_energy": {
                            "process_3": 0.00416469638730993,
                            "process_1": 0.00411517773658393,
                            "process_2": 0.004135424141669952,
                            "process_0": 0.004161735551607981
                        },
                        "ram_energy": {
                            "process_3": 7.81887350919857e-06,
                            "process_1": 7.757410907263172e-06,
                            "process_2": 7.96883740943101e-06,
                            "process_0": 7.6058170166688375e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005329713866600463,
                            "process_1": 0.005263290067085384,
                            "process_2": 0.005291695672173636,
                            "process_0": 0.005326714147187251
                        },
                        "total_energy_joules": {
                            "process_3": 19186.96991976167,
                            "process_1": 18947.84424150738,
                            "process_2": 19050.10441982509,
                            "process_0": 19176.170929874104
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 373.17186175313213,
                        "ram_power_avg": 0.9598445892333984,
                        "cpu_energy_total": 0.004603228997032375,
                        "gpu_energy_total": 0.016577033817171793,
                        "ram_energy_total": 3.1150938842561587e-05,
                        "total_energy_kwh": 0.021211413753046734,
                        "total_energy_joules": 76361.08951096825
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21455953686525986,
                        "joules_per_token": 4.660711029722183,
                        "flops_per_joule": 221971308.9703541,
                        "joules_per_flop": 4.505086736833891e-09
                    },
                    "per-process_emissions": [
                        0.0020303544974814465,
                        0.0020050503510561767,
                        0.0020158714663145467,
                        0.002029211754370983
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0281": {
            "setup": {
                "experiment_id": "0281",
                "date_time": "April 11, 2025 at 06:07:16 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.42052311400039,
                        "average_latency_ms_per_batch": 4677.565389250049,
                        "throughput_queries_per_sec": 3.4205828606417987,
                        "throughput_tokens_per_sec": 437.83460616215024
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2698768384
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0281",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 320.67068582389214,
                            "process_1": 430.5514573826835,
                            "process_2": 408.89376410486153,
                            "process_0": 332.57153970109135
                        },
                        "ram_power": {
                            "process_3": 0.967979907989502,
                            "process_1": 0.9679012298583984,
                            "process_2": 0.9617486000061035,
                            "process_0": 0.9417486190795898
                        },
                        "cpu_energy": {
                            "process_3": 0.0011571986057813321,
                            "process_1": 0.0011403549195941882,
                            "process_2": 0.0011483026930942512,
                            "process_0": 0.0011573727785626033
                        },
                        "gpu_energy": {
                            "process_3": 0.00416469638730993,
                            "process_1": 0.00411517773658393,
                            "process_2": 0.004135424141669952,
                            "process_0": 0.004161735551607981
                        },
                        "ram_energy": {
                            "process_3": 7.81887350919857e-06,
                            "process_1": 7.757410907263172e-06,
                            "process_2": 7.96883740943101e-06,
                            "process_0": 7.6058170166688375e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005329713866600463,
                            "process_1": 0.005263290067085384,
                            "process_2": 0.005291695672173636,
                            "process_0": 0.005326714147187251
                        },
                        "total_energy_joules": {
                            "process_3": 19186.96991976167,
                            "process_1": 18947.84424150738,
                            "process_2": 19050.10441982509,
                            "process_0": 19176.170929874104
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 373.17186175313213,
                        "ram_power_avg": 0.9598445892333984,
                        "cpu_energy_total": 0.004603228997032375,
                        "gpu_energy_total": 0.016577033817171793,
                        "ram_energy_total": 3.1150938842561587e-05,
                        "total_energy_kwh": 0.021211413753046734,
                        "total_energy_joules": 76361.08951096825
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21455953686525986,
                        "joules_per_token": 4.660711029722183,
                        "flops_per_joule": 221971308.9703541,
                        "joules_per_flop": 4.505086736833891e-09
                    },
                    "per-process_emissions": [
                        0.0020303544974814465,
                        0.0020050503510561767,
                        0.0020158714663145467,
                        0.002029211754370983
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0282": {
            "setup": {
                "experiment_id": "0282",
                "date_time": "April 11, 2025 at 06:08:21 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.23992333899514,
                        "average_latency_ms_per_batch": 4654.990417374393,
                        "throughput_queries_per_sec": 3.437171415064838,
                        "throughput_tokens_per_sec": 439.95794112829924
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2679017472
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0282",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 315.6121738236365,
                            "process_1": 555.2209120207334,
                            "process_3": 1022.682414082239,
                            "process_2": 433.4740750712552
                        },
                        "ram_power": {
                            "process_0": 0.9347434043884277,
                            "process_1": 0.9530611038208008,
                            "process_3": 0.9802980422973632,
                            "process_2": 0.9856696128845216
                        },
                        "cpu_energy": {
                            "process_0": 0.0011487739449371473,
                            "process_1": 0.0011471098759997177,
                            "process_3": 0.0011465595788748713,
                            "process_2": 0.0011347920952812275
                        },
                        "gpu_energy": {
                            "process_0": 0.004161173606713975,
                            "process_1": 0.004151583043486007,
                            "process_3": 0.004151583043486007,
                            "process_2": 0.00410628189613399
                        },
                        "ram_energy": {
                            "process_0": 7.524246030854335e-06,
                            "process_1": 7.876099234077217e-06,
                            "process_3": 8.119056745404707e-06,
                            "process_2": 7.837070380787701e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0053174717976819785,
                            "process_1": 0.0053065690187198,
                            "process_3": 0.005306261679106286,
                            "process_2": 0.005248911061796005
                        },
                        "total_energy_joules": {
                            "process_0": 19142.898471655124,
                            "process_1": 19103.64846739128,
                            "process_3": 19102.54204478263,
                            "process_2": 18896.079822465617
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 581.747393749466,
                        "ram_power_avg": 0.9634430408477783,
                        "cpu_energy_total": 0.004577235495092964,
                        "gpu_energy_total": 0.016570621589819978,
                        "ram_energy_total": 3.1356472391123956e-05,
                        "total_energy_kwh": 0.021179213557304066,
                        "total_energy_joules": 76245.16880629465
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21488574629068655,
                        "joules_per_token": 4.65363579139982,
                        "flops_per_joule": 222308787.01592755,
                        "joules_per_flop": 4.4982477455034375e-09
                    },
                    "per-process_emissions": [
                        0.00202569088132695,
                        0.0020215374676813076,
                        0.0020214203866555395,
                        0.001999572668991188
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0282": {
            "setup": {
                "experiment_id": "0282",
                "date_time": "April 11, 2025 at 06:08:21 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.23992333899514,
                        "average_latency_ms_per_batch": 4654.990417374393,
                        "throughput_queries_per_sec": 3.437171415064838,
                        "throughput_tokens_per_sec": 439.95794112829924
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2679017472
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0282",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 315.6121738236365,
                            "process_1": 555.2209120207334,
                            "process_3": 1022.682414082239,
                            "process_2": 433.4740750712552
                        },
                        "ram_power": {
                            "process_0": 0.9347434043884277,
                            "process_1": 0.9530611038208008,
                            "process_3": 0.9802980422973632,
                            "process_2": 0.9856696128845216
                        },
                        "cpu_energy": {
                            "process_0": 0.0011487739449371473,
                            "process_1": 0.0011471098759997177,
                            "process_3": 0.0011465595788748713,
                            "process_2": 0.0011347920952812275
                        },
                        "gpu_energy": {
                            "process_0": 0.004161173606713975,
                            "process_1": 0.004151583043486007,
                            "process_3": 0.004151583043486007,
                            "process_2": 0.00410628189613399
                        },
                        "ram_energy": {
                            "process_0": 7.524246030854335e-06,
                            "process_1": 7.876099234077217e-06,
                            "process_3": 8.119056745404707e-06,
                            "process_2": 7.837070380787701e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0053174717976819785,
                            "process_1": 0.0053065690187198,
                            "process_3": 0.005306261679106286,
                            "process_2": 0.005248911061796005
                        },
                        "total_energy_joules": {
                            "process_0": 19142.898471655124,
                            "process_1": 19103.64846739128,
                            "process_3": 19102.54204478263,
                            "process_2": 18896.079822465617
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 581.747393749466,
                        "ram_power_avg": 0.9634430408477783,
                        "cpu_energy_total": 0.004577235495092964,
                        "gpu_energy_total": 0.016570621589819978,
                        "ram_energy_total": 3.1356472391123956e-05,
                        "total_energy_kwh": 0.021179213557304066,
                        "total_energy_joules": 76245.16880629465
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21488574629068655,
                        "joules_per_token": 4.65363579139982,
                        "flops_per_joule": 222308787.01592755,
                        "joules_per_flop": 4.4982477455034375e-09
                    },
                    "per-process_emissions": [
                        0.00202569088132695,
                        0.0020215374676813076,
                        0.0020214203866555395,
                        0.001999572668991188
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0283": {
            "setup": {
                "experiment_id": "0283",
                "date_time": "April 11, 2025 at 06:09:27 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.278837409001426,
                        "average_latency_ms_per_batch": 4659.854676125178,
                        "throughput_queries_per_sec": 3.433583472458099,
                        "throughput_tokens_per_sec": 439.49868447463666
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            10.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2653843456
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0283",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 437.56591252634587,
                            "process_1": 380.30606111624434,
                            "process_3": 449.5710395670026,
                            "process_0": 317.36318218404375
                        },
                        "ram_power": {
                            "process_2": 0.9806957244873048,
                            "process_1": 0.986286163330078,
                            "process_3": 0.9601478576660156,
                            "process_0": 0.9263577461242676
                        },
                        "cpu_energy": {
                            "process_2": 0.0011416736117503203,
                            "process_1": 0.0011530402771880973,
                            "process_3": 0.0011334216451250542,
                            "process_0": 0.00115348654937452
                        },
                        "gpu_energy": {
                            "process_2": 0.004123862743531881,
                            "process_1": 0.0041568222143439915,
                            "process_3": 0.004102067170540125,
                            "process_0": 0.004154977212868061
                        },
                        "ram_energy": {
                            "process_2": 7.872162720309353e-06,
                            "process_1": 7.965184633750345e-06,
                            "process_3": 7.645092022755634e-06,
                            "process_0": 7.456374845608522e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005273408518002509,
                            "process_1": 0.0053178276761658375,
                            "process_3": 0.005243133907687937,
                            "process_0": 0.005315920137088188
                        },
                        "total_energy_joules": {
                            "process_2": 18984.27066480903,
                            "process_1": 19144.179634197015,
                            "process_3": 18875.282067676573,
                            "process_0": 19137.31249351748
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 396.2015488484091,
                        "ram_power_avg": 0.9633718729019165,
                        "cpu_energy_total": 0.004581622083437992,
                        "gpu_energy_total": 0.016537729341284058,
                        "ram_energy_total": 3.093881422242385e-05,
                        "total_energy_kwh": 0.02115029023894447,
                        "total_energy_joules": 76141.04486020008
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2151796055607339,
                        "joules_per_token": 4.6472805700805715,
                        "flops_per_joule": 222612797.39821342,
                        "joules_per_flop": 4.492104729321485e-09
                    },
                    "per-process_emissions": [
                        0.002008904974933056,
                        0.002025826453235376,
                        0.0019973718621337194,
                        0.0020250997762237453
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0283": {
            "setup": {
                "experiment_id": "0283",
                "date_time": "April 11, 2025 at 06:09:27 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.278837409001426,
                        "average_latency_ms_per_batch": 4659.854676125178,
                        "throughput_queries_per_sec": 3.433583472458099,
                        "throughput_tokens_per_sec": 439.49868447463666
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            10.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2653843456
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0283",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 437.56591252634587,
                            "process_1": 380.30606111624434,
                            "process_3": 449.5710395670026,
                            "process_0": 317.36318218404375
                        },
                        "ram_power": {
                            "process_2": 0.9806957244873048,
                            "process_1": 0.986286163330078,
                            "process_3": 0.9601478576660156,
                            "process_0": 0.9263577461242676
                        },
                        "cpu_energy": {
                            "process_2": 0.0011416736117503203,
                            "process_1": 0.0011530402771880973,
                            "process_3": 0.0011334216451250542,
                            "process_0": 0.00115348654937452
                        },
                        "gpu_energy": {
                            "process_2": 0.004123862743531881,
                            "process_1": 0.0041568222143439915,
                            "process_3": 0.004102067170540125,
                            "process_0": 0.004154977212868061
                        },
                        "ram_energy": {
                            "process_2": 7.872162720309353e-06,
                            "process_1": 7.965184633750345e-06,
                            "process_3": 7.645092022755634e-06,
                            "process_0": 7.456374845608522e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005273408518002509,
                            "process_1": 0.0053178276761658375,
                            "process_3": 0.005243133907687937,
                            "process_0": 0.005315920137088188
                        },
                        "total_energy_joules": {
                            "process_2": 18984.27066480903,
                            "process_1": 19144.179634197015,
                            "process_3": 18875.282067676573,
                            "process_0": 19137.31249351748
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 396.2015488484091,
                        "ram_power_avg": 0.9633718729019165,
                        "cpu_energy_total": 0.004581622083437992,
                        "gpu_energy_total": 0.016537729341284058,
                        "ram_energy_total": 3.093881422242385e-05,
                        "total_energy_kwh": 0.02115029023894447,
                        "total_energy_joules": 76141.04486020008
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2151796055607339,
                        "joules_per_token": 4.6472805700805715,
                        "flops_per_joule": 222612797.39821342,
                        "joules_per_flop": 4.492104729321485e-09
                    },
                    "per-process_emissions": [
                        0.002008904974933056,
                        0.002025826453235376,
                        0.0019973718621337194,
                        0.0020250997762237453
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0284": {
            "setup": {
                "experiment_id": "0284",
                "date_time": "April 11, 2025 at 06:10:32 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.056080158999976,
                        "average_latency_ms_per_batch": 4632.010019874997,
                        "throughput_queries_per_sec": 3.454223961379036,
                        "throughput_tokens_per_sec": 442.14066705651663
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2680303616
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0284",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 923.6037290130146,
                            "process_1": 314.0525103192753,
                            "process_2": 433.76790434023195,
                            "process_3": 430.8460779738844
                        },
                        "ram_power": {
                            "process_0": 0.9347920417785645,
                            "process_1": 0.9724860191345215,
                            "process_2": 0.9719667434692383,
                            "process_3": 0.9849929809570311
                        },
                        "cpu_energy": {
                            "process_0": 0.0011456778798744833,
                            "process_1": 0.0011495814117816966,
                            "process_2": 0.001141673809218901,
                            "process_3": 0.0011368114448124511
                        },
                        "gpu_energy": {
                            "process_0": 0.0041454574830301905,
                            "process_1": 0.004158076104236019,
                            "process_2": 0.004131220804974134,
                            "process_3": 0.004123397187604239
                        },
                        "ram_energy": {
                            "process_0": 7.72012631360629e-06,
                            "process_1": 7.78838025725957e-06,
                            "process_2": 7.839698041987093e-06,
                            "process_3": 7.87604357585929e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005298855489218278,
                            "process_1": 0.0053154458962749755,
                            "process_2": 0.005280734312235022,
                            "process_3": 0.0052680846759925485
                        },
                        "total_energy_joules": {
                            "process_0": 19075.8797611858,
                            "process_1": 19135.605226589912,
                            "process_2": 19010.64352404608,
                            "process_3": 18965.104833573176
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 525.5675554116016,
                        "ram_power_avg": 0.9660594463348389,
                        "cpu_energy_total": 0.004573744545687533,
                        "gpu_energy_total": 0.016558151579844582,
                        "ram_energy_total": 3.122424818871224e-05,
                        "total_energy_kwh": 0.021163120373720826,
                        "total_energy_joules": 76187.23334539497
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21504915299553015,
                        "joules_per_token": 4.650099691491392,
                        "flops_per_joule": 222477838.46289933,
                        "joules_per_flop": 4.494829718362088e-09
                    },
                    "per-process_emissions": [
                        0.0020185989986177028,
                        0.002024919114185952,
                        0.002011695736245932,
                        0.0020068768573193612
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0284": {
            "setup": {
                "experiment_id": "0284",
                "date_time": "April 11, 2025 at 06:10:32 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.056080158999976,
                        "average_latency_ms_per_batch": 4632.010019874997,
                        "throughput_queries_per_sec": 3.454223961379036,
                        "throughput_tokens_per_sec": 442.14066705651663
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2680303616
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0284",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 923.6037290130146,
                            "process_1": 314.0525103192753,
                            "process_2": 433.76790434023195,
                            "process_3": 430.8460779738844
                        },
                        "ram_power": {
                            "process_0": 0.9347920417785645,
                            "process_1": 0.9724860191345215,
                            "process_2": 0.9719667434692383,
                            "process_3": 0.9849929809570311
                        },
                        "cpu_energy": {
                            "process_0": 0.0011456778798744833,
                            "process_1": 0.0011495814117816966,
                            "process_2": 0.001141673809218901,
                            "process_3": 0.0011368114448124511
                        },
                        "gpu_energy": {
                            "process_0": 0.0041454574830301905,
                            "process_1": 0.004158076104236019,
                            "process_2": 0.004131220804974134,
                            "process_3": 0.004123397187604239
                        },
                        "ram_energy": {
                            "process_0": 7.72012631360629e-06,
                            "process_1": 7.78838025725957e-06,
                            "process_2": 7.839698041987093e-06,
                            "process_3": 7.87604357585929e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005298855489218278,
                            "process_1": 0.0053154458962749755,
                            "process_2": 0.005280734312235022,
                            "process_3": 0.0052680846759925485
                        },
                        "total_energy_joules": {
                            "process_0": 19075.8797611858,
                            "process_1": 19135.605226589912,
                            "process_2": 19010.64352404608,
                            "process_3": 18965.104833573176
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 525.5675554116016,
                        "ram_power_avg": 0.9660594463348389,
                        "cpu_energy_total": 0.004573744545687533,
                        "gpu_energy_total": 0.016558151579844582,
                        "ram_energy_total": 3.122424818871224e-05,
                        "total_energy_kwh": 0.021163120373720826,
                        "total_energy_joules": 76187.23334539497
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21504915299553015,
                        "joules_per_token": 4.650099691491392,
                        "flops_per_joule": 222477838.46289933,
                        "joules_per_flop": 4.494829718362088e-09
                    },
                    "per-process_emissions": [
                        0.0020185989986177028,
                        0.002024919114185952,
                        0.002011695736245932,
                        0.0020068768573193612
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0285": {
            "setup": {
                "experiment_id": "0285",
                "date_time": "April 11, 2025 at 06:11:38 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.89626298499934,
                        "average_latency_ms_per_batch": 4612.032873124917,
                        "throughput_queries_per_sec": 3.469186027106325,
                        "throughput_tokens_per_sec": 444.0558114696096
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            2.0,
                            100.0,
                            98.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2592911360
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0285",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 409.6293127774312,
                            "process_1": 421.616475996957,
                            "process_2": 318.3932440091747,
                            "process_3": 449.4503059932046
                        },
                        "ram_power": {
                            "process_0": 0.9043407440185547,
                            "process_1": 0.9719667434692383,
                            "process_2": 0.9604225158691406,
                            "process_3": 0.9801034927368164
                        },
                        "cpu_energy": {
                            "process_0": 0.0011402216813121411,
                            "process_1": 0.0011408969635003814,
                            "process_2": 0.0011541587551874954,
                            "process_3": 0.0011283666217810834
                        },
                        "gpu_energy": {
                            "process_0": 0.004133836918177924,
                            "process_1": 0.004140392478977906,
                            "process_2": 0.0041648447207617845,
                            "process_3": 0.0041000255022400345
                        },
                        "ram_energy": {
                            "process_0": 7.234196894660289e-06,
                            "process_1": 7.782081287271757e-06,
                            "process_2": 7.75253423581526e-06,
                            "process_3": 7.752935783333107e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005281292796384726,
                            "process_1": 0.0052890715237655615,
                            "process_2": 0.005326756010185095,
                            "process_3": 0.005236145059804451
                        },
                        "total_energy_joules": {
                            "process_0": 19012.654066985015,
                            "process_1": 19040.65748555602,
                            "process_2": 19176.321636666344,
                            "process_3": 18850.12221529602
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 399.7723346941919,
                        "ram_power_avg": 0.9542083740234375,
                        "cpu_energy_total": 0.004563644021781102,
                        "gpu_energy_total": 0.01653909962015765,
                        "ram_energy_total": 3.052174820108041e-05,
                        "total_energy_kwh": 0.021133265390139832,
                        "total_energy_joules": 76079.7554045034
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21535295313305092,
                        "joules_per_token": 4.643539758575647,
                        "flops_per_joule": 222792133.11125705,
                        "joules_per_flop": 4.488488826042273e-09
                    },
                    "per-process_emissions": [
                        0.0020119084907827614,
                        0.0020148717969784907,
                        0.002029227702080012,
                        0.0019947094605325057
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0285": {
            "setup": {
                "experiment_id": "0285",
                "date_time": "April 11, 2025 at 06:11:38 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.89626298499934,
                        "average_latency_ms_per_batch": 4612.032873124917,
                        "throughput_queries_per_sec": 3.469186027106325,
                        "throughput_tokens_per_sec": 444.0558114696096
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            2.0,
                            100.0,
                            98.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2592911360
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0285",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 409.6293127774312,
                            "process_1": 421.616475996957,
                            "process_2": 318.3932440091747,
                            "process_3": 449.4503059932046
                        },
                        "ram_power": {
                            "process_0": 0.9043407440185547,
                            "process_1": 0.9719667434692383,
                            "process_2": 0.9604225158691406,
                            "process_3": 0.9801034927368164
                        },
                        "cpu_energy": {
                            "process_0": 0.0011402216813121411,
                            "process_1": 0.0011408969635003814,
                            "process_2": 0.0011541587551874954,
                            "process_3": 0.0011283666217810834
                        },
                        "gpu_energy": {
                            "process_0": 0.004133836918177924,
                            "process_1": 0.004140392478977906,
                            "process_2": 0.0041648447207617845,
                            "process_3": 0.0041000255022400345
                        },
                        "ram_energy": {
                            "process_0": 7.234196894660289e-06,
                            "process_1": 7.782081287271757e-06,
                            "process_2": 7.75253423581526e-06,
                            "process_3": 7.752935783333107e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005281292796384726,
                            "process_1": 0.0052890715237655615,
                            "process_2": 0.005326756010185095,
                            "process_3": 0.005236145059804451
                        },
                        "total_energy_joules": {
                            "process_0": 19012.654066985015,
                            "process_1": 19040.65748555602,
                            "process_2": 19176.321636666344,
                            "process_3": 18850.12221529602
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 399.7723346941919,
                        "ram_power_avg": 0.9542083740234375,
                        "cpu_energy_total": 0.004563644021781102,
                        "gpu_energy_total": 0.01653909962015765,
                        "ram_energy_total": 3.052174820108041e-05,
                        "total_energy_kwh": 0.021133265390139832,
                        "total_energy_joules": 76079.7554045034
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21535295313305092,
                        "joules_per_token": 4.643539758575647,
                        "flops_per_joule": 222792133.11125705,
                        "joules_per_flop": 4.488488826042273e-09
                    },
                    "per-process_emissions": [
                        0.0020119084907827614,
                        0.0020148717969784907,
                        0.002029227702080012,
                        0.0019947094605325057
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0286": {
            "setup": {
                "experiment_id": "0286",
                "date_time": "April 11, 2025 at 06:12:45 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.303934518997266,
                        "average_latency_ms_per_batch": 4662.991814874658,
                        "throughput_queries_per_sec": 3.431273447437969,
                        "throughput_tokens_per_sec": 439.20300127206
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            13.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2699710464
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0286",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 433.1320111531084,
                            "process_3": 333.29895681739083,
                            "process_1": 419.1433936254986,
                            "process_0": 370.84861585578403
                        },
                        "ram_power": {
                            "process_2": 0.9618115425109863,
                            "process_3": 0.9719266891479492,
                            "process_1": 0.9781508445739747,
                            "process_0": 0.9415283203125
                        },
                        "cpu_energy": {
                            "process_2": 0.0011434431965000158,
                            "process_3": 0.0011624681586873747,
                            "process_1": 0.0011522386622186789,
                            "process_0": 0.001154290716499645
                        },
                        "gpu_energy": {
                            "process_2": 0.004099908557702092,
                            "process_3": 0.004158209159898452,
                            "process_1": 0.004129244136726107,
                            "process_0": 0.004134581640996282
                        },
                        "ram_energy": {
                            "process_2": 7.727092729353722e-06,
                            "process_3": 7.90900995028182e-06,
                            "process_1": 7.878535747645437e-06,
                            "process_0": 7.61264605184936e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005251078846931461,
                            "process_3": 0.00532858632853611,
                            "process_1": 0.0052893613346924285,
                            "process_0": 0.005296485003547777
                        },
                        "total_energy_joules": {
                            "process_2": 18903.88384895326,
                            "process_3": 19182.910782729996,
                            "process_1": 19041.700804892742,
                            "process_0": 19067.346012771995
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 389.10574436294553,
                        "ram_power_avg": 0.9633543491363525,
                        "cpu_energy_total": 0.004612440733905715,
                        "gpu_energy_total": 0.016521943495322933,
                        "ram_energy_total": 3.112728447913034e-05,
                        "total_energy_kwh": 0.021165511513707776,
                        "total_energy_joules": 76195.841449348
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21502485815963382,
                        "joules_per_token": 4.650625088461181,
                        "flops_per_joule": 222452704.38308206,
                        "joules_per_flop": 4.495337571971779e-09
                    },
                    "per-process_emissions": [
                        0.00200039848673854,
                        0.0020299249618558313,
                        0.002014982200451081,
                        0.0020176959621015255
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0286": {
            "setup": {
                "experiment_id": "0286",
                "date_time": "April 11, 2025 at 06:12:45 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.303934518997266,
                        "average_latency_ms_per_batch": 4662.991814874658,
                        "throughput_queries_per_sec": 3.431273447437969,
                        "throughput_tokens_per_sec": 439.20300127206
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            13.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2699710464
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0286",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 433.1320111531084,
                            "process_3": 333.29895681739083,
                            "process_1": 419.1433936254986,
                            "process_0": 370.84861585578403
                        },
                        "ram_power": {
                            "process_2": 0.9618115425109863,
                            "process_3": 0.9719266891479492,
                            "process_1": 0.9781508445739747,
                            "process_0": 0.9415283203125
                        },
                        "cpu_energy": {
                            "process_2": 0.0011434431965000158,
                            "process_3": 0.0011624681586873747,
                            "process_1": 0.0011522386622186789,
                            "process_0": 0.001154290716499645
                        },
                        "gpu_energy": {
                            "process_2": 0.004099908557702092,
                            "process_3": 0.004158209159898452,
                            "process_1": 0.004129244136726107,
                            "process_0": 0.004134581640996282
                        },
                        "ram_energy": {
                            "process_2": 7.727092729353722e-06,
                            "process_3": 7.90900995028182e-06,
                            "process_1": 7.878535747645437e-06,
                            "process_0": 7.61264605184936e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005251078846931461,
                            "process_3": 0.00532858632853611,
                            "process_1": 0.0052893613346924285,
                            "process_0": 0.005296485003547777
                        },
                        "total_energy_joules": {
                            "process_2": 18903.88384895326,
                            "process_3": 19182.910782729996,
                            "process_1": 19041.700804892742,
                            "process_0": 19067.346012771995
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 389.10574436294553,
                        "ram_power_avg": 0.9633543491363525,
                        "cpu_energy_total": 0.004612440733905715,
                        "gpu_energy_total": 0.016521943495322933,
                        "ram_energy_total": 3.112728447913034e-05,
                        "total_energy_kwh": 0.021165511513707776,
                        "total_energy_joules": 76195.841449348
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21502485815963382,
                        "joules_per_token": 4.650625088461181,
                        "flops_per_joule": 222452704.38308206,
                        "joules_per_flop": 4.495337571971779e-09
                    },
                    "per-process_emissions": [
                        0.00200039848673854,
                        0.0020299249618558313,
                        0.002014982200451081,
                        0.0020176959621015255
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0287": {
            "setup": {
                "experiment_id": "0287",
                "date_time": "April 11, 2025 at 06:13:51 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.00523649099341,
                        "average_latency_ms_per_batch": 4625.654561374176,
                        "throughput_queries_per_sec": 3.458969922571729,
                        "throughput_tokens_per_sec": 442.7481500891813
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            99.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2629054464
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0287",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 0.0,
                            "process_2": 443.8376252277481,
                            "process_3": 349.0281451005805,
                            "process_1": 319.0213217004817
                        },
                        "ram_power": {
                            "process_0": 0.9170436859130859,
                            "process_2": 0.9760279655456544,
                            "process_3": 0.9799861907958983,
                            "process_1": 0.9766001701354982
                        },
                        "cpu_energy": {
                            "process_0": 0.001178748577780994,
                            "process_2": 0.0011396375084373176,
                            "process_3": 0.0011566713103442227,
                            "process_1": 0.0011590954138126792
                        },
                        "gpu_energy": {
                            "process_0": 0.0041171194048039705,
                            "process_2": 0.004087209658654001,
                            "process_3": 0.004143068870008038,
                            "process_1": 0.004148684430056004
                        },
                        "ram_energy": {
                            "process_0": 7.569655942303139e-06,
                            "process_2": 7.857105836558186e-06,
                            "process_3": 7.943699804419125e-06,
                            "process_1": 7.947455498247189e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005303437638527266,
                            "process_2": 0.005234704272927878,
                            "process_3": 0.005307683880156677,
                            "process_1": 0.0053157272993669315
                        },
                        "total_energy_joules": {
                            "process_0": 19092.37549869816,
                            "process_2": 18844.93538254036,
                            "process_3": 19107.66196856404,
                            "process_1": 19136.618277720954
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 277.97177300720256,
                        "ram_power_avg": 0.9624145030975342,
                        "cpu_energy_total": 0.004634152810375214,
                        "gpu_energy_total": 0.016496082363522013,
                        "ram_energy_total": 3.131791708152764e-05,
                        "total_energy_kwh": 0.021161553090978752,
                        "total_energy_joules": 76181.59112752351
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.215065080126433,
                        "joules_per_token": 4.649755317842011,
                        "flops_per_joule": 222494315.78265074,
                        "joules_per_flop": 4.494496843581728e-09
                    },
                    "per-process_emissions": [
                        0.002020344568396962,
                        0.001994160592771875,
                        0.002021962174145686,
                        0.0020250263146938327
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0287": {
            "setup": {
                "experiment_id": "0287",
                "date_time": "April 11, 2025 at 06:13:51 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.00523649099341,
                        "average_latency_ms_per_batch": 4625.654561374176,
                        "throughput_queries_per_sec": 3.458969922571729,
                        "throughput_tokens_per_sec": 442.7481500891813
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            99.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2629054464
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0287",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 0.0,
                            "process_2": 443.8376252277481,
                            "process_3": 349.0281451005805,
                            "process_1": 319.0213217004817
                        },
                        "ram_power": {
                            "process_0": 0.9170436859130859,
                            "process_2": 0.9760279655456544,
                            "process_3": 0.9799861907958983,
                            "process_1": 0.9766001701354982
                        },
                        "cpu_energy": {
                            "process_0": 0.001178748577780994,
                            "process_2": 0.0011396375084373176,
                            "process_3": 0.0011566713103442227,
                            "process_1": 0.0011590954138126792
                        },
                        "gpu_energy": {
                            "process_0": 0.0041171194048039705,
                            "process_2": 0.004087209658654001,
                            "process_3": 0.004143068870008038,
                            "process_1": 0.004148684430056004
                        },
                        "ram_energy": {
                            "process_0": 7.569655942303139e-06,
                            "process_2": 7.857105836558186e-06,
                            "process_3": 7.943699804419125e-06,
                            "process_1": 7.947455498247189e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005303437638527266,
                            "process_2": 0.005234704272927878,
                            "process_3": 0.005307683880156677,
                            "process_1": 0.0053157272993669315
                        },
                        "total_energy_joules": {
                            "process_0": 19092.37549869816,
                            "process_2": 18844.93538254036,
                            "process_3": 19107.66196856404,
                            "process_1": 19136.618277720954
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 277.97177300720256,
                        "ram_power_avg": 0.9624145030975342,
                        "cpu_energy_total": 0.004634152810375214,
                        "gpu_energy_total": 0.016496082363522013,
                        "ram_energy_total": 3.131791708152764e-05,
                        "total_energy_kwh": 0.021161553090978752,
                        "total_energy_joules": 76181.59112752351
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.215065080126433,
                        "joules_per_token": 4.649755317842011,
                        "flops_per_joule": 222494315.78265074,
                        "joules_per_flop": 4.494496843581728e-09
                    },
                    "per-process_emissions": [
                        0.002020344568396962,
                        0.001994160592771875,
                        0.002021962174145686,
                        0.0020250263146938327
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0288": {
            "setup": {
                "experiment_id": "0288",
                "date_time": "April 11, 2025 at 06:14:58 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.3776513039993,
                        "average_latency_ms_per_batch": 4672.206412999913,
                        "throughput_queries_per_sec": 3.424506236599847,
                        "throughput_tokens_per_sec": 438.3367982847804
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            99.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.5,
                        "cpu_memory_usage_bytes": 2676060160
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0288",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 392.7788986902051,
                            "process_3": 448.7109705113396,
                            "process_1": 329.05223013518423,
                            "process_0": 365.5587253377288
                        },
                        "ram_power": {
                            "process_2": 0.9603424072265626,
                            "process_3": 0.9726276397705078,
                            "process_1": 0.9653792381286621,
                            "process_0": 0.9332470893859863
                        },
                        "cpu_energy": {
                            "process_2": 0.0011500015437500223,
                            "process_3": 0.0011510067458125376,
                            "process_1": 0.0011672183879063598,
                            "process_0": 0.001156956679687596
                        },
                        "gpu_energy": {
                            "process_2": 0.004136584142598154,
                            "process_3": 0.004135577752904129,
                            "process_1": 0.004176825563680042,
                            "process_0": 0.00414834248533813
                        },
                        "ram_energy": {
                            "process_2": 7.707482275861918e-06,
                            "process_3": 7.848652862540811e-06,
                            "process_1": 7.871278043935633e-06,
                            "process_0": 7.555357360497081e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005294293168624039,
                            "process_3": 0.005294433151579209,
                            "process_1": 0.005351915229630338,
                            "process_0": 0.0053128545223862225
                        },
                        "total_energy_joules": {
                            "process_2": 19059.45540704654,
                            "process_3": 19059.95934568515,
                            "process_1": 19266.894826669217,
                            "process_0": 19126.276280590402
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 384.02520616861443,
                        "ram_power_avg": 0.9578990936279297,
                        "cpu_energy_total": 0.004625183357156516,
                        "gpu_energy_total": 0.016597329944520456,
                        "ram_energy_total": 3.0982770542835446e-05,
                        "total_energy_kwh": 0.02125349607221981,
                        "total_energy_joules": 76512.5858599913
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21413470497495302,
                        "joules_per_token": 4.66995763305611,
                        "flops_per_joule": 221531801.63285002,
                        "joules_per_flop": 4.514024589829879e-09
                    },
                    "per-process_emissions": [
                        0.0020168609825873276,
                        0.0020169143090940996,
                        0.002038812106727677,
                        0.0020239319303030313
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0288": {
            "setup": {
                "experiment_id": "0288",
                "date_time": "April 11, 2025 at 06:14:58 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.3776513039993,
                        "average_latency_ms_per_batch": 4672.206412999913,
                        "throughput_queries_per_sec": 3.424506236599847,
                        "throughput_tokens_per_sec": 438.3367982847804
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            99.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.5,
                        "cpu_memory_usage_bytes": 2676060160
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0288",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 392.7788986902051,
                            "process_3": 448.7109705113396,
                            "process_1": 329.05223013518423,
                            "process_0": 365.5587253377288
                        },
                        "ram_power": {
                            "process_2": 0.9603424072265626,
                            "process_3": 0.9726276397705078,
                            "process_1": 0.9653792381286621,
                            "process_0": 0.9332470893859863
                        },
                        "cpu_energy": {
                            "process_2": 0.0011500015437500223,
                            "process_3": 0.0011510067458125376,
                            "process_1": 0.0011672183879063598,
                            "process_0": 0.001156956679687596
                        },
                        "gpu_energy": {
                            "process_2": 0.004136584142598154,
                            "process_3": 0.004135577752904129,
                            "process_1": 0.004176825563680042,
                            "process_0": 0.00414834248533813
                        },
                        "ram_energy": {
                            "process_2": 7.707482275861918e-06,
                            "process_3": 7.848652862540811e-06,
                            "process_1": 7.871278043935633e-06,
                            "process_0": 7.555357360497081e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005294293168624039,
                            "process_3": 0.005294433151579209,
                            "process_1": 0.005351915229630338,
                            "process_0": 0.0053128545223862225
                        },
                        "total_energy_joules": {
                            "process_2": 19059.45540704654,
                            "process_3": 19059.95934568515,
                            "process_1": 19266.894826669217,
                            "process_0": 19126.276280590402
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 384.02520616861443,
                        "ram_power_avg": 0.9578990936279297,
                        "cpu_energy_total": 0.004625183357156516,
                        "gpu_energy_total": 0.016597329944520456,
                        "ram_energy_total": 3.0982770542835446e-05,
                        "total_energy_kwh": 0.02125349607221981,
                        "total_energy_joules": 76512.5858599913
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21413470497495302,
                        "joules_per_token": 4.66995763305611,
                        "flops_per_joule": 221531801.63285002,
                        "joules_per_flop": 4.514024589829879e-09
                    },
                    "per-process_emissions": [
                        0.0020168609825873276,
                        0.0020169143090940996,
                        0.002038812106727677,
                        0.0020239319303030313
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0289": {
            "setup": {
                "experiment_id": "0289",
                "date_time": "April 11, 2025 at 06:16:04 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.01407191800172,
                        "average_latency_ms_per_batch": 4626.758989750215,
                        "throughput_queries_per_sec": 3.4581442507477127,
                        "throughput_tokens_per_sec": 442.6424640957072
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            85.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2679005184
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0289",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 301.5271970768851,
                            "process_3": 409.63537295503727,
                            "process_0": 0.0,
                            "process_2": 5198.226099780574
                        },
                        "ram_power": {
                            "process_1": 0.9816126823425293,
                            "process_3": 0.9685549736022949,
                            "process_0": 0.934844970703125,
                            "process_2": 0.9636282920837402
                        },
                        "cpu_energy": {
                            "process_1": 0.001153393463249813,
                            "process_3": 0.0011509885512813295,
                            "process_0": 0.001145981969312743,
                            "process_2": 0.0011476573128123846
                        },
                        "gpu_energy": {
                            "process_1": 0.004167972778819917,
                            "process_3": 0.00416994083595007,
                            "process_0": 0.004150126653431785,
                            "process_2": 0.004154283601201791
                        },
                        "ram_energy": {
                            "process_1": 7.912226004245615e-06,
                            "process_3": 7.825878974372939e-06,
                            "process_0": 7.744124668107656e-06,
                            "process_2": 7.982325185358708e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005329278468073977,
                            "process_3": 0.005328755266205774,
                            "process_0": 0.005303852747412636,
                            "process_2": 0.005309923239199536
                        },
                        "total_energy_joules": {
                            "process_1": 19185.402485066315,
                            "process_3": 19183.518958340785,
                            "process_0": 19093.869890685488,
                            "process_2": 19115.72366111833
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1477.3471674531243,
                        "ram_power_avg": 0.9621602296829224,
                        "cpu_energy_total": 0.00459802129665627,
                        "gpu_energy_total": 0.016642323869403564,
                        "ram_energy_total": 3.146455483208492e-05,
                        "total_energy_kwh": 0.02127180972089192,
                        "total_energy_joules": 76578.51499521092
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2139503488808137,
                        "joules_per_token": 4.6739816281256665,
                        "flops_per_joule": 221341077.11819717,
                        "joules_per_flop": 4.517914220983009e-09
                    },
                    "per-process_emissions": [
                        0.0020301886324127815,
                        0.0020299893186610895,
                        0.0020205027041268436,
                        0.0020228152579730633
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0289": {
            "setup": {
                "experiment_id": "0289",
                "date_time": "April 11, 2025 at 06:16:04 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.01407191800172,
                        "average_latency_ms_per_batch": 4626.758989750215,
                        "throughput_queries_per_sec": 3.4581442507477127,
                        "throughput_tokens_per_sec": 442.6424640957072
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            85.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2679005184
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0289",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 301.5271970768851,
                            "process_3": 409.63537295503727,
                            "process_0": 0.0,
                            "process_2": 5198.226099780574
                        },
                        "ram_power": {
                            "process_1": 0.9816126823425293,
                            "process_3": 0.9685549736022949,
                            "process_0": 0.934844970703125,
                            "process_2": 0.9636282920837402
                        },
                        "cpu_energy": {
                            "process_1": 0.001153393463249813,
                            "process_3": 0.0011509885512813295,
                            "process_0": 0.001145981969312743,
                            "process_2": 0.0011476573128123846
                        },
                        "gpu_energy": {
                            "process_1": 0.004167972778819917,
                            "process_3": 0.00416994083595007,
                            "process_0": 0.004150126653431785,
                            "process_2": 0.004154283601201791
                        },
                        "ram_energy": {
                            "process_1": 7.912226004245615e-06,
                            "process_3": 7.825878974372939e-06,
                            "process_0": 7.744124668107656e-06,
                            "process_2": 7.982325185358708e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005329278468073977,
                            "process_3": 0.005328755266205774,
                            "process_0": 0.005303852747412636,
                            "process_2": 0.005309923239199536
                        },
                        "total_energy_joules": {
                            "process_1": 19185.402485066315,
                            "process_3": 19183.518958340785,
                            "process_0": 19093.869890685488,
                            "process_2": 19115.72366111833
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1477.3471674531243,
                        "ram_power_avg": 0.9621602296829224,
                        "cpu_energy_total": 0.00459802129665627,
                        "gpu_energy_total": 0.016642323869403564,
                        "ram_energy_total": 3.146455483208492e-05,
                        "total_energy_kwh": 0.02127180972089192,
                        "total_energy_joules": 76578.51499521092
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2139503488808137,
                        "joules_per_token": 4.6739816281256665,
                        "flops_per_joule": 221341077.11819717,
                        "joules_per_flop": 4.517914220983009e-09
                    },
                    "per-process_emissions": [
                        0.0020301886324127815,
                        0.0020299893186610895,
                        0.0020205027041268436,
                        0.0020228152579730633
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0290": {
            "setup": {
                "experiment_id": "0290",
                "date_time": "April 11, 2025 at 06:17:15 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.463349108998955,
                        "average_latency_ms_per_batch": 4682.918638624869,
                        "throughput_queries_per_sec": 3.4166726425762484,
                        "throughput_tokens_per_sec": 437.3340982497598
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2652667904
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0290",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 341.44113574779465,
                            "process_3": 454.6761057337131,
                            "process_1": 604.9123105979407,
                            "process_2": 404.57594578259335
                        },
                        "ram_power": {
                            "process_0": 0.9251289367675781,
                            "process_3": 0.9611163139343262,
                            "process_1": 0.9806141853332518,
                            "process_2": 0.9758119583129883
                        },
                        "cpu_energy": {
                            "process_0": 0.0011569758758126908,
                            "process_3": 0.0011404603646249143,
                            "process_1": 0.0011451689439373922,
                            "process_2": 0.001151182837312149
                        },
                        "gpu_energy": {
                            "process_0": 0.004195553911995886,
                            "process_3": 0.0041489819302941155,
                            "process_1": 0.00417058500313211,
                            "process_2": 0.004174842228760023
                        },
                        "ram_energy": {
                            "process_0": 7.48305323381441e-06,
                            "process_3": 7.707149320715297e-06,
                            "process_1": 8.133438561889527e-06,
                            "process_2": 7.855850339191684e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005360012841042392,
                            "process_3": 0.005297149444239744,
                            "process_1": 0.00532388738563139,
                            "process_2": 0.005333880916411361
                        },
                        "total_energy_joules": {
                            "process_0": 19296.04622775261,
                            "process_3": 19069.737999263078,
                            "process_1": 19165.994588273003,
                            "process_2": 19201.9712990809
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 451.40137446551046,
                        "ram_power_avg": 0.9606678485870361,
                        "cpu_energy_total": 0.0045937880216871466,
                        "gpu_energy_total": 0.016689963074182135,
                        "ram_energy_total": 3.1179491455610915e-05,
                        "total_energy_kwh": 0.021314930587324886,
                        "total_energy_joules": 76733.75011436958
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21351751967784827,
                        "joules_per_token": 4.683456427879003,
                        "flops_per_joule": 220893296.20784238,
                        "joules_per_flop": 4.527072650765655e-09
                    },
                    "per-process_emissions": [
                        0.002041896891795099,
                        0.0020179490807831304,
                        0.002028134899556278,
                        0.002031941935106908
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0290": {
            "setup": {
                "experiment_id": "0290",
                "date_time": "April 11, 2025 at 06:17:15 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.463349108998955,
                        "average_latency_ms_per_batch": 4682.918638624869,
                        "throughput_queries_per_sec": 3.4166726425762484,
                        "throughput_tokens_per_sec": 437.3340982497598
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2652667904
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0290",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 341.44113574779465,
                            "process_3": 454.6761057337131,
                            "process_1": 604.9123105979407,
                            "process_2": 404.57594578259335
                        },
                        "ram_power": {
                            "process_0": 0.9251289367675781,
                            "process_3": 0.9611163139343262,
                            "process_1": 0.9806141853332518,
                            "process_2": 0.9758119583129883
                        },
                        "cpu_energy": {
                            "process_0": 0.0011569758758126908,
                            "process_3": 0.0011404603646249143,
                            "process_1": 0.0011451689439373922,
                            "process_2": 0.001151182837312149
                        },
                        "gpu_energy": {
                            "process_0": 0.004195553911995886,
                            "process_3": 0.0041489819302941155,
                            "process_1": 0.00417058500313211,
                            "process_2": 0.004174842228760023
                        },
                        "ram_energy": {
                            "process_0": 7.48305323381441e-06,
                            "process_3": 7.707149320715297e-06,
                            "process_1": 8.133438561889527e-06,
                            "process_2": 7.855850339191684e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005360012841042392,
                            "process_3": 0.005297149444239744,
                            "process_1": 0.00532388738563139,
                            "process_2": 0.005333880916411361
                        },
                        "total_energy_joules": {
                            "process_0": 19296.04622775261,
                            "process_3": 19069.737999263078,
                            "process_1": 19165.994588273003,
                            "process_2": 19201.9712990809
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 451.40137446551046,
                        "ram_power_avg": 0.9606678485870361,
                        "cpu_energy_total": 0.0045937880216871466,
                        "gpu_energy_total": 0.016689963074182135,
                        "ram_energy_total": 3.1179491455610915e-05,
                        "total_energy_kwh": 0.021314930587324886,
                        "total_energy_joules": 76733.75011436958
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21351751967784827,
                        "joules_per_token": 4.683456427879003,
                        "flops_per_joule": 220893296.20784238,
                        "joules_per_flop": 4.527072650765655e-09
                    },
                    "per-process_emissions": [
                        0.002041896891795099,
                        0.0020179490807831304,
                        0.002028134899556278,
                        0.002031941935106908
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0296": {
            "setup": {
                "experiment_id": "0296",
                "date_time": "April 11, 2025 at 06:20:42 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.610643549996894,
                        "average_latency_ms_per_batch": 4576.330443749612,
                        "throughput_queries_per_sec": 3.496251023973351,
                        "throughput_tokens_per_sec": 447.5201310685889
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            99.0,
                            99.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2638675968
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0296",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 372.3921721111116,
                            "process_2": 402.7174652300277,
                            "process_0": 353.87291626260924,
                            "process_3": 1092.5381481756428
                        },
                        "ram_power": {
                            "process_1": 0.9625768661499023,
                            "process_2": 0.9543256759643555,
                            "process_0": 0.9204812049865724,
                            "process_3": 0.9748821258544922
                        },
                        "cpu_energy": {
                            "process_1": 0.0011284985478437189,
                            "process_2": 0.00112479365712511,
                            "process_0": 0.001129105555156116,
                            "process_3": 0.001114388519781301
                        },
                        "gpu_energy": {
                            "process_1": 0.003997037642071932,
                            "process_2": 0.003992889583197989,
                            "process_0": 0.00400510014852179,
                            "process_3": 0.0039605195573019225
                        },
                        "ram_energy": {
                            "process_1": 7.62526294241484e-06,
                            "process_2": 7.515553537781442e-06,
                            "process_0": 7.284200389121117e-06,
                            "process_3": 7.822469271114613e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005133161452858066,
                            "process_2": 0.005125198793860882,
                            "process_0": 0.005141489904067024,
                            "process_3": 0.005082730546354339
                        },
                        "total_energy_joules": {
                            "process_1": 18479.381230289036,
                            "process_2": 18450.715657899174,
                            "process_0": 18509.36365464129,
                            "process_3": 18297.82996687562
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 555.3801754448479,
                        "ram_power_avg": 0.9530664682388306,
                        "cpu_energy_total": 0.004496786279906246,
                        "gpu_energy_total": 0.015955546931093634,
                        "ram_energy_total": 3.0247486140432015e-05,
                        "total_energy_kwh": 0.02048258069714031,
                        "total_energy_joules": 73737.29050970513
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2221942233942482,
                        "joules_per_token": 4.5005670477114945,
                        "flops_per_joule": 229869729.08803973,
                        "joules_per_flop": 4.350290070673036e-09
                    },
                    "per-process_emissions": [
                        0.0019554778554662804,
                        0.0019524444805213029,
                        0.001958650578954333,
                        0.0019362662016336855
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0296": {
            "setup": {
                "experiment_id": "0296",
                "date_time": "April 11, 2025 at 06:20:42 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.610643549996894,
                        "average_latency_ms_per_batch": 4576.330443749612,
                        "throughput_queries_per_sec": 3.496251023973351,
                        "throughput_tokens_per_sec": 447.5201310685889
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            99.0,
                            99.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2638675968
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0296",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 372.3921721111116,
                            "process_2": 402.7174652300277,
                            "process_0": 353.87291626260924,
                            "process_3": 1092.5381481756428
                        },
                        "ram_power": {
                            "process_1": 0.9625768661499023,
                            "process_2": 0.9543256759643555,
                            "process_0": 0.9204812049865724,
                            "process_3": 0.9748821258544922
                        },
                        "cpu_energy": {
                            "process_1": 0.0011284985478437189,
                            "process_2": 0.00112479365712511,
                            "process_0": 0.001129105555156116,
                            "process_3": 0.001114388519781301
                        },
                        "gpu_energy": {
                            "process_1": 0.003997037642071932,
                            "process_2": 0.003992889583197989,
                            "process_0": 0.00400510014852179,
                            "process_3": 0.0039605195573019225
                        },
                        "ram_energy": {
                            "process_1": 7.62526294241484e-06,
                            "process_2": 7.515553537781442e-06,
                            "process_0": 7.284200389121117e-06,
                            "process_3": 7.822469271114613e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005133161452858066,
                            "process_2": 0.005125198793860882,
                            "process_0": 0.005141489904067024,
                            "process_3": 0.005082730546354339
                        },
                        "total_energy_joules": {
                            "process_1": 18479.381230289036,
                            "process_2": 18450.715657899174,
                            "process_0": 18509.36365464129,
                            "process_3": 18297.82996687562
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 555.3801754448479,
                        "ram_power_avg": 0.9530664682388306,
                        "cpu_energy_total": 0.004496786279906246,
                        "gpu_energy_total": 0.015955546931093634,
                        "ram_energy_total": 3.0247486140432015e-05,
                        "total_energy_kwh": 0.02048258069714031,
                        "total_energy_joules": 73737.29050970513
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2221942233942482,
                        "joules_per_token": 4.5005670477114945,
                        "flops_per_joule": 229869729.08803973,
                        "joules_per_flop": 4.350290070673036e-09
                    },
                    "per-process_emissions": [
                        0.0019554778554662804,
                        0.0019524444805213029,
                        0.001958650578954333,
                        0.0019362662016336855
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0297": {
            "setup": {
                "experiment_id": "0297",
                "date_time": "April 11, 2025 at 06:21:47 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.430529735000164,
                        "average_latency_ms_per_batch": 4553.8162168750205,
                        "throughput_queries_per_sec": 3.5135366114927953,
                        "throughput_tokens_per_sec": 449.7326862710778
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2633592832
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0297",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 411.0982434262747,
                            "process_2": 403.4928685869104,
                            "process_3": 356.21093531541266,
                            "process_1": 455.48279822865896
                        },
                        "ram_power": {
                            "process_0": 0.9191365242004395,
                            "process_2": 0.9453063011169434,
                            "process_3": 0.9698138236999512,
                            "process_1": 0.9642262458801271
                        },
                        "cpu_energy": {
                            "process_0": 0.0011270251540000799,
                            "process_2": 0.001122974109937445,
                            "process_3": 0.0011366059548746535,
                            "process_1": 0.0011253427911251496
                        },
                        "gpu_energy": {
                            "process_0": 0.004004919315043987,
                            "process_2": 0.003999206254918175,
                            "process_3": 0.004027337666312081,
                            "process_1": 0.004000361811397901
                        },
                        "ram_energy": {
                            "process_0": 7.244058326863752e-06,
                            "process_2": 7.486519781910739e-06,
                            "process_3": 7.680471186803883e-06,
                            "process_1": 7.593408558319037e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005139188527370931,
                            "process_2": 0.005129666884637531,
                            "process_3": 0.005171624092373539,
                            "process_1": 0.005133298011081368
                        },
                        "total_energy_joules": {
                            "process_0": 18501.07869853535,
                            "process_2": 18466.80078469511,
                            "process_3": 18617.846732544742,
                            "process_1": 18479.872839892927
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 406.57121138931416,
                        "ram_power_avg": 0.9496207237243652,
                        "cpu_energy_total": 0.004511948009937328,
                        "gpu_energy_total": 0.016031825047672144,
                        "ram_energy_total": 3.000445785389741e-05,
                        "total_energy_kwh": 0.020573777515463368,
                        "total_energy_joules": 74065.59905566814
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22120930916505097,
                        "joules_per_token": 4.520605411112557,
                        "flops_per_joule": 228850791.85564005,
                        "joules_per_flop": 4.369659339570053e-09
                    },
                    "per-process_emissions": [
                        0.0019577738695019564,
                        0.0019541465997026677,
                        0.0019701301979896998,
                        0.0019555298773214474
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0297": {
            "setup": {
                "experiment_id": "0297",
                "date_time": "April 11, 2025 at 06:21:47 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.430529735000164,
                        "average_latency_ms_per_batch": 4553.8162168750205,
                        "throughput_queries_per_sec": 3.5135366114927953,
                        "throughput_tokens_per_sec": 449.7326862710778
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2633592832
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0297",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 411.0982434262747,
                            "process_2": 403.4928685869104,
                            "process_3": 356.21093531541266,
                            "process_1": 455.48279822865896
                        },
                        "ram_power": {
                            "process_0": 0.9191365242004395,
                            "process_2": 0.9453063011169434,
                            "process_3": 0.9698138236999512,
                            "process_1": 0.9642262458801271
                        },
                        "cpu_energy": {
                            "process_0": 0.0011270251540000799,
                            "process_2": 0.001122974109937445,
                            "process_3": 0.0011366059548746535,
                            "process_1": 0.0011253427911251496
                        },
                        "gpu_energy": {
                            "process_0": 0.004004919315043987,
                            "process_2": 0.003999206254918175,
                            "process_3": 0.004027337666312081,
                            "process_1": 0.004000361811397901
                        },
                        "ram_energy": {
                            "process_0": 7.244058326863752e-06,
                            "process_2": 7.486519781910739e-06,
                            "process_3": 7.680471186803883e-06,
                            "process_1": 7.593408558319037e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005139188527370931,
                            "process_2": 0.005129666884637531,
                            "process_3": 0.005171624092373539,
                            "process_1": 0.005133298011081368
                        },
                        "total_energy_joules": {
                            "process_0": 18501.07869853535,
                            "process_2": 18466.80078469511,
                            "process_3": 18617.846732544742,
                            "process_1": 18479.872839892927
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 406.57121138931416,
                        "ram_power_avg": 0.9496207237243652,
                        "cpu_energy_total": 0.004511948009937328,
                        "gpu_energy_total": 0.016031825047672144,
                        "ram_energy_total": 3.000445785389741e-05,
                        "total_energy_kwh": 0.020573777515463368,
                        "total_energy_joules": 74065.59905566814
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22120930916505097,
                        "joules_per_token": 4.520605411112557,
                        "flops_per_joule": 228850791.85564005,
                        "joules_per_flop": 4.369659339570053e-09
                    },
                    "per-process_emissions": [
                        0.0019577738695019564,
                        0.0019541465997026677,
                        0.0019701301979896998,
                        0.0019555298773214474
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0298": {
            "setup": {
                "experiment_id": "0298",
                "date_time": "April 11, 2025 at 06:22:52 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.47186338600659,
                        "average_latency_ms_per_batch": 4558.982923250824,
                        "throughput_queries_per_sec": 3.5095547119512034,
                        "throughput_tokens_per_sec": 449.22300312975403
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2657009664
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0298",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 376.4052444283301,
                            "process_2": 396.84101591486467,
                            "process_1": 336.62381119975385,
                            "process_3": 393.13631659534985
                        },
                        "ram_power": {
                            "process_0": 0.9275722503662109,
                            "process_2": 0.9721870422363281,
                            "process_1": 0.9647154808044434,
                            "process_3": 0.9574184417724609
                        },
                        "cpu_energy": {
                            "process_0": 0.0011265324139376391,
                            "process_2": 0.0011273988356878136,
                            "process_1": 0.0011178707360621728,
                            "process_3": 0.0011246913671560607
                        },
                        "gpu_energy": {
                            "process_0": 0.0040370893407799535,
                            "process_2": 0.0040370893407799535,
                            "process_1": 0.0040063434828498945,
                            "process_3": 0.00402798461127396
                        },
                        "ram_energy": {
                            "process_0": 7.324347532702139e-06,
                            "process_2": 7.659196336590407e-06,
                            "process_1": 7.571200171164137e-06,
                            "process_3": 7.558865903864901e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005170946102250293,
                            "process_2": 0.005172147372804357,
                            "process_1": 0.005131785419083231,
                            "process_3": 0.005160234844333889
                        },
                        "total_energy_joules": {
                            "process_0": 18615.405968101055,
                            "process_2": 18619.730542095684,
                            "process_1": 18474.42750869963,
                            "process_3": 18576.845439602
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 375.7515970345746,
                        "ram_power_avg": 0.9554733037948608,
                        "cpu_energy_total": 0.004496493352843686,
                        "gpu_energy_total": 0.016108506775683762,
                        "ram_energy_total": 3.0113609944321583e-05,
                        "total_energy_kwh": 0.020635113738471766,
                        "total_energy_joules": 74286.40945849837
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22055178220927826,
                        "joules_per_token": 4.534082608550926,
                        "flops_per_joule": 228170551.20454368,
                        "joules_per_flop": 4.382686524272579e-09
                    },
                    "per-process_emissions": [
                        0.001969871917652249,
                        0.00197032954166982,
                        0.0019549536553997568,
                        0.001965791463948995
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0298": {
            "setup": {
                "experiment_id": "0298",
                "date_time": "April 11, 2025 at 06:22:52 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.47186338600659,
                        "average_latency_ms_per_batch": 4558.982923250824,
                        "throughput_queries_per_sec": 3.5095547119512034,
                        "throughput_tokens_per_sec": 449.22300312975403
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2657009664
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0298",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 376.4052444283301,
                            "process_2": 396.84101591486467,
                            "process_1": 336.62381119975385,
                            "process_3": 393.13631659534985
                        },
                        "ram_power": {
                            "process_0": 0.9275722503662109,
                            "process_2": 0.9721870422363281,
                            "process_1": 0.9647154808044434,
                            "process_3": 0.9574184417724609
                        },
                        "cpu_energy": {
                            "process_0": 0.0011265324139376391,
                            "process_2": 0.0011273988356878136,
                            "process_1": 0.0011178707360621728,
                            "process_3": 0.0011246913671560607
                        },
                        "gpu_energy": {
                            "process_0": 0.0040370893407799535,
                            "process_2": 0.0040370893407799535,
                            "process_1": 0.0040063434828498945,
                            "process_3": 0.00402798461127396
                        },
                        "ram_energy": {
                            "process_0": 7.324347532702139e-06,
                            "process_2": 7.659196336590407e-06,
                            "process_1": 7.571200171164137e-06,
                            "process_3": 7.558865903864901e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005170946102250293,
                            "process_2": 0.005172147372804357,
                            "process_1": 0.005131785419083231,
                            "process_3": 0.005160234844333889
                        },
                        "total_energy_joules": {
                            "process_0": 18615.405968101055,
                            "process_2": 18619.730542095684,
                            "process_1": 18474.42750869963,
                            "process_3": 18576.845439602
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 375.7515970345746,
                        "ram_power_avg": 0.9554733037948608,
                        "cpu_energy_total": 0.004496493352843686,
                        "gpu_energy_total": 0.016108506775683762,
                        "ram_energy_total": 3.0113609944321583e-05,
                        "total_energy_kwh": 0.020635113738471766,
                        "total_energy_joules": 74286.40945849837
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22055178220927826,
                        "joules_per_token": 4.534082608550926,
                        "flops_per_joule": 228170551.20454368,
                        "joules_per_flop": 4.382686524272579e-09
                    },
                    "per-process_emissions": [
                        0.001969871917652249,
                        0.00197032954166982,
                        0.0019549536553997568,
                        0.001965791463948995
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0299": {
            "setup": {
                "experiment_id": "0299",
                "date_time": "April 11, 2025 at 06:23:58 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.4072339129998,
                        "average_latency_ms_per_batch": 4550.904239124975,
                        "throughput_queries_per_sec": 3.515784810949219,
                        "throughput_tokens_per_sec": 450.0204558015
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2613207040
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0299",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 344.3327479871836,
                            "process_3": 431.56001548642996,
                            "process_1": 373.7821955830307,
                            "process_2": 337.79182205638836
                        },
                        "ram_power": {
                            "process_0": 0.9117136001586914,
                            "process_3": 0.9694705009460449,
                            "process_1": 0.9740839004516603,
                            "process_2": 0.9704689979553223
                        },
                        "cpu_energy": {
                            "process_0": 0.0011248617520312788,
                            "process_3": 0.0011026449954374584,
                            "process_1": 0.0011197061152189464,
                            "process_2": 0.0011370996205628213
                        },
                        "gpu_energy": {
                            "process_0": 0.00403987767634395,
                            "process_3": 0.003967022895838002,
                            "process_1": 0.004020186549480054,
                            "process_2": 0.0040689001995621865
                        },
                        "ram_energy": {
                            "process_0": 7.202336395773373e-06,
                            "process_3": 7.5634955140371264e-06,
                            "process_1": 7.658792596092139e-06,
                            "process_2": 7.701114679636079e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005171941764771004,
                            "process_3": 0.005077231386789497,
                            "process_1": 0.00514755145729509,
                            "process_2": 0.0052137009348046425
                        },
                        "total_energy_joules": {
                            "process_0": 18618.990353175614,
                            "process_3": 18278.03299244219,
                            "process_1": 18531.185246262325,
                            "process_2": 18769.323365296714
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 371.86669527825813,
                        "ram_power_avg": 0.9564342498779297,
                        "cpu_energy_total": 0.004484312483250505,
                        "gpu_energy_total": 0.016095987321224192,
                        "ram_energy_total": 3.012573918553872e-05,
                        "total_energy_kwh": 0.020610425543660233,
                        "total_energy_joules": 74197.53195717685
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2208159701249367,
                        "joules_per_token": 4.528657956370657,
                        "flops_per_joule": 228443865.2613767,
                        "joules_per_flop": 4.377443004896797e-09
                    },
                    "per-process_emissions": [
                        0.0019702512152895138,
                        0.001934171296797459,
                        0.0019609597276565647,
                        0.0019861593711138284
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0299": {
            "setup": {
                "experiment_id": "0299",
                "date_time": "April 11, 2025 at 06:23:58 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.4072339129998,
                        "average_latency_ms_per_batch": 4550.904239124975,
                        "throughput_queries_per_sec": 3.515784810949219,
                        "throughput_tokens_per_sec": 450.0204558015
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2613207040
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0299",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 344.3327479871836,
                            "process_3": 431.56001548642996,
                            "process_1": 373.7821955830307,
                            "process_2": 337.79182205638836
                        },
                        "ram_power": {
                            "process_0": 0.9117136001586914,
                            "process_3": 0.9694705009460449,
                            "process_1": 0.9740839004516603,
                            "process_2": 0.9704689979553223
                        },
                        "cpu_energy": {
                            "process_0": 0.0011248617520312788,
                            "process_3": 0.0011026449954374584,
                            "process_1": 0.0011197061152189464,
                            "process_2": 0.0011370996205628213
                        },
                        "gpu_energy": {
                            "process_0": 0.00403987767634395,
                            "process_3": 0.003967022895838002,
                            "process_1": 0.004020186549480054,
                            "process_2": 0.0040689001995621865
                        },
                        "ram_energy": {
                            "process_0": 7.202336395773373e-06,
                            "process_3": 7.5634955140371264e-06,
                            "process_1": 7.658792596092139e-06,
                            "process_2": 7.701114679636079e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005171941764771004,
                            "process_3": 0.005077231386789497,
                            "process_1": 0.00514755145729509,
                            "process_2": 0.0052137009348046425
                        },
                        "total_energy_joules": {
                            "process_0": 18618.990353175614,
                            "process_3": 18278.03299244219,
                            "process_1": 18531.185246262325,
                            "process_2": 18769.323365296714
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 371.86669527825813,
                        "ram_power_avg": 0.9564342498779297,
                        "cpu_energy_total": 0.004484312483250505,
                        "gpu_energy_total": 0.016095987321224192,
                        "ram_energy_total": 3.012573918553872e-05,
                        "total_energy_kwh": 0.020610425543660233,
                        "total_energy_joules": 74197.53195717685
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2208159701249367,
                        "joules_per_token": 4.528657956370657,
                        "flops_per_joule": 228443865.2613767,
                        "joules_per_flop": 4.377443004896797e-09
                    },
                    "per-process_emissions": [
                        0.0019702512152895138,
                        0.001934171296797459,
                        0.0019609597276565647,
                        0.0019861593711138284
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0300": {
            "setup": {
                "experiment_id": "0300",
                "date_time": "April 11, 2025 at 06:25:04 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.23065119199964,
                        "average_latency_ms_per_batch": 4653.831398999955,
                        "throughput_queries_per_sec": 3.4380274290637565,
                        "throughput_tokens_per_sec": 440.0675109201608
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2558296064
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0300",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 415.740948254861,
                            "process_1": 456.8359573594204,
                            "process_3": 0.0,
                            "process_0": 375.38526560195
                        },
                        "ram_power": {
                            "process_2": 0.9709396362304688,
                            "process_1": 0.9707021713256836,
                            "process_3": 0.9501228332519532,
                            "process_0": 0.8920612335205078
                        },
                        "cpu_energy": {
                            "process_2": 0.0011361071433127565,
                            "process_1": 0.0011262399479686563,
                            "process_3": 0.0011719106633436244,
                            "process_0": 0.001152011785843456
                        },
                        "gpu_energy": {
                            "process_2": 0.004117583571841776,
                            "process_1": 0.004086087157756102,
                            "process_3": 0.0041463524837460075,
                            "process_0": 0.004160898606493935
                        },
                        "ram_energy": {
                            "process_2": 7.779169064469106e-06,
                            "process_1": 7.657706694033373e-06,
                            "process_3": 7.80852132449874e-06,
                            "process_0": 7.159155372218623e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005261469884219003,
                            "process_1": 0.005219984812418791,
                            "process_3": 0.005326071668414134,
                            "process_0": 0.0053200695477096105
                        },
                        "total_energy_joules": {
                            "process_2": 18941.291583188413,
                            "process_1": 18791.945324707645,
                            "process_3": 19173.85800629088,
                            "process_0": 19152.2503717546
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 311.99054280405784,
                        "ram_power_avg": 0.9459564685821533,
                        "cpu_energy_total": 0.004586269540468493,
                        "gpu_energy_total": 0.01651092181983782,
                        "ram_energy_total": 3.040455245521984e-05,
                        "total_energy_kwh": 0.02112759591276154,
                        "total_energy_joules": 76059.34528594154
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21541074194637255,
                        "joules_per_token": 4.642294023800143,
                        "flops_per_joule": 222851918.188743,
                        "joules_per_flop": 4.4872846871932974e-09
                    },
                    "per-process_emissions": [
                        0.0020043569523932295,
                        0.0019885532142909384,
                        0.0020289670020823642,
                        0.002026680494199976
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0300": {
            "setup": {
                "experiment_id": "0300",
                "date_time": "April 11, 2025 at 06:25:04 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.23065119199964,
                        "average_latency_ms_per_batch": 4653.831398999955,
                        "throughput_queries_per_sec": 3.4380274290637565,
                        "throughput_tokens_per_sec": 440.0675109201608
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2558296064
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0300",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 415.740948254861,
                            "process_1": 456.8359573594204,
                            "process_3": 0.0,
                            "process_0": 375.38526560195
                        },
                        "ram_power": {
                            "process_2": 0.9709396362304688,
                            "process_1": 0.9707021713256836,
                            "process_3": 0.9501228332519532,
                            "process_0": 0.8920612335205078
                        },
                        "cpu_energy": {
                            "process_2": 0.0011361071433127565,
                            "process_1": 0.0011262399479686563,
                            "process_3": 0.0011719106633436244,
                            "process_0": 0.001152011785843456
                        },
                        "gpu_energy": {
                            "process_2": 0.004117583571841776,
                            "process_1": 0.004086087157756102,
                            "process_3": 0.0041463524837460075,
                            "process_0": 0.004160898606493935
                        },
                        "ram_energy": {
                            "process_2": 7.779169064469106e-06,
                            "process_1": 7.657706694033373e-06,
                            "process_3": 7.80852132449874e-06,
                            "process_0": 7.159155372218623e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005261469884219003,
                            "process_1": 0.005219984812418791,
                            "process_3": 0.005326071668414134,
                            "process_0": 0.0053200695477096105
                        },
                        "total_energy_joules": {
                            "process_2": 18941.291583188413,
                            "process_1": 18791.945324707645,
                            "process_3": 19173.85800629088,
                            "process_0": 19152.2503717546
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 311.99054280405784,
                        "ram_power_avg": 0.9459564685821533,
                        "cpu_energy_total": 0.004586269540468493,
                        "gpu_energy_total": 0.01651092181983782,
                        "ram_energy_total": 3.040455245521984e-05,
                        "total_energy_kwh": 0.02112759591276154,
                        "total_energy_joules": 76059.34528594154
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21541074194637255,
                        "joules_per_token": 4.642294023800143,
                        "flops_per_joule": 222851918.188743,
                        "joules_per_flop": 4.4872846871932974e-09
                    },
                    "per-process_emissions": [
                        0.0020043569523932295,
                        0.0019885532142909384,
                        0.0020289670020823642,
                        0.002026680494199976
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0301": {
            "setup": {
                "experiment_id": "0301",
                "date_time": "April 11, 2025 at 06:26:10 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.06134638599542,
                        "average_latency_ms_per_batch": 4632.668298249428,
                        "throughput_queries_per_sec": 3.4537331338930546,
                        "throughput_tokens_per_sec": 442.077841138311
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2625011712
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0301",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 443.79984857483123,
                            "process_3": 302.8619839605174,
                            "process_1": 443.8863071531142,
                            "process_0": 0.0
                        },
                        "ram_power": {
                            "process_2": 0.9707450866699219,
                            "process_3": 0.9590520858764648,
                            "process_1": 0.9751009941101074,
                            "process_0": 0.9160952568054199
                        },
                        "cpu_energy": {
                            "process_2": 0.0011360947903125411,
                            "process_3": 0.0011563648477501826,
                            "process_1": 0.0011249567326558465,
                            "process_0": 0.0011477762492189637
                        },
                        "gpu_energy": {
                            "process_2": 0.004106108284884058,
                            "process_3": 0.0041637441643260775,
                            "process_1": 0.004081763543185957,
                            "process_0": 0.004137360532108059
                        },
                        "ram_energy": {
                            "process_2": 7.722364906680388e-06,
                            "process_3": 7.762235993057113e-06,
                            "process_1": 7.715356228594965e-06,
                            "process_0": 7.604475222364343e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0052499254401032775,
                            "process_3": 0.005327871248069319,
                            "process_1": 0.005214435632070397,
                            "process_0": 0.005292741256549387
                        },
                        "total_energy_joules": {
                            "process_2": 18899.7315843718,
                            "process_3": 19180.336493049548,
                            "process_1": 18771.96827545343,
                            "process_0": 19053.868523577792
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 297.63703492211573,
                        "ram_power_avg": 0.9552483558654785,
                        "cpu_energy_total": 0.004565192619937534,
                        "gpu_energy_total": 0.01648897652450415,
                        "ram_energy_total": 3.0804432350696814e-05,
                        "total_energy_kwh": 0.021084973576792378,
                        "total_energy_joules": 75905.90487645257
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21584618517712478,
                        "joules_per_token": 4.632928764431919,
                        "flops_per_joule": 223302403.42619509,
                        "joules_per_flop": 4.478232140168234e-09
                    },
                    "per-process_emissions": [
                        0.0019999590964073434,
                        0.002029652551952007,
                        0.001986439254037218,
                        0.002016269781682489
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0301": {
            "setup": {
                "experiment_id": "0301",
                "date_time": "April 11, 2025 at 06:26:10 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.06134638599542,
                        "average_latency_ms_per_batch": 4632.668298249428,
                        "throughput_queries_per_sec": 3.4537331338930546,
                        "throughput_tokens_per_sec": 442.077841138311
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2625011712
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0301",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 443.79984857483123,
                            "process_3": 302.8619839605174,
                            "process_1": 443.8863071531142,
                            "process_0": 0.0
                        },
                        "ram_power": {
                            "process_2": 0.9707450866699219,
                            "process_3": 0.9590520858764648,
                            "process_1": 0.9751009941101074,
                            "process_0": 0.9160952568054199
                        },
                        "cpu_energy": {
                            "process_2": 0.0011360947903125411,
                            "process_3": 0.0011563648477501826,
                            "process_1": 0.0011249567326558465,
                            "process_0": 0.0011477762492189637
                        },
                        "gpu_energy": {
                            "process_2": 0.004106108284884058,
                            "process_3": 0.0041637441643260775,
                            "process_1": 0.004081763543185957,
                            "process_0": 0.004137360532108059
                        },
                        "ram_energy": {
                            "process_2": 7.722364906680388e-06,
                            "process_3": 7.762235993057113e-06,
                            "process_1": 7.715356228594965e-06,
                            "process_0": 7.604475222364343e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0052499254401032775,
                            "process_3": 0.005327871248069319,
                            "process_1": 0.005214435632070397,
                            "process_0": 0.005292741256549387
                        },
                        "total_energy_joules": {
                            "process_2": 18899.7315843718,
                            "process_3": 19180.336493049548,
                            "process_1": 18771.96827545343,
                            "process_0": 19053.868523577792
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 297.63703492211573,
                        "ram_power_avg": 0.9552483558654785,
                        "cpu_energy_total": 0.004565192619937534,
                        "gpu_energy_total": 0.01648897652450415,
                        "ram_energy_total": 3.0804432350696814e-05,
                        "total_energy_kwh": 0.021084973576792378,
                        "total_energy_joules": 75905.90487645257
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21584618517712478,
                        "joules_per_token": 4.632928764431919,
                        "flops_per_joule": 223302403.42619509,
                        "joules_per_flop": 4.478232140168234e-09
                    },
                    "per-process_emissions": [
                        0.0019999590964073434,
                        0.002029652551952007,
                        0.001986439254037218,
                        0.002016269781682489
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0302": {
            "setup": {
                "experiment_id": "0302",
                "date_time": "April 11, 2025 at 06:27:16 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.94766097399406,
                        "average_latency_ms_per_batch": 4618.457621749258,
                        "throughput_queries_per_sec": 3.464360033239829,
                        "throughput_tokens_per_sec": 443.43808425469814
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2649886720
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0302",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 422.5950535074562,
                            "process_3": 0.0,
                            "process_0": 425.0000449984457,
                            "process_1": 428.8016495184117
                        },
                        "ram_power": {
                            "process_2": 0.9751567840576173,
                            "process_3": 0.9590234756469727,
                            "process_0": 0.9253263473510742,
                            "process_1": 0.9793009757995605
                        },
                        "cpu_energy": {
                            "process_2": 0.00113344947637529,
                            "process_3": 0.0011483055740313833,
                            "process_0": 0.0011443010092500572,
                            "process_1": 0.0011342009950625423
                        },
                        "gpu_energy": {
                            "process_2": 0.004113688013170136,
                            "process_3": 0.004147301373394141,
                            "process_0": 0.00413771747683811,
                            "process_1": 0.004113688013170136
                        },
                        "ram_energy": {
                            "process_2": 7.752193471164641e-06,
                            "process_3": 7.923955074155306e-06,
                            "process_0": 7.466373932549577e-06,
                            "process_1": 7.812283439053733e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005254889683016591,
                            "process_3": 0.005303530902499676,
                            "process_0": 0.005289484860020717,
                            "process_1": 0.005255701291671733
                        },
                        "total_energy_joules": {
                            "process_2": 18917.602858859726,
                            "process_3": 19092.711248998836,
                            "process_0": 19042.14549607458,
                            "process_1": 18920.52465001824
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 319.0991870060784,
                        "ram_power_avg": 0.9597018957138062,
                        "cpu_energy_total": 0.004560257054719273,
                        "gpu_energy_total": 0.016512394876572523,
                        "ram_energy_total": 3.095480591692326e-05,
                        "total_energy_kwh": 0.021103606737208718,
                        "total_energy_joules": 75972.98425395138
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.215655606540793,
                        "joules_per_token": 4.637022964718712,
                        "flops_per_joule": 223105241.4170558,
                        "joules_per_flop": 4.482189632338923e-09
                    },
                    "per-process_emissions": [
                        0.0020018502247451703,
                        0.0020203800973072517,
                        0.002015029257424892,
                        0.002002159407062347
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0302": {
            "setup": {
                "experiment_id": "0302",
                "date_time": "April 11, 2025 at 06:27:16 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.94766097399406,
                        "average_latency_ms_per_batch": 4618.457621749258,
                        "throughput_queries_per_sec": 3.464360033239829,
                        "throughput_tokens_per_sec": 443.43808425469814
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2649886720
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0302",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 422.5950535074562,
                            "process_3": 0.0,
                            "process_0": 425.0000449984457,
                            "process_1": 428.8016495184117
                        },
                        "ram_power": {
                            "process_2": 0.9751567840576173,
                            "process_3": 0.9590234756469727,
                            "process_0": 0.9253263473510742,
                            "process_1": 0.9793009757995605
                        },
                        "cpu_energy": {
                            "process_2": 0.00113344947637529,
                            "process_3": 0.0011483055740313833,
                            "process_0": 0.0011443010092500572,
                            "process_1": 0.0011342009950625423
                        },
                        "gpu_energy": {
                            "process_2": 0.004113688013170136,
                            "process_3": 0.004147301373394141,
                            "process_0": 0.00413771747683811,
                            "process_1": 0.004113688013170136
                        },
                        "ram_energy": {
                            "process_2": 7.752193471164641e-06,
                            "process_3": 7.923955074155306e-06,
                            "process_0": 7.466373932549577e-06,
                            "process_1": 7.812283439053733e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005254889683016591,
                            "process_3": 0.005303530902499676,
                            "process_0": 0.005289484860020717,
                            "process_1": 0.005255701291671733
                        },
                        "total_energy_joules": {
                            "process_2": 18917.602858859726,
                            "process_3": 19092.711248998836,
                            "process_0": 19042.14549607458,
                            "process_1": 18920.52465001824
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 319.0991870060784,
                        "ram_power_avg": 0.9597018957138062,
                        "cpu_energy_total": 0.004560257054719273,
                        "gpu_energy_total": 0.016512394876572523,
                        "ram_energy_total": 3.095480591692326e-05,
                        "total_energy_kwh": 0.021103606737208718,
                        "total_energy_joules": 75972.98425395138
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.215655606540793,
                        "joules_per_token": 4.637022964718712,
                        "flops_per_joule": 223105241.4170558,
                        "joules_per_flop": 4.482189632338923e-09
                    },
                    "per-process_emissions": [
                        0.0020018502247451703,
                        0.0020203800973072517,
                        0.002015029257424892,
                        0.002002159407062347
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0303": {
            "setup": {
                "experiment_id": "0303",
                "date_time": "April 11, 2025 at 06:28:21 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.86678839499291,
                        "average_latency_ms_per_batch": 4608.348549374114,
                        "throughput_queries_per_sec": 3.4719596030063853,
                        "throughput_tokens_per_sec": 444.4108291848173
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2647212032
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0303",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 438.03804865911286,
                            "process_1": 441.4947199911335,
                            "process_2": 429.67052937505724,
                            "process_0": 380.7620667851534
                        },
                        "ram_power": {
                            "process_3": 0.9588503837585449,
                            "process_1": 0.9750080108642578,
                            "process_2": 0.9753842353820801,
                            "process_0": 0.9237427711486816
                        },
                        "cpu_energy": {
                            "process_3": 0.0011268567554058107,
                            "process_1": 0.0011303659404370595,
                            "process_2": 0.0011300531373433387,
                            "process_0": 0.001141364287093893
                        },
                        "gpu_energy": {
                            "process_3": 0.004102539393140203,
                            "process_1": 0.004110483288383926,
                            "process_2": 0.004124589688558,
                            "process_0": 0.004140963034990008
                        },
                        "ram_energy": {
                            "process_3": 7.594406198218584e-06,
                            "process_1": 7.818276048345094e-06,
                            "process_2": 7.793794807502773e-06,
                            "process_0": 7.434084268082258e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0052369905547442345,
                            "process_1": 0.005248667504869331,
                            "process_2": 0.0052624366207088425,
                            "process_0": 0.005289761406351982
                        },
                        "total_energy_joules": {
                            "process_3": 18853.165997079243,
                            "process_1": 18895.20301752959,
                            "process_2": 18944.771834551833,
                            "process_0": 19043.141062867136
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 422.49134120261425,
                        "ram_power_avg": 0.9582463502883911,
                        "cpu_energy_total": 0.004528640120280101,
                        "gpu_energy_total": 0.016478575405072138,
                        "ram_energy_total": 3.0640561322148706e-05,
                        "total_energy_kwh": 0.02103785608667439,
                        "total_energy_joules": 75736.2819120278
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21632960565757625,
                        "joules_per_token": 4.6225758002946655,
                        "flops_per_joule": 223802523.2456011,
                        "joules_per_flop": 4.468224868504271e-09
                    },
                    "per-process_emissions": [
                        0.001995031551829816,
                        0.0019994798859799714,
                        0.0020047252306590337,
                        0.0020151346077497876
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0303": {
            "setup": {
                "experiment_id": "0303",
                "date_time": "April 11, 2025 at 06:28:21 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.86678839499291,
                        "average_latency_ms_per_batch": 4608.348549374114,
                        "throughput_queries_per_sec": 3.4719596030063853,
                        "throughput_tokens_per_sec": 444.4108291848173
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2647212032
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0303",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 438.03804865911286,
                            "process_1": 441.4947199911335,
                            "process_2": 429.67052937505724,
                            "process_0": 380.7620667851534
                        },
                        "ram_power": {
                            "process_3": 0.9588503837585449,
                            "process_1": 0.9750080108642578,
                            "process_2": 0.9753842353820801,
                            "process_0": 0.9237427711486816
                        },
                        "cpu_energy": {
                            "process_3": 0.0011268567554058107,
                            "process_1": 0.0011303659404370595,
                            "process_2": 0.0011300531373433387,
                            "process_0": 0.001141364287093893
                        },
                        "gpu_energy": {
                            "process_3": 0.004102539393140203,
                            "process_1": 0.004110483288383926,
                            "process_2": 0.004124589688558,
                            "process_0": 0.004140963034990008
                        },
                        "ram_energy": {
                            "process_3": 7.594406198218584e-06,
                            "process_1": 7.818276048345094e-06,
                            "process_2": 7.793794807502773e-06,
                            "process_0": 7.434084268082258e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0052369905547442345,
                            "process_1": 0.005248667504869331,
                            "process_2": 0.0052624366207088425,
                            "process_0": 0.005289761406351982
                        },
                        "total_energy_joules": {
                            "process_3": 18853.165997079243,
                            "process_1": 18895.20301752959,
                            "process_2": 18944.771834551833,
                            "process_0": 19043.141062867136
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 422.49134120261425,
                        "ram_power_avg": 0.9582463502883911,
                        "cpu_energy_total": 0.004528640120280101,
                        "gpu_energy_total": 0.016478575405072138,
                        "ram_energy_total": 3.0640561322148706e-05,
                        "total_energy_kwh": 0.02103785608667439,
                        "total_energy_joules": 75736.2819120278
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21632960565757625,
                        "joules_per_token": 4.6225758002946655,
                        "flops_per_joule": 223802523.2456011,
                        "joules_per_flop": 4.468224868504271e-09
                    },
                    "per-process_emissions": [
                        0.001995031551829816,
                        0.0019994798859799714,
                        0.0020047252306590337,
                        0.0020151346077497876
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0304": {
            "setup": {
                "experiment_id": "0304",
                "date_time": "April 11, 2025 at 06:29:27 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.912186677000136,
                        "average_latency_ms_per_batch": 4614.023334625017,
                        "throughput_queries_per_sec": 3.4676894414319914,
                        "throughput_tokens_per_sec": 443.8642485032949
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2652401664
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0304",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 340.2019840041833,
                            "process_2": 438.6025726078089,
                            "process_0": 445.41548963061155,
                            "process_3": 317.236460219269
                        },
                        "ram_power": {
                            "process_1": 0.9737691879272461,
                            "process_2": 0.9590134620666504,
                            "process_0": 0.9257054328918457,
                            "process_3": 0.9788718223571776
                        },
                        "cpu_energy": {
                            "process_1": 0.0011497868373124907,
                            "process_2": 0.0011403098808124242,
                            "process_0": 0.0011402579954693693,
                            "process_3": 0.0011527976269062492
                        },
                        "gpu_energy": {
                            "process_1": 0.004137138309708133,
                            "process_2": 0.004112438289948062,
                            "process_0": 0.004110462455034103,
                            "process_3": 0.00414919276379605
                        },
                        "ram_energy": {
                            "process_1": 7.819246854494539e-06,
                            "process_2": 7.698612961833827e-06,
                            "process_0": 7.393220178037956e-06,
                            "process_3": 7.868973767493226e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005294744393875117,
                            "process_2": 0.005260446783722317,
                            "process_0": 0.0052581136706815095,
                            "process_3": 0.005309859364469795
                        },
                        "total_energy_joules": {
                            "process_1": 19061.07981795042,
                            "process_2": 18937.608421400342,
                            "process_0": 18929.209214453433,
                            "process_3": 19115.493712091262
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 385.3641266154682,
                        "ram_power_avg": 0.95933997631073,
                        "cpu_energy_total": 0.004583152340500533,
                        "gpu_energy_total": 0.016509231818486347,
                        "ram_energy_total": 3.078005376185955e-05,
                        "total_energy_kwh": 0.02112316421274874,
                        "total_energy_joules": 76043.39116589546
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21545593573354505,
                        "joules_per_token": 4.641320261590299,
                        "flops_per_joule": 222898673.15588441,
                        "joules_per_flop": 4.4863434395621055e-09
                    },
                    "per-process_emissions": [
                        0.002017032876846726,
                        0.002003967202259017,
                        0.002003078402846121,
                        0.0020227909248947684
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0304": {
            "setup": {
                "experiment_id": "0304",
                "date_time": "April 11, 2025 at 06:29:27 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.912186677000136,
                        "average_latency_ms_per_batch": 4614.023334625017,
                        "throughput_queries_per_sec": 3.4676894414319914,
                        "throughput_tokens_per_sec": 443.8642485032949
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2652401664
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0304",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 340.2019840041833,
                            "process_2": 438.6025726078089,
                            "process_0": 445.41548963061155,
                            "process_3": 317.236460219269
                        },
                        "ram_power": {
                            "process_1": 0.9737691879272461,
                            "process_2": 0.9590134620666504,
                            "process_0": 0.9257054328918457,
                            "process_3": 0.9788718223571776
                        },
                        "cpu_energy": {
                            "process_1": 0.0011497868373124907,
                            "process_2": 0.0011403098808124242,
                            "process_0": 0.0011402579954693693,
                            "process_3": 0.0011527976269062492
                        },
                        "gpu_energy": {
                            "process_1": 0.004137138309708133,
                            "process_2": 0.004112438289948062,
                            "process_0": 0.004110462455034103,
                            "process_3": 0.00414919276379605
                        },
                        "ram_energy": {
                            "process_1": 7.819246854494539e-06,
                            "process_2": 7.698612961833827e-06,
                            "process_0": 7.393220178037956e-06,
                            "process_3": 7.868973767493226e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005294744393875117,
                            "process_2": 0.005260446783722317,
                            "process_0": 0.0052581136706815095,
                            "process_3": 0.005309859364469795
                        },
                        "total_energy_joules": {
                            "process_1": 19061.07981795042,
                            "process_2": 18937.608421400342,
                            "process_0": 18929.209214453433,
                            "process_3": 19115.493712091262
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 385.3641266154682,
                        "ram_power_avg": 0.95933997631073,
                        "cpu_energy_total": 0.004583152340500533,
                        "gpu_energy_total": 0.016509231818486347,
                        "ram_energy_total": 3.078005376185955e-05,
                        "total_energy_kwh": 0.02112316421274874,
                        "total_energy_joules": 76043.39116589546
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21545593573354505,
                        "joules_per_token": 4.641320261590299,
                        "flops_per_joule": 222898673.15588441,
                        "joules_per_flop": 4.4863434395621055e-09
                    },
                    "per-process_emissions": [
                        0.002017032876846726,
                        0.002003967202259017,
                        0.002003078402846121,
                        0.0020227909248947684
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0305": {
            "setup": {
                "experiment_id": "0305",
                "date_time": "April 11, 2025 at 06:30:33 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.25063190000583,
                        "average_latency_ms_per_batch": 4656.3289875007285,
                        "throughput_queries_per_sec": 3.4361833201540932,
                        "throughput_tokens_per_sec": 439.83146497972393
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2694619136
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0305",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 566.2170327922812,
                            "process_1": 450.618838201581,
                            "process_0": 413.4561972506076,
                            "process_2": 689.2566462292624
                        },
                        "ram_power": {
                            "process_3": 0.9619989395141602,
                            "process_1": 0.9776401519775392,
                            "process_0": 0.9402565956115723,
                            "process_2": 0.9790921211242675
                        },
                        "cpu_energy": {
                            "process_3": 0.001149004184124692,
                            "process_1": 0.0011417217090940995,
                            "process_0": 0.0011507003279999709,
                            "process_2": 0.0011472500294373734
                        },
                        "gpu_energy": {
                            "process_3": 0.004133142750955887,
                            "process_1": 0.004116103292880091,
                            "process_0": 0.004140954979428069,
                            "process_2": 0.004131420805134067
                        },
                        "ram_energy": {
                            "process_3": 7.968043110409637e-06,
                            "process_1": 7.842128923791522e-06,
                            "process_0": 7.568413683244292e-06,
                            "process_2": 8.137994902795606e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.00529011497819099,
                            "process_1": 0.005265667130897983,
                            "process_0": 0.005299223721111283,
                            "process_2": 0.0052868088294742354
                        },
                        "total_energy_joules": {
                            "process_3": 19044.413921487565,
                            "process_1": 18956.40167123274,
                            "process_0": 19077.20539600062,
                            "process_2": 19032.511786107247
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 529.887178618433,
                        "ram_power_avg": 0.9647469520568848,
                        "cpu_energy_total": 0.0045886762506561355,
                        "gpu_energy_total": 0.016521621828398114,
                        "ram_energy_total": 3.1516580620241056e-05,
                        "total_energy_kwh": 0.021141814659674494,
                        "total_energy_joules": 76110.53277482817
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21526586929133462,
                        "joules_per_token": 4.6454182601823835,
                        "flops_per_joule": 222702041.03416574,
                        "joules_per_flop": 4.490304603210105e-09
                    },
                    "per-process_emissions": [
                        0.002015269300941858,
                        0.002005955893515587,
                        0.0020187392765573433,
                        0.00201400982358821
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0305": {
            "setup": {
                "experiment_id": "0305",
                "date_time": "April 11, 2025 at 06:30:33 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.25063190000583,
                        "average_latency_ms_per_batch": 4656.3289875007285,
                        "throughput_queries_per_sec": 3.4361833201540932,
                        "throughput_tokens_per_sec": 439.83146497972393
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2694619136
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0305",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 566.2170327922812,
                            "process_1": 450.618838201581,
                            "process_0": 413.4561972506076,
                            "process_2": 689.2566462292624
                        },
                        "ram_power": {
                            "process_3": 0.9619989395141602,
                            "process_1": 0.9776401519775392,
                            "process_0": 0.9402565956115723,
                            "process_2": 0.9790921211242675
                        },
                        "cpu_energy": {
                            "process_3": 0.001149004184124692,
                            "process_1": 0.0011417217090940995,
                            "process_0": 0.0011507003279999709,
                            "process_2": 0.0011472500294373734
                        },
                        "gpu_energy": {
                            "process_3": 0.004133142750955887,
                            "process_1": 0.004116103292880091,
                            "process_0": 0.004140954979428069,
                            "process_2": 0.004131420805134067
                        },
                        "ram_energy": {
                            "process_3": 7.968043110409637e-06,
                            "process_1": 7.842128923791522e-06,
                            "process_0": 7.568413683244292e-06,
                            "process_2": 8.137994902795606e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.00529011497819099,
                            "process_1": 0.005265667130897983,
                            "process_0": 0.005299223721111283,
                            "process_2": 0.0052868088294742354
                        },
                        "total_energy_joules": {
                            "process_3": 19044.413921487565,
                            "process_1": 18956.40167123274,
                            "process_0": 19077.20539600062,
                            "process_2": 19032.511786107247
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 529.887178618433,
                        "ram_power_avg": 0.9647469520568848,
                        "cpu_energy_total": 0.0045886762506561355,
                        "gpu_energy_total": 0.016521621828398114,
                        "ram_energy_total": 3.1516580620241056e-05,
                        "total_energy_kwh": 0.021141814659674494,
                        "total_energy_joules": 76110.53277482817
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21526586929133462,
                        "joules_per_token": 4.6454182601823835,
                        "flops_per_joule": 222702041.03416574,
                        "joules_per_flop": 4.490304603210105e-09
                    },
                    "per-process_emissions": [
                        0.002015269300941858,
                        0.002005955893515587,
                        0.0020187392765573433,
                        0.00201400982358821
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0306": {
            "setup": {
                "experiment_id": "0306",
                "date_time": "April 11, 2025 at 06:31:40 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.874463046005985,
                        "average_latency_ms_per_batch": 4609.307880750748,
                        "throughput_queries_per_sec": 3.471236986971236,
                        "throughput_tokens_per_sec": 444.3183343323182
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            91.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2651906048
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0306",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 411.29174579572503,
                            "process_1": 387.1080956286788,
                            "process_3": 368.749821081881,
                            "process_2": 372.68417664042113
                        },
                        "ram_power": {
                            "process_0": 0.9260330200195314,
                            "process_1": 0.9680972099304199,
                            "process_3": 0.9745745658874513,
                            "process_2": 0.9589490890502931
                        },
                        "cpu_energy": {
                            "process_0": 0.0011361622156250638,
                            "process_1": 0.0011315450821249445,
                            "process_3": 0.0011505893765308885,
                            "process_2": 0.0011530512320620122
                        },
                        "gpu_energy": {
                            "process_0": 0.00411967885129616,
                            "process_1": 0.004086146602248042,
                            "process_3": 0.004154474712466105,
                            "process_2": 0.004162983330383896
                        },
                        "ram_energy": {
                            "process_0": 7.37563822613839e-06,
                            "process_1": 7.721923976944302e-06,
                            "process_3": 7.852320432116734e-06,
                            "process_2": 7.777821388871636e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005263216705147363,
                            "process_1": 0.005225413608349931,
                            "process_3": 0.0053129164094291095,
                            "process_2": 0.005323812383834778
                        },
                        "total_energy_joules": {
                            "process_0": 18947.580138530506,
                            "process_1": 18811.48899005975,
                            "process_3": 19126.499073944793,
                            "process_2": 19165.7245818052
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 384.9584597866765,
                        "ram_power_avg": 0.9569134712219238,
                        "cpu_energy_total": 0.0045713479063429095,
                        "gpu_energy_total": 0.016523283496394203,
                        "ram_energy_total": 3.072770402407106e-05,
                        "total_energy_kwh": 0.02112535910676118,
                        "total_energy_joules": 76051.29278434026
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21543355017593643,
                        "joules_per_token": 4.641802538106705,
                        "flops_per_joule": 222875514.3087084,
                        "joules_per_flop": 4.48680961253951e-09
                    },
                    "per-process_emissions": [
                        0.002005022403825888,
                        0.0019906213141009065,
                        0.0020239555061720193,
                        0.002028106327621859
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0306": {
            "setup": {
                "experiment_id": "0306",
                "date_time": "April 11, 2025 at 06:31:40 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.874463046005985,
                        "average_latency_ms_per_batch": 4609.307880750748,
                        "throughput_queries_per_sec": 3.471236986971236,
                        "throughput_tokens_per_sec": 444.3183343323182
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            91.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2651906048
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0306",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 411.29174579572503,
                            "process_1": 387.1080956286788,
                            "process_3": 368.749821081881,
                            "process_2": 372.68417664042113
                        },
                        "ram_power": {
                            "process_0": 0.9260330200195314,
                            "process_1": 0.9680972099304199,
                            "process_3": 0.9745745658874513,
                            "process_2": 0.9589490890502931
                        },
                        "cpu_energy": {
                            "process_0": 0.0011361622156250638,
                            "process_1": 0.0011315450821249445,
                            "process_3": 0.0011505893765308885,
                            "process_2": 0.0011530512320620122
                        },
                        "gpu_energy": {
                            "process_0": 0.00411967885129616,
                            "process_1": 0.004086146602248042,
                            "process_3": 0.004154474712466105,
                            "process_2": 0.004162983330383896
                        },
                        "ram_energy": {
                            "process_0": 7.37563822613839e-06,
                            "process_1": 7.721923976944302e-06,
                            "process_3": 7.852320432116734e-06,
                            "process_2": 7.777821388871636e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005263216705147363,
                            "process_1": 0.005225413608349931,
                            "process_3": 0.0053129164094291095,
                            "process_2": 0.005323812383834778
                        },
                        "total_energy_joules": {
                            "process_0": 18947.580138530506,
                            "process_1": 18811.48899005975,
                            "process_3": 19126.499073944793,
                            "process_2": 19165.7245818052
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 384.9584597866765,
                        "ram_power_avg": 0.9569134712219238,
                        "cpu_energy_total": 0.0045713479063429095,
                        "gpu_energy_total": 0.016523283496394203,
                        "ram_energy_total": 3.072770402407106e-05,
                        "total_energy_kwh": 0.02112535910676118,
                        "total_energy_joules": 76051.29278434026
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21543355017593643,
                        "joules_per_token": 4.641802538106705,
                        "flops_per_joule": 222875514.3087084,
                        "joules_per_flop": 4.48680961253951e-09
                    },
                    "per-process_emissions": [
                        0.002005022403825888,
                        0.0019906213141009065,
                        0.0020239555061720193,
                        0.002028106327621859
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0307": {
            "setup": {
                "experiment_id": "0307",
                "date_time": "April 11, 2025 at 06:32:49 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.22712447800586,
                        "average_latency_ms_per_batch": 4653.390559750733,
                        "throughput_queries_per_sec": 3.4383531308098645,
                        "throughput_tokens_per_sec": 440.10920074366265
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2672742400
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0307",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 431.4207266804503,
                            "process_0": 432.7705472408762,
                            "process_3": 0.0,
                            "process_2": 431.0388161794864
                        },
                        "ram_power": {
                            "process_1": 0.9687638282775879,
                            "process_0": 0.9323186874389648,
                            "process_3": 0.9501700401306152,
                            "process_2": 0.9677939414978027
                        },
                        "cpu_energy": {
                            "process_1": 0.001140352462656324,
                            "process_0": 0.0011507653811879664,
                            "process_3": 0.0011458788179999142,
                            "process_2": 0.0011376030963124323
                        },
                        "gpu_energy": {
                            "process_1": 0.004128513025029867,
                            "process_0": 0.004157017214500014,
                            "process_3": 0.0041456802609857735,
                            "process_2": 0.00412227579781782
                        },
                        "ram_energy": {
                            "process_1": 7.737812169353967e-06,
                            "process_0": 7.484034727327974e-06,
                            "process_3": 7.879037949463737e-06,
                            "process_2": 7.710932369738852e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0052766032998555435,
                            "process_0": 0.005315266630415309,
                            "process_3": 0.005299438116935155,
                            "process_2": 0.005267589826499991
                        },
                        "total_energy_joules": {
                            "process_1": 18995.77187947996,
                            "process_0": 19134.959869495113,
                            "process_3": 19077.977220966557,
                            "process_2": 18963.32337539997
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 323.80752252520324,
                        "ram_power_avg": 0.9547616243362427,
                        "cpu_energy_total": 0.004574599758156637,
                        "gpu_energy_total": 0.016553486298333475,
                        "ram_energy_total": 3.081181721588453e-05,
                        "total_energy_kwh": 0.021158897873706,
                        "total_energy_joules": 76172.03234534159
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21509206851301751,
                        "joules_per_token": 4.649171896077978,
                        "flops_per_joule": 222522236.45951596,
                        "joules_per_flop": 4.49393290266491e-09
                    },
                    "per-process_emissions": [
                        0.0020101220270799694,
                        0.002024850822856712,
                        0.0020188209506464476,
                        0.002006688344405172
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0307": {
            "setup": {
                "experiment_id": "0307",
                "date_time": "April 11, 2025 at 06:32:49 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.22712447800586,
                        "average_latency_ms_per_batch": 4653.390559750733,
                        "throughput_queries_per_sec": 3.4383531308098645,
                        "throughput_tokens_per_sec": 440.10920074366265
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2672742400
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0307",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 431.4207266804503,
                            "process_0": 432.7705472408762,
                            "process_3": 0.0,
                            "process_2": 431.0388161794864
                        },
                        "ram_power": {
                            "process_1": 0.9687638282775879,
                            "process_0": 0.9323186874389648,
                            "process_3": 0.9501700401306152,
                            "process_2": 0.9677939414978027
                        },
                        "cpu_energy": {
                            "process_1": 0.001140352462656324,
                            "process_0": 0.0011507653811879664,
                            "process_3": 0.0011458788179999142,
                            "process_2": 0.0011376030963124323
                        },
                        "gpu_energy": {
                            "process_1": 0.004128513025029867,
                            "process_0": 0.004157017214500014,
                            "process_3": 0.0041456802609857735,
                            "process_2": 0.00412227579781782
                        },
                        "ram_energy": {
                            "process_1": 7.737812169353967e-06,
                            "process_0": 7.484034727327974e-06,
                            "process_3": 7.879037949463737e-06,
                            "process_2": 7.710932369738852e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0052766032998555435,
                            "process_0": 0.005315266630415309,
                            "process_3": 0.005299438116935155,
                            "process_2": 0.005267589826499991
                        },
                        "total_energy_joules": {
                            "process_1": 18995.77187947996,
                            "process_0": 19134.959869495113,
                            "process_3": 19077.977220966557,
                            "process_2": 18963.32337539997
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 323.80752252520324,
                        "ram_power_avg": 0.9547616243362427,
                        "cpu_energy_total": 0.004574599758156637,
                        "gpu_energy_total": 0.016553486298333475,
                        "ram_energy_total": 3.081181721588453e-05,
                        "total_energy_kwh": 0.021158897873706,
                        "total_energy_joules": 76172.03234534159
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21509206851301751,
                        "joules_per_token": 4.649171896077978,
                        "flops_per_joule": 222522236.45951596,
                        "joules_per_flop": 4.49393290266491e-09
                    },
                    "per-process_emissions": [
                        0.0020101220270799694,
                        0.002024850822856712,
                        0.0020188209506464476,
                        0.002006688344405172
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0308": {
            "setup": {
                "experiment_id": "0308",
                "date_time": "April 11, 2025 at 06:33:55 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.74441998500333,
                        "average_latency_ms_per_batch": 4593.052498125417,
                        "throughput_queries_per_sec": 3.4835221253251847,
                        "throughput_tokens_per_sec": 445.89083204162364
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            19.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2628280320
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0308",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 437.84975047151204,
                            "process_1": 340.5068475137976,
                            "process_3": 443.02797731755857,
                            "process_2": 444.2620825310913
                        },
                        "ram_power": {
                            "process_0": 0.9174957275390625,
                            "process_1": 0.9789104461669922,
                            "process_3": 0.9760794639587402,
                            "process_2": 0.9588189125061035
                        },
                        "cpu_energy": {
                            "process_0": 0.0011375277889057996,
                            "process_1": 0.0011510260955004692,
                            "process_3": 0.0011308565689686248,
                            "process_2": 0.0011377099602810858
                        },
                        "gpu_energy": {
                            "process_0": 0.0041419097024139595,
                            "process_1": 0.004180389177641852,
                            "process_3": 0.004131342471737964,
                            "process_2": 0.00414657053947598
                        },
                        "ram_energy": {
                            "process_0": 7.322863111530788e-06,
                            "process_1": 7.867775460567995e-06,
                            "process_3": 7.779717200243565e-06,
                            "process_2": 7.69341892957711e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.00528676035443129,
                            "process_1": 0.005339283048602891,
                            "process_3": 0.0052699787579068335,
                            "process_2": 0.005291973918686643
                        },
                        "total_energy_joules": {
                            "process_0": 19032.337275952643,
                            "process_1": 19221.41897497041,
                            "process_3": 18971.923528464602,
                            "process_2": 19051.106107271917
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 416.4116644584899,
                        "ram_power_avg": 0.9578261375427246,
                        "cpu_energy_total": 0.004557120413655979,
                        "gpu_energy_total": 0.016600211891269756,
                        "ram_energy_total": 3.066377470191946e-05,
                        "total_energy_kwh": 0.021187996079627656,
                        "total_energy_joules": 76276.78588665956
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.214796675155468,
                        "joules_per_token": 4.655565544840061,
                        "flops_per_joule": 222216638.9960129,
                        "joules_per_flop": 4.5001130631713965e-09
                    },
                    "per-process_emissions": [
                        0.0020139913570206,
                        0.0020339998773652715,
                        0.0020075984078246085,
                        0.0020159774643236767
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0308": {
            "setup": {
                "experiment_id": "0308",
                "date_time": "April 11, 2025 at 06:33:55 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.74441998500333,
                        "average_latency_ms_per_batch": 4593.052498125417,
                        "throughput_queries_per_sec": 3.4835221253251847,
                        "throughput_tokens_per_sec": 445.89083204162364
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            19.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2628280320
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0308",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 437.84975047151204,
                            "process_1": 340.5068475137976,
                            "process_3": 443.02797731755857,
                            "process_2": 444.2620825310913
                        },
                        "ram_power": {
                            "process_0": 0.9174957275390625,
                            "process_1": 0.9789104461669922,
                            "process_3": 0.9760794639587402,
                            "process_2": 0.9588189125061035
                        },
                        "cpu_energy": {
                            "process_0": 0.0011375277889057996,
                            "process_1": 0.0011510260955004692,
                            "process_3": 0.0011308565689686248,
                            "process_2": 0.0011377099602810858
                        },
                        "gpu_energy": {
                            "process_0": 0.0041419097024139595,
                            "process_1": 0.004180389177641852,
                            "process_3": 0.004131342471737964,
                            "process_2": 0.00414657053947598
                        },
                        "ram_energy": {
                            "process_0": 7.322863111530788e-06,
                            "process_1": 7.867775460567995e-06,
                            "process_3": 7.779717200243565e-06,
                            "process_2": 7.69341892957711e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.00528676035443129,
                            "process_1": 0.005339283048602891,
                            "process_3": 0.0052699787579068335,
                            "process_2": 0.005291973918686643
                        },
                        "total_energy_joules": {
                            "process_0": 19032.337275952643,
                            "process_1": 19221.41897497041,
                            "process_3": 18971.923528464602,
                            "process_2": 19051.106107271917
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 416.4116644584899,
                        "ram_power_avg": 0.9578261375427246,
                        "cpu_energy_total": 0.004557120413655979,
                        "gpu_energy_total": 0.016600211891269756,
                        "ram_energy_total": 3.066377470191946e-05,
                        "total_energy_kwh": 0.021187996079627656,
                        "total_energy_joules": 76276.78588665956
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.214796675155468,
                        "joules_per_token": 4.655565544840061,
                        "flops_per_joule": 222216638.9960129,
                        "joules_per_flop": 4.5001130631713965e-09
                    },
                    "per-process_emissions": [
                        0.0020139913570206,
                        0.0020339998773652715,
                        0.0020075984078246085,
                        0.0020159774643236767
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0309": {
            "setup": {
                "experiment_id": "0309",
                "date_time": "April 11, 2025 at 06:35:01 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.353906473999814,
                        "average_latency_ms_per_batch": 4669.238309249977,
                        "throughput_queries_per_sec": 3.426683099104893,
                        "throughput_tokens_per_sec": 438.6154366854263
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            99.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2675462144
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0309",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 307.02573110267167,
                            "process_3": 480.2869778291142,
                            "process_2": 274.3732385153243,
                            "process_1": 439.35126304282267
                        },
                        "ram_power": {
                            "process_0": 0.9338879585266113,
                            "process_3": 0.9711627960205078,
                            "process_2": 0.9512114524841309,
                            "process_1": 0.9711928367614746
                        },
                        "cpu_energy": {
                            "process_0": 0.0011550524647815337,
                            "process_3": 0.0011463496021559651,
                            "process_2": 0.0011514209234999271,
                            "process_1": 0.0011365753374681167
                        },
                        "gpu_energy": {
                            "process_0": 0.004168873335095968,
                            "process_3": 0.004152211377322024,
                            "process_2": 0.0041623074965098095,
                            "process_1": 0.004112234678674054
                        },
                        "ram_energy": {
                            "process_0": 7.557070887821451e-06,
                            "process_3": 8.086995881437649e-06,
                            "process_2": 7.631172462586074e-06,
                            "process_1": 7.74450249923901e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005331482870765325,
                            "process_3": 0.005306647975359425,
                            "process_2": 0.005321359592472322,
                            "process_1": 0.00525655451864141
                        },
                        "total_energy_joules": {
                            "process_0": 19193.338334755168,
                            "process_3": 19103.932711293928,
                            "process_2": 19156.89453290036,
                            "process_1": 18923.59626710908
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 375.25930262248323,
                        "ram_power_avg": 0.9568637609481812,
                        "cpu_energy_total": 0.004589398327905543,
                        "gpu_energy_total": 0.016595626887601855,
                        "ram_energy_total": 3.101974173108418e-05,
                        "total_energy_kwh": 0.021216044957238483,
                        "total_energy_joules": 76377.76184605854
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21451270113180848,
                        "joules_per_token": 4.661728628299471,
                        "flops_per_joule": 221922855.3373314,
                        "joules_per_flop": 4.506070357106576e-09
                    },
                    "per-process_emissions": [
                        0.0020310283996180503,
                        0.002021567546213173,
                        0.0020271719367523314,
                        0.0020024844438764453
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0309": {
            "setup": {
                "experiment_id": "0309",
                "date_time": "April 11, 2025 at 06:35:01 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.353906473999814,
                        "average_latency_ms_per_batch": 4669.238309249977,
                        "throughput_queries_per_sec": 3.426683099104893,
                        "throughput_tokens_per_sec": 438.6154366854263
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            99.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2675462144
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0309",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 307.02573110267167,
                            "process_3": 480.2869778291142,
                            "process_2": 274.3732385153243,
                            "process_1": 439.35126304282267
                        },
                        "ram_power": {
                            "process_0": 0.9338879585266113,
                            "process_3": 0.9711627960205078,
                            "process_2": 0.9512114524841309,
                            "process_1": 0.9711928367614746
                        },
                        "cpu_energy": {
                            "process_0": 0.0011550524647815337,
                            "process_3": 0.0011463496021559651,
                            "process_2": 0.0011514209234999271,
                            "process_1": 0.0011365753374681167
                        },
                        "gpu_energy": {
                            "process_0": 0.004168873335095968,
                            "process_3": 0.004152211377322024,
                            "process_2": 0.0041623074965098095,
                            "process_1": 0.004112234678674054
                        },
                        "ram_energy": {
                            "process_0": 7.557070887821451e-06,
                            "process_3": 8.086995881437649e-06,
                            "process_2": 7.631172462586074e-06,
                            "process_1": 7.74450249923901e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005331482870765325,
                            "process_3": 0.005306647975359425,
                            "process_2": 0.005321359592472322,
                            "process_1": 0.00525655451864141
                        },
                        "total_energy_joules": {
                            "process_0": 19193.338334755168,
                            "process_3": 19103.932711293928,
                            "process_2": 19156.89453290036,
                            "process_1": 18923.59626710908
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 375.25930262248323,
                        "ram_power_avg": 0.9568637609481812,
                        "cpu_energy_total": 0.004589398327905543,
                        "gpu_energy_total": 0.016595626887601855,
                        "ram_energy_total": 3.101974173108418e-05,
                        "total_energy_kwh": 0.021216044957238483,
                        "total_energy_joules": 76377.76184605854
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21451270113180848,
                        "joules_per_token": 4.661728628299471,
                        "flops_per_joule": 221922855.3373314,
                        "joules_per_flop": 4.506070357106576e-09
                    },
                    "per-process_emissions": [
                        0.0020310283996180503,
                        0.002021567546213173,
                        0.0020271719367523314,
                        0.0020024844438764453
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0310": {
            "setup": {
                "experiment_id": "0310",
                "date_time": "April 11, 2025 at 06:36:06 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.85616371299693,
                        "average_latency_ms_per_batch": 4607.020464124616,
                        "throughput_queries_per_sec": 3.47296047946689,
                        "throughput_tokens_per_sec": 444.5389413717619
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2673082368
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0310",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 1204.8414784427964,
                            "process_2": 286.5936494905088,
                            "process_3": 298.2000713000924,
                            "process_0": 431.9912031488015
                        },
                        "ram_power": {
                            "process_1": 0.9503860473632812,
                            "process_2": 0.9707221984863281,
                            "process_3": 0.9786787033081055,
                            "process_0": 0.9334688186645508
                        },
                        "cpu_energy": {
                            "process_1": 0.0011477094566565713,
                            "process_2": 0.0011477575752815025,
                            "process_3": 0.0011517842162188572,
                            "process_0": 0.0011390094181876979
                        },
                        "gpu_energy": {
                            "process_1": 0.004169906391478184,
                            "process_2": 0.004167538056250142,
                            "process_3": 0.0041755680626741065,
                            "process_0": 0.004144614149022285
                        },
                        "ram_energy": {
                            "process_1": 7.88604104456642e-06,
                            "process_2": 8.060230954973849e-06,
                            "process_3": 7.899663356815042e-06,
                            "process_0": 7.452646592718817e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.00532550188917932,
                            "process_2": 0.005323355862486617,
                            "process_3": 0.005335251942249778,
                            "process_0": 0.005291076213802702
                        },
                        "total_energy_joules": {
                            "process_1": 19171.806801045554,
                            "process_2": 19164.08110495182,
                            "process_3": 19206.9069920992,
                            "process_0": 19047.874369689725
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 555.4066005955498,
                        "ram_power_avg": 0.9583139419555664,
                        "cpu_energy_total": 0.004586260666344629,
                        "gpu_energy_total": 0.016657626659424718,
                        "ram_energy_total": 3.1298581949074126e-05,
                        "total_energy_kwh": 0.021275185907718416,
                        "total_energy_joules": 76590.6692677863
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21391639682264846,
                        "joules_per_token": 4.67472346605141,
                        "flops_per_joule": 221305952.2157888,
                        "joules_per_flop": 4.518631288438776e-09
                    },
                    "per-process_emissions": [
                        0.002028749944682862,
                        0.0020279324158142768,
                        0.002032464227400053,
                        0.0020156354836481393
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0310": {
            "setup": {
                "experiment_id": "0310",
                "date_time": "April 11, 2025 at 06:36:06 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.85616371299693,
                        "average_latency_ms_per_batch": 4607.020464124616,
                        "throughput_queries_per_sec": 3.47296047946689,
                        "throughput_tokens_per_sec": 444.5389413717619
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2673082368
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0310",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 1204.8414784427964,
                            "process_2": 286.5936494905088,
                            "process_3": 298.2000713000924,
                            "process_0": 431.9912031488015
                        },
                        "ram_power": {
                            "process_1": 0.9503860473632812,
                            "process_2": 0.9707221984863281,
                            "process_3": 0.9786787033081055,
                            "process_0": 0.9334688186645508
                        },
                        "cpu_energy": {
                            "process_1": 0.0011477094566565713,
                            "process_2": 0.0011477575752815025,
                            "process_3": 0.0011517842162188572,
                            "process_0": 0.0011390094181876979
                        },
                        "gpu_energy": {
                            "process_1": 0.004169906391478184,
                            "process_2": 0.004167538056250142,
                            "process_3": 0.0041755680626741065,
                            "process_0": 0.004144614149022285
                        },
                        "ram_energy": {
                            "process_1": 7.88604104456642e-06,
                            "process_2": 8.060230954973849e-06,
                            "process_3": 7.899663356815042e-06,
                            "process_0": 7.452646592718817e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.00532550188917932,
                            "process_2": 0.005323355862486617,
                            "process_3": 0.005335251942249778,
                            "process_0": 0.005291076213802702
                        },
                        "total_energy_joules": {
                            "process_1": 19171.806801045554,
                            "process_2": 19164.08110495182,
                            "process_3": 19206.9069920992,
                            "process_0": 19047.874369689725
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 555.4066005955498,
                        "ram_power_avg": 0.9583139419555664,
                        "cpu_energy_total": 0.004586260666344629,
                        "gpu_energy_total": 0.016657626659424718,
                        "ram_energy_total": 3.1298581949074126e-05,
                        "total_energy_kwh": 0.021275185907718416,
                        "total_energy_joules": 76590.6692677863
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21391639682264846,
                        "joules_per_token": 4.67472346605141,
                        "flops_per_joule": 221305952.2157888,
                        "joules_per_flop": 4.518631288438776e-09
                    },
                    "per-process_emissions": [
                        0.002028749944682862,
                        0.0020279324158142768,
                        0.002032464227400053,
                        0.0020156354836481393
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0311": {
            "setup": {
                "experiment_id": "0311",
                "date_time": "April 11, 2025 at 06:37:12 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.47841536100168,
                        "average_latency_ms_per_batch": 4559.80192012521,
                        "throughput_queries_per_sec": 3.5089243524772775,
                        "throughput_tokens_per_sec": 449.1423171170915
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            92.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2555088896
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0311",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 426.0523418012017,
                            "process_3": 441.41001992045943,
                            "process_2": 398.2740537791687,
                            "process_1": 382.4901620029904
                        },
                        "ram_power": {
                            "process_0": 0.891782283782959,
                            "process_3": 0.9619402885437012,
                            "process_2": 0.959099292755127,
                            "process_1": 0.9786386489868165
                        },
                        "cpu_energy": {
                            "process_0": 0.0011260705640937657,
                            "process_3": 0.0011334558067500212,
                            "process_2": 0.001138230240530902,
                            "process_1": 0.0011419885041873386
                        },
                        "gpu_energy": {
                            "process_0": 0.004124934966612093,
                            "process_3": 0.004144084981931917,
                            "process_2": 0.004165580276905945,
                            "process_1": 0.004170276113995897
                        },
                        "ram_energy": {
                            "process_0": 7.056061646868521e-06,
                            "process_3": 7.693919699286686e-06,
                            "process_2": 7.702355208278876e-06,
                            "process_1": 7.922240791792477e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0052580615923527255,
                            "process_3": 0.005285234708381226,
                            "process_2": 0.005311512872645126,
                            "process_1": 0.005320186858975028
                        },
                        "total_energy_joules": {
                            "process_0": 18929.021732469813,
                            "process_3": 19026.844950172414,
                            "process_2": 19121.446341522456,
                            "process_1": 19152.672692310098
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 412.0566443759551,
                        "ram_power_avg": 0.9478651285171509,
                        "cpu_energy_total": 0.004539745115562027,
                        "gpu_energy_total": 0.01660487633944585,
                        "ram_energy_total": 3.037457734622656e-05,
                        "total_energy_kwh": 0.021174996032354107,
                        "total_energy_joules": 76229.98571647478
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2149285461096328,
                        "joules_per_token": 4.6527090891403065,
                        "flops_per_joule": 222353065.31729788,
                        "joules_per_flop": 4.497351986459012e-09
                    },
                    "per-process_emissions": [
                        0.002003058563606771,
                        0.002013410162157828,
                        0.0020234208288341608,
                        0.002026725183926537
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0311": {
            "setup": {
                "experiment_id": "0311",
                "date_time": "April 11, 2025 at 06:37:12 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.47841536100168,
                        "average_latency_ms_per_batch": 4559.80192012521,
                        "throughput_queries_per_sec": 3.5089243524772775,
                        "throughput_tokens_per_sec": 449.1423171170915
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            92.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2555088896
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0311",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 426.0523418012017,
                            "process_3": 441.41001992045943,
                            "process_2": 398.2740537791687,
                            "process_1": 382.4901620029904
                        },
                        "ram_power": {
                            "process_0": 0.891782283782959,
                            "process_3": 0.9619402885437012,
                            "process_2": 0.959099292755127,
                            "process_1": 0.9786386489868165
                        },
                        "cpu_energy": {
                            "process_0": 0.0011260705640937657,
                            "process_3": 0.0011334558067500212,
                            "process_2": 0.001138230240530902,
                            "process_1": 0.0011419885041873386
                        },
                        "gpu_energy": {
                            "process_0": 0.004124934966612093,
                            "process_3": 0.004144084981931917,
                            "process_2": 0.004165580276905945,
                            "process_1": 0.004170276113995897
                        },
                        "ram_energy": {
                            "process_0": 7.056061646868521e-06,
                            "process_3": 7.693919699286686e-06,
                            "process_2": 7.702355208278876e-06,
                            "process_1": 7.922240791792477e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0052580615923527255,
                            "process_3": 0.005285234708381226,
                            "process_2": 0.005311512872645126,
                            "process_1": 0.005320186858975028
                        },
                        "total_energy_joules": {
                            "process_0": 18929.021732469813,
                            "process_3": 19026.844950172414,
                            "process_2": 19121.446341522456,
                            "process_1": 19152.672692310098
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 412.0566443759551,
                        "ram_power_avg": 0.9478651285171509,
                        "cpu_energy_total": 0.004539745115562027,
                        "gpu_energy_total": 0.01660487633944585,
                        "ram_energy_total": 3.037457734622656e-05,
                        "total_energy_kwh": 0.021174996032354107,
                        "total_energy_joules": 76229.98571647478
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2149285461096328,
                        "joules_per_token": 4.6527090891403065,
                        "flops_per_joule": 222353065.31729788,
                        "joules_per_flop": 4.497351986459012e-09
                    },
                    "per-process_emissions": [
                        0.002003058563606771,
                        0.002013410162157828,
                        0.0020234208288341608,
                        0.002026725183926537
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0312": {
            "setup": {
                "experiment_id": "0312",
                "date_time": "April 11, 2025 at 06:38:18 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.80444539099699,
                        "average_latency_ms_per_batch": 4600.555673874624,
                        "throughput_queries_per_sec": 3.4778407510335976,
                        "throughput_tokens_per_sec": 445.1636161323005
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2555072512
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0312",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 460.131541408428,
                            "process_1": 12.641982191390031,
                            "process_0": 432.73041210059444,
                            "process_3": 430.2639473379498
                        },
                        "ram_power": {
                            "process_2": 0.9704747200012207,
                            "process_1": 0.9750795364379883,
                            "process_0": 0.8919467926025391,
                            "process_3": 0.9697794914245605
                        },
                        "cpu_energy": {
                            "process_2": 0.0011324940094062867,
                            "process_1": 0.001173244973281385,
                            "process_0": 0.0011377906212500194,
                            "process_3": 0.0011403083679061865
                        },
                        "gpu_energy": {
                            "process_2": 0.0041273096907339935,
                            "process_1": 0.00417128139257783,
                            "process_0": 0.004147589151402009,
                            "process_3": 0.0041571394368199455
                        },
                        "ram_energy": {
                            "process_2": 7.74746216091196e-06,
                            "process_1": 8.03458384540423e-06,
                            "process_0": 7.14300294075635e-06,
                            "process_3": 7.785883158040544e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005267551162301191,
                            "process_1": 0.005352560949704621,
                            "process_0": 0.0052925227755927845,
                            "process_3": 0.00530523368788417
                        },
                        "total_energy_joules": {
                            "process_2": 18963.184184284288,
                            "process_1": 19269.219418936635,
                            "process_0": 19053.081992134023,
                            "process_3": 19098.841276383013
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 333.94197075959056,
                        "ram_power_avg": 0.9518201351165771,
                        "cpu_energy_total": 0.004583837971843877,
                        "gpu_energy_total": 0.01660331967153378,
                        "ram_energy_total": 3.071093210511308e-05,
                        "total_energy_kwh": 0.021217868575482766,
                        "total_energy_joules": 76384.32687173795
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21449426434707572,
                        "joules_per_token": 4.6621293256676,
                        "flops_per_joule": 221903781.66994694,
                        "joules_per_flop": 4.5064576749186284e-09
                    },
                    "per-process_emissions": [
                        0.002006673615278639,
                        0.002039058093789975,
                        0.002016186551362071,
                        0.0020210287733994745
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0312": {
            "setup": {
                "experiment_id": "0312",
                "date_time": "April 11, 2025 at 06:38:18 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.80444539099699,
                        "average_latency_ms_per_batch": 4600.555673874624,
                        "throughput_queries_per_sec": 3.4778407510335976,
                        "throughput_tokens_per_sec": 445.1636161323005
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2555072512
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0312",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 460.131541408428,
                            "process_1": 12.641982191390031,
                            "process_0": 432.73041210059444,
                            "process_3": 430.2639473379498
                        },
                        "ram_power": {
                            "process_2": 0.9704747200012207,
                            "process_1": 0.9750795364379883,
                            "process_0": 0.8919467926025391,
                            "process_3": 0.9697794914245605
                        },
                        "cpu_energy": {
                            "process_2": 0.0011324940094062867,
                            "process_1": 0.001173244973281385,
                            "process_0": 0.0011377906212500194,
                            "process_3": 0.0011403083679061865
                        },
                        "gpu_energy": {
                            "process_2": 0.0041273096907339935,
                            "process_1": 0.00417128139257783,
                            "process_0": 0.004147589151402009,
                            "process_3": 0.0041571394368199455
                        },
                        "ram_energy": {
                            "process_2": 7.74746216091196e-06,
                            "process_1": 8.03458384540423e-06,
                            "process_0": 7.14300294075635e-06,
                            "process_3": 7.785883158040544e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005267551162301191,
                            "process_1": 0.005352560949704621,
                            "process_0": 0.0052925227755927845,
                            "process_3": 0.00530523368788417
                        },
                        "total_energy_joules": {
                            "process_2": 18963.184184284288,
                            "process_1": 19269.219418936635,
                            "process_0": 19053.081992134023,
                            "process_3": 19098.841276383013
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 333.94197075959056,
                        "ram_power_avg": 0.9518201351165771,
                        "cpu_energy_total": 0.004583837971843877,
                        "gpu_energy_total": 0.01660331967153378,
                        "ram_energy_total": 3.071093210511308e-05,
                        "total_energy_kwh": 0.021217868575482766,
                        "total_energy_joules": 76384.32687173795
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21449426434707572,
                        "joules_per_token": 4.6621293256676,
                        "flops_per_joule": 221903781.66994694,
                        "joules_per_flop": 4.5064576749186284e-09
                    },
                    "per-process_emissions": [
                        0.002006673615278639,
                        0.002039058093789975,
                        0.002016186551362071,
                        0.0020210287733994745
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0313": {
            "setup": {
                "experiment_id": "0313",
                "date_time": "April 11, 2025 at 06:39:24 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.74930079700425,
                        "average_latency_ms_per_batch": 4593.662599625532,
                        "throughput_queries_per_sec": 3.483059465730961,
                        "throughput_tokens_per_sec": 445.831611613563
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            97.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.5,
                        "cpu_memory_usage_bytes": 2673180672
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0313",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 435.1718741816514,
                            "process_1": 427.4147994352998,
                            "process_2": 445.3208580371184,
                            "process_0": 456.36535942241545
                        },
                        "ram_power": {
                            "process_3": 0.9774656295776368,
                            "process_1": 0.9790263175964355,
                            "process_2": 0.9673504829406738,
                            "process_0": 0.9324259757995605
                        },
                        "cpu_energy": {
                            "process_3": 0.0011338120156879085,
                            "process_1": 0.0011368195720616542,
                            "process_2": 0.001149498950125121,
                            "process_0": 0.00113598189465597
                        },
                        "gpu_energy": {
                            "process_3": 0.004128691358506076,
                            "process_1": 0.004124869411003984,
                            "process_2": 0.004161756384957971,
                            "process_0": 0.004130475526600175
                        },
                        "ram_energy": {
                            "process_3": 7.757641338733848e-06,
                            "process_1": 7.802977768050764e-06,
                            "process_2": 8.026877641272774e-06,
                            "process_0": 7.424530164804228e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005270261015532718,
                            "process_1": 0.005269491960833687,
                            "process_2": 0.005319282212724365,
                            "process_0": 0.0052738819514209475
                        },
                        "total_energy_joules": {
                            "process_3": 18972.939655917788,
                            "process_1": 18970.171059001274,
                            "process_2": 19149.415965807715,
                            "process_0": 18985.97502511541
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 441.06822276912123,
                        "ram_power_avg": 0.9640671014785767,
                        "cpu_energy_total": 0.004556112432530654,
                        "gpu_energy_total": 0.016545792681068205,
                        "ram_energy_total": 3.101202691286161e-05,
                        "total_energy_kwh": 0.021132917140511718,
                        "total_energy_joules": 76078.50170584218
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21535650193728575,
                        "joules_per_token": 4.643463238881969,
                        "flops_per_joule": 222795804.50583962,
                        "joules_per_flop": 4.48841486139291e-09
                    },
                    "per-process_emissions": [
                        0.002007705933867189,
                        0.002007412962479593,
                        0.002026380558937347,
                        0.00200908532939381
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0313": {
            "setup": {
                "experiment_id": "0313",
                "date_time": "April 11, 2025 at 06:39:24 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.74930079700425,
                        "average_latency_ms_per_batch": 4593.662599625532,
                        "throughput_queries_per_sec": 3.483059465730961,
                        "throughput_tokens_per_sec": 445.831611613563
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            97.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.5,
                        "cpu_memory_usage_bytes": 2673180672
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0313",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 435.1718741816514,
                            "process_1": 427.4147994352998,
                            "process_2": 445.3208580371184,
                            "process_0": 456.36535942241545
                        },
                        "ram_power": {
                            "process_3": 0.9774656295776368,
                            "process_1": 0.9790263175964355,
                            "process_2": 0.9673504829406738,
                            "process_0": 0.9324259757995605
                        },
                        "cpu_energy": {
                            "process_3": 0.0011338120156879085,
                            "process_1": 0.0011368195720616542,
                            "process_2": 0.001149498950125121,
                            "process_0": 0.00113598189465597
                        },
                        "gpu_energy": {
                            "process_3": 0.004128691358506076,
                            "process_1": 0.004124869411003984,
                            "process_2": 0.004161756384957971,
                            "process_0": 0.004130475526600175
                        },
                        "ram_energy": {
                            "process_3": 7.757641338733848e-06,
                            "process_1": 7.802977768050764e-06,
                            "process_2": 8.026877641272774e-06,
                            "process_0": 7.424530164804228e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005270261015532718,
                            "process_1": 0.005269491960833687,
                            "process_2": 0.005319282212724365,
                            "process_0": 0.0052738819514209475
                        },
                        "total_energy_joules": {
                            "process_3": 18972.939655917788,
                            "process_1": 18970.171059001274,
                            "process_2": 19149.415965807715,
                            "process_0": 18985.97502511541
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 441.06822276912123,
                        "ram_power_avg": 0.9640671014785767,
                        "cpu_energy_total": 0.004556112432530654,
                        "gpu_energy_total": 0.016545792681068205,
                        "ram_energy_total": 3.101202691286161e-05,
                        "total_energy_kwh": 0.021132917140511718,
                        "total_energy_joules": 76078.50170584218
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21535650193728575,
                        "joules_per_token": 4.643463238881969,
                        "flops_per_joule": 222795804.50583962,
                        "joules_per_flop": 4.48841486139291e-09
                    },
                    "per-process_emissions": [
                        0.002007705933867189,
                        0.002007412962479593,
                        0.002026380558937347,
                        0.00200908532939381
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0314": {
            "setup": {
                "experiment_id": "0314",
                "date_time": "April 11, 2025 at 06:40:30 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.2675071320009,
                        "average_latency_ms_per_batch": 4658.438391500113,
                        "throughput_queries_per_sec": 3.4346273698057157,
                        "throughput_tokens_per_sec": 439.6323033351316
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2674552832
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0314",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 245.7845533810929,
                            "process_2": 395.18199874798864,
                            "process_3": 402.0546785712256,
                            "process_1": 427.1167597242292
                        },
                        "ram_power": {
                            "process_0": 0.9333558082580568,
                            "process_2": 0.9752626419067383,
                            "process_3": 0.9594311714172363,
                            "process_1": 0.9662818908691406
                        },
                        "cpu_energy": {
                            "process_0": 0.001152431789156594,
                            "process_2": 0.001153821296718547,
                            "process_3": 0.0011343179064374452,
                            "process_1": 0.0011307776068749718
                        },
                        "gpu_energy": {
                            "process_0": 0.004163469164105871,
                            "process_2": 0.004170053613817781,
                            "process_3": 0.004107624674985932,
                            "process_1": 0.004110263565985983
                        },
                        "ram_energy": {
                            "process_0": 7.488628917134048e-06,
                            "process_2": 7.837211065726147e-06,
                            "process_3": 7.616490424078213e-06,
                            "process_1": 7.681703091092155e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005323389582179599,
                            "process_2": 0.005331712121602054,
                            "process_3": 0.005249559071847454,
                            "process_1": 0.005248722875952047
                        },
                        "total_energy_joules": {
                            "process_0": 19164.202495846555,
                            "process_2": 19194.163637767393,
                            "process_3": 18898.412658650836,
                            "process_1": 18895.40235342737
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 367.5344976061341,
                        "ram_power_avg": 0.958582878112793,
                        "cpu_energy_total": 0.004571348599187558,
                        "gpu_energy_total": 0.016551411018895568,
                        "ram_energy_total": 3.062403349803056e-05,
                        "total_energy_kwh": 0.021153383651581155,
                        "total_energy_joules": 76152.18114569216
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21514813828713067,
                        "joules_per_token": 4.647960275005625,
                        "flops_per_joule": 222580243.115083,
                        "joules_per_flop": 4.49276173843947e-09
                    },
                    "per-process_emissions": [
                        0.0020279452613313183,
                        0.0020311157327243022,
                        0.001999819528420288,
                        0.0019995009795939324
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0314": {
            "setup": {
                "experiment_id": "0314",
                "date_time": "April 11, 2025 at 06:40:30 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.2675071320009,
                        "average_latency_ms_per_batch": 4658.438391500113,
                        "throughput_queries_per_sec": 3.4346273698057157,
                        "throughput_tokens_per_sec": 439.6323033351316
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2674552832
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0314",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 245.7845533810929,
                            "process_2": 395.18199874798864,
                            "process_3": 402.0546785712256,
                            "process_1": 427.1167597242292
                        },
                        "ram_power": {
                            "process_0": 0.9333558082580568,
                            "process_2": 0.9752626419067383,
                            "process_3": 0.9594311714172363,
                            "process_1": 0.9662818908691406
                        },
                        "cpu_energy": {
                            "process_0": 0.001152431789156594,
                            "process_2": 0.001153821296718547,
                            "process_3": 0.0011343179064374452,
                            "process_1": 0.0011307776068749718
                        },
                        "gpu_energy": {
                            "process_0": 0.004163469164105871,
                            "process_2": 0.004170053613817781,
                            "process_3": 0.004107624674985932,
                            "process_1": 0.004110263565985983
                        },
                        "ram_energy": {
                            "process_0": 7.488628917134048e-06,
                            "process_2": 7.837211065726147e-06,
                            "process_3": 7.616490424078213e-06,
                            "process_1": 7.681703091092155e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005323389582179599,
                            "process_2": 0.005331712121602054,
                            "process_3": 0.005249559071847454,
                            "process_1": 0.005248722875952047
                        },
                        "total_energy_joules": {
                            "process_0": 19164.202495846555,
                            "process_2": 19194.163637767393,
                            "process_3": 18898.412658650836,
                            "process_1": 18895.40235342737
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 367.5344976061341,
                        "ram_power_avg": 0.958582878112793,
                        "cpu_energy_total": 0.004571348599187558,
                        "gpu_energy_total": 0.016551411018895568,
                        "ram_energy_total": 3.062403349803056e-05,
                        "total_energy_kwh": 0.021153383651581155,
                        "total_energy_joules": 76152.18114569216
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21514813828713067,
                        "joules_per_token": 4.647960275005625,
                        "flops_per_joule": 222580243.115083,
                        "joules_per_flop": 4.49276173843947e-09
                    },
                    "per-process_emissions": [
                        0.0020279452613313183,
                        0.0020311157327243022,
                        0.001999819528420288,
                        0.0019995009795939324
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0315": {
            "setup": {
                "experiment_id": "0315",
                "date_time": "April 11, 2025 at 06:41:37 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.3497220989957,
                        "average_latency_ms_per_batch": 4668.715262374462,
                        "throughput_queries_per_sec": 3.427066998269361,
                        "throughput_tokens_per_sec": 438.6645757784782
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2693410816
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0315",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 444.2362146365718,
                            "process_2": 0.0,
                            "process_0": 328.2044619660668,
                            "process_3": 412.0253862086145
                        },
                        "ram_power": {
                            "process_1": 0.9673776626586914,
                            "process_2": 0.9753613471984863,
                            "process_0": 0.9401121139526367,
                            "process_3": 0.9753785133361816
                        },
                        "cpu_energy": {
                            "process_1": 0.0011295780764999106,
                            "process_2": 0.0011464341345309777,
                            "process_0": 0.0011545280689692846,
                            "process_3": 0.0011436426305937175
                        },
                        "gpu_energy": {
                            "process_1": 0.004090368272291867,
                            "process_2": 0.0041427035919379684,
                            "process_0": 0.004166296666367975,
                            "process_3": 0.004134590252114101
                        },
                        "ram_energy": {
                            "process_1": 7.66086093228601e-06,
                            "process_2": 8.045843570171781e-06,
                            "process_0": 7.566576221588479e-06,
                            "process_3": 7.84094684890939e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005227607209724063,
                            "process_2": 0.005297183570039118,
                            "process_0": 0.005328391311558846,
                            "process_3": 0.00528607382955673
                        },
                        "total_energy_joules": {
                            "process_1": 18819.385955006626,
                            "process_2": 19069.860852140824,
                            "process_0": 19182.208721611845,
                            "process_3": 19029.86578640423
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 296.1165157028133,
                        "ram_power_avg": 0.964557409286499,
                        "cpu_energy_total": 0.00457418291059389,
                        "gpu_energy_total": 0.01653395878271191,
                        "ram_energy_total": 3.111422757295566e-05,
                        "total_energy_kwh": 0.021139255920878758,
                        "total_energy_joules": 76101.32131516353
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21529192551267062,
                        "joules_per_token": 4.644856037302461,
                        "flops_per_joule": 222728997.3449468,
                        "joules_per_flop": 4.489761153332322e-09
                    },
                    "per-process_emissions": [
                        0.001991456966544382,
                        0.002017962081006402,
                        0.002029850670138342,
                        0.0020137298253696365
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0315": {
            "setup": {
                "experiment_id": "0315",
                "date_time": "April 11, 2025 at 06:41:37 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.3497220989957,
                        "average_latency_ms_per_batch": 4668.715262374462,
                        "throughput_queries_per_sec": 3.427066998269361,
                        "throughput_tokens_per_sec": 438.6645757784782
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2693410816
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0315",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 444.2362146365718,
                            "process_2": 0.0,
                            "process_0": 328.2044619660668,
                            "process_3": 412.0253862086145
                        },
                        "ram_power": {
                            "process_1": 0.9673776626586914,
                            "process_2": 0.9753613471984863,
                            "process_0": 0.9401121139526367,
                            "process_3": 0.9753785133361816
                        },
                        "cpu_energy": {
                            "process_1": 0.0011295780764999106,
                            "process_2": 0.0011464341345309777,
                            "process_0": 0.0011545280689692846,
                            "process_3": 0.0011436426305937175
                        },
                        "gpu_energy": {
                            "process_1": 0.004090368272291867,
                            "process_2": 0.0041427035919379684,
                            "process_0": 0.004166296666367975,
                            "process_3": 0.004134590252114101
                        },
                        "ram_energy": {
                            "process_1": 7.66086093228601e-06,
                            "process_2": 8.045843570171781e-06,
                            "process_0": 7.566576221588479e-06,
                            "process_3": 7.84094684890939e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005227607209724063,
                            "process_2": 0.005297183570039118,
                            "process_0": 0.005328391311558846,
                            "process_3": 0.00528607382955673
                        },
                        "total_energy_joules": {
                            "process_1": 18819.385955006626,
                            "process_2": 19069.860852140824,
                            "process_0": 19182.208721611845,
                            "process_3": 19029.86578640423
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 296.1165157028133,
                        "ram_power_avg": 0.964557409286499,
                        "cpu_energy_total": 0.00457418291059389,
                        "gpu_energy_total": 0.01653395878271191,
                        "ram_energy_total": 3.111422757295566e-05,
                        "total_energy_kwh": 0.021139255920878758,
                        "total_energy_joules": 76101.32131516353
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21529192551267062,
                        "joules_per_token": 4.644856037302461,
                        "flops_per_joule": 222728997.3449468,
                        "joules_per_flop": 4.489761153332322e-09
                    },
                    "per-process_emissions": [
                        0.001991456966544382,
                        0.002017962081006402,
                        0.002029850670138342,
                        0.0020137298253696365
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0316": {
            "setup": {
                "experiment_id": "0316",
                "date_time": "April 11, 2025 at 06:42:44 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.40434449300301,
                        "average_latency_ms_per_batch": 4550.543061625376,
                        "throughput_queries_per_sec": 3.516063859482537,
                        "throughput_tokens_per_sec": 450.05617401376475
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2628284416
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0316",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 394.9727373961627,
                            "process_2": 414.9788273192332,
                            "process_0": 476.138974085396,
                            "process_3": 418.59567974046576
                        },
                        "ram_power": {
                            "process_1": 0.9753842353820801,
                            "process_2": 0.974520206451416,
                            "process_0": 0.9168534278869629,
                            "process_3": 0.9767503738403321
                        },
                        "cpu_energy": {
                            "process_1": 0.0011434048649380203,
                            "process_2": 0.001139900537093695,
                            "process_0": 0.0011270531146567466,
                            "process_3": 0.0011448689370309924
                        },
                        "gpu_energy": {
                            "process_1": 0.004129466359126088,
                            "process_2": 0.004120523018637967,
                            "process_0": 0.004073313814203838,
                            "process_3": 0.004142602202967971
                        },
                        "ram_energy": {
                            "process_1": 7.83189723815221e-06,
                            "process_2": 7.77886821250845e-06,
                            "process_0": 7.215928724179839e-06,
                            "process_3": 8.056487863929863e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005280703121302261,
                            "process_2": 0.005268202423944172,
                            "process_0": 0.005207582857584764,
                            "process_3": 0.005295527627862891
                        },
                        "total_energy_joules": {
                            "process_1": 19010.53123668814,
                            "process_2": 18965.52872619902,
                            "process_0": 18747.29828730515,
                            "process_3": 19063.89946030641
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 426.1715546353144,
                        "ram_power_avg": 0.9608770608901978,
                        "cpu_energy_total": 0.004555227453719455,
                        "gpu_energy_total": 0.016465905394935865,
                        "ram_energy_total": 3.0883182038770365e-05,
                        "total_energy_kwh": 0.02105201603069409,
                        "total_energy_joules": 75787.25771049871
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2161840986856336,
                        "joules_per_token": 4.625687116119306,
                        "flops_per_joule": 223651989.86219475,
                        "joules_per_flop": 4.471232295389633e-09
                    },
                    "per-process_emissions": [
                        0.0020116838540600963,
                        0.0020069217134015323,
                        0.001983828689596916,
                        0.0020173312498343687
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0316": {
            "setup": {
                "experiment_id": "0316",
                "date_time": "April 11, 2025 at 06:42:44 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.40434449300301,
                        "average_latency_ms_per_batch": 4550.543061625376,
                        "throughput_queries_per_sec": 3.516063859482537,
                        "throughput_tokens_per_sec": 450.05617401376475
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2628284416
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0316",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 394.9727373961627,
                            "process_2": 414.9788273192332,
                            "process_0": 476.138974085396,
                            "process_3": 418.59567974046576
                        },
                        "ram_power": {
                            "process_1": 0.9753842353820801,
                            "process_2": 0.974520206451416,
                            "process_0": 0.9168534278869629,
                            "process_3": 0.9767503738403321
                        },
                        "cpu_energy": {
                            "process_1": 0.0011434048649380203,
                            "process_2": 0.001139900537093695,
                            "process_0": 0.0011270531146567466,
                            "process_3": 0.0011448689370309924
                        },
                        "gpu_energy": {
                            "process_1": 0.004129466359126088,
                            "process_2": 0.004120523018637967,
                            "process_0": 0.004073313814203838,
                            "process_3": 0.004142602202967971
                        },
                        "ram_energy": {
                            "process_1": 7.83189723815221e-06,
                            "process_2": 7.77886821250845e-06,
                            "process_0": 7.215928724179839e-06,
                            "process_3": 8.056487863929863e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005280703121302261,
                            "process_2": 0.005268202423944172,
                            "process_0": 0.005207582857584764,
                            "process_3": 0.005295527627862891
                        },
                        "total_energy_joules": {
                            "process_1": 19010.53123668814,
                            "process_2": 18965.52872619902,
                            "process_0": 18747.29828730515,
                            "process_3": 19063.89946030641
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 426.1715546353144,
                        "ram_power_avg": 0.9608770608901978,
                        "cpu_energy_total": 0.004555227453719455,
                        "gpu_energy_total": 0.016465905394935865,
                        "ram_energy_total": 3.0883182038770365e-05,
                        "total_energy_kwh": 0.02105201603069409,
                        "total_energy_joules": 75787.25771049871
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2161840986856336,
                        "joules_per_token": 4.625687116119306,
                        "flops_per_joule": 223651989.86219475,
                        "joules_per_flop": 4.471232295389633e-09
                    },
                    "per-process_emissions": [
                        0.0020116838540600963,
                        0.0020069217134015323,
                        0.001983828689596916,
                        0.0020173312498343687
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0317": {
            "setup": {
                "experiment_id": "0317",
                "date_time": "April 11, 2025 at 06:43:50 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.76496392899935,
                        "average_latency_ms_per_batch": 4595.620491124919,
                        "throughput_queries_per_sec": 3.4815755632779655,
                        "throughput_tokens_per_sec": 445.6416720995796
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2672754688
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0317",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 727.7180229456367,
                            "process_0": 438.79439058613633,
                            "process_3": 329.7989161279816,
                            "process_1": 362.42268303523065
                        },
                        "ram_power": {
                            "process_2": 0.9750766754150391,
                            "process_0": 0.9326605796813965,
                            "process_3": 0.9745316505432129,
                            "process_1": 0.9666409492492676
                        },
                        "cpu_energy": {
                            "process_2": 0.001145705444062969,
                            "process_0": 0.001137369137562814,
                            "process_3": 0.0011548779088436734,
                            "process_1": 0.0011553591032186432
                        },
                        "gpu_energy": {
                            "process_2": 0.004109945510176127,
                            "process_0": 0.0040802796531101415,
                            "process_3": 0.004139588311668108,
                            "process_1": 0.004135161641460161
                        },
                        "ram_energy": {
                            "process_2": 8.040321934352712e-06,
                            "process_0": 7.450671167259162e-06,
                            "process_3": 7.871472183989206e-06,
                            "process_1": 7.78753076943648e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0052636912761734505,
                            "process_0": 0.005225099461840216,
                            "process_3": 0.00530233769269577,
                            "process_1": 0.005298308275448242
                        },
                        "total_energy_joules": {
                            "process_2": 18949.28859422442,
                            "process_0": 18810.35806262478,
                            "process_3": 19088.415693704774,
                            "process_1": 19073.90979161367
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 464.6835031737463,
                        "ram_power_avg": 0.962227463722229,
                        "cpu_energy_total": 0.0045933115936881,
                        "gpu_energy_total": 0.016464975116414537,
                        "ram_energy_total": 3.114999605503756e-05,
                        "total_energy_kwh": 0.021089436706157677,
                        "total_energy_joules": 75921.97214216765
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21580050593680772,
                        "joules_per_token": 4.63390943250535,
                        "flops_per_joule": 223255146.23635355,
                        "joules_per_flop": 4.4791800630715575e-09
                    },
                    "per-process_emissions": [
                        0.002005203191658276,
                        0.0019905016399880304,
                        0.0020199255440324537,
                        0.0020183905375320077
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0317": {
            "setup": {
                "experiment_id": "0317",
                "date_time": "April 11, 2025 at 06:43:50 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.76496392899935,
                        "average_latency_ms_per_batch": 4595.620491124919,
                        "throughput_queries_per_sec": 3.4815755632779655,
                        "throughput_tokens_per_sec": 445.6416720995796
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2672754688
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0317",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 727.7180229456367,
                            "process_0": 438.79439058613633,
                            "process_3": 329.7989161279816,
                            "process_1": 362.42268303523065
                        },
                        "ram_power": {
                            "process_2": 0.9750766754150391,
                            "process_0": 0.9326605796813965,
                            "process_3": 0.9745316505432129,
                            "process_1": 0.9666409492492676
                        },
                        "cpu_energy": {
                            "process_2": 0.001145705444062969,
                            "process_0": 0.001137369137562814,
                            "process_3": 0.0011548779088436734,
                            "process_1": 0.0011553591032186432
                        },
                        "gpu_energy": {
                            "process_2": 0.004109945510176127,
                            "process_0": 0.0040802796531101415,
                            "process_3": 0.004139588311668108,
                            "process_1": 0.004135161641460161
                        },
                        "ram_energy": {
                            "process_2": 8.040321934352712e-06,
                            "process_0": 7.450671167259162e-06,
                            "process_3": 7.871472183989206e-06,
                            "process_1": 7.78753076943648e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0052636912761734505,
                            "process_0": 0.005225099461840216,
                            "process_3": 0.00530233769269577,
                            "process_1": 0.005298308275448242
                        },
                        "total_energy_joules": {
                            "process_2": 18949.28859422442,
                            "process_0": 18810.35806262478,
                            "process_3": 19088.415693704774,
                            "process_1": 19073.90979161367
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 464.6835031737463,
                        "ram_power_avg": 0.962227463722229,
                        "cpu_energy_total": 0.0045933115936881,
                        "gpu_energy_total": 0.016464975116414537,
                        "ram_energy_total": 3.114999605503756e-05,
                        "total_energy_kwh": 0.021089436706157677,
                        "total_energy_joules": 75921.97214216765
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21580050593680772,
                        "joules_per_token": 4.63390943250535,
                        "flops_per_joule": 223255146.23635355,
                        "joules_per_flop": 4.4791800630715575e-09
                    },
                    "per-process_emissions": [
                        0.002005203191658276,
                        0.0019905016399880304,
                        0.0020199255440324537,
                        0.0020183905375320077
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0318": {
            "setup": {
                "experiment_id": "0318",
                "date_time": "April 11, 2025 at 06:44:56 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.75161749900144,
                        "average_latency_ms_per_batch": 4593.95218737518,
                        "throughput_queries_per_sec": 3.4828399050321477,
                        "throughput_tokens_per_sec": 445.8035078441149
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            23.0,
                            100.0,
                            88.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2672275456
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0318",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 444.4633429125873,
                            "process_2": 628.773780409627,
                            "process_1": 0.0,
                            "process_0": 419.8588411823655
                        },
                        "ram_power": {
                            "process_3": 0.9686007499694824,
                            "process_2": 0.9751825332641602,
                            "process_1": 0.9787602424621582,
                            "process_0": 0.9320340156555176
                        },
                        "cpu_energy": {
                            "process_3": 0.0011258621165308109,
                            "process_2": 0.0011487436302810464,
                            "process_1": 0.0011454743207189042,
                            "process_0": 0.0011377526975624049
                        },
                        "gpu_energy": {
                            "process_3": 0.004076732428050067,
                            "process_2": 0.00414881720793997,
                            "process_1": 0.004144733038005999,
                            "process_0": 0.0041163535708581
                        },
                        "ram_energy": {
                            "process_3": 7.6269847167496166e-06,
                            "process_2": 8.092928218485627e-06,
                            "process_1": 8.062530849183237e-06,
                            "process_0": 7.444777466315261e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005210221529297628,
                            "process_2": 0.005305653766439502,
                            "process_1": 0.005298269889574087,
                            "process_0": 0.00526155104588682
                        },
                        "total_energy_joules": {
                            "process_3": 18756.797505471463,
                            "process_2": 19100.353559182207,
                            "process_1": 19073.771602466713,
                            "process_0": 18941.583765192554
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 373.27399112614495,
                        "ram_power_avg": 0.9636443853378296,
                        "cpu_energy_total": 0.004557832765093166,
                        "gpu_energy_total": 0.016486636244854136,
                        "ram_energy_total": 3.122722125073374e-05,
                        "total_energy_kwh": 0.021075696231198038,
                        "total_energy_joules": 75872.50643231293
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21594119886650148,
                        "joules_per_token": 4.63089028517535,
                        "flops_per_joule": 223400699.28061935,
                        "joules_per_flop": 4.4762617271124755e-09
                    },
                    "per-process_emissions": [
                        0.0019848338915859315,
                        0.0020211888023251284,
                        0.0020183759144332485,
                        0.002004387870930584
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0318": {
            "setup": {
                "experiment_id": "0318",
                "date_time": "April 11, 2025 at 06:44:56 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.75161749900144,
                        "average_latency_ms_per_batch": 4593.95218737518,
                        "throughput_queries_per_sec": 3.4828399050321477,
                        "throughput_tokens_per_sec": 445.8035078441149
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            23.0,
                            100.0,
                            88.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2672275456
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0318",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 444.4633429125873,
                            "process_2": 628.773780409627,
                            "process_1": 0.0,
                            "process_0": 419.8588411823655
                        },
                        "ram_power": {
                            "process_3": 0.9686007499694824,
                            "process_2": 0.9751825332641602,
                            "process_1": 0.9787602424621582,
                            "process_0": 0.9320340156555176
                        },
                        "cpu_energy": {
                            "process_3": 0.0011258621165308109,
                            "process_2": 0.0011487436302810464,
                            "process_1": 0.0011454743207189042,
                            "process_0": 0.0011377526975624049
                        },
                        "gpu_energy": {
                            "process_3": 0.004076732428050067,
                            "process_2": 0.00414881720793997,
                            "process_1": 0.004144733038005999,
                            "process_0": 0.0041163535708581
                        },
                        "ram_energy": {
                            "process_3": 7.6269847167496166e-06,
                            "process_2": 8.092928218485627e-06,
                            "process_1": 8.062530849183237e-06,
                            "process_0": 7.444777466315261e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005210221529297628,
                            "process_2": 0.005305653766439502,
                            "process_1": 0.005298269889574087,
                            "process_0": 0.00526155104588682
                        },
                        "total_energy_joules": {
                            "process_3": 18756.797505471463,
                            "process_2": 19100.353559182207,
                            "process_1": 19073.771602466713,
                            "process_0": 18941.583765192554
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 373.27399112614495,
                        "ram_power_avg": 0.9636443853378296,
                        "cpu_energy_total": 0.004557832765093166,
                        "gpu_energy_total": 0.016486636244854136,
                        "ram_energy_total": 3.122722125073374e-05,
                        "total_energy_kwh": 0.021075696231198038,
                        "total_energy_joules": 75872.50643231293
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21594119886650148,
                        "joules_per_token": 4.63089028517535,
                        "flops_per_joule": 223400699.28061935,
                        "joules_per_flop": 4.4762617271124755e-09
                    },
                    "per-process_emissions": [
                        0.0019848338915859315,
                        0.0020211888023251284,
                        0.0020183759144332485,
                        0.002004387870930584
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0319": {
            "setup": {
                "experiment_id": "0319",
                "date_time": "April 11, 2025 at 06:46:02 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.28953127200293,
                        "average_latency_ms_per_batch": 4536.191409000367,
                        "throughput_queries_per_sec": 3.527188021266919,
                        "throughput_tokens_per_sec": 451.48006672216565
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2673119232
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0319",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 6.115260106872989,
                            "process_0": 400.19802281006855,
                            "process_3": 5.655756902183269,
                            "process_1": 0.0
                        },
                        "ram_power": {
                            "process_2": 0.9711027145385742,
                            "process_0": 0.9332900047302246,
                            "process_3": 0.9690113067626953,
                            "process_1": 0.9773712158203125
                        },
                        "cpu_energy": {
                            "process_2": 0.0011773284859060598,
                            "process_0": 0.0011206866758747085,
                            "process_3": 0.0011762950589061351,
                            "process_1": 0.001176790259593531
                        },
                        "gpu_energy": {
                            "process_2": 0.004144954427072134,
                            "process_0": 0.004059726303334232,
                            "process_3": 0.004142751647532195,
                            "process_1": 0.004143302759084144
                        },
                        "ram_energy": {
                            "process_2": 8.007008294083422e-06,
                            "process_0": 7.3323139159918754e-06,
                            "process_3": 7.996867787775997e-06,
                            "process_1": 8.067444075074723e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005330289921272278,
                            "process_0": 0.005187745293124931,
                            "process_3": 0.005327043574226105,
                            "process_1": 0.005328160462752747
                        },
                        "total_energy_joules": {
                            "process_2": 19189.043716580203,
                            "process_0": 18675.883055249753,
                            "process_3": 19177.356867213977,
                            "process_1": 19181.377665909888
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 102.9922599547812,
                        "ram_power_avg": 0.9626938104629517,
                        "cpu_energy_total": 0.004651100480280434,
                        "gpu_energy_total": 0.016490735137022705,
                        "ram_energy_total": 3.140363407292602e-05,
                        "total_energy_kwh": 0.021173239251376062,
                        "total_energy_joules": 76223.66130495383
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2149463791099102,
                        "joules_per_token": 4.652323077694936,
                        "flops_per_joule": 222371514.3430195,
                        "joules_per_flop": 4.496978864196826e-09
                    },
                    "per-process_emissions": [
                        0.0020305739455086743,
                        0.0019762715694159427,
                        0.002029337249601435,
                        0.002029762728285659
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0319": {
            "setup": {
                "experiment_id": "0319",
                "date_time": "April 11, 2025 at 06:46:02 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.28953127200293,
                        "average_latency_ms_per_batch": 4536.191409000367,
                        "throughput_queries_per_sec": 3.527188021266919,
                        "throughput_tokens_per_sec": 451.48006672216565
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2673119232
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0319",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 6.115260106872989,
                            "process_0": 400.19802281006855,
                            "process_3": 5.655756902183269,
                            "process_1": 0.0
                        },
                        "ram_power": {
                            "process_2": 0.9711027145385742,
                            "process_0": 0.9332900047302246,
                            "process_3": 0.9690113067626953,
                            "process_1": 0.9773712158203125
                        },
                        "cpu_energy": {
                            "process_2": 0.0011773284859060598,
                            "process_0": 0.0011206866758747085,
                            "process_3": 0.0011762950589061351,
                            "process_1": 0.001176790259593531
                        },
                        "gpu_energy": {
                            "process_2": 0.004144954427072134,
                            "process_0": 0.004059726303334232,
                            "process_3": 0.004142751647532195,
                            "process_1": 0.004143302759084144
                        },
                        "ram_energy": {
                            "process_2": 8.007008294083422e-06,
                            "process_0": 7.3323139159918754e-06,
                            "process_3": 7.996867787775997e-06,
                            "process_1": 8.067444075074723e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005330289921272278,
                            "process_0": 0.005187745293124931,
                            "process_3": 0.005327043574226105,
                            "process_1": 0.005328160462752747
                        },
                        "total_energy_joules": {
                            "process_2": 19189.043716580203,
                            "process_0": 18675.883055249753,
                            "process_3": 19177.356867213977,
                            "process_1": 19181.377665909888
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 102.9922599547812,
                        "ram_power_avg": 0.9626938104629517,
                        "cpu_energy_total": 0.004651100480280434,
                        "gpu_energy_total": 0.016490735137022705,
                        "ram_energy_total": 3.140363407292602e-05,
                        "total_energy_kwh": 0.021173239251376062,
                        "total_energy_joules": 76223.66130495383
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2149463791099102,
                        "joules_per_token": 4.652323077694936,
                        "flops_per_joule": 222371514.3430195,
                        "joules_per_flop": 4.496978864196826e-09
                    },
                    "per-process_emissions": [
                        0.0020305739455086743,
                        0.0019762715694159427,
                        0.002029337249601435,
                        0.002029762728285659
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0320": {
            "setup": {
                "experiment_id": "0320",
                "date_time": "April 11, 2025 at 06:47:09 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.688996045999374,
                        "average_latency_ms_per_batch": 4586.124505749922,
                        "throughput_queries_per_sec": 3.4887844802163053,
                        "throughput_tokens_per_sec": 446.5644134676871
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            9.0,
                            100.0,
                            100.0,
                            89.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2648391680
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0320",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 436.92118438130547,
                            "process_0": 445.1247417925214,
                            "process_3": 338.88162977860026,
                            "process_2": 442.0856546888017
                        },
                        "ram_power": {
                            "process_1": 0.9782481193542479,
                            "process_0": 0.924656867980957,
                            "process_3": 0.9749236106872559,
                            "process_2": 0.9711456298828125
                        },
                        "cpu_energy": {
                            "process_1": 0.0011456740541872248,
                            "process_0": 0.0011339328076254561,
                            "process_3": 0.0011488051350314664,
                            "process_2": 0.0011360918230936931
                        },
                        "gpu_energy": {
                            "process_1": 0.004159865550111885,
                            "process_0": 0.004120358018506132,
                            "process_3": 0.00416298833038814,
                            "process_2": 0.004130204415271954
                        },
                        "ram_energy": {
                            "process_1": 8.058037173251429e-06,
                            "process_0": 7.363611227187148e-06,
                            "process_3": 8.051447914666084e-06,
                            "process_2": 7.698389448603264e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005313597641472363,
                            "process_0": 0.005261654437358776,
                            "process_3": 0.005319844913334273,
                            "process_2": 0.005273994627814249
                        },
                        "total_energy_joules": {
                            "process_1": 19128.951509300507,
                            "process_0": 18941.955974491593,
                            "process_3": 19151.441688003382,
                            "process_2": 18986.380660131297
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 415.7533026603072,
                        "ram_power_avg": 0.9622435569763184,
                        "cpu_energy_total": 0.004564503819937841,
                        "gpu_energy_total": 0.01657341631427811,
                        "ram_energy_total": 3.117148576370792e-05,
                        "total_energy_kwh": 0.021169091619979662,
                        "total_energy_joules": 76208.72983192679
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21498849326230482,
                        "joules_per_token": 4.651411732905688,
                        "flops_per_joule": 222415083.29208502,
                        "joules_per_flop": 4.496097949826348e-09
                    },
                    "per-process_emissions": [
                        0.0020242150215188967,
                        0.0020044272579118256,
                        0.0020265949197346912,
                        0.0020091282534658384
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0320": {
            "setup": {
                "experiment_id": "0320",
                "date_time": "April 11, 2025 at 06:47:09 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.688996045999374,
                        "average_latency_ms_per_batch": 4586.124505749922,
                        "throughput_queries_per_sec": 3.4887844802163053,
                        "throughput_tokens_per_sec": 446.5644134676871
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            9.0,
                            100.0,
                            100.0,
                            89.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2648391680
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0320",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 436.92118438130547,
                            "process_0": 445.1247417925214,
                            "process_3": 338.88162977860026,
                            "process_2": 442.0856546888017
                        },
                        "ram_power": {
                            "process_1": 0.9782481193542479,
                            "process_0": 0.924656867980957,
                            "process_3": 0.9749236106872559,
                            "process_2": 0.9711456298828125
                        },
                        "cpu_energy": {
                            "process_1": 0.0011456740541872248,
                            "process_0": 0.0011339328076254561,
                            "process_3": 0.0011488051350314664,
                            "process_2": 0.0011360918230936931
                        },
                        "gpu_energy": {
                            "process_1": 0.004159865550111885,
                            "process_0": 0.004120358018506132,
                            "process_3": 0.00416298833038814,
                            "process_2": 0.004130204415271954
                        },
                        "ram_energy": {
                            "process_1": 8.058037173251429e-06,
                            "process_0": 7.363611227187148e-06,
                            "process_3": 8.051447914666084e-06,
                            "process_2": 7.698389448603264e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005313597641472363,
                            "process_0": 0.005261654437358776,
                            "process_3": 0.005319844913334273,
                            "process_2": 0.005273994627814249
                        },
                        "total_energy_joules": {
                            "process_1": 19128.951509300507,
                            "process_0": 18941.955974491593,
                            "process_3": 19151.441688003382,
                            "process_2": 18986.380660131297
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 415.7533026603072,
                        "ram_power_avg": 0.9622435569763184,
                        "cpu_energy_total": 0.004564503819937841,
                        "gpu_energy_total": 0.01657341631427811,
                        "ram_energy_total": 3.117148576370792e-05,
                        "total_energy_kwh": 0.021169091619979662,
                        "total_energy_joules": 76208.72983192679
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21498849326230482,
                        "joules_per_token": 4.651411732905688,
                        "flops_per_joule": 222415083.29208502,
                        "joules_per_flop": 4.496097949826348e-09
                    },
                    "per-process_emissions": [
                        0.0020242150215188967,
                        0.0020044272579118256,
                        0.0020265949197346912,
                        0.0020091282534658384
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0321": {
            "setup": {
                "experiment_id": "0321",
                "date_time": "April 11, 2025 at 06:48:16 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.79045879399564,
                        "average_latency_ms_per_batch": 4598.807349249455,
                        "throughput_queries_per_sec": 3.4791629187535476,
                        "throughput_tokens_per_sec": 445.3328536004541
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2650079232
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0321",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 451.4220464335529,
                            "process_1": 438.90896434586176,
                            "process_2": 447.85237849352956,
                            "process_3": 427.50226929396956
                        },
                        "ram_power": {
                            "process_0": 0.9254350662231445,
                            "process_1": 0.9783897399902343,
                            "process_2": 0.9627914428710939,
                            "process_3": 0.9663891792297363
                        },
                        "cpu_energy": {
                            "process_0": 0.0011343776087185233,
                            "process_1": 0.0011382171950629074,
                            "process_2": 0.001133920455156158,
                            "process_3": 0.001139285035968555
                        },
                        "gpu_energy": {
                            "process_0": 0.004144614149022174,
                            "process_1": 0.004156548880791866,
                            "process_2": 0.004142776925330105,
                            "process_3": 0.004156548880791866
                        },
                        "ram_energy": {
                            "process_0": 7.3636583962315404e-06,
                            "process_1": 7.806009336573465e-06,
                            "process_2": 7.676399715927792e-06,
                            "process_3": 7.721663301640018e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.00528635541613693,
                            "process_1": 0.005302572085191348,
                            "process_2": 0.005284373780202192,
                            "process_3": 0.005303555580062061
                        },
                        "total_energy_joules": {
                            "process_0": 19030.87949809295,
                            "process_1": 19089.259506688853,
                            "process_2": 19023.74560872789,
                            "process_3": 19092.80008822342
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 441.42141464172846,
                        "ram_power_avg": 0.9582513570785522,
                        "cpu_energy_total": 0.004545800294906144,
                        "gpu_energy_total": 0.01660048883593601,
                        "ram_energy_total": 3.056773075037282e-05,
                        "total_energy_kwh": 0.021176856861592534,
                        "total_energy_joules": 76236.68470173312
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.214909660147217,
                        "joules_per_token": 4.653117962752265,
                        "flops_per_joule": 222333526.95577896,
                        "joules_per_flop": 4.497747207504584e-09
                    },
                    "per-process_emissions": [
                        0.0020138370957773636,
                        0.002020014835853644,
                        0.002013082191568025,
                        0.002020389498224642
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0321": {
            "setup": {
                "experiment_id": "0321",
                "date_time": "April 11, 2025 at 06:48:16 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.79045879399564,
                        "average_latency_ms_per_batch": 4598.807349249455,
                        "throughput_queries_per_sec": 3.4791629187535476,
                        "throughput_tokens_per_sec": 445.3328536004541
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2650079232
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0321",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 451.4220464335529,
                            "process_1": 438.90896434586176,
                            "process_2": 447.85237849352956,
                            "process_3": 427.50226929396956
                        },
                        "ram_power": {
                            "process_0": 0.9254350662231445,
                            "process_1": 0.9783897399902343,
                            "process_2": 0.9627914428710939,
                            "process_3": 0.9663891792297363
                        },
                        "cpu_energy": {
                            "process_0": 0.0011343776087185233,
                            "process_1": 0.0011382171950629074,
                            "process_2": 0.001133920455156158,
                            "process_3": 0.001139285035968555
                        },
                        "gpu_energy": {
                            "process_0": 0.004144614149022174,
                            "process_1": 0.004156548880791866,
                            "process_2": 0.004142776925330105,
                            "process_3": 0.004156548880791866
                        },
                        "ram_energy": {
                            "process_0": 7.3636583962315404e-06,
                            "process_1": 7.806009336573465e-06,
                            "process_2": 7.676399715927792e-06,
                            "process_3": 7.721663301640018e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.00528635541613693,
                            "process_1": 0.005302572085191348,
                            "process_2": 0.005284373780202192,
                            "process_3": 0.005303555580062061
                        },
                        "total_energy_joules": {
                            "process_0": 19030.87949809295,
                            "process_1": 19089.259506688853,
                            "process_2": 19023.74560872789,
                            "process_3": 19092.80008822342
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 441.42141464172846,
                        "ram_power_avg": 0.9582513570785522,
                        "cpu_energy_total": 0.004545800294906144,
                        "gpu_energy_total": 0.01660048883593601,
                        "ram_energy_total": 3.056773075037282e-05,
                        "total_energy_kwh": 0.021176856861592534,
                        "total_energy_joules": 76236.68470173312
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.214909660147217,
                        "joules_per_token": 4.653117962752265,
                        "flops_per_joule": 222333526.95577896,
                        "joules_per_flop": 4.497747207504584e-09
                    },
                    "per-process_emissions": [
                        0.0020138370957773636,
                        0.002020014835853644,
                        0.002013082191568025,
                        0.002020389498224642
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0322": {
            "setup": {
                "experiment_id": "0322",
                "date_time": "April 11, 2025 at 06:49:22 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.692678189003345,
                        "average_latency_ms_per_batch": 4586.584773625418,
                        "throughput_queries_per_sec": 3.488434377580024,
                        "throughput_tokens_per_sec": 446.51960033024307
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2672320512
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0322",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 0.0,
                            "process_1": 355.11502407746883,
                            "process_0": 450.80011333613163,
                            "process_3": 319.78165388031056
                        },
                        "ram_power": {
                            "process_2": 0.958432674407959,
                            "process_1": 0.9587817192077637,
                            "process_0": 0.9331626892089845,
                            "process_3": 0.9664864540100099
                        },
                        "cpu_energy": {
                            "process_2": 0.0011469235454375163,
                            "process_1": 0.0011483953722815842,
                            "process_0": 0.0011315514682185039,
                            "process_3": 0.0011596272316561455
                        },
                        "gpu_energy": {
                            "process_2": 0.0041465272061081415,
                            "process_1": 0.004152525266461982,
                            "process_0": 0.004105904395832027,
                            "process_3": 0.0041863927935561485
                        },
                        "ram_energy": {
                            "process_2": 7.931999132639978e-06,
                            "process_1": 7.941031061899845e-06,
                            "process_0": 7.399765928793849e-06,
                            "process_3": 7.848076240771746e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0053013827506782985,
                            "process_1": 0.005308861669805468,
                            "process_0": 0.005244855629979325,
                            "process_3": 0.005353868101453066
                        },
                        "total_energy_joules": {
                            "process_2": 19084.977902441875,
                            "process_1": 19111.902011299684,
                            "process_0": 18881.48026792557,
                            "process_3": 19273.925165231038
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 281.42419782347775,
                        "ram_power_avg": 0.9542158842086792,
                        "cpu_energy_total": 0.00458649761759375,
                        "gpu_energy_total": 0.0165913496619583,
                        "ram_energy_total": 3.112087236410542e-05,
                        "total_energy_kwh": 0.02120896815191616,
                        "total_energy_joules": 76352.28534689816
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21458427767500485,
                        "joules_per_token": 4.660173666192515,
                        "flops_per_joule": 221996904.42979777,
                        "joules_per_flop": 4.504567316235847e-09
                    },
                    "per-process_emissions": [
                        0.002019561758870898,
                        0.002022410853112393,
                        0.001998027752240624,
                        0.0020395560532485454
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0322": {
            "setup": {
                "experiment_id": "0322",
                "date_time": "April 11, 2025 at 06:49:22 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.692678189003345,
                        "average_latency_ms_per_batch": 4586.584773625418,
                        "throughput_queries_per_sec": 3.488434377580024,
                        "throughput_tokens_per_sec": 446.51960033024307
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2672320512
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0322",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 0.0,
                            "process_1": 355.11502407746883,
                            "process_0": 450.80011333613163,
                            "process_3": 319.78165388031056
                        },
                        "ram_power": {
                            "process_2": 0.958432674407959,
                            "process_1": 0.9587817192077637,
                            "process_0": 0.9331626892089845,
                            "process_3": 0.9664864540100099
                        },
                        "cpu_energy": {
                            "process_2": 0.0011469235454375163,
                            "process_1": 0.0011483953722815842,
                            "process_0": 0.0011315514682185039,
                            "process_3": 0.0011596272316561455
                        },
                        "gpu_energy": {
                            "process_2": 0.0041465272061081415,
                            "process_1": 0.004152525266461982,
                            "process_0": 0.004105904395832027,
                            "process_3": 0.0041863927935561485
                        },
                        "ram_energy": {
                            "process_2": 7.931999132639978e-06,
                            "process_1": 7.941031061899845e-06,
                            "process_0": 7.399765928793849e-06,
                            "process_3": 7.848076240771746e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0053013827506782985,
                            "process_1": 0.005308861669805468,
                            "process_0": 0.005244855629979325,
                            "process_3": 0.005353868101453066
                        },
                        "total_energy_joules": {
                            "process_2": 19084.977902441875,
                            "process_1": 19111.902011299684,
                            "process_0": 18881.48026792557,
                            "process_3": 19273.925165231038
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 281.42419782347775,
                        "ram_power_avg": 0.9542158842086792,
                        "cpu_energy_total": 0.00458649761759375,
                        "gpu_energy_total": 0.0165913496619583,
                        "ram_energy_total": 3.112087236410542e-05,
                        "total_energy_kwh": 0.02120896815191616,
                        "total_energy_joules": 76352.28534689816
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21458427767500485,
                        "joules_per_token": 4.660173666192515,
                        "flops_per_joule": 221996904.42979777,
                        "joules_per_flop": 4.504567316235847e-09
                    },
                    "per-process_emissions": [
                        0.002019561758870898,
                        0.002022410853112393,
                        0.001998027752240624,
                        0.0020395560532485454
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0323": {
            "setup": {
                "experiment_id": "0323",
                "date_time": "April 11, 2025 at 06:50:29 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.88714080999853,
                        "average_latency_ms_per_batch": 4610.892601249816,
                        "throughput_queries_per_sec": 3.470043955407481,
                        "throughput_tokens_per_sec": 444.1656262921576
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.5,
                        "cpu_memory_usage_bytes": 2671546368
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0323",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 421.0102653294041,
                            "process_0": 429.57394355753735,
                            "process_1": 445.0623162517838,
                            "process_3": 394.5104225202925
                        },
                        "ram_power": {
                            "process_2": 0.9754815101623535,
                            "process_0": 0.9323272705078125,
                            "process_1": 0.9672732353210449,
                            "process_3": 0.974414348602295
                        },
                        "cpu_energy": {
                            "process_2": 0.0011385763301245788,
                            "process_0": 0.0011387713251565402,
                            "process_1": 0.0011318130630625092,
                            "process_3": 0.001139242698781118
                        },
                        "gpu_energy": {
                            "process_2": 0.004151656376877977,
                            "process_0": 0.004154614990355965,
                            "process_1": 0.004118613294887996,
                            "process_3": 0.00415326026704993
                        },
                        "ram_energy": {
                            "process_2": 7.786744219517089e-06,
                            "process_0": 7.455656332887617e-06,
                            "process_1": 7.726367364210634e-06,
                            "process_3": 7.814864489418439e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005298019451222074,
                            "process_0": 0.005300841971845393,
                            "process_1": 0.005258152725314715,
                            "process_3": 0.0053003178303204675
                        },
                        "total_energy_joules": {
                            "process_2": 19072.870024399464,
                            "process_0": 19083.031098643416,
                            "process_1": 18929.34981113297,
                            "process_3": 19081.14418915368
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 422.53923691475444,
                        "ram_power_avg": 0.9623740911483765,
                        "cpu_energy_total": 0.004548403417124746,
                        "gpu_energy_total": 0.016578144929171867,
                        "ram_energy_total": 3.0783632406033783e-05,
                        "total_energy_kwh": 0.02115733197870265,
                        "total_energy_joules": 76166.39512332954
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21510798789244037,
                        "joules_per_token": 4.648827827351656,
                        "flops_per_joule": 222538705.7600193,
                        "joules_per_flop": 4.493600322626021e-09
                    },
                    "per-process_emissions": [
                        0.002018280509943049,
                        0.0020193557491745024,
                        0.002003093280708641,
                        0.002019156077460582
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0323": {
            "setup": {
                "experiment_id": "0323",
                "date_time": "April 11, 2025 at 06:50:29 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.88714080999853,
                        "average_latency_ms_per_batch": 4610.892601249816,
                        "throughput_queries_per_sec": 3.470043955407481,
                        "throughput_tokens_per_sec": 444.1656262921576
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.5,
                        "cpu_memory_usage_bytes": 2671546368
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0323",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 421.0102653294041,
                            "process_0": 429.57394355753735,
                            "process_1": 445.0623162517838,
                            "process_3": 394.5104225202925
                        },
                        "ram_power": {
                            "process_2": 0.9754815101623535,
                            "process_0": 0.9323272705078125,
                            "process_1": 0.9672732353210449,
                            "process_3": 0.974414348602295
                        },
                        "cpu_energy": {
                            "process_2": 0.0011385763301245788,
                            "process_0": 0.0011387713251565402,
                            "process_1": 0.0011318130630625092,
                            "process_3": 0.001139242698781118
                        },
                        "gpu_energy": {
                            "process_2": 0.004151656376877977,
                            "process_0": 0.004154614990355965,
                            "process_1": 0.004118613294887996,
                            "process_3": 0.00415326026704993
                        },
                        "ram_energy": {
                            "process_2": 7.786744219517089e-06,
                            "process_0": 7.455656332887617e-06,
                            "process_1": 7.726367364210634e-06,
                            "process_3": 7.814864489418439e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005298019451222074,
                            "process_0": 0.005300841971845393,
                            "process_1": 0.005258152725314715,
                            "process_3": 0.0053003178303204675
                        },
                        "total_energy_joules": {
                            "process_2": 19072.870024399464,
                            "process_0": 19083.031098643416,
                            "process_1": 18929.34981113297,
                            "process_3": 19081.14418915368
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 422.53923691475444,
                        "ram_power_avg": 0.9623740911483765,
                        "cpu_energy_total": 0.004548403417124746,
                        "gpu_energy_total": 0.016578144929171867,
                        "ram_energy_total": 3.0783632406033783e-05,
                        "total_energy_kwh": 0.02115733197870265,
                        "total_energy_joules": 76166.39512332954
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21510798789244037,
                        "joules_per_token": 4.648827827351656,
                        "flops_per_joule": 222538705.7600193,
                        "joules_per_flop": 4.493600322626021e-09
                    },
                    "per-process_emissions": [
                        0.002018280509943049,
                        0.0020193557491745024,
                        0.002003093280708641,
                        0.002019156077460582
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0328": {
            "setup": {
                "experiment_id": "0328",
                "date_time": "April 11, 2025 at 06:53:27 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.50405484700241,
                        "average_latency_ms_per_batch": 4563.006855875301,
                        "throughput_queries_per_sec": 3.5064597764955128,
                        "throughput_tokens_per_sec": 448.82685139142563
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            2.0,
                            84.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2672652288
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0328",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 431.01580427412665,
                            "process_3": 425.93977061205146,
                            "process_1": 308.7306284440971,
                            "process_2": 250.55954226266775
                        },
                        "ram_power": {
                            "process_0": 0.9332571029663086,
                            "process_3": 0.9788289070129396,
                            "process_1": 0.9603252410888672,
                            "process_2": 0.9786529541015625
                        },
                        "cpu_energy": {
                            "process_0": 0.001129519240343825,
                            "process_3": 0.001135983484343342,
                            "process_1": 0.0011509676934064147,
                            "process_2": 0.0011499685041562772
                        },
                        "gpu_energy": {
                            "process_0": 0.004033769060346137,
                            "process_3": 0.004069773811372079,
                            "process_1": 0.0040991918904621505,
                            "process_2": 0.004096998555374043
                        },
                        "ram_energy": {
                            "process_0": 7.407263451927785e-06,
                            "process_3": 7.83116533890817e-06,
                            "process_1": 7.729522544790178e-06,
                            "process_2": 8.096928245220375e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005170695564141891,
                            "process_3": 0.005213588461054329,
                            "process_1": 0.005257889106413357,
                            "process_2": 0.005255063987775541
                        },
                        "total_energy_joules": {
                            "process_0": 18614.504030910808,
                            "process_3": 18768.918459795586,
                            "process_1": 18928.400783088087,
                            "process_2": 18918.23035599195
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 354.0614363982357,
                        "ram_power_avg": 0.9627660512924194,
                        "cpu_energy_total": 0.004566438922249859,
                        "gpu_energy_total": 0.01629973331755441,
                        "ram_energy_total": 3.106487958084651e-05,
                        "total_energy_kwh": 0.02089723711938512,
                        "total_energy_joules": 75230.05362978643
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21778530267474053,
                        "joules_per_token": 4.5916780779898945,
                        "flops_per_joule": 225308506.0468555,
                        "joules_per_flop": 4.438358842040515e-09
                    },
                    "per-process_emissions": [
                        0.001969776475159853,
                        0.001986116524238647,
                        0.0020029928550881686,
                        0.0020019166261430925
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0328": {
            "setup": {
                "experiment_id": "0328",
                "date_time": "April 11, 2025 at 06:53:27 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.50405484700241,
                        "average_latency_ms_per_batch": 4563.006855875301,
                        "throughput_queries_per_sec": 3.5064597764955128,
                        "throughput_tokens_per_sec": 448.82685139142563
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            2.0,
                            84.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2672652288
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0328",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 431.01580427412665,
                            "process_3": 425.93977061205146,
                            "process_1": 308.7306284440971,
                            "process_2": 250.55954226266775
                        },
                        "ram_power": {
                            "process_0": 0.9332571029663086,
                            "process_3": 0.9788289070129396,
                            "process_1": 0.9603252410888672,
                            "process_2": 0.9786529541015625
                        },
                        "cpu_energy": {
                            "process_0": 0.001129519240343825,
                            "process_3": 0.001135983484343342,
                            "process_1": 0.0011509676934064147,
                            "process_2": 0.0011499685041562772
                        },
                        "gpu_energy": {
                            "process_0": 0.004033769060346137,
                            "process_3": 0.004069773811372079,
                            "process_1": 0.0040991918904621505,
                            "process_2": 0.004096998555374043
                        },
                        "ram_energy": {
                            "process_0": 7.407263451927785e-06,
                            "process_3": 7.83116533890817e-06,
                            "process_1": 7.729522544790178e-06,
                            "process_2": 8.096928245220375e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005170695564141891,
                            "process_3": 0.005213588461054329,
                            "process_1": 0.005257889106413357,
                            "process_2": 0.005255063987775541
                        },
                        "total_energy_joules": {
                            "process_0": 18614.504030910808,
                            "process_3": 18768.918459795586,
                            "process_1": 18928.400783088087,
                            "process_2": 18918.23035599195
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 354.0614363982357,
                        "ram_power_avg": 0.9627660512924194,
                        "cpu_energy_total": 0.004566438922249859,
                        "gpu_energy_total": 0.01629973331755441,
                        "ram_energy_total": 3.106487958084651e-05,
                        "total_energy_kwh": 0.02089723711938512,
                        "total_energy_joules": 75230.05362978643
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21778530267474053,
                        "joules_per_token": 4.5916780779898945,
                        "flops_per_joule": 225308506.0468555,
                        "joules_per_flop": 4.438358842040515e-09
                    },
                    "per-process_emissions": [
                        0.001969776475159853,
                        0.001986116524238647,
                        0.0020029928550881686,
                        0.0020019166261430925
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0329": {
            "setup": {
                "experiment_id": "0329",
                "date_time": "April 11, 2025 at 06:54:34 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_True_latency_0.05_latency_0.2_latency_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.919570180001756,
                        "average_latency_ms_per_batch": 4739.9462725002195,
                        "throughput_queries_per_sec": 3.375565687912396,
                        "throughput_tokens_per_sec": 432.07240805278667
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            31.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2649559040
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0329",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 0.0,
                            "process_3": 437.76301009530715,
                            "process_1": 0.0,
                            "process_0": 448.6483652133049
                        },
                        "ram_power": {
                            "process_2": 0.9664492607116699,
                            "process_3": 0.9792795181274413,
                            "process_1": 0.971034049987793,
                            "process_0": 0.9240989685058594
                        },
                        "cpu_energy": {
                            "process_2": 0.0011755553185316783,
                            "process_3": 0.0011696065990939814,
                            "process_1": 0.001207839963468473,
                            "process_0": 0.0011726690163441163
                        },
                        "gpu_energy": {
                            "process_2": 0.004154955546183947,
                            "process_3": 0.0041311955271758904,
                            "process_1": 0.004154036101003844,
                            "process_0": 0.004144809426955942
                        },
                        "ram_energy": {
                            "process_2": 8.238433493975284e-06,
                            "process_3": 8.064387368817376e-06,
                            "process_1": 8.310454689229334e-06,
                            "process_0": 7.624423793127069e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0053387492982096,
                            "process_3": 0.0053088665136386895,
                            "process_1": 0.005370186519161545,
                            "process_0": 0.005325102867093186
                        },
                        "total_energy_joules": {
                            "process_2": 19219.49747355456,
                            "process_3": 19111.91944909928,
                            "process_1": 19332.671468981564,
                            "process_0": 19170.37032153547
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 221.602843827153,
                        "ram_power_avg": 0.9602154493331909,
                        "cpu_energy_total": 0.004725670897438249,
                        "gpu_energy_total": 0.016584996601319624,
                        "ram_energy_total": 3.223769934514906e-05,
                        "total_energy_kwh": 0.021342905198103022,
                        "total_energy_joules": 76834.45871317087
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21323765761352953,
                        "joules_per_token": 4.689603192942558,
                        "flops_per_joule": 220603766.55255148,
                        "joules_per_flop": 4.5330141711872515e-09
                    },
                    "per-process_emissions": [
                        0.002033796545152947,
                        0.002022412698370659,
                        0.0020457725544745908,
                        0.0020285979372191495
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0329": {
            "setup": {
                "experiment_id": "0329",
                "date_time": "April 11, 2025 at 06:54:34 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_True_latency_0.05_latency_0.2_latency_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.919570180001756,
                        "average_latency_ms_per_batch": 4739.9462725002195,
                        "throughput_queries_per_sec": 3.375565687912396,
                        "throughput_tokens_per_sec": 432.07240805278667
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            31.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2649559040
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0329",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 0.0,
                            "process_3": 437.76301009530715,
                            "process_1": 0.0,
                            "process_0": 448.6483652133049
                        },
                        "ram_power": {
                            "process_2": 0.9664492607116699,
                            "process_3": 0.9792795181274413,
                            "process_1": 0.971034049987793,
                            "process_0": 0.9240989685058594
                        },
                        "cpu_energy": {
                            "process_2": 0.0011755553185316783,
                            "process_3": 0.0011696065990939814,
                            "process_1": 0.001207839963468473,
                            "process_0": 0.0011726690163441163
                        },
                        "gpu_energy": {
                            "process_2": 0.004154955546183947,
                            "process_3": 0.0041311955271758904,
                            "process_1": 0.004154036101003844,
                            "process_0": 0.004144809426955942
                        },
                        "ram_energy": {
                            "process_2": 8.238433493975284e-06,
                            "process_3": 8.064387368817376e-06,
                            "process_1": 8.310454689229334e-06,
                            "process_0": 7.624423793127069e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0053387492982096,
                            "process_3": 0.0053088665136386895,
                            "process_1": 0.005370186519161545,
                            "process_0": 0.005325102867093186
                        },
                        "total_energy_joules": {
                            "process_2": 19219.49747355456,
                            "process_3": 19111.91944909928,
                            "process_1": 19332.671468981564,
                            "process_0": 19170.37032153547
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 221.602843827153,
                        "ram_power_avg": 0.9602154493331909,
                        "cpu_energy_total": 0.004725670897438249,
                        "gpu_energy_total": 0.016584996601319624,
                        "ram_energy_total": 3.223769934514906e-05,
                        "total_energy_kwh": 0.021342905198103022,
                        "total_energy_joules": 76834.45871317087
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21323765761352953,
                        "joules_per_token": 4.689603192942558,
                        "flops_per_joule": 220603766.55255148,
                        "joules_per_flop": 4.5330141711872515e-09
                    },
                    "per-process_emissions": [
                        0.002033796545152947,
                        0.002022412698370659,
                        0.0020457725544745908,
                        0.0020285979372191495
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0330": {
            "setup": {
                "experiment_id": "0330",
                "date_time": "April 11, 2025 at 06:55:45 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_True_latency_0.2_latency_0.6_latency_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.6,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 40.77431774299839,
                        "average_latency_ms_per_batch": 5096.789717874799,
                        "throughput_queries_per_sec": 3.139230944507457,
                        "throughput_tokens_per_sec": 401.8215608969545
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2599944192
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0330",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 368.50873578091176,
                            "process_3": 359.45410357203036,
                            "process_0": 337.6562198380756,
                            "process_1": 428.9298561902056
                        },
                        "ram_power": {
                            "process_2": 0.9684247970581055,
                            "process_3": 0.9748692512512207,
                            "process_0": 0.9072790145874023,
                            "process_1": 0.9763941764831543
                        },
                        "cpu_energy": {
                            "process_2": 0.0012539194769055938,
                            "process_3": 0.001246445036750174,
                            "process_0": 0.00125943987562539,
                            "process_1": 0.001224131389187505
                        },
                        "gpu_energy": {
                            "process_2": 0.004295461214143925,
                            "process_3": 0.004273142029622035,
                            "process_0": 0.004315517896856025,
                            "process_1": 0.004202419473044006
                        },
                        "ram_energy": {
                            "process_2": 8.50134698044207e-06,
                            "process_3": 8.651951662402772e-06,
                            "process_0": 8.118531002223695e-06,
                            "process_1": 8.5784329810978e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005557882038029959,
                            "process_3": 0.005528239018034611,
                            "process_0": 0.005583076303483637,
                            "process_1": 0.00543512929521261
                        },
                        "total_energy_joules": {
                            "process_2": 20008.37533690785,
                            "process_3": 19901.6604649246,
                            "process_0": 20099.074692541093,
                            "process_1": 19566.465462765394
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 373.6372288453058,
                        "ram_power_avg": 0.9567418098449707,
                        "cpu_energy_total": 0.004983935778468663,
                        "gpu_energy_total": 0.01708654061366599,
                        "ram_energy_total": 3.3850262626166334e-05,
                        "total_energy_kwh": 0.022104326654760816,
                        "total_energy_joules": 79575.57595713894
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20589232063899562,
                        "joules_per_token": 4.856907712227718,
                        "flops_per_joule": 213004691.31736612,
                        "joules_per_flop": 4.6947322794409775e-09
                    },
                    "per-process_emissions": [
                        0.002117275162387513,
                        0.0021059826539202853,
                        0.0021268729178120917,
                        0.002070512505011244
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0330": {
            "setup": {
                "experiment_id": "0330",
                "date_time": "April 11, 2025 at 06:55:45 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_True_latency_0.2_latency_0.6_latency_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.6,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 40.77431774299839,
                        "average_latency_ms_per_batch": 5096.789717874799,
                        "throughput_queries_per_sec": 3.139230944507457,
                        "throughput_tokens_per_sec": 401.8215608969545
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2599944192
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0330",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 368.50873578091176,
                            "process_3": 359.45410357203036,
                            "process_0": 337.6562198380756,
                            "process_1": 428.9298561902056
                        },
                        "ram_power": {
                            "process_2": 0.9684247970581055,
                            "process_3": 0.9748692512512207,
                            "process_0": 0.9072790145874023,
                            "process_1": 0.9763941764831543
                        },
                        "cpu_energy": {
                            "process_2": 0.0012539194769055938,
                            "process_3": 0.001246445036750174,
                            "process_0": 0.00125943987562539,
                            "process_1": 0.001224131389187505
                        },
                        "gpu_energy": {
                            "process_2": 0.004295461214143925,
                            "process_3": 0.004273142029622035,
                            "process_0": 0.004315517896856025,
                            "process_1": 0.004202419473044006
                        },
                        "ram_energy": {
                            "process_2": 8.50134698044207e-06,
                            "process_3": 8.651951662402772e-06,
                            "process_0": 8.118531002223695e-06,
                            "process_1": 8.5784329810978e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005557882038029959,
                            "process_3": 0.005528239018034611,
                            "process_0": 0.005583076303483637,
                            "process_1": 0.00543512929521261
                        },
                        "total_energy_joules": {
                            "process_2": 20008.37533690785,
                            "process_3": 19901.6604649246,
                            "process_0": 20099.074692541093,
                            "process_1": 19566.465462765394
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 373.6372288453058,
                        "ram_power_avg": 0.9567418098449707,
                        "cpu_energy_total": 0.004983935778468663,
                        "gpu_energy_total": 0.01708654061366599,
                        "ram_energy_total": 3.3850262626166334e-05,
                        "total_energy_kwh": 0.022104326654760816,
                        "total_energy_joules": 79575.57595713894
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20589232063899562,
                        "joules_per_token": 4.856907712227718,
                        "flops_per_joule": 213004691.31736612,
                        "joules_per_flop": 4.6947322794409775e-09
                    },
                    "per-process_emissions": [
                        0.002117275162387513,
                        0.0021059826539202853,
                        0.0021268729178120917,
                        0.002070512505011244
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0331": {
            "setup": {
                "experiment_id": "0331",
                "date_time": "April 11, 2025 at 06:56:55 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_True_latency_0.05_latency_0.2_latency_True_latency_4.0_latency_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.93022625600497,
                        "average_latency_ms_per_batch": 5241.278282000621,
                        "throughput_queries_per_sec": 3.0526904199966887,
                        "throughput_tokens_per_sec": 390.74437375957615
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            19.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2624958464
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0331",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 376.718852244718,
                            "process_1": 514.1052050251058,
                            "process_3": 391.0407959623868,
                            "process_2": 251.8417757942326
                        },
                        "ram_power": {
                            "process_0": 0.916534423828125,
                            "process_1": 0.9670085906982422,
                            "process_3": 0.9756917953491211,
                            "process_2": 0.9603881835937501
                        },
                        "cpu_energy": {
                            "process_0": 0.001291256826843778,
                            "process_1": 0.0012742074446247214,
                            "process_3": 0.0012829651515311297,
                            "process_2": 0.001304153403562054
                        },
                        "gpu_energy": {
                            "process_0": 0.004347373755674067,
                            "process_1": 0.004280352313168034,
                            "process_3": 0.004306847889919974,
                            "process_2": 0.0043664871042981
                        },
                        "ram_energy": {
                            "process_0": 8.442594742664905e-06,
                            "process_1": 8.821813575644333e-06,
                            "process_3": 8.944218272185448e-06,
                            "process_2": 8.9128036409247e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.00564707317726051,
                            "process_1": 0.0055633815713684,
                            "process_3": 0.005598757259723288,
                            "process_2": 0.005679553311501079
                        },
                        "total_energy_joules": {
                            "process_0": 20329.463438137835,
                            "process_1": 20028.17365692624,
                            "process_3": 20155.526135003838,
                            "process_2": 20446.391921403883
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 383.4266572566107,
                        "ram_power_avg": 0.9549057483673096,
                        "cpu_energy_total": 0.005152582826561683,
                        "gpu_energy_total": 0.017301061063060175,
                        "ram_energy_total": 3.5121430231419384e-05,
                        "total_energy_kwh": 0.022488765319853277,
                        "total_energy_joules": 80959.55515147178
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20237265347303668,
                        "joules_per_token": 4.941379098600573,
                        "flops_per_joule": 209363440.3183089,
                        "joules_per_flop": 4.7763831091026914e-09
                    },
                    "per-process_emissions": [
                        0.0021512525268773913,
                        0.0021193702096127917,
                        0.0021328465780915866,
                        0.002163625834016336
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0331": {
            "setup": {
                "experiment_id": "0331",
                "date_time": "April 11, 2025 at 06:56:55 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_True_latency_0.05_latency_0.2_latency_True_latency_4.0_latency_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.93022625600497,
                        "average_latency_ms_per_batch": 5241.278282000621,
                        "throughput_queries_per_sec": 3.0526904199966887,
                        "throughput_tokens_per_sec": 390.74437375957615
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            19.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 2624958464
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0331",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 376.718852244718,
                            "process_1": 514.1052050251058,
                            "process_3": 391.0407959623868,
                            "process_2": 251.8417757942326
                        },
                        "ram_power": {
                            "process_0": 0.916534423828125,
                            "process_1": 0.9670085906982422,
                            "process_3": 0.9756917953491211,
                            "process_2": 0.9603881835937501
                        },
                        "cpu_energy": {
                            "process_0": 0.001291256826843778,
                            "process_1": 0.0012742074446247214,
                            "process_3": 0.0012829651515311297,
                            "process_2": 0.001304153403562054
                        },
                        "gpu_energy": {
                            "process_0": 0.004347373755674067,
                            "process_1": 0.004280352313168034,
                            "process_3": 0.004306847889919974,
                            "process_2": 0.0043664871042981
                        },
                        "ram_energy": {
                            "process_0": 8.442594742664905e-06,
                            "process_1": 8.821813575644333e-06,
                            "process_3": 8.944218272185448e-06,
                            "process_2": 8.9128036409247e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.00564707317726051,
                            "process_1": 0.0055633815713684,
                            "process_3": 0.005598757259723288,
                            "process_2": 0.005679553311501079
                        },
                        "total_energy_joules": {
                            "process_0": 20329.463438137835,
                            "process_1": 20028.17365692624,
                            "process_3": 20155.526135003838,
                            "process_2": 20446.391921403883
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 383.4266572566107,
                        "ram_power_avg": 0.9549057483673096,
                        "cpu_energy_total": 0.005152582826561683,
                        "gpu_energy_total": 0.017301061063060175,
                        "ram_energy_total": 3.5121430231419384e-05,
                        "total_energy_kwh": 0.022488765319853277,
                        "total_energy_joules": 80959.55515147178
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20237265347303668,
                        "joules_per_token": 4.941379098600573,
                        "flops_per_joule": 209363440.3183089,
                        "joules_per_flop": 4.7763831091026914e-09
                    },
                    "per-process_emissions": [
                        0.0021512525268773913,
                        0.0021193702096127917,
                        0.0021328465780915866,
                        0.002163625834016336
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0332": {
            "setup": {
                "experiment_id": "0332",
                "date_time": "April 11, 2025 at 06:58:10 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_True_latency_0.2_latency_0.6_latency_True_latency_5.0_latency_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.6,
                    "simulate_burst": true,
                    "burst_interval": 5.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 45.240207012997416,
                        "average_latency_ms_per_batch": 5655.025876624677,
                        "throughput_queries_per_sec": 2.8293416067531667,
                        "throughput_tokens_per_sec": 362.15572566440534
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2628886528
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0332",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 309.0588786128148,
                            "process_1": 398.1893540214666,
                            "process_2": 337.4327131091097,
                            "process_3": 299.5179472319089
                        },
                        "ram_power": {
                            "process_0": 0.9169220924377441,
                            "process_1": 0.9751253128051758,
                            "process_2": 0.9786887168884276,
                            "process_3": 0.9758305549621583
                        },
                        "cpu_energy": {
                            "process_0": 0.0013977257129373583,
                            "process_1": 0.0013753497628754302,
                            "process_2": 0.0014077143324684586,
                            "process_3": 0.0014364733160625747
                        },
                        "gpu_energy": {
                            "process_0": 0.0044302018774918794,
                            "process_1": 0.004370046273811989,
                            "process_2": 0.004468829963949927,
                            "process_3": 0.004539245575837958
                        },
                        "ram_energy": {
                            "process_0": 9.146326378428975e-06,
                            "process_1": 9.688351347638887e-06,
                            "process_2": 9.95485464278705e-06,
                            "process_3": 1.0061019205793366e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0058370739168076705,
                            "process_1": 0.005755084388035059,
                            "process_2": 0.005886499151061172,
                            "process_3": 0.005985779911106326
                        },
                        "total_energy_joules": {
                            "process_0": 21013.466100507612,
                            "process_1": 20718.30379692621,
                            "process_2": 21191.39694382022,
                            "process_3": 21548.807679982772
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 336.049723243825,
                        "ram_power_avg": 0.9616416692733765,
                        "cpu_energy_total": 0.005617263124343821,
                        "gpu_energy_total": 0.017808323691091754,
                        "ram_energy_total": 3.8850551574648276e-05,
                        "total_energy_kwh": 0.023464437367010225,
                        "total_energy_joules": 84471.97452123681
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.19395781965391318,
                        "joules_per_token": 5.155760163649708,
                        "flops_per_joule": 200657923.40263888,
                        "joules_per_flop": 4.9836058454238386e-09
                    },
                    "per-process_emissions": [
                        0.0022236333086078823,
                        0.0021923993976219556,
                        0.0022424618515967535,
                        0.002280282857135955
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0332": {
            "setup": {
                "experiment_id": "0332",
                "date_time": "April 11, 2025 at 06:58:10 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_True_latency_0.2_latency_0.6_latency_True_latency_5.0_latency_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.6,
                    "simulate_burst": true,
                    "burst_interval": 5.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 45.240207012997416,
                        "average_latency_ms_per_batch": 5655.025876624677,
                        "throughput_queries_per_sec": 2.8293416067531667,
                        "throughput_tokens_per_sec": 362.15572566440534
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.3,
                        "cpu_memory_usage_bytes": 2628886528
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0332",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 309.0588786128148,
                            "process_1": 398.1893540214666,
                            "process_2": 337.4327131091097,
                            "process_3": 299.5179472319089
                        },
                        "ram_power": {
                            "process_0": 0.9169220924377441,
                            "process_1": 0.9751253128051758,
                            "process_2": 0.9786887168884276,
                            "process_3": 0.9758305549621583
                        },
                        "cpu_energy": {
                            "process_0": 0.0013977257129373583,
                            "process_1": 0.0013753497628754302,
                            "process_2": 0.0014077143324684586,
                            "process_3": 0.0014364733160625747
                        },
                        "gpu_energy": {
                            "process_0": 0.0044302018774918794,
                            "process_1": 0.004370046273811989,
                            "process_2": 0.004468829963949927,
                            "process_3": 0.004539245575837958
                        },
                        "ram_energy": {
                            "process_0": 9.146326378428975e-06,
                            "process_1": 9.688351347638887e-06,
                            "process_2": 9.95485464278705e-06,
                            "process_3": 1.0061019205793366e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0058370739168076705,
                            "process_1": 0.005755084388035059,
                            "process_2": 0.005886499151061172,
                            "process_3": 0.005985779911106326
                        },
                        "total_energy_joules": {
                            "process_0": 21013.466100507612,
                            "process_1": 20718.30379692621,
                            "process_2": 21191.39694382022,
                            "process_3": 21548.807679982772
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 336.049723243825,
                        "ram_power_avg": 0.9616416692733765,
                        "cpu_energy_total": 0.005617263124343821,
                        "gpu_energy_total": 0.017808323691091754,
                        "ram_energy_total": 3.8850551574648276e-05,
                        "total_energy_kwh": 0.023464437367010225,
                        "total_energy_joules": 84471.97452123681
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.19395781965391318,
                        "joules_per_token": 5.155760163649708,
                        "flops_per_joule": 200657923.40263888,
                        "joules_per_flop": 4.9836058454238386e-09
                    },
                    "per-process_emissions": [
                        0.0022236333086078823,
                        0.0021923993976219556,
                        0.0022424618515967535,
                        0.002280282857135955
                    ]
                },
                "local_energy_results": {}
            }
        }
    }
]