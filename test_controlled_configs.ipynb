{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of configurations: 32\n",
      "================================================================================\n",
      "Configuration 1:\n",
      "Config Name: num_processes_1\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'num_processes_1',\n",
      " 'controlled_variation': {'num_processes': 1},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.0, 'decoder_top_k': 0, 'decoder_top_p': 0.0},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 1,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'suite': 'controlled',\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 2:\n",
      "Config Name: num_processes_2\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'num_processes_2',\n",
      " 'controlled_variation': {'num_processes': 2},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.0, 'decoder_top_k': 0, 'decoder_top_p': 0.0},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 2,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'suite': 'controlled',\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 3:\n",
      "Config Name: num_processes_3\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'num_processes_3',\n",
      " 'controlled_variation': {'num_processes': 3},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.0, 'decoder_top_k': 0, 'decoder_top_p': 0.0},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 3,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'suite': 'controlled',\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 4:\n",
      "Config Name: num_processes_4\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'num_processes_4',\n",
      " 'controlled_variation': {'num_processes': 4},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.0, 'decoder_top_k': 0, 'decoder_top_p': 0.0},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'suite': 'controlled',\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 5:\n",
      "Config Name: batching_1\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 1,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'batching_1',\n",
      " 'controlled_variation': {'batching_options.batch_size___fixed_batching': 1},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.0, 'decoder_top_k': 0, 'decoder_top_p': 0.0},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'suite': 'controlled',\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 6:\n",
      "Config Name: batching_2\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 2,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'batching_2',\n",
      " 'controlled_variation': {'batching_options.batch_size___fixed_batching': 2},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.0, 'decoder_top_k': 0, 'decoder_top_p': 0.0},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'suite': 'controlled',\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 7:\n",
      "Config Name: batching_4\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 4,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'batching_4',\n",
      " 'controlled_variation': {'batching_options.batch_size___fixed_batching': 4},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.0, 'decoder_top_k': 0, 'decoder_top_p': 0.0},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'suite': 'controlled',\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 8:\n",
      "Config Name: batching_8\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 8,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'batching_8',\n",
      " 'controlled_variation': {'batching_options.batch_size___fixed_batching': 8},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.0, 'decoder_top_k': 0, 'decoder_top_p': 0.0},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'suite': 'controlled',\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 9:\n",
      "Config Name: batching_16\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'batching_16',\n",
      " 'controlled_variation': {'batching_options.batch_size___fixed_batching': 16},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.0, 'decoder_top_k': 0, 'decoder_top_p': 0.0},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'suite': 'controlled',\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 10:\n",
      "Config Name: batching_32\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 32,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'batching_32',\n",
      " 'controlled_variation': {'batching_options.batch_size___fixed_batching': 32},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.0, 'decoder_top_k': 0, 'decoder_top_p': 0.0},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'suite': 'controlled',\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 11:\n",
      "Config Name: batching_64\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 64,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'batching_64',\n",
      " 'controlled_variation': {'batching_options.batch_size___fixed_batching': 64},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.0, 'decoder_top_k': 0, 'decoder_top_p': 0.0},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'suite': 'controlled',\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 12:\n",
      "Config Name: precis_float32_quant_False_quant8_False_quant4_False\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'precis_float32_quant_False_quant8_False_quant4_False',\n",
      " 'controlled_variation': {'fp_precision': 'float32',\n",
      "                          'quantization_config.load_in_4bit': False,\n",
      "                          'quantization_config.load_in_8bit': False,\n",
      "                          'quantization_config.quantization': False},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.0, 'decoder_top_k': 0, 'decoder_top_p': 0.0},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 13:\n",
      "Config Name: precis_float16_quant_False_quant8_False_quant4_False\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'precis_float16_quant_False_quant8_False_quant4_False',\n",
      " 'controlled_variation': {'fp_precision': 'float16',\n",
      "                          'quantization_config.load_in_4bit': False,\n",
      "                          'quantization_config.load_in_8bit': False,\n",
      "                          'quantization_config.quantization': False},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.0, 'decoder_top_k': 0, 'decoder_top_p': 0.0},\n",
      " 'fp_precision': 'float16',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 14:\n",
      "Config Name: precis_float16_quant_True_quant8_True_quant4_False\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'precis_float16_quant_True_quant8_True_quant4_False',\n",
      " 'controlled_variation': {'fp_precision': 'float16',\n",
      "                          'quantization_config.load_in_4bit': False,\n",
      "                          'quantization_config.load_in_8bit': True,\n",
      "                          'quantization_config.quantization': True},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.0, 'decoder_top_k': 0, 'decoder_top_p': 0.0},\n",
      " 'fp_precision': 'float16',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': True,\n",
      "                         'quantization': True},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 15:\n",
      "Config Name: precis_float16_quant_True_quant8_False_quant4_True\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'precis_float16_quant_True_quant8_False_quant4_True',\n",
      " 'controlled_variation': {'fp_precision': 'float16',\n",
      "                          'quantization_config.load_in_4bit': True,\n",
      "                          'quantization_config.load_in_8bit': False,\n",
      "                          'quantization_config.quantization': True},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.0, 'decoder_top_k': 0, 'decoder_top_p': 0.0},\n",
      " 'fp_precision': 'float16',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': True,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': True},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 16:\n",
      "Config Name: decoding_greedy_decoder_temperature_0\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'decoding_greedy_decoder_temperature_0',\n",
      " 'controlled_variation': {'decoder_config.decoder_temperature': 0, 'decoder_config.decoding_mode': 'greedy'},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 0, 'decoder_top_k': 0, 'decoder_top_p': 0.0, 'decoding_mode': 'greedy'},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 17:\n",
      "Config Name: decoding_greedy_decoder_temperature_0.7\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'decoding_greedy_decoder_temperature_0.7',\n",
      " 'controlled_variation': {'decoder_config.decoder_temperature': 0.7, 'decoder_config.decoding_mode': 'greedy'},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 0.7, 'decoder_top_k': 0, 'decoder_top_p': 0.0, 'decoding_mode': 'greedy'},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 18:\n",
      "Config Name: decoding_greedy_decoder_temperature_1.0\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'decoding_greedy_decoder_temperature_1.0',\n",
      " 'controlled_variation': {'decoder_config.decoder_temperature': 1.0, 'decoder_config.decoding_mode': 'greedy'},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.0, 'decoder_top_k': 0, 'decoder_top_p': 0.0, 'decoding_mode': 'greedy'},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 19:\n",
      "Config Name: decoding_greedy_decoder_temperature_1.3\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'decoding_greedy_decoder_temperature_1.3',\n",
      " 'controlled_variation': {'decoder_config.decoder_temperature': 1.3, 'decoder_config.decoding_mode': 'greedy'},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.3, 'decoder_top_k': 0, 'decoder_top_p': 0.0, 'decoding_mode': 'greedy'},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 20:\n",
      "Config Name: decoding_top_k_decoder_top_k_50_decoder_temperature_0\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'decoding_top_k_decoder_top_k_50_decoder_temperature_0',\n",
      " 'controlled_variation': {'decoder_config.decoder_temperature': 0,\n",
      "                          'decoder_config.decoder_top_k': 50,\n",
      "                          'decoder_config.decoding_mode': 'top_k'},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 0, 'decoder_top_k': 50, 'decoder_top_p': 0.0, 'decoding_mode': 'top_k'},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 21:\n",
      "Config Name: decoding_top_k_decoder_top_k_50_decoder_temperature_0.7\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'decoding_top_k_decoder_top_k_50_decoder_temperature_0.7',\n",
      " 'controlled_variation': {'decoder_config.decoder_temperature': 0.7,\n",
      "                          'decoder_config.decoder_top_k': 50,\n",
      "                          'decoder_config.decoding_mode': 'top_k'},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 0.7, 'decoder_top_k': 50, 'decoder_top_p': 0.0, 'decoding_mode': 'top_k'},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 22:\n",
      "Config Name: decoding_top_k_decoder_top_k_50_decoder_temperature_1.0\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'decoding_top_k_decoder_top_k_50_decoder_temperature_1.0',\n",
      " 'controlled_variation': {'decoder_config.decoder_temperature': 1.0,\n",
      "                          'decoder_config.decoder_top_k': 50,\n",
      "                          'decoder_config.decoding_mode': 'top_k'},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.0, 'decoder_top_k': 50, 'decoder_top_p': 0.0, 'decoding_mode': 'top_k'},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 23:\n",
      "Config Name: decoding_top_k_decoder_top_k_50_decoder_temperature_1.3\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'decoding_top_k_decoder_top_k_50_decoder_temperature_1.3',\n",
      " 'controlled_variation': {'decoder_config.decoder_temperature': 1.3,\n",
      "                          'decoder_config.decoder_top_k': 50,\n",
      "                          'decoder_config.decoding_mode': 'top_k'},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.3, 'decoder_top_k': 50, 'decoder_top_p': 0.0, 'decoding_mode': 'top_k'},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 24:\n",
      "Config Name: decoding_top_p_decoder_top_p_0.9_decoder_temperature_0\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'decoding_top_p_decoder_top_p_0.9_decoder_temperature_0',\n",
      " 'controlled_variation': {'decoder_config.decoder_temperature': 0,\n",
      "                          'decoder_config.decoder_top_p': 0.9,\n",
      "                          'decoder_config.decoding_mode': 'top_p'},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 0, 'decoder_top_k': 0, 'decoder_top_p': 0.9, 'decoding_mode': 'top_p'},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 25:\n",
      "Config Name: decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.7\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.7',\n",
      " 'controlled_variation': {'decoder_config.decoder_temperature': 0.7,\n",
      "                          'decoder_config.decoder_top_p': 0.9,\n",
      "                          'decoder_config.decoding_mode': 'top_p'},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 0.7, 'decoder_top_k': 0, 'decoder_top_p': 0.9, 'decoding_mode': 'top_p'},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 26:\n",
      "Config Name: decoding_top_p_decoder_top_p_0.9_decoder_temperature_1.0\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'decoding_top_p_decoder_top_p_0.9_decoder_temperature_1.0',\n",
      " 'controlled_variation': {'decoder_config.decoder_temperature': 1.0,\n",
      "                          'decoder_config.decoder_top_p': 0.9,\n",
      "                          'decoder_config.decoding_mode': 'top_p'},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.0, 'decoder_top_k': 0, 'decoder_top_p': 0.9, 'decoding_mode': 'top_p'},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 27:\n",
      "Config Name: decoding_top_p_decoder_top_p_0.9_decoder_temperature_1.3\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'decoding_top_p_decoder_top_p_0.9_decoder_temperature_1.3',\n",
      " 'controlled_variation': {'decoder_config.decoder_temperature': 1.3,\n",
      "                          'decoder_config.decoder_top_p': 0.9,\n",
      "                          'decoder_config.decoding_mode': 'top_p'},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.3, 'decoder_top_k': 0, 'decoder_top_p': 0.9, 'decoding_mode': 'top_p'},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 28:\n",
      "Config Name: latency_False\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'latency_False',\n",
      " 'controlled_variation': {'latency_simulation.simulate': False},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.0, 'decoder_top_k': 0, 'decoder_top_p': 0.0},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0,\n",
      "                        'delay_min': 0,\n",
      "                        'simulate': False,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 29:\n",
      "Config Name: latency_True_latency_0.05_latency_0.2_latency_False\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'latency_True_latency_0.05_latency_0.2_latency_False',\n",
      " 'controlled_variation': {'latency_simulation.delay_max': 0.2,\n",
      "                          'latency_simulation.delay_min': 0.05,\n",
      "                          'latency_simulation.simulate': True,\n",
      "                          'latency_simulation.simulate_burst': False},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.0, 'decoder_top_k': 0, 'decoder_top_p': 0.0},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0.2,\n",
      "                        'delay_min': 0.05,\n",
      "                        'simulate': True,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 30:\n",
      "Config Name: latency_True_latency_0.2_latency_0.6_latency_False\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'latency_True_latency_0.2_latency_0.6_latency_False',\n",
      " 'controlled_variation': {'latency_simulation.delay_max': 0.6,\n",
      "                          'latency_simulation.delay_min': 0.2,\n",
      "                          'latency_simulation.simulate': True,\n",
      "                          'latency_simulation.simulate_burst': False},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.0, 'decoder_top_k': 0, 'decoder_top_p': 0.0},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 0.0,\n",
      "                        'burst_size': 0,\n",
      "                        'delay_max': 0.6,\n",
      "                        'delay_min': 0.2,\n",
      "                        'simulate': True,\n",
      "                        'simulate_burst': False},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 31:\n",
      "Config Name: latency_True_latency_0.05_latency_0.2_latency_True_latency_4.0_latency_5\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'latency_True_latency_0.05_latency_0.2_latency_True_latency_4.0_latency_5',\n",
      " 'controlled_variation': {'latency_simulation.burst_interval': 4.0,\n",
      "                          'latency_simulation.burst_size': 5,\n",
      "                          'latency_simulation.delay_max': 0.2,\n",
      "                          'latency_simulation.delay_min': 0.05,\n",
      "                          'latency_simulation.simulate': True,\n",
      "                          'latency_simulation.simulate_burst': True},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.0, 'decoder_top_k': 0, 'decoder_top_p': 0.0},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 4.0,\n",
      "                        'burst_size': 5,\n",
      "                        'delay_max': 0.2,\n",
      "                        'delay_min': 0.05,\n",
      "                        'simulate': True,\n",
      "                        'simulate_burst': True},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n",
      "Configuration 32:\n",
      "Config Name: latency_True_latency_0.2_latency_0.6_latency_True_latency_5.0_latency_8\n",
      "{'backend': 'pytorch',\n",
      " 'batching_options': {'adaptive_batching': False,\n",
      "                      'adaptive_max_tokens': 0,\n",
      "                      'batch_size___fixed_batching': 16,\n",
      "                      'max_batch_size___adaptive_batching': 0},\n",
      " 'config_name': 'latency_True_latency_0.2_latency_0.6_latency_True_latency_5.0_latency_8',\n",
      " 'controlled_variation': {'latency_simulation.burst_interval': 5.0,\n",
      "                          'latency_simulation.burst_size': 8,\n",
      "                          'latency_simulation.delay_max': 0.6,\n",
      "                          'latency_simulation.delay_min': 0.2,\n",
      "                          'latency_simulation.simulate': True,\n",
      "                          'latency_simulation.simulate_burst': True},\n",
      " 'decode_token_to_text': True,\n",
      " 'decoder_config': {'decoder_temperature': 1.0, 'decoder_top_k': 0, 'decoder_top_p': 0.0},\n",
      " 'fp_precision': 'float32',\n",
      " 'gpu_list': [0, 1, 2, 3],\n",
      " 'inference_type': 'pure_generative',\n",
      " 'is_encoder_decoder': False,\n",
      " 'latency_simulation': {'burst_interval': 5.0,\n",
      "                        'burst_size': 8,\n",
      "                        'delay_max': 0.6,\n",
      "                        'delay_min': 0.2,\n",
      "                        'simulate': True,\n",
      "                        'simulate_burst': True},\n",
      " 'max_input_tokens': 100,\n",
      " 'max_output_tokens': 100,\n",
      " 'model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      " 'num_input_prompts': 10,\n",
      " 'num_processes': 4,\n",
      " 'quantization_config': {'cached_flops_for_quantised_models': 82763530240000,\n",
      "                         'load_in_4bit': False,\n",
      "                         'load_in_8bit': False,\n",
      "                         'quantization': False},\n",
      " 'query_rate': 1.0,\n",
      " 'save_outputs': True,\n",
      " 'sharding_config': {'fsdp_config': {'cpu_offload': False, 'use_orig_params': False}, 'sharding_strategy': 'NO_SHARD'},\n",
      " 'task_type': 'text_generation'}\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#!/usr/bin/env python\n",
    "import pprint\n",
    "\n",
    "from configs.c_controlled_configs import controlled_config_list\n",
    "\n",
    "def main():\n",
    "    print(\"Number of configurations:\", len(controlled_config_list))\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for idx, cfg in enumerate(controlled_config_list, start=1):\n",
    "        print(f\"Configuration {idx}:\")\n",
    "        print(\"Config Name:\", cfg.get(\"config_name\", \"No config_name field\"))\n",
    "        # Pretty-print the full configuration dictionary.\n",
    "        pprint.pprint(cfg, width=120)\n",
    "        print(\"=\"*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
