{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will later move this into function script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_energy_metrics_local_process_results_gpu_power_process_3\n",
      "compute_metrics_compute_utilisation_gpu_utilization_percent_1\n",
      "variables_decoder_config_decoder_top_k\n",
      "variables_quantisation_load_in_8bit\n",
      "global_energy_metrics_per-process_emissions_2\n",
      "global_energy_metrics_local_process_results_total_energy_joules_process_2\n",
      "compute_metrics_compute_utilisation_gpu_utilization_percent_2\n",
      "variables_backend\n",
      "global_energy_metrics_local_process_results_total_energy_kwh_process_2\n",
      "global_energy_metrics_global_derived_quantities_flops_per_joule\n",
      "variables_max_output_tokens\n",
      "variables_batching_options_adaptive_batching\n",
      "global_energy_metrics_local_process_results_ram_energy_process_2\n",
      "global_energy_metrics_local_process_results_ram_power_process_0\n",
      "setup_country\n",
      "setup_available_gpu_count\n",
      "variables_latency_simulation_simulate\n",
      "setup_is_encoder_decoder\n",
      "model_architecture_architecture\n",
      "global_energy_metrics_local_process_results_cpu_power_process_3\n",
      "inference_metrics_inference_performance_total_inference_time_sec\n",
      "setup_gpu_model\n",
      "variables_latency_simulation_delay_min\n",
      "variables_quantisation_cached_flops_for_quantised_models\n",
      "global_energy_metrics_local_process_results_total_energy_kwh_process_0\n",
      "setup_date_time\n",
      "global_energy_metrics_global_experiment_results_ram_power_avg\n",
      "global_energy_metrics_local_process_results_ram_energy_process_0\n",
      "compute_metrics_compute_utilisation_cpu_usage_percent\n",
      "variables_decoder_config_decoder_temperature\n",
      "global_energy_metrics_global_derived_quantities_joules_per_token\n",
      "global_energy_metrics_local_process_results_gpu_power_process_2\n",
      "global_energy_metrics_local_process_results_gpu_energy_process_1\n",
      "global_energy_metrics_local_process_results_cpu_energy_process_2\n",
      "global_energy_metrics_per-process_emissions_1\n",
      "inference_metrics_inference_performance_throughput_tokens_per_sec\n",
      "global_energy_metrics_global_experiment_results_ram_energy_total\n",
      "variables_sharding_config_sharding_strategy\n",
      "variables_latency_simulation_burst_size\n",
      "variables_sharding_config_fsdp_config_cpu_offload\n",
      "variables_sharding_config_fsdp_config_use_orig_params\n",
      "compute_metrics_flops\n",
      "global_energy_metrics_local_process_results_cpu_power_process_1\n",
      "compute_metrics_memory_gpu_max_memory_reserved_bytes\n",
      "compute_metrics_compute_utilisation_gpu_utilization_percent_0\n",
      "global_energy_metrics_local_process_results_ram_power_process_1\n",
      "variables_batching_options_max_batch_size___adaptive_batching\n",
      "variables_quantisation_load_in_4bit\n",
      "global_energy_metrics_local_process_results_ram_energy_process_1\n",
      "setup_task_type\n",
      "variables_accelerate_config_distributed_type\n",
      "variables_accelerate_config_num_processes\n",
      "variables_latency_simulation_delay_max\n",
      "global_energy_metrics_local_process_results_ram_power_process_2\n",
      "variables_batching_options_adaptive_max_tokens\n",
      "global_energy_metrics_global_experiment_results_total_energy_kwh\n",
      "global_energy_metrics_local_process_results_total_energy_kwh_process_1\n",
      "setup_python_version\n",
      "global_energy_metrics_global_experiment_results_total_energy_joules\n",
      "global_energy_metrics_local_process_results_gpu_energy_process_0\n",
      "inference_metrics_inference_performance_throughput_queries_per_sec\n",
      "inference_metrics_raw_inference_metrics_total_generated_tokens\n",
      "model_architecture_total_params\n",
      "global_energy_metrics_global_derived_quantities_tokens_per_joule\n",
      "variables_latency_simulation_simulate_burst\n",
      "global_energy_metrics_global_experiment_results_gpu_energy_total\n",
      "global_energy_metrics_local_process_results_gpu_energy_process_2\n",
      "variables_decode_token_to_text\n",
      "global_energy_metrics_local_process_results_cpu_energy_process_1\n",
      "inference_metrics_inference_performance_average_latency_ms_per_batch\n",
      "variables_decoder_config_decoder_top_p\n",
      "variables_quantisation_quantization\n",
      "setup_os\n",
      "variables_fp_precision\n",
      "global_energy_metrics_local_process_results_total_energy_joules_process_3\n",
      "compute_metrics_memory_gpu_current_memory_reserved_bytes\n",
      "compute_metrics_memory_gpu_max_memory_allocated_bytes\n",
      "compute_metrics_compute_utilisation_cpu_memory_usage_bytes\n",
      "variables_latency_simulation_burst_interval\n",
      "variables_number_input_prompts\n",
      "variables_query_rate\n",
      "global_energy_metrics_global_experiment_results_cpu_energy_total\n",
      "global_energy_metrics_global_derived_quantities_joules_per_flop\n",
      "setup_region\n",
      "variables_batching_options_batch_size___fixed_batching\n",
      "variables_max_input_tokens\n",
      "setup_cpu_model\n",
      "setup_available_cpu_count\n",
      "variables_decoder_config_decoding_mode\n",
      "global_energy_metrics_local_process_results_cpu_energy_process_3\n",
      "compute_metrics_compute_utilisation_gpu_utilization_percent_3\n",
      "global_energy_metrics_local_process_results_cpu_power_process_2\n",
      "variables_inference_type\n",
      "global_energy_metrics_local_process_results_total_energy_kwh_process_3\n",
      "global_energy_metrics_global_experiment_results_cpu_power_avg\n",
      "global_energy_metrics_local_process_results_gpu_power_process_0\n",
      "global_energy_metrics_local_process_results_gpu_power_process_1\n",
      "global_energy_metrics_experiment_id\n",
      "global_energy_metrics_local_process_results_gpu_energy_process_3\n",
      "global_energy_metrics_local_process_results_cpu_power_process_0\n",
      "global_energy_metrics_per-process_emissions_3\n",
      "inference_metrics_raw_inference_metrics_total_input_tokens\n",
      "global_energy_metrics_per-process_emissions_0\n",
      "compute_metrics_memory_gpu_current_memory_allocated_bytes\n",
      "variables_config_name\n",
      "global_energy_metrics_local_process_results_total_energy_joules_process_1\n",
      "global_energy_metrics_local_process_results_cpu_energy_process_0\n",
      "global_energy_metrics_local_process_results_total_energy_joules_process_0\n",
      "global_energy_metrics_local_process_results_ram_power_process_3\n",
      "setup_model\n",
      "inference_metrics_raw_inference_metrics_number_input_prompts\n",
      "global_energy_metrics_local_process_results_ram_energy_process_3\n",
      "global_energy_metrics_global_experiment_results_gpu_power_avg\n",
      "setup_experiment_id\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('results/controlled_results.csv')\n",
    "columns = list(df.columns)\n",
    "for col in columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column(col: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean a single column name by:\n",
    "      - Stripping whitespace and replacing any non-standard quotes.\n",
    "      - Checking for per-process metric patterns.\n",
    "      - Applying special renames.\n",
    "      - Removing the 'variables_' prefix if present.\n",
    "      - Otherwise, attempting to strip off any messy prefixes using a known list of tokens.\n",
    "    \n",
    "    If no known token is found, the original (normalized) column name is returned.\n",
    "    \"\"\"\n",
    "    # Normalize the column string: remove extra whitespace and fix common issues with quotes.\n",
    "    col = col.strip()\n",
    "    col = col.replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\")\n",
    "    \n",
    "    # 1. Special exact mappings.\n",
    "    special_mappings = {\n",
    "        \"setup_cpu_model\": \"cpu_model\",\n",
    "        \"setup_gpu_model\": \"gpu_model\",\n",
    "        \"model_architecture_total_params\": \"total_params\",  # now maps to total_params\n",
    "        \"model_architecture_architecture\": \"model_arch\"\n",
    "    }\n",
    "    if col in special_mappings:\n",
    "        return special_mappings[col]\n",
    "    \n",
    "    # 2. Remove the 'variables_' prefix if it exists.\n",
    "    if col.startswith(\"variables_\"):\n",
    "        col = col[len(\"variables_\"):]\n",
    "    \n",
    "    # 3. First, check if it is a per-process metric column.\n",
    "    per_process_patterns = [\n",
    "        r'(cpu_power_process_\\d+)',\n",
    "        r'(gpu_power_process_\\d+)',\n",
    "        r'(ram_power_process_\\d+)',\n",
    "        r'(cpu_energy_process_\\d+)',\n",
    "        r'(gpu_energy_process_\\d+)',\n",
    "        r'(ram_energy_process_\\d+)',\n",
    "        r'(total_energy_kwh_process_\\d+)',\n",
    "        r'(total_energy_joules_process_\\d+)'\n",
    "    ]\n",
    "    for pattern in per_process_patterns:\n",
    "        match = re.search(pattern, col)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    \n",
    "    # 4. For non-per-process columns, search for a known token in the cleaned column.\n",
    "    tokens = [\n",
    "        \"config_name\", \"experiment_id\", \"date_time\", \"model\", \"is_encoder_decoder\",\n",
    "        \"task_type\", \"available_gpu_count\", \"gpu_model\", \"available_cpu_count\", \"cpu_model\",\n",
    "        \"os\", \"python_version\", \"country\", \"region\", \"fsdp_use_orig_params\", \"fsdp_cpu_offload\",\n",
    "        \"sharding_strategy\", \"distributed_type\", \"num_processes\", \"max_input_tokens\", \"max_output_tokens\",\n",
    "        \"number_input_prompts\", \"decode_token_to_text\", \"decoder_temperature\", \"decoder_top_k\", \"decoder_top_p\",\n",
    "        \"query_rate\", \"latency_simulate\", \"latency_delay_min\", \"latency_delay_max\", \"latency_simulate_burst\",\n",
    "        \"latency_burst_interval\", \"latency_burst_size\", \"fp_precision\", \"quantization\", \"load_in_8bit\",\n",
    "        \"load_in_4bit\", \"cached_flops_for_quantised_models\", \"batch_size___fixed_batching\", \"adaptive_batching\",\n",
    "        \"adaptive_max_tokens\", \"max_batch_size___adaptive_batching\", \"inference_type\", \"backend\", \"total_params\",\n",
    "        \"architecture\", \"total_input_tokens\", \"total_generated_tokens\", \"total_inference_time_sec\", \n",
    "        \"average_latency_ms_per_batch\", \"throughput_queries_per_sec\", \"throughput_tokens_per_sec\", \"flops\",\n",
    "        \"gpu_current_memory_allocated_bytes\", \"gpu_max_memory_allocated_bytes\", \"gpu_current_memory_reserved_bytes\",\n",
    "        \"gpu_max_memory_reserved_bytes\", \"gpu_utilization_percent\", \"cpu_usage_percent\", \"cpu_memory_usage_bytes\",\n",
    "        # Per-process metrics:\n",
    "        \"cpu_power_process_0\", \"gpu_power_process_0\", \"ram_power_process_0\",\n",
    "        \"cpu_energy_process_0\", \"gpu_energy_process_0\", \"ram_energy_process_0\",\n",
    "        \"total_energy_kwh_process_0\", \"total_energy_joules_process_0\",\n",
    "        # Global averages and totals:\n",
    "        \"cpu_power_avg\", \"gpu_power_avg\", \"ram_power_avg\", \"cpu_energy_total\", \"gpu_energy_total\", \"ram_energy_total\",\n",
    "        \"total_energy_kwh\", \"total_energy_joules\", \"tokens_per_joule\", \"joules_per_token\", \"flops_per_joule\", \"joules_per_flop\",\n",
    "        \"per-process_emissions\"\n",
    "    ]\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in col:\n",
    "            idx = col.find(token)\n",
    "            return col[idx:]\n",
    "    \n",
    "    return col\n",
    "\n",
    "def resolve_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Resolve duplicate columns in the DataFrame.\n",
    "    \n",
    "    For any column that appears more than once:\n",
    "      - For 'adaptive_batching', if duplicates exist, prefer the one with a boolean dtype;\n",
    "        otherwise, pick the first occurrence.\n",
    "      - For all other columns (including 'experiment_id' and 'number_input_prompts'),\n",
    "        keep only the first occurrence.\n",
    "    \"\"\"\n",
    "    # Build a mapping of column name to list of indices where it occurs.\n",
    "    seen = {}\n",
    "    for idx, col in enumerate(df.columns):\n",
    "        seen.setdefault(col, []).append(idx)\n",
    "    \n",
    "    # Choose one index per duplicate group.\n",
    "    chosen_indices = []\n",
    "    for col, indices in seen.items():\n",
    "        if len(indices) == 1:\n",
    "            chosen_indices.append(indices[0])\n",
    "        else:\n",
    "            if col == \"adaptive_batching\":\n",
    "                # Look for a column with boolean type.\n",
    "                bool_idx = None\n",
    "                for i in indices:\n",
    "                    if pd.api.types.is_bool_dtype(df.iloc[:, i]):\n",
    "                        bool_idx = i\n",
    "                        break\n",
    "                chosen_indices.append(bool_idx if bool_idx is not None else indices[0])\n",
    "            else:\n",
    "                # For experiment_id, number_input_prompts, or any duplicate, keep the first occurrence.\n",
    "                chosen_indices.append(indices[0])\n",
    "    \n",
    "    # Sort indices to preserve the original order.\n",
    "    chosen_indices.sort()\n",
    "    return df.iloc[:, chosen_indices]\n",
    "\n",
    "def clean_and_reorder_columns(df: pd.DataFrame, desired_order: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean DataFrame columns by:\n",
    "      1. Renaming each column to remove extraneous prefixes and apply special mappings.\n",
    "      2. Removing duplicates (applying special resolution for some columns).\n",
    "      3. Reordering columns into the order specified by 'desired_order'. Any columns not explicitly mentioned\n",
    "         will be appended at the end.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with messy, flattened column names.\n",
    "        desired_order (list): List of column names (after cleaning) indicating the preferred ordering.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with cleaned and reordered columns.\n",
    "    \"\"\"\n",
    "    # Build mapping from original column names to cleaned names.\n",
    "    mapping = {col: clean_column(col) for col in df.columns}\n",
    "    \n",
    "    # Rename columns in the DataFrame.\n",
    "    df = df.rename(columns=mapping)\n",
    "    \n",
    "    # Resolve duplicates as required.\n",
    "    df = resolve_duplicates(df)\n",
    "    \n",
    "    # Reorder columns: first, the ones matching the desired order.\n",
    "    ordered_cols = [col for col in desired_order if col in df.columns]\n",
    "    # Then, append any remaining columns.\n",
    "    remaining_cols = [col for col in df.columns if col not in desired_order]\n",
    "    final_order = ordered_cols + remaining_cols\n",
    "    \n",
    "    return df[final_order]\n",
    "\n",
    "# Example desired order list\n",
    "desired_order = [\n",
    "    \"config_name\",\n",
    "    \"experiment_id\",\n",
    "    \"date_time\",\n",
    "    \"model\",\n",
    "    # num_process\n",
    "    \"num_processes\",\n",
    "    # batching\n",
    "    \"batch_size___fixed_batching\",\n",
    "    # decodeer\n",
    "    \"decoder_temperature\",\n",
    "    \"decoder_top_k\",\n",
    "    \"decoder_top_p\",\n",
    "    # latency\n",
    "    \"latency_simulation_simulate\"\n",
    "    \"latency_simulation_delay_max\",\n",
    "    \"latency_simulation_delay_min\",\n",
    "    \"latency_simulation_simulate_burst\",\n",
    "    \"latency_simulation_burst_size\",\n",
    "    \"latency_simulation_burst_interval\",\n",
    "    # precision / quantisation\n",
    "    \"fp_precision\",\n",
    "    \"quantization\",\n",
    "    \"load_in_8bit\",\n",
    "    \"load_in_4bit\",\n",
    "    \"cached_flops_for_quantised_models\",\n",
    "    \n",
    "    # UNUSED PARAMS\n",
    "    \"sharding_strategy\",\n",
    "    \"sharding_config_fsdp_config_use_orig_params\",\n",
    "    \"sharding_config_fsdp_config_cpu_offload\",\n",
    "    \"adaptive_batching\",\n",
    "    \"adaptive_max_tokens\",\n",
    "    \"query_rate\",\n",
    "    \"total_input_tokens\",\n",
    "    \"total_generated_tokens\"\n",
    "    \n",
    "    # CONSTANT SETUP ====\n",
    "    \"date_time\",\n",
    "    \"is_encoder_decoder\",\n",
    "    \"task_type\",\n",
    "    \"available_gpu_count\",\n",
    "    \"gpu_model\",\n",
    "    \"available_cpu_count\",\n",
    "    \"cpu_model\",\n",
    "    \"os\",\n",
    "    \"python_version\",\n",
    "    \"country\",\n",
    "    \"region\",\n",
    "    \"distributed_type\",\n",
    "    \"decode_token_to_text\",\n",
    "    \"inference_type\",\n",
    "    \"backend\",\n",
    "    \"total_params\",\n",
    "    \"model_arch\",\n",
    "\n",
    "    # Validation (should be same):\n",
    "    \"max_input_tokens\",\n",
    "    \"max_output_tokens\",\n",
    "    \"number_input_prompts\",\n",
    "    \n",
    "    # RESULTS =====\n",
    "    # energy\n",
    "    \"total_energy_kwh\",\n",
    "    \"total_energy_joules\",\n",
    "    # FLOPS\n",
    "    \"flops\",\n",
    "    \"tokens_per_joule\",\n",
    "    \"joules_per_token\",\n",
    "    \"flops_per_joule\",\n",
    "    \"joules_per_flop\",\n",
    "    \"total_inference_time_sec\", \n",
    "    # inference performance\n",
    "    \"average_latency_ms_per_batch\",\n",
    "    \"throughput_queries_per_sec\",\n",
    "    \"throughput_tokens_per_sec\",\n",
    "    # CPU utilization\n",
    "    \"cpu_usage_percent\",\n",
    "    \"cpu_memory_usage_bytes\",\n",
    "    # GPU utilization\n",
    "    \"gpu_utilization_percent_0\", \"gpu_utilization_percent_1\", \"gpu_utilization_percent_2\", \"gpu_utilization_percent_3\",\n",
    "    # Compute mem\n",
    "    \"gpu_current_memory_allocated_bytes\",\n",
    "    \"gpu_max_memory_allocated_bytes\",\n",
    "    \"gpu_current_memory_reserved_bytes\",\n",
    "    \"gpu_max_memory_reserved_bytes\",\n",
    "    # Per-process metrics:\n",
    "    \"cpu_power_process_0\", \"cpu_power_process_1\", \"cpu_power_process_2\", \"cpu_power_process_3\",\n",
    "    \"gpu_power_process_0\", \"gpu_power_process_1\", \"gpu_power_process_2\", \"gpu_power_process_3\",\n",
    "    \"ram_power_process_0\", \"ram_power_process_1\", \"ram_power_process_2\", \"ram_power_process_3\",\n",
    "    \"cpu_energy_process_0\", \"cpu_energy_process_1\", \"cpu_energy_process_2\", \"cpu_energy_process_3\",\n",
    "    \"gpu_energy_process_0\", \"gpu_energy_process_1\", \"gpu_energy_process_2\", \"gpu_energy_process_3\",\n",
    "    \"ram_energy_process_0\", \"ram_energy_process_1\", \"ram_energy_process_2\", \"ram_energy_process_3\",\n",
    "    \"total_energy_kwh_process_0\", \"total_energy_kwh_process_1\", \"total_energy_kwh_process_2\", \"total_energy_kwh_process_3\",\n",
    "    \"total_energy_joules_process_0\", \"total_energy_joules_process_1\", \"total_energy_joules_process_2\", \"total_energy_joules_process_3\",\n",
    "    # Global averages and totals:\n",
    "    \"cpu_power_avg\",\n",
    "    \"gpu_power_avg\",\n",
    "    \"ram_power_avg\",\n",
    "    \"cpu_energy_total\",\n",
    "    \"gpu_energy_total\",\n",
    "    \"ram_energy_total\",\n",
    "    # per-process_emsisisons\n",
    "    \"per-process_emissions_0\", \"per-process_emissions_1\", \"per-process_emissions_2\",\"per-process_emissions_3\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "controlled did not exist: name 'desired_order' is not defined\n",
      "scenarios did not exist: name 'desired_order' is not defined\n",
      "grid did not exist: name 'desired_order' is not defined\n",
      "text_generation did not exist: name 'desired_order' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'desired_order' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m did not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m df_controlled \u001b[38;5;241m=\u001b[39m inspect_results(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontrolled\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mdesired_order\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'desired_order' is not defined"
     ]
    }
   ],
   "source": [
    "def inspect_results(name, desired_order):\n",
    "    input_file = f\"results/{name}_results.csv\"\n",
    "    output_file = f\"results/cleaned_{name}.csv\"\n",
    "    \n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    df_cleaned = clean_and_reorder_columns(df, desired_order)\n",
    "        \n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    \n",
    "    display(df_cleaned.T)\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "possible_files = [\"controlled\", \"scenarios\", \"grid\", \"text_generation\"]\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "for file in possible_files:\n",
    "    try:\n",
    "        dfs[file] = inspect_results(file, desired_order)\n",
    "        print(f\"found & inspecting: {file}\")\n",
    "    except Exception as e:\n",
    "            print(f\"{file} did not exist: {e}\")\n",
    "     \n",
    "#df_controlled = inspect_results('controlled', desired_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second round ordering\n",
    "#existing_cols = [col for col in desired_order if col in df.columns]\n",
    "#df_controlled = df[existing_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>config_name</th>\n",
       "      <td>num_processes_1</td>\n",
       "      <td>num_processes_2</td>\n",
       "      <td>num_processes_3</td>\n",
       "      <td>num_processes_4</td>\n",
       "      <td>batching_1</td>\n",
       "      <td>batching_2</td>\n",
       "      <td>batching_4</td>\n",
       "      <td>batching_8</td>\n",
       "      <td>batching_16</td>\n",
       "      <td>batching_32</td>\n",
       "      <td>batching_64</td>\n",
       "      <td>precis_float32_quant_False_quant8_False_quant4...</td>\n",
       "      <td>precis_float16_quant_False_quant8_False_quant4...</td>\n",
       "      <td>precis_float16_quant_True_quant8_True_quant4_F...</td>\n",
       "      <td>precis_float16_quant_True_quant8_False_quant4_...</td>\n",
       "      <td>decoding_greedy_decoder_temperature_0</td>\n",
       "      <td>decoding_greedy_decoder_temperature_0.7</td>\n",
       "      <td>decoding_greedy_decoder_temperature_1.0</td>\n",
       "      <td>decoding_greedy_decoder_temperature_1.3</td>\n",
       "      <td>decoding_top_k_decoder_top_k_50_decoder_temper...</td>\n",
       "      <td>decoding_top_k_decoder_top_k_50_decoder_temper...</td>\n",
       "      <td>decoding_top_k_decoder_top_k_50_decoder_temper...</td>\n",
       "      <td>decoding_top_k_decoder_top_k_50_decoder_temper...</td>\n",
       "      <td>decoding_top_p_decoder_top_p_0.9_decoder_tempe...</td>\n",
       "      <td>decoding_top_p_decoder_top_p_0.9_decoder_tempe...</td>\n",
       "      <td>decoding_top_p_decoder_top_p_0.9_decoder_tempe...</td>\n",
       "      <td>decoding_top_p_decoder_top_p_0.9_decoder_tempe...</td>\n",
       "      <td>latency_False</td>\n",
       "      <td>latency_True_latency_0.05_latency_0.2_latency_...</td>\n",
       "      <td>latency_True_latency_0.2_latency_0.6_latency_F...</td>\n",
       "      <td>latency_True_latency_0.05_latency_0.2_latency_...</td>\n",
       "      <td>latency_True_latency_0.2_latency_0.6_latency_T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment_id</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_time</th>\n",
       "      <td>April 08, 2025 at 04:31:33 PM</td>\n",
       "      <td>April 08, 2025 at 04:32:06 PM</td>\n",
       "      <td>April 08, 2025 at 04:32:41 PM</td>\n",
       "      <td>April 08, 2025 at 04:33:15 PM</td>\n",
       "      <td>April 08, 2025 at 04:34:12 PM</td>\n",
       "      <td>April 08, 2025 at 04:34:59 PM</td>\n",
       "      <td>April 08, 2025 at 04:35:43 PM</td>\n",
       "      <td>April 08, 2025 at 04:36:20 PM</td>\n",
       "      <td>April 08, 2025 at 04:36:58 PM</td>\n",
       "      <td>April 08, 2025 at 04:37:33 PM</td>\n",
       "      <td>April 08, 2025 at 04:38:19 PM</td>\n",
       "      <td>April 08, 2025 at 04:38:55 PM</td>\n",
       "      <td>April 08, 2025 at 04:39:28 PM</td>\n",
       "      <td>April 08, 2025 at 04:40:33 PM</td>\n",
       "      <td>April 08, 2025 at 04:41:10 PM</td>\n",
       "      <td>April 08, 2025 at 04:41:43 PM</td>\n",
       "      <td>April 08, 2025 at 04:42:19 PM</td>\n",
       "      <td>April 08, 2025 at 04:42:54 PM</td>\n",
       "      <td>April 08, 2025 at 04:43:29 PM</td>\n",
       "      <td>April 08, 2025 at 04:44:03 PM</td>\n",
       "      <td>April 08, 2025 at 04:44:38 PM</td>\n",
       "      <td>April 08, 2025 at 04:45:14 PM</td>\n",
       "      <td>April 08, 2025 at 04:45:49 PM</td>\n",
       "      <td>April 08, 2025 at 04:46:23 PM</td>\n",
       "      <td>April 08, 2025 at 04:47:02 PM</td>\n",
       "      <td>April 08, 2025 at 04:47:37 PM</td>\n",
       "      <td>April 08, 2025 at 04:48:15 PM</td>\n",
       "      <td>April 08, 2025 at 04:48:49 PM</td>\n",
       "      <td>April 08, 2025 at 04:49:27 PM</td>\n",
       "      <td>April 08, 2025 at 04:50:02 PM</td>\n",
       "      <td>April 08, 2025 at 04:50:37 PM</td>\n",
       "      <td>April 08, 2025 at 04:51:12 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_processes</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch_size___fixed_batching</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decoder_temperature</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decoder_top_k</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decoder_top_p</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latency_simulation_delay_min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latency_simulation_simulate_burst</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latency_simulation_burst_size</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latency_simulation_burst_interval</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fp_precision</th>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float16</td>\n",
       "      <td>torch.float16</td>\n",
       "      <td>torch.float16</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "      <td>torch.float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quantization</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>load_in_8bit</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>load_in_4bit</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_input_tokens</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_params</th>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>615606272</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "      <td>1100048384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_input_tokens</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_output_tokens</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_input_prompts</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_energy_kwh</th>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.001153</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>0.00258</td>\n",
       "      <td>0.016064</td>\n",
       "      <td>0.009953</td>\n",
       "      <td>0.006337</td>\n",
       "      <td>0.004281</td>\n",
       "      <td>0.002686</td>\n",
       "      <td>0.002528</td>\n",
       "      <td>0.002687</td>\n",
       "      <td>0.002929</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>0.009094</td>\n",
       "      <td>0.002708</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.002392</td>\n",
       "      <td>0.00258</td>\n",
       "      <td>0.00262</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>0.00261</td>\n",
       "      <td>0.002658</td>\n",
       "      <td>0.002661</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.002628</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.002529</td>\n",
       "      <td>0.002987</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>0.002472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_energy_joules</th>\n",
       "      <td>1561.168622</td>\n",
       "      <td>4151.58235</td>\n",
       "      <td>6486.403256</td>\n",
       "      <td>9287.658482</td>\n",
       "      <td>57830.498773</td>\n",
       "      <td>35832.430964</td>\n",
       "      <td>22811.407515</td>\n",
       "      <td>15412.070402</td>\n",
       "      <td>9669.475684</td>\n",
       "      <td>9099.859349</td>\n",
       "      <td>9673.588298</td>\n",
       "      <td>10544.0582</td>\n",
       "      <td>6938.191835</td>\n",
       "      <td>32739.910723</td>\n",
       "      <td>9749.962662</td>\n",
       "      <td>7767.530628</td>\n",
       "      <td>8612.615549</td>\n",
       "      <td>9287.457917</td>\n",
       "      <td>9431.681788</td>\n",
       "      <td>8861.743317</td>\n",
       "      <td>9394.350592</td>\n",
       "      <td>9569.324332</td>\n",
       "      <td>9578.843492</td>\n",
       "      <td>10361.488063</td>\n",
       "      <td>9881.56245</td>\n",
       "      <td>9782.421027</td>\n",
       "      <td>9460.731311</td>\n",
       "      <td>11004.24041</td>\n",
       "      <td>9104.932397</td>\n",
       "      <td>10752.746732</td>\n",
       "      <td>8354.124101</td>\n",
       "      <td>8900.869755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flops</th>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens_per_joule</th>\n",
       "      <td>0.640546</td>\n",
       "      <td>0.240872</td>\n",
       "      <td>0.154169</td>\n",
       "      <td>0.10767</td>\n",
       "      <td>0.017292</td>\n",
       "      <td>0.027908</td>\n",
       "      <td>0.043838</td>\n",
       "      <td>0.064884</td>\n",
       "      <td>0.103418</td>\n",
       "      <td>0.109892</td>\n",
       "      <td>0.103374</td>\n",
       "      <td>0.09484</td>\n",
       "      <td>0.14413</td>\n",
       "      <td>0.030544</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.128741</td>\n",
       "      <td>0.116109</td>\n",
       "      <td>0.107672</td>\n",
       "      <td>0.106026</td>\n",
       "      <td>0.112845</td>\n",
       "      <td>0.106447</td>\n",
       "      <td>0.104501</td>\n",
       "      <td>0.104397</td>\n",
       "      <td>0.096511</td>\n",
       "      <td>0.101199</td>\n",
       "      <td>0.102224</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>0.090874</td>\n",
       "      <td>0.109831</td>\n",
       "      <td>0.092999</td>\n",
       "      <td>0.119701</td>\n",
       "      <td>0.112349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joules_per_token</th>\n",
       "      <td>1.561169</td>\n",
       "      <td>4.151582</td>\n",
       "      <td>6.486403</td>\n",
       "      <td>9.287658</td>\n",
       "      <td>57.830499</td>\n",
       "      <td>35.832431</td>\n",
       "      <td>22.811408</td>\n",
       "      <td>15.41207</td>\n",
       "      <td>9.669476</td>\n",
       "      <td>9.099859</td>\n",
       "      <td>9.673588</td>\n",
       "      <td>10.544058</td>\n",
       "      <td>6.938192</td>\n",
       "      <td>32.739911</td>\n",
       "      <td>9.749963</td>\n",
       "      <td>7.767531</td>\n",
       "      <td>8.612616</td>\n",
       "      <td>9.287458</td>\n",
       "      <td>9.431682</td>\n",
       "      <td>8.861743</td>\n",
       "      <td>9.394351</td>\n",
       "      <td>9.569324</td>\n",
       "      <td>9.578843</td>\n",
       "      <td>10.361488</td>\n",
       "      <td>9.881562</td>\n",
       "      <td>9.782421</td>\n",
       "      <td>9.460731</td>\n",
       "      <td>11.00424</td>\n",
       "      <td>9.104932</td>\n",
       "      <td>10.752747</td>\n",
       "      <td>8.354124</td>\n",
       "      <td>8.90087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flops_per_joule</th>\n",
       "      <td>662672893.521465</td>\n",
       "      <td>249192727.209392</td>\n",
       "      <td>159494266.256535</td>\n",
       "      <td>111389122.454932</td>\n",
       "      <td>17889247.88724</td>\n",
       "      <td>28871725.980626</td>\n",
       "      <td>45352051.482635</td>\n",
       "      <td>67125577.616509</td>\n",
       "      <td>106990716.129887</td>\n",
       "      <td>113687925.088365</td>\n",
       "      <td>106945230.268081</td>\n",
       "      <td>98116314.268051</td>\n",
       "      <td>149108608.219022</td>\n",
       "      <td>31598868.327846</td>\n",
       "      <td>106107496.397334</td>\n",
       "      <td>133188290.785709</td>\n",
       "      <td>120119622.447774</td>\n",
       "      <td>111391527.936536</td>\n",
       "      <td>109688192.54092</td>\n",
       "      <td>116742732.320793</td>\n",
       "      <td>110124070.617504</td>\n",
       "      <td>108110467.588197</td>\n",
       "      <td>108003030.722911</td>\n",
       "      <td>99845130.52061</td>\n",
       "      <td>104694387.473973</td>\n",
       "      <td>105755428.550843</td>\n",
       "      <td>109351390.926845</td>\n",
       "      <td>94013224.855566</td>\n",
       "      <td>113624580.931623</td>\n",
       "      <td>96212079.926237</td>\n",
       "      <td>123836337.055097</td>\n",
       "      <td>116229554.686645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joules_per_flop</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_inference_time_sec</th>\n",
       "      <td>2.356892</td>\n",
       "      <td>2.401629</td>\n",
       "      <td>2.346066</td>\n",
       "      <td>2.372563</td>\n",
       "      <td>20.824448</td>\n",
       "      <td>10.598905</td>\n",
       "      <td>6.579609</td>\n",
       "      <td>4.488748</td>\n",
       "      <td>2.56615</td>\n",
       "      <td>2.408858</td>\n",
       "      <td>2.385437</td>\n",
       "      <td>2.286825</td>\n",
       "      <td>2.363797</td>\n",
       "      <td>7.218139</td>\n",
       "      <td>3.677875</td>\n",
       "      <td>2.272839</td>\n",
       "      <td>2.381273</td>\n",
       "      <td>2.350439</td>\n",
       "      <td>2.395863</td>\n",
       "      <td>2.303065</td>\n",
       "      <td>2.387908</td>\n",
       "      <td>2.412971</td>\n",
       "      <td>2.478181</td>\n",
       "      <td>3.440782</td>\n",
       "      <td>2.714286</td>\n",
       "      <td>2.56474</td>\n",
       "      <td>2.419328</td>\n",
       "      <td>3.559638</td>\n",
       "      <td>2.453797</td>\n",
       "      <td>4.07742</td>\n",
       "      <td>2.509436</td>\n",
       "      <td>2.615047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_latency_ms_per_batch</th>\n",
       "      <td>2356.891729</td>\n",
       "      <td>2401.628917</td>\n",
       "      <td>2346.066005</td>\n",
       "      <td>2372.563443</td>\n",
       "      <td>2082.444752</td>\n",
       "      <td>2119.780987</td>\n",
       "      <td>2193.20311</td>\n",
       "      <td>2244.3741</td>\n",
       "      <td>2566.149628</td>\n",
       "      <td>2408.858126</td>\n",
       "      <td>2385.437419</td>\n",
       "      <td>2286.825385</td>\n",
       "      <td>2363.7969</td>\n",
       "      <td>7218.139077</td>\n",
       "      <td>3677.875369</td>\n",
       "      <td>2272.839022</td>\n",
       "      <td>2381.273368</td>\n",
       "      <td>2350.439308</td>\n",
       "      <td>2395.863263</td>\n",
       "      <td>2303.064523</td>\n",
       "      <td>2387.907913</td>\n",
       "      <td>2412.971</td>\n",
       "      <td>2478.180916</td>\n",
       "      <td>3440.782234</td>\n",
       "      <td>2714.285777</td>\n",
       "      <td>2564.739563</td>\n",
       "      <td>2419.327923</td>\n",
       "      <td>3559.637986</td>\n",
       "      <td>2453.797072</td>\n",
       "      <td>4077.420104</td>\n",
       "      <td>2509.436437</td>\n",
       "      <td>2615.046637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>throughput_queries_per_sec</th>\n",
       "      <td>4.242876</td>\n",
       "      <td>4.163841</td>\n",
       "      <td>4.262455</td>\n",
       "      <td>4.21485</td>\n",
       "      <td>0.480205</td>\n",
       "      <td>0.943494</td>\n",
       "      <td>1.519847</td>\n",
       "      <td>2.227793</td>\n",
       "      <td>3.896889</td>\n",
       "      <td>4.151345</td>\n",
       "      <td>4.192103</td>\n",
       "      <td>4.372874</td>\n",
       "      <td>4.230482</td>\n",
       "      <td>1.385399</td>\n",
       "      <td>2.718961</td>\n",
       "      <td>4.399784</td>\n",
       "      <td>4.199434</td>\n",
       "      <td>4.254524</td>\n",
       "      <td>4.173861</td>\n",
       "      <td>4.342041</td>\n",
       "      <td>4.187766</td>\n",
       "      <td>4.144269</td>\n",
       "      <td>4.035218</td>\n",
       "      <td>2.906316</td>\n",
       "      <td>3.68421</td>\n",
       "      <td>3.899031</td>\n",
       "      <td>4.133379</td>\n",
       "      <td>2.809274</td>\n",
       "      <td>4.075317</td>\n",
       "      <td>2.452531</td>\n",
       "      <td>3.984958</td>\n",
       "      <td>3.824024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>throughput_tokens_per_sec</th>\n",
       "      <td>424.287627</td>\n",
       "      <td>416.38406</td>\n",
       "      <td>426.245467</td>\n",
       "      <td>421.485041</td>\n",
       "      <td>48.020482</td>\n",
       "      <td>94.34937</td>\n",
       "      <td>151.984708</td>\n",
       "      <td>222.779259</td>\n",
       "      <td>389.68889</td>\n",
       "      <td>415.134453</td>\n",
       "      <td>419.210327</td>\n",
       "      <td>437.287432</td>\n",
       "      <td>423.04819</td>\n",
       "      <td>138.539863</td>\n",
       "      <td>271.896108</td>\n",
       "      <td>439.978366</td>\n",
       "      <td>419.943386</td>\n",
       "      <td>425.452381</td>\n",
       "      <td>417.38609</td>\n",
       "      <td>434.204075</td>\n",
       "      <td>418.776618</td>\n",
       "      <td>414.426862</td>\n",
       "      <td>403.52179</td>\n",
       "      <td>290.631587</td>\n",
       "      <td>368.421044</td>\n",
       "      <td>389.903137</td>\n",
       "      <td>413.337932</td>\n",
       "      <td>280.927444</td>\n",
       "      <td>407.531662</td>\n",
       "      <td>245.253119</td>\n",
       "      <td>398.495848</td>\n",
       "      <td>382.402358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cpu_usage_percent</th>\n",
       "      <td>3.9</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.3</td>\n",
       "      <td>47.4</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.8</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>55.3</td>\n",
       "      <td>5.5</td>\n",
       "      <td>16.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>54.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>17.3</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cpu_memory_usage_bytes</th>\n",
       "      <td>2050256896</td>\n",
       "      <td>2070810624</td>\n",
       "      <td>2068250624</td>\n",
       "      <td>2075455488</td>\n",
       "      <td>2015477760</td>\n",
       "      <td>2049409024</td>\n",
       "      <td>2057400320</td>\n",
       "      <td>2067193856</td>\n",
       "      <td>2073178112</td>\n",
       "      <td>2076033024</td>\n",
       "      <td>2071547904</td>\n",
       "      <td>2073534464</td>\n",
       "      <td>3161710592</td>\n",
       "      <td>2766688256</td>\n",
       "      <td>2705195008</td>\n",
       "      <td>2056642560</td>\n",
       "      <td>2073894912</td>\n",
       "      <td>2070286336</td>\n",
       "      <td>2072047616</td>\n",
       "      <td>2056028160</td>\n",
       "      <td>2075602944</td>\n",
       "      <td>2075750400</td>\n",
       "      <td>2077544448</td>\n",
       "      <td>2057662464</td>\n",
       "      <td>2070953984</td>\n",
       "      <td>2052939776</td>\n",
       "      <td>2072227840</td>\n",
       "      <td>2074927104</td>\n",
       "      <td>2070724608</td>\n",
       "      <td>2072039424</td>\n",
       "      <td>2073120768</td>\n",
       "      <td>2070740992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpu_utilization_percent_0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpu_utilization_percent_1</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpu_utilization_percent_2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpu_utilization_percent_3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cpu_power_process_0</th>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cpu_power_process_1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cpu_power_process_2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cpu_power_process_3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpu_power_process_0</th>\n",
       "      <td>414.856645</td>\n",
       "      <td>605.276451</td>\n",
       "      <td>560.689296</td>\n",
       "      <td>788.015735</td>\n",
       "      <td>1313.057996</td>\n",
       "      <td>641.064988</td>\n",
       "      <td>677.58912</td>\n",
       "      <td>623.64748</td>\n",
       "      <td>641.468533</td>\n",
       "      <td>705.181102</td>\n",
       "      <td>651.046027</td>\n",
       "      <td>619.770725</td>\n",
       "      <td>675.28713</td>\n",
       "      <td>444.462409</td>\n",
       "      <td>557.53898</td>\n",
       "      <td>612.704186</td>\n",
       "      <td>762.470124</td>\n",
       "      <td>795.129191</td>\n",
       "      <td>740.388</td>\n",
       "      <td>776.321292</td>\n",
       "      <td>809.680864</td>\n",
       "      <td>646.756882</td>\n",
       "      <td>652.001595</td>\n",
       "      <td>567.629617</td>\n",
       "      <td>727.260463</td>\n",
       "      <td>701.330117</td>\n",
       "      <td>756.244221</td>\n",
       "      <td>551.612807</td>\n",
       "      <td>716.271282</td>\n",
       "      <td>381.983252</td>\n",
       "      <td>615.725547</td>\n",
       "      <td>717.525238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpu_power_process_1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>491.304513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>424.010995</td>\n",
       "      <td>466.001815</td>\n",
       "      <td>518.360923</td>\n",
       "      <td>369.103744</td>\n",
       "      <td>483.293236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>505.669169</td>\n",
       "      <td>1743.091048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119.894599</td>\n",
       "      <td>374.736474</td>\n",
       "      <td>633.739616</td>\n",
       "      <td>556.747136</td>\n",
       "      <td>453.252145</td>\n",
       "      <td>18.628497</td>\n",
       "      <td>0.0</td>\n",
       "      <td>495.301506</td>\n",
       "      <td>1679.540024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2219.419073</td>\n",
       "      <td>771.336972</td>\n",
       "      <td>555.824567</td>\n",
       "      <td>471.022131</td>\n",
       "      <td>413.266378</td>\n",
       "      <td>404.622742</td>\n",
       "      <td>468.725355</td>\n",
       "      <td>559.126046</td>\n",
       "      <td>310.173365</td>\n",
       "      <td>533.99169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpu_power_process_2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>531.830444</td>\n",
       "      <td>708.880427</td>\n",
       "      <td>469.151347</td>\n",
       "      <td>629.644402</td>\n",
       "      <td>632.178172</td>\n",
       "      <td>561.420978</td>\n",
       "      <td>588.174565</td>\n",
       "      <td>683.577268</td>\n",
       "      <td>742.100489</td>\n",
       "      <td>667.109686</td>\n",
       "      <td>594.917177</td>\n",
       "      <td>518.21448</td>\n",
       "      <td>564.387603</td>\n",
       "      <td>660.52256</td>\n",
       "      <td>683.491168</td>\n",
       "      <td>688.585224</td>\n",
       "      <td>685.891048</td>\n",
       "      <td>759.717472</td>\n",
       "      <td>708.550319</td>\n",
       "      <td>731.480916</td>\n",
       "      <td>670.105119</td>\n",
       "      <td>615.131361</td>\n",
       "      <td>705.838975</td>\n",
       "      <td>718.217363</td>\n",
       "      <td>780.127003</td>\n",
       "      <td>516.23026</td>\n",
       "      <td>694.223236</td>\n",
       "      <td>611.717499</td>\n",
       "      <td>631.657895</td>\n",
       "      <td>734.177811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpu_power_process_3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>533.950626</td>\n",
       "      <td>580.455824</td>\n",
       "      <td>658.01506</td>\n",
       "      <td>667.772397</td>\n",
       "      <td>545.313817</td>\n",
       "      <td>595.323068</td>\n",
       "      <td>598.674237</td>\n",
       "      <td>555.219656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>589.702252</td>\n",
       "      <td>480.250829</td>\n",
       "      <td>0.0</td>\n",
       "      <td>627.405103</td>\n",
       "      <td>658.562704</td>\n",
       "      <td>610.801172</td>\n",
       "      <td>604.058121</td>\n",
       "      <td>574.829885</td>\n",
       "      <td>601.787839</td>\n",
       "      <td>1382.946973</td>\n",
       "      <td>708.385754</td>\n",
       "      <td>657.908802</td>\n",
       "      <td>944.800435</td>\n",
       "      <td>363.407429</td>\n",
       "      <td>250.763626</td>\n",
       "      <td>604.277492</td>\n",
       "      <td>618.121217</td>\n",
       "      <td>479.491748</td>\n",
       "      <td>796.653119</td>\n",
       "      <td>514.542529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ram_power_process_0</th>\n",
       "      <td>0.714289</td>\n",
       "      <td>0.721865</td>\n",
       "      <td>0.719776</td>\n",
       "      <td>0.722456</td>\n",
       "      <td>0.702599</td>\n",
       "      <td>0.71335</td>\n",
       "      <td>0.71698</td>\n",
       "      <td>0.720617</td>\n",
       "      <td>0.72172</td>\n",
       "      <td>0.722474</td>\n",
       "      <td>0.721355</td>\n",
       "      <td>0.721694</td>\n",
       "      <td>1.102643</td>\n",
       "      <td>0.965406</td>\n",
       "      <td>0.944375</td>\n",
       "      <td>0.716738</td>\n",
       "      <td>0.721763</td>\n",
       "      <td>0.721812</td>\n",
       "      <td>0.722298</td>\n",
       "      <td>0.716374</td>\n",
       "      <td>0.722797</td>\n",
       "      <td>0.723407</td>\n",
       "      <td>0.723691</td>\n",
       "      <td>0.716706</td>\n",
       "      <td>0.721634</td>\n",
       "      <td>0.714984</td>\n",
       "      <td>0.722484</td>\n",
       "      <td>0.722661</td>\n",
       "      <td>0.721959</td>\n",
       "      <td>0.722185</td>\n",
       "      <td>0.721872</td>\n",
       "      <td>0.720926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ram_power_process_1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.680336</td>\n",
       "      <td>0.678097</td>\n",
       "      <td>0.666578</td>\n",
       "      <td>0.659914</td>\n",
       "      <td>0.656784</td>\n",
       "      <td>0.662802</td>\n",
       "      <td>0.667093</td>\n",
       "      <td>0.667399</td>\n",
       "      <td>0.67005</td>\n",
       "      <td>0.670575</td>\n",
       "      <td>0.668138</td>\n",
       "      <td>0.871312</td>\n",
       "      <td>1.010028</td>\n",
       "      <td>1.002962</td>\n",
       "      <td>0.653096</td>\n",
       "      <td>0.666492</td>\n",
       "      <td>0.668358</td>\n",
       "      <td>0.672742</td>\n",
       "      <td>0.65193</td>\n",
       "      <td>0.681414</td>\n",
       "      <td>0.691215</td>\n",
       "      <td>0.681756</td>\n",
       "      <td>0.639193</td>\n",
       "      <td>0.662752</td>\n",
       "      <td>0.673425</td>\n",
       "      <td>0.663541</td>\n",
       "      <td>0.665721</td>\n",
       "      <td>0.678437</td>\n",
       "      <td>0.680659</td>\n",
       "      <td>0.673545</td>\n",
       "      <td>0.674317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ram_power_process_2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.670814</td>\n",
       "      <td>0.677396</td>\n",
       "      <td>0.652318</td>\n",
       "      <td>0.656422</td>\n",
       "      <td>0.671325</td>\n",
       "      <td>0.665362</td>\n",
       "      <td>0.666369</td>\n",
       "      <td>0.669521</td>\n",
       "      <td>0.680642</td>\n",
       "      <td>0.666388</td>\n",
       "      <td>0.913636</td>\n",
       "      <td>1.010149</td>\n",
       "      <td>0.979697</td>\n",
       "      <td>0.652359</td>\n",
       "      <td>0.667424</td>\n",
       "      <td>0.668231</td>\n",
       "      <td>0.678045</td>\n",
       "      <td>0.638948</td>\n",
       "      <td>0.686724</td>\n",
       "      <td>0.691328</td>\n",
       "      <td>0.684342</td>\n",
       "      <td>0.646367</td>\n",
       "      <td>0.667909</td>\n",
       "      <td>0.666428</td>\n",
       "      <td>0.666791</td>\n",
       "      <td>0.67092</td>\n",
       "      <td>0.669261</td>\n",
       "      <td>0.671914</td>\n",
       "      <td>0.667791</td>\n",
       "      <td>0.668052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ram_power_process_3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.672838</td>\n",
       "      <td>0.650243</td>\n",
       "      <td>0.657912</td>\n",
       "      <td>0.66566</td>\n",
       "      <td>0.666917</td>\n",
       "      <td>0.666261</td>\n",
       "      <td>0.678216</td>\n",
       "      <td>0.676693</td>\n",
       "      <td>0.671143</td>\n",
       "      <td>0.893091</td>\n",
       "      <td>1.000088</td>\n",
       "      <td>0.9912</td>\n",
       "      <td>0.652561</td>\n",
       "      <td>0.668657</td>\n",
       "      <td>0.670784</td>\n",
       "      <td>0.669389</td>\n",
       "      <td>0.639038</td>\n",
       "      <td>0.688542</td>\n",
       "      <td>0.681649</td>\n",
       "      <td>0.689416</td>\n",
       "      <td>0.651056</td>\n",
       "      <td>0.675183</td>\n",
       "      <td>0.667291</td>\n",
       "      <td>0.67208</td>\n",
       "      <td>0.685136</td>\n",
       "      <td>0.67303</td>\n",
       "      <td>0.671625</td>\n",
       "      <td>0.667638</td>\n",
       "      <td>0.667927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cpu_energy_process_0</th>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.00021</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cpu_energy_process_1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cpu_energy_process_2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.00021</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cpu_energy_process_3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.00018</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpu_energy_process_0</th>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.00038</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.003039</td>\n",
       "      <td>0.001836</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.00035</td>\n",
       "      <td>0.00093</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.00046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpu_energy_process_1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.00064</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.003765</td>\n",
       "      <td>0.002517</td>\n",
       "      <td>0.001569</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.000543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpu_energy_process_2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.003001</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.00052</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpu_energy_process_3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.001397</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.00053</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.00053</td>\n",
       "      <td>0.000619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ram_energy_process_0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ram_energy_process_1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ram_energy_process_2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ram_energy_process_3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_energy_kwh_process_0</th>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.00046</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.003694</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>0.001406</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.00055</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.000547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_energy_kwh_process_1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.004591</td>\n",
       "      <td>0.003007</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>0.00123</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.000809</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.005024</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.00085</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0.000646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_energy_kwh_process_2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.003648</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.001402</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.00057</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.00054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_energy_kwh_process_3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.004131</td>\n",
       "      <td>0.002615</td>\n",
       "      <td>0.001657</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.00073</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.001751</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.00066</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.000739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_energy_joules_process_0</th>\n",
       "      <td>1561.168622</td>\n",
       "      <td>1655.254476</td>\n",
       "      <td>1785.737139</td>\n",
       "      <td>1967.774708</td>\n",
       "      <td>13298.517006</td>\n",
       "      <td>7821.340985</td>\n",
       "      <td>5062.259515</td>\n",
       "      <td>3433.921539</td>\n",
       "      <td>2099.764701</td>\n",
       "      <td>2024.987954</td>\n",
       "      <td>1956.394685</td>\n",
       "      <td>2222.158137</td>\n",
       "      <td>1542.311349</td>\n",
       "      <td>4171.360098</td>\n",
       "      <td>2298.536562</td>\n",
       "      <td>1833.577944</td>\n",
       "      <td>1935.222914</td>\n",
       "      <td>1999.907206</td>\n",
       "      <td>2033.157579</td>\n",
       "      <td>1980.873995</td>\n",
       "      <td>2036.742215</td>\n",
       "      <td>1985.841167</td>\n",
       "      <td>2041.046993</td>\n",
       "      <td>2654.411522</td>\n",
       "      <td>2218.098723</td>\n",
       "      <td>2130.240098</td>\n",
       "      <td>2031.708541</td>\n",
       "      <td>2753.201388</td>\n",
       "      <td>2030.512325</td>\n",
       "      <td>2877.54669</td>\n",
       "      <td>1905.424207</td>\n",
       "      <td>1968.818093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_energy_joules_process_1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2496.327873</td>\n",
       "      <td>2892.296754</td>\n",
       "      <td>2786.096248</td>\n",
       "      <td>16527.902193</td>\n",
       "      <td>10826.222637</td>\n",
       "      <td>6735.872403</td>\n",
       "      <td>4427.86806</td>\n",
       "      <td>3150.834691</td>\n",
       "      <td>2912.954866</td>\n",
       "      <td>3038.993424</td>\n",
       "      <td>3386.549539</td>\n",
       "      <td>1989.664818</td>\n",
       "      <td>18084.969546</td>\n",
       "      <td>2527.941143</td>\n",
       "      <td>2092.726127</td>\n",
       "      <td>2539.553048</td>\n",
       "      <td>3109.550092</td>\n",
       "      <td>3121.067615</td>\n",
       "      <td>2774.728283</td>\n",
       "      <td>3003.487737</td>\n",
       "      <td>3054.210497</td>\n",
       "      <td>3047.636726</td>\n",
       "      <td>3041.883795</td>\n",
       "      <td>3042.360366</td>\n",
       "      <td>3028.890264</td>\n",
       "      <td>2890.460805</td>\n",
       "      <td>3061.352417</td>\n",
       "      <td>2805.720536</td>\n",
       "      <td>2856.492483</td>\n",
       "      <td>2287.806555</td>\n",
       "      <td>2326.154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_energy_joules_process_2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1808.369364</td>\n",
       "      <td>2018.489243</td>\n",
       "      <td>13131.137615</td>\n",
       "      <td>7769.089104</td>\n",
       "      <td>5047.998719</td>\n",
       "      <td>3381.365592</td>\n",
       "      <td>2120.050649</td>\n",
       "      <td>1989.456368</td>\n",
       "      <td>2051.496196</td>\n",
       "      <td>2259.662788</td>\n",
       "      <td>1606.976883</td>\n",
       "      <td>4179.91919</td>\n",
       "      <td>2320.70705</td>\n",
       "      <td>1832.894399</td>\n",
       "      <td>2010.031883</td>\n",
       "      <td>2003.210191</td>\n",
       "      <td>2055.629763</td>\n",
       "      <td>1960.037607</td>\n",
       "      <td>2068.396405</td>\n",
       "      <td>2070.639691</td>\n",
       "      <td>2044.85402</td>\n",
       "      <td>2205.460381</td>\n",
       "      <td>2245.070151</td>\n",
       "      <td>2189.903237</td>\n",
       "      <td>2059.903488</td>\n",
       "      <td>2440.732517</td>\n",
       "      <td>2019.285212</td>\n",
       "      <td>2363.976182</td>\n",
       "      <td>1887.933024</td>\n",
       "      <td>1944.850933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_energy_joules_process_3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2515.298283</td>\n",
       "      <td>14872.94196</td>\n",
       "      <td>9415.778239</td>\n",
       "      <td>5965.276878</td>\n",
       "      <td>4168.915212</td>\n",
       "      <td>2298.825642</td>\n",
       "      <td>2172.460161</td>\n",
       "      <td>2626.703993</td>\n",
       "      <td>2675.687736</td>\n",
       "      <td>1799.238784</td>\n",
       "      <td>6303.66189</td>\n",
       "      <td>2602.777907</td>\n",
       "      <td>2008.332158</td>\n",
       "      <td>2127.807703</td>\n",
       "      <td>2174.790428</td>\n",
       "      <td>2221.826831</td>\n",
       "      <td>2146.103432</td>\n",
       "      <td>2285.724235</td>\n",
       "      <td>2458.632976</td>\n",
       "      <td>2445.305753</td>\n",
       "      <td>2459.732365</td>\n",
       "      <td>2376.033211</td>\n",
       "      <td>2433.387427</td>\n",
       "      <td>2478.658477</td>\n",
       "      <td>2748.954088</td>\n",
       "      <td>2249.414323</td>\n",
       "      <td>2654.731377</td>\n",
       "      <td>2272.960315</td>\n",
       "      <td>2661.04673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cpu_power_avg</th>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "      <td>112.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpu_power_avg</th>\n",
       "      <td>414.856645</td>\n",
       "      <td>548.290482</td>\n",
       "      <td>364.173247</td>\n",
       "      <td>613.714446</td>\n",
       "      <td>707.166746</td>\n",
       "      <td>611.771343</td>\n",
       "      <td>586.660858</td>\n",
       "      <td>553.418878</td>\n",
       "      <td>456.241542</td>\n",
       "      <td>623.275444</td>\n",
       "      <td>922.864305</td>\n",
       "      <td>321.720103</td>\n",
       "      <td>494.950289</td>\n",
       "      <td>454.416048</td>\n",
       "      <td>438.91655</td>\n",
       "      <td>614.344746</td>\n",
       "      <td>639.444035</td>\n",
       "      <td>528.286021</td>\n",
       "      <td>507.584292</td>\n",
       "      <td>651.542539</td>\n",
       "      <td>949.889761</td>\n",
       "      <td>690.296192</td>\n",
       "      <td>1062.477885</td>\n",
       "      <td>653.001688</td>\n",
       "      <td>733.43111</td>\n",
       "      <td>563.49426</td>\n",
       "      <td>550.100307</td>\n",
       "      <td>519.185825</td>\n",
       "      <td>624.335273</td>\n",
       "      <td>508.079636</td>\n",
       "      <td>588.552482</td>\n",
       "      <td>625.059317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ram_power_avg</th>\n",
       "      <td>0.714289</td>\n",
       "      <td>0.7011</td>\n",
       "      <td>0.689562</td>\n",
       "      <td>0.684817</td>\n",
       "      <td>0.666268</td>\n",
       "      <td>0.671117</td>\n",
       "      <td>0.679191</td>\n",
       "      <td>0.679998</td>\n",
       "      <td>0.680437</td>\n",
       "      <td>0.685065</td>\n",
       "      <td>0.687316</td>\n",
       "      <td>0.681841</td>\n",
       "      <td>0.94517</td>\n",
       "      <td>0.996418</td>\n",
       "      <td>0.979558</td>\n",
       "      <td>0.668688</td>\n",
       "      <td>0.681084</td>\n",
       "      <td>0.682296</td>\n",
       "      <td>0.685619</td>\n",
       "      <td>0.661573</td>\n",
       "      <td>0.69487</td>\n",
       "      <td>0.696899</td>\n",
       "      <td>0.694802</td>\n",
       "      <td>0.66333</td>\n",
       "      <td>0.681869</td>\n",
       "      <td>0.680532</td>\n",
       "      <td>0.681224</td>\n",
       "      <td>0.68611</td>\n",
       "      <td>0.685672</td>\n",
       "      <td>0.686596</td>\n",
       "      <td>0.682712</td>\n",
       "      <td>0.682806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cpu_energy_total</th>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.002848</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.001809</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpu_energy_total</th>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>0.00148</td>\n",
       "      <td>0.002194</td>\n",
       "      <td>0.013203</td>\n",
       "      <td>0.008378</td>\n",
       "      <td>0.005357</td>\n",
       "      <td>0.003613</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>0.002288</td>\n",
       "      <td>0.002353</td>\n",
       "      <td>0.001579</td>\n",
       "      <td>0.007272</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>0.001828</td>\n",
       "      <td>0.002027</td>\n",
       "      <td>0.002169</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.002263</td>\n",
       "      <td>0.002265</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.002298</td>\n",
       "      <td>0.00224</td>\n",
       "      <td>0.00258</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.002483</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.002076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ram_energy_total</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latency_simulation_simulate</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>models</th>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "      <td>1034544128000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latency_simulation_delay_max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_generated_tokens</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decoder_config_decoding_mode</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>greedy</td>\n",
       "      <td>greedy</td>\n",
       "      <td>greedy</td>\n",
       "      <td>greedy</td>\n",
       "      <td>top_k</td>\n",
       "      <td>top_k</td>\n",
       "      <td>top_k</td>\n",
       "      <td>top_k</td>\n",
       "      <td>top_p</td>\n",
       "      <td>top_p</td>\n",
       "      <td>top_p</td>\n",
       "      <td>top_p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   0   \\\n",
       "config_name                                           num_processes_1   \n",
       "experiment_id                                                       1   \n",
       "date_time                               April 08, 2025 at 04:31:33 PM   \n",
       "model                              TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                       1   \n",
       "batch_size___fixed_batching                                        16   \n",
       "decoder_temperature                                               1.0   \n",
       "decoder_top_k                                                       0   \n",
       "decoder_top_p                                                     0.0   \n",
       "latency_simulation_delay_min                                      0.0   \n",
       "latency_simulation_simulate_burst                               False   \n",
       "latency_simulation_burst_size                                       0   \n",
       "latency_simulation_burst_interval                                 0.0   \n",
       "fp_precision                                            torch.float32   \n",
       "quantization                                                    False   \n",
       "load_in_8bit                                                    False   \n",
       "load_in_4bit                                                    False   \n",
       "total_input_tokens                                               1000   \n",
       "total_params                                               1100048384   \n",
       "max_input_tokens                                                  100   \n",
       "max_output_tokens                                                 100   \n",
       "number_input_prompts                                               10   \n",
       "total_energy_kwh                                             0.000434   \n",
       "total_energy_joules                                       1561.168622   \n",
       "flops                                                   1034544128000   \n",
       "tokens_per_joule                                             0.640546   \n",
       "joules_per_token                                             1.561169   \n",
       "flops_per_joule                                      662672893.521465   \n",
       "joules_per_flop                                                   0.0   \n",
       "total_inference_time_sec                                     2.356892   \n",
       "average_latency_ms_per_batch                              2356.891729   \n",
       "throughput_queries_per_sec                                   4.242876   \n",
       "throughput_tokens_per_sec                                  424.287627   \n",
       "cpu_usage_percent                                                 3.9   \n",
       "cpu_memory_usage_bytes                                     2050256896   \n",
       "gpu_utilization_percent_0                                        11.0   \n",
       "gpu_utilization_percent_1                                       100.0   \n",
       "gpu_utilization_percent_2                                         0.0   \n",
       "gpu_utilization_percent_3                                        35.0   \n",
       "cpu_power_process_0                                             112.5   \n",
       "cpu_power_process_1                                               NaN   \n",
       "cpu_power_process_2                                               NaN   \n",
       "cpu_power_process_3                                               NaN   \n",
       "gpu_power_process_0                                        414.856645   \n",
       "gpu_power_process_1                                               NaN   \n",
       "gpu_power_process_2                                               NaN   \n",
       "gpu_power_process_3                                               NaN   \n",
       "ram_power_process_0                                          0.714289   \n",
       "ram_power_process_1                                               NaN   \n",
       "ram_power_process_2                                               NaN   \n",
       "ram_power_process_3                                               NaN   \n",
       "cpu_energy_process_0                                         0.000078   \n",
       "cpu_energy_process_1                                              NaN   \n",
       "cpu_energy_process_2                                              NaN   \n",
       "cpu_energy_process_3                                              NaN   \n",
       "gpu_energy_process_0                                         0.000355   \n",
       "gpu_energy_process_1                                              NaN   \n",
       "gpu_energy_process_2                                              NaN   \n",
       "gpu_energy_process_3                                              NaN   \n",
       "ram_energy_process_0                                              0.0   \n",
       "ram_energy_process_1                                              NaN   \n",
       "ram_energy_process_2                                              NaN   \n",
       "ram_energy_process_3                                              NaN   \n",
       "total_energy_kwh_process_0                                   0.000434   \n",
       "total_energy_kwh_process_1                                        NaN   \n",
       "total_energy_kwh_process_2                                        NaN   \n",
       "total_energy_kwh_process_3                                        NaN   \n",
       "total_energy_joules_process_0                             1561.168622   \n",
       "total_energy_joules_process_1                                     NaN   \n",
       "total_energy_joules_process_2                                     NaN   \n",
       "total_energy_joules_process_3                                     NaN   \n",
       "cpu_power_avg                                                   112.5   \n",
       "gpu_power_avg                                              414.856645   \n",
       "ram_power_avg                                                0.714289   \n",
       "cpu_energy_total                                             0.000078   \n",
       "gpu_energy_total                                             0.000355   \n",
       "ram_energy_total                                                  0.0   \n",
       "latency_simulation_simulate                                     False   \n",
       "models                                                  1034544128000   \n",
       "latency_simulation_delay_max                                      0.0   \n",
       "total_generated_tokens                                           1000   \n",
       "decoder_config_decoding_mode                                      NaN   \n",
       "\n",
       "                                                                   1   \\\n",
       "config_name                                           num_processes_2   \n",
       "experiment_id                                                       2   \n",
       "date_time                               April 08, 2025 at 04:32:06 PM   \n",
       "model                              TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                       2   \n",
       "batch_size___fixed_batching                                        16   \n",
       "decoder_temperature                                               1.0   \n",
       "decoder_top_k                                                       0   \n",
       "decoder_top_p                                                     0.0   \n",
       "latency_simulation_delay_min                                      0.0   \n",
       "latency_simulation_simulate_burst                               False   \n",
       "latency_simulation_burst_size                                       0   \n",
       "latency_simulation_burst_interval                                 0.0   \n",
       "fp_precision                                            torch.float32   \n",
       "quantization                                                    False   \n",
       "load_in_8bit                                                    False   \n",
       "load_in_4bit                                                    False   \n",
       "total_input_tokens                                               1000   \n",
       "total_params                                               1100048384   \n",
       "max_input_tokens                                                  100   \n",
       "max_output_tokens                                                 100   \n",
       "number_input_prompts                                               10   \n",
       "total_energy_kwh                                             0.001153   \n",
       "total_energy_joules                                        4151.58235   \n",
       "flops                                                   1034544128000   \n",
       "tokens_per_joule                                             0.240872   \n",
       "joules_per_token                                             4.151582   \n",
       "flops_per_joule                                      249192727.209392   \n",
       "joules_per_flop                                                   0.0   \n",
       "total_inference_time_sec                                     2.401629   \n",
       "average_latency_ms_per_batch                              2401.628917   \n",
       "throughput_queries_per_sec                                   4.163841   \n",
       "throughput_tokens_per_sec                                   416.38406   \n",
       "cpu_usage_percent                                                 2.8   \n",
       "cpu_memory_usage_bytes                                     2070810624   \n",
       "gpu_utilization_percent_0                                        65.0   \n",
       "gpu_utilization_percent_1                                       100.0   \n",
       "gpu_utilization_percent_2                                         0.0   \n",
       "gpu_utilization_percent_3                                        71.0   \n",
       "cpu_power_process_0                                             112.5   \n",
       "cpu_power_process_1                                             112.5   \n",
       "cpu_power_process_2                                               NaN   \n",
       "cpu_power_process_3                                               NaN   \n",
       "gpu_power_process_0                                        605.276451   \n",
       "gpu_power_process_1                                        491.304513   \n",
       "gpu_power_process_2                                               NaN   \n",
       "gpu_power_process_3                                               NaN   \n",
       "ram_power_process_0                                          0.721865   \n",
       "ram_power_process_1                                          0.680336   \n",
       "ram_power_process_2                                               NaN   \n",
       "ram_power_process_3                                               NaN   \n",
       "cpu_energy_process_0                                          0.00008   \n",
       "cpu_energy_process_1                                         0.000126   \n",
       "cpu_energy_process_2                                              NaN   \n",
       "cpu_energy_process_3                                              NaN   \n",
       "gpu_energy_process_0                                          0.00038   \n",
       "gpu_energy_process_1                                         0.000567   \n",
       "gpu_energy_process_2                                              NaN   \n",
       "gpu_energy_process_3                                              NaN   \n",
       "ram_energy_process_0                                              0.0   \n",
       "ram_energy_process_1                                         0.000001   \n",
       "ram_energy_process_2                                              NaN   \n",
       "ram_energy_process_3                                              NaN   \n",
       "total_energy_kwh_process_0                                    0.00046   \n",
       "total_energy_kwh_process_1                                   0.000693   \n",
       "total_energy_kwh_process_2                                        NaN   \n",
       "total_energy_kwh_process_3                                        NaN   \n",
       "total_energy_joules_process_0                             1655.254476   \n",
       "total_energy_joules_process_1                             2496.327873   \n",
       "total_energy_joules_process_2                                     NaN   \n",
       "total_energy_joules_process_3                                     NaN   \n",
       "cpu_power_avg                                                   112.5   \n",
       "gpu_power_avg                                              548.290482   \n",
       "ram_power_avg                                                  0.7011   \n",
       "cpu_energy_total                                             0.000205   \n",
       "gpu_energy_total                                             0.000947   \n",
       "ram_energy_total                                             0.000001   \n",
       "latency_simulation_simulate                                     False   \n",
       "models                                                  1034544128000   \n",
       "latency_simulation_delay_max                                      0.0   \n",
       "total_generated_tokens                                           1000   \n",
       "decoder_config_decoding_mode                                      NaN   \n",
       "\n",
       "                                                                   2   \\\n",
       "config_name                                           num_processes_3   \n",
       "experiment_id                                                       3   \n",
       "date_time                               April 08, 2025 at 04:32:41 PM   \n",
       "model                              TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                       3   \n",
       "batch_size___fixed_batching                                        16   \n",
       "decoder_temperature                                               1.0   \n",
       "decoder_top_k                                                       0   \n",
       "decoder_top_p                                                     0.0   \n",
       "latency_simulation_delay_min                                      0.0   \n",
       "latency_simulation_simulate_burst                               False   \n",
       "latency_simulation_burst_size                                       0   \n",
       "latency_simulation_burst_interval                                 0.0   \n",
       "fp_precision                                            torch.float32   \n",
       "quantization                                                    False   \n",
       "load_in_8bit                                                    False   \n",
       "load_in_4bit                                                    False   \n",
       "total_input_tokens                                               1000   \n",
       "total_params                                               1100048384   \n",
       "max_input_tokens                                                  100   \n",
       "max_output_tokens                                                 100   \n",
       "number_input_prompts                                               10   \n",
       "total_energy_kwh                                             0.001802   \n",
       "total_energy_joules                                       6486.403256   \n",
       "flops                                                   1034544128000   \n",
       "tokens_per_joule                                             0.154169   \n",
       "joules_per_token                                             6.486403   \n",
       "flops_per_joule                                      159494266.256535   \n",
       "joules_per_flop                                                   0.0   \n",
       "total_inference_time_sec                                     2.346066   \n",
       "average_latency_ms_per_batch                              2346.066005   \n",
       "throughput_queries_per_sec                                   4.262455   \n",
       "throughput_tokens_per_sec                                  426.245467   \n",
       "cpu_usage_percent                                                 4.9   \n",
       "cpu_memory_usage_bytes                                     2068250624   \n",
       "gpu_utilization_percent_0                                        11.0   \n",
       "gpu_utilization_percent_1                                       100.0   \n",
       "gpu_utilization_percent_2                                       100.0   \n",
       "gpu_utilization_percent_3                                       100.0   \n",
       "cpu_power_process_0                                             112.5   \n",
       "cpu_power_process_1                                             112.5   \n",
       "cpu_power_process_2                                             112.5   \n",
       "cpu_power_process_3                                               NaN   \n",
       "gpu_power_process_0                                        560.689296   \n",
       "gpu_power_process_1                                               0.0   \n",
       "gpu_power_process_2                                        531.830444   \n",
       "gpu_power_process_3                                               NaN   \n",
       "ram_power_process_0                                          0.719776   \n",
       "ram_power_process_1                                          0.678097   \n",
       "ram_power_process_2                                          0.670814   \n",
       "ram_power_process_3                                               NaN   \n",
       "cpu_energy_process_0                                         0.000078   \n",
       "cpu_energy_process_1                                         0.000162   \n",
       "cpu_energy_process_2                                          0.00008   \n",
       "cpu_energy_process_3                                              NaN   \n",
       "gpu_energy_process_0                                         0.000417   \n",
       "gpu_energy_process_1                                          0.00064   \n",
       "gpu_energy_process_2                                         0.000422   \n",
       "gpu_energy_process_3                                              NaN   \n",
       "ram_energy_process_0                                              0.0   \n",
       "ram_energy_process_1                                         0.000001   \n",
       "ram_energy_process_2                                              0.0   \n",
       "ram_energy_process_3                                              NaN   \n",
       "total_energy_kwh_process_0                                   0.000496   \n",
       "total_energy_kwh_process_1                                   0.000803   \n",
       "total_energy_kwh_process_2                                   0.000502   \n",
       "total_energy_kwh_process_3                                        NaN   \n",
       "total_energy_joules_process_0                             1785.737139   \n",
       "total_energy_joules_process_1                             2892.296754   \n",
       "total_energy_joules_process_2                             1808.369364   \n",
       "total_energy_joules_process_3                                     NaN   \n",
       "cpu_power_avg                                                   112.5   \n",
       "gpu_power_avg                                              364.173247   \n",
       "ram_power_avg                                                0.689562   \n",
       "cpu_energy_total                                             0.000321   \n",
       "gpu_energy_total                                              0.00148   \n",
       "ram_energy_total                                             0.000002   \n",
       "latency_simulation_simulate                                     False   \n",
       "models                                                  1034544128000   \n",
       "latency_simulation_delay_max                                      0.0   \n",
       "total_generated_tokens                                           1000   \n",
       "decoder_config_decoding_mode                                      NaN   \n",
       "\n",
       "                                                                   3   \\\n",
       "config_name                                           num_processes_4   \n",
       "experiment_id                                                       4   \n",
       "date_time                               April 08, 2025 at 04:33:15 PM   \n",
       "model                              TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                       4   \n",
       "batch_size___fixed_batching                                        16   \n",
       "decoder_temperature                                               1.0   \n",
       "decoder_top_k                                                       0   \n",
       "decoder_top_p                                                     0.0   \n",
       "latency_simulation_delay_min                                      0.0   \n",
       "latency_simulation_simulate_burst                               False   \n",
       "latency_simulation_burst_size                                       0   \n",
       "latency_simulation_burst_interval                                 0.0   \n",
       "fp_precision                                            torch.float32   \n",
       "quantization                                                    False   \n",
       "load_in_8bit                                                    False   \n",
       "load_in_4bit                                                    False   \n",
       "total_input_tokens                                               1000   \n",
       "total_params                                               1100048384   \n",
       "max_input_tokens                                                  100   \n",
       "max_output_tokens                                                 100   \n",
       "number_input_prompts                                               10   \n",
       "total_energy_kwh                                              0.00258   \n",
       "total_energy_joules                                       9287.658482   \n",
       "flops                                                   1034544128000   \n",
       "tokens_per_joule                                              0.10767   \n",
       "joules_per_token                                             9.287658   \n",
       "flops_per_joule                                      111389122.454932   \n",
       "joules_per_flop                                                   0.0   \n",
       "total_inference_time_sec                                     2.372563   \n",
       "average_latency_ms_per_batch                              2372.563443   \n",
       "throughput_queries_per_sec                                    4.21485   \n",
       "throughput_tokens_per_sec                                  421.485041   \n",
       "cpu_usage_percent                                                 4.4   \n",
       "cpu_memory_usage_bytes                                     2075455488   \n",
       "gpu_utilization_percent_0                                        11.0   \n",
       "gpu_utilization_percent_1                                       100.0   \n",
       "gpu_utilization_percent_2                                       100.0   \n",
       "gpu_utilization_percent_3                                       100.0   \n",
       "cpu_power_process_0                                             112.5   \n",
       "cpu_power_process_1                                             112.5   \n",
       "cpu_power_process_2                                             112.5   \n",
       "cpu_power_process_3                                             112.5   \n",
       "gpu_power_process_0                                        788.015735   \n",
       "gpu_power_process_1                                        424.010995   \n",
       "gpu_power_process_2                                        708.880427   \n",
       "gpu_power_process_3                                        533.950626   \n",
       "ram_power_process_0                                          0.722456   \n",
       "ram_power_process_1                                          0.666578   \n",
       "ram_power_process_2                                          0.677396   \n",
       "ram_power_process_3                                          0.672838   \n",
       "cpu_energy_process_0                                         0.000079   \n",
       "cpu_energy_process_1                                          0.00012   \n",
       "cpu_energy_process_2                                         0.000081   \n",
       "cpu_energy_process_3                                         0.000105   \n",
       "gpu_energy_process_0                                         0.000467   \n",
       "gpu_energy_process_1                                         0.000654   \n",
       "gpu_energy_process_2                                         0.000479   \n",
       "gpu_energy_process_3                                         0.000594   \n",
       "ram_energy_process_0                                              0.0   \n",
       "ram_energy_process_1                                         0.000001   \n",
       "ram_energy_process_2                                              0.0   \n",
       "ram_energy_process_3                                         0.000001   \n",
       "total_energy_kwh_process_0                                   0.000547   \n",
       "total_energy_kwh_process_1                                   0.000774   \n",
       "total_energy_kwh_process_2                                   0.000561   \n",
       "total_energy_kwh_process_3                                   0.000699   \n",
       "total_energy_joules_process_0                             1967.774708   \n",
       "total_energy_joules_process_1                             2786.096248   \n",
       "total_energy_joules_process_2                             2018.489243   \n",
       "total_energy_joules_process_3                             2515.298283   \n",
       "cpu_power_avg                                                   112.5   \n",
       "gpu_power_avg                                              613.714446   \n",
       "ram_power_avg                                                0.684817   \n",
       "cpu_energy_total                                             0.000384   \n",
       "gpu_energy_total                                             0.002194   \n",
       "ram_energy_total                                             0.000002   \n",
       "latency_simulation_simulate                                     False   \n",
       "models                                                  1034544128000   \n",
       "latency_simulation_delay_max                                      0.0   \n",
       "total_generated_tokens                                           1000   \n",
       "decoder_config_decoding_mode                                      NaN   \n",
       "\n",
       "                                                                   4   \\\n",
       "config_name                                                batching_1   \n",
       "experiment_id                                                       5   \n",
       "date_time                               April 08, 2025 at 04:34:12 PM   \n",
       "model                              TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                       4   \n",
       "batch_size___fixed_batching                                         1   \n",
       "decoder_temperature                                               1.0   \n",
       "decoder_top_k                                                       0   \n",
       "decoder_top_p                                                     0.0   \n",
       "latency_simulation_delay_min                                      0.0   \n",
       "latency_simulation_simulate_burst                               False   \n",
       "latency_simulation_burst_size                                       0   \n",
       "latency_simulation_burst_interval                                 0.0   \n",
       "fp_precision                                            torch.float32   \n",
       "quantization                                                    False   \n",
       "load_in_8bit                                                    False   \n",
       "load_in_4bit                                                    False   \n",
       "total_input_tokens                                               1000   \n",
       "total_params                                               1100048384   \n",
       "max_input_tokens                                                  100   \n",
       "max_output_tokens                                                 100   \n",
       "number_input_prompts                                               10   \n",
       "total_energy_kwh                                             0.016064   \n",
       "total_energy_joules                                      57830.498773   \n",
       "flops                                                   1034544128000   \n",
       "tokens_per_joule                                             0.017292   \n",
       "joules_per_token                                            57.830499   \n",
       "flops_per_joule                                        17889247.88724   \n",
       "joules_per_flop                                                   0.0   \n",
       "total_inference_time_sec                                    20.824448   \n",
       "average_latency_ms_per_batch                              2082.444752   \n",
       "throughput_queries_per_sec                                   0.480205   \n",
       "throughput_tokens_per_sec                                   48.020482   \n",
       "cpu_usage_percent                                                 5.0   \n",
       "cpu_memory_usage_bytes                                     2015477760   \n",
       "gpu_utilization_percent_0                                        57.0   \n",
       "gpu_utilization_percent_1                                       100.0   \n",
       "gpu_utilization_percent_2                                       100.0   \n",
       "gpu_utilization_percent_3                                       100.0   \n",
       "cpu_power_process_0                                             112.5   \n",
       "cpu_power_process_1                                             112.5   \n",
       "cpu_power_process_2                                             112.5   \n",
       "cpu_power_process_3                                             112.5   \n",
       "gpu_power_process_0                                       1313.057996   \n",
       "gpu_power_process_1                                        466.001815   \n",
       "gpu_power_process_2                                        469.151347   \n",
       "gpu_power_process_3                                        580.455824   \n",
       "ram_power_process_0                                          0.702599   \n",
       "ram_power_process_1                                          0.659914   \n",
       "ram_power_process_2                                          0.652318   \n",
       "ram_power_process_3                                          0.650243   \n",
       "cpu_energy_process_0                                         0.000652   \n",
       "cpu_energy_process_1                                         0.000822   \n",
       "cpu_energy_process_2                                         0.000644   \n",
       "cpu_energy_process_3                                         0.000729   \n",
       "gpu_energy_process_0                                         0.003039   \n",
       "gpu_energy_process_1                                         0.003765   \n",
       "gpu_energy_process_2                                         0.003001   \n",
       "gpu_energy_process_3                                         0.003399   \n",
       "ram_energy_process_0                                         0.000003   \n",
       "ram_energy_process_1                                         0.000004   \n",
       "ram_energy_process_2                                         0.000003   \n",
       "ram_energy_process_3                                         0.000003   \n",
       "total_energy_kwh_process_0                                   0.003694   \n",
       "total_energy_kwh_process_1                                   0.004591   \n",
       "total_energy_kwh_process_2                                   0.003648   \n",
       "total_energy_kwh_process_3                                   0.004131   \n",
       "total_energy_joules_process_0                            13298.517006   \n",
       "total_energy_joules_process_1                            16527.902193   \n",
       "total_energy_joules_process_2                            13131.137615   \n",
       "total_energy_joules_process_3                             14872.94196   \n",
       "cpu_power_avg                                                   112.5   \n",
       "gpu_power_avg                                              707.166746   \n",
       "ram_power_avg                                                0.666268   \n",
       "cpu_energy_total                                             0.002848   \n",
       "gpu_energy_total                                             0.013203   \n",
       "ram_energy_total                                             0.000014   \n",
       "latency_simulation_simulate                                     False   \n",
       "models                                                  1034544128000   \n",
       "latency_simulation_delay_max                                      0.0   \n",
       "total_generated_tokens                                           1000   \n",
       "decoder_config_decoding_mode                                      NaN   \n",
       "\n",
       "                                                                   5   \\\n",
       "config_name                                                batching_2   \n",
       "experiment_id                                                       6   \n",
       "date_time                               April 08, 2025 at 04:34:59 PM   \n",
       "model                              TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                       4   \n",
       "batch_size___fixed_batching                                         2   \n",
       "decoder_temperature                                               1.0   \n",
       "decoder_top_k                                                       0   \n",
       "decoder_top_p                                                     0.0   \n",
       "latency_simulation_delay_min                                      0.0   \n",
       "latency_simulation_simulate_burst                               False   \n",
       "latency_simulation_burst_size                                       0   \n",
       "latency_simulation_burst_interval                                 0.0   \n",
       "fp_precision                                            torch.float32   \n",
       "quantization                                                    False   \n",
       "load_in_8bit                                                    False   \n",
       "load_in_4bit                                                    False   \n",
       "total_input_tokens                                               1000   \n",
       "total_params                                               1100048384   \n",
       "max_input_tokens                                                  100   \n",
       "max_output_tokens                                                 100   \n",
       "number_input_prompts                                               10   \n",
       "total_energy_kwh                                             0.009953   \n",
       "total_energy_joules                                      35832.430964   \n",
       "flops                                                   1034544128000   \n",
       "tokens_per_joule                                             0.027908   \n",
       "joules_per_token                                            35.832431   \n",
       "flops_per_joule                                       28871725.980626   \n",
       "joules_per_flop                                                   0.0   \n",
       "total_inference_time_sec                                    10.598905   \n",
       "average_latency_ms_per_batch                              2119.780987   \n",
       "throughput_queries_per_sec                                   0.943494   \n",
       "throughput_tokens_per_sec                                    94.34937   \n",
       "cpu_usage_percent                                                17.6   \n",
       "cpu_memory_usage_bytes                                     2049409024   \n",
       "gpu_utilization_percent_0                                        20.0   \n",
       "gpu_utilization_percent_1                                       100.0   \n",
       "gpu_utilization_percent_2                                       100.0   \n",
       "gpu_utilization_percent_3                                       100.0   \n",
       "cpu_power_process_0                                             112.5   \n",
       "cpu_power_process_1                                             112.5   \n",
       "cpu_power_process_2                                             112.5   \n",
       "cpu_power_process_3                                             112.5   \n",
       "gpu_power_process_0                                        641.064988   \n",
       "gpu_power_process_1                                        518.360923   \n",
       "gpu_power_process_2                                        629.644402   \n",
       "gpu_power_process_3                                         658.01506   \n",
       "ram_power_process_0                                           0.71335   \n",
       "ram_power_process_1                                          0.656784   \n",
       "ram_power_process_2                                          0.656422   \n",
       "ram_power_process_3                                          0.657912   \n",
       "cpu_energy_process_0                                         0.000335   \n",
       "cpu_energy_process_1                                         0.000488   \n",
       "cpu_energy_process_2                                         0.000333   \n",
       "cpu_energy_process_3                                         0.000412   \n",
       "gpu_energy_process_0                                         0.001836   \n",
       "gpu_energy_process_1                                         0.002517   \n",
       "gpu_energy_process_2                                         0.001823   \n",
       "gpu_energy_process_3                                         0.002202   \n",
       "ram_energy_process_0                                         0.000002   \n",
       "ram_energy_process_1                                         0.000002   \n",
       "ram_energy_process_2                                         0.000002   \n",
       "ram_energy_process_3                                         0.000002   \n",
       "total_energy_kwh_process_0                                   0.002173   \n",
       "total_energy_kwh_process_1                                   0.003007   \n",
       "total_energy_kwh_process_2                                   0.002158   \n",
       "total_energy_kwh_process_3                                   0.002615   \n",
       "total_energy_joules_process_0                             7821.340985   \n",
       "total_energy_joules_process_1                            10826.222637   \n",
       "total_energy_joules_process_2                             7769.089104   \n",
       "total_energy_joules_process_3                             9415.778239   \n",
       "cpu_power_avg                                                   112.5   \n",
       "gpu_power_avg                                              611.771343   \n",
       "ram_power_avg                                                0.671117   \n",
       "cpu_energy_total                                             0.001568   \n",
       "gpu_energy_total                                             0.008378   \n",
       "ram_energy_total                                             0.000008   \n",
       "latency_simulation_simulate                                     False   \n",
       "models                                                  1034544128000   \n",
       "latency_simulation_delay_max                                      0.0   \n",
       "total_generated_tokens                                           1000   \n",
       "decoder_config_decoding_mode                                      NaN   \n",
       "\n",
       "                                                                   6   \\\n",
       "config_name                                                batching_4   \n",
       "experiment_id                                                       7   \n",
       "date_time                               April 08, 2025 at 04:35:43 PM   \n",
       "model                              TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                       4   \n",
       "batch_size___fixed_batching                                         4   \n",
       "decoder_temperature                                               1.0   \n",
       "decoder_top_k                                                       0   \n",
       "decoder_top_p                                                     0.0   \n",
       "latency_simulation_delay_min                                      0.0   \n",
       "latency_simulation_simulate_burst                               False   \n",
       "latency_simulation_burst_size                                       0   \n",
       "latency_simulation_burst_interval                                 0.0   \n",
       "fp_precision                                            torch.float32   \n",
       "quantization                                                    False   \n",
       "load_in_8bit                                                    False   \n",
       "load_in_4bit                                                    False   \n",
       "total_input_tokens                                               1000   \n",
       "total_params                                               1100048384   \n",
       "max_input_tokens                                                  100   \n",
       "max_output_tokens                                                 100   \n",
       "number_input_prompts                                               10   \n",
       "total_energy_kwh                                             0.006337   \n",
       "total_energy_joules                                      22811.407515   \n",
       "flops                                                   1034544128000   \n",
       "tokens_per_joule                                             0.043838   \n",
       "joules_per_token                                            22.811408   \n",
       "flops_per_joule                                       45352051.482635   \n",
       "joules_per_flop                                                   0.0   \n",
       "total_inference_time_sec                                     6.579609   \n",
       "average_latency_ms_per_batch                               2193.20311   \n",
       "throughput_queries_per_sec                                   1.519847   \n",
       "throughput_tokens_per_sec                                  151.984708   \n",
       "cpu_usage_percent                                                 4.0   \n",
       "cpu_memory_usage_bytes                                     2057400320   \n",
       "gpu_utilization_percent_0                                        67.0   \n",
       "gpu_utilization_percent_1                                        73.0   \n",
       "gpu_utilization_percent_2                                        99.0   \n",
       "gpu_utilization_percent_3                                       100.0   \n",
       "cpu_power_process_0                                             112.5   \n",
       "cpu_power_process_1                                             112.5   \n",
       "cpu_power_process_2                                             112.5   \n",
       "cpu_power_process_3                                             112.5   \n",
       "gpu_power_process_0                                         677.58912   \n",
       "gpu_power_process_1                                        369.103744   \n",
       "gpu_power_process_2                                        632.178172   \n",
       "gpu_power_process_3                                        667.772397   \n",
       "ram_power_process_0                                           0.71698   \n",
       "ram_power_process_1                                          0.662802   \n",
       "ram_power_process_2                                          0.671325   \n",
       "ram_power_process_3                                           0.66566   \n",
       "cpu_energy_process_0                                          0.00021   \n",
       "cpu_energy_process_1                                           0.0003   \n",
       "cpu_energy_process_2                                          0.00021   \n",
       "cpu_energy_process_3                                         0.000255   \n",
       "gpu_energy_process_0                                         0.001195   \n",
       "gpu_energy_process_1                                         0.001569   \n",
       "gpu_energy_process_2                                         0.001192   \n",
       "gpu_energy_process_3                                         0.001401   \n",
       "ram_energy_process_0                                         0.000001   \n",
       "ram_energy_process_1                                         0.000001   \n",
       "ram_energy_process_2                                         0.000001   \n",
       "ram_energy_process_3                                         0.000001   \n",
       "total_energy_kwh_process_0                                   0.001406   \n",
       "total_energy_kwh_process_1                                   0.001871   \n",
       "total_energy_kwh_process_2                                   0.001402   \n",
       "total_energy_kwh_process_3                                   0.001657   \n",
       "total_energy_joules_process_0                             5062.259515   \n",
       "total_energy_joules_process_1                             6735.872403   \n",
       "total_energy_joules_process_2                             5047.998719   \n",
       "total_energy_joules_process_3                             5965.276878   \n",
       "cpu_power_avg                                                   112.5   \n",
       "gpu_power_avg                                              586.660858   \n",
       "ram_power_avg                                                0.679191   \n",
       "cpu_energy_total                                             0.000975   \n",
       "gpu_energy_total                                             0.005357   \n",
       "ram_energy_total                                             0.000005   \n",
       "latency_simulation_simulate                                     False   \n",
       "models                                                  1034544128000   \n",
       "latency_simulation_delay_max                                      0.0   \n",
       "total_generated_tokens                                           1000   \n",
       "decoder_config_decoding_mode                                      NaN   \n",
       "\n",
       "                                                                   7   \\\n",
       "config_name                                                batching_8   \n",
       "experiment_id                                                       8   \n",
       "date_time                               April 08, 2025 at 04:36:20 PM   \n",
       "model                              TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                       4   \n",
       "batch_size___fixed_batching                                         8   \n",
       "decoder_temperature                                               1.0   \n",
       "decoder_top_k                                                       0   \n",
       "decoder_top_p                                                     0.0   \n",
       "latency_simulation_delay_min                                      0.0   \n",
       "latency_simulation_simulate_burst                               False   \n",
       "latency_simulation_burst_size                                       0   \n",
       "latency_simulation_burst_interval                                 0.0   \n",
       "fp_precision                                            torch.float32   \n",
       "quantization                                                    False   \n",
       "load_in_8bit                                                    False   \n",
       "load_in_4bit                                                    False   \n",
       "total_input_tokens                                               1000   \n",
       "total_params                                               1100048384   \n",
       "max_input_tokens                                                  100   \n",
       "max_output_tokens                                                 100   \n",
       "number_input_prompts                                               10   \n",
       "total_energy_kwh                                             0.004281   \n",
       "total_energy_joules                                      15412.070402   \n",
       "flops                                                   1034544128000   \n",
       "tokens_per_joule                                             0.064884   \n",
       "joules_per_token                                             15.41207   \n",
       "flops_per_joule                                       67125577.616509   \n",
       "joules_per_flop                                                   0.0   \n",
       "total_inference_time_sec                                     4.488748   \n",
       "average_latency_ms_per_batch                                2244.3741   \n",
       "throughput_queries_per_sec                                   2.227793   \n",
       "throughput_tokens_per_sec                                  222.779259   \n",
       "cpu_usage_percent                                                16.3   \n",
       "cpu_memory_usage_bytes                                     2067193856   \n",
       "gpu_utilization_percent_0                                        11.0   \n",
       "gpu_utilization_percent_1                                       100.0   \n",
       "gpu_utilization_percent_2                                       100.0   \n",
       "gpu_utilization_percent_3                                       100.0   \n",
       "cpu_power_process_0                                             112.5   \n",
       "cpu_power_process_1                                             112.5   \n",
       "cpu_power_process_2                                             112.5   \n",
       "cpu_power_process_3                                             112.5   \n",
       "gpu_power_process_0                                         623.64748   \n",
       "gpu_power_process_1                                        483.293236   \n",
       "gpu_power_process_2                                        561.420978   \n",
       "gpu_power_process_3                                        545.313817   \n",
       "ram_power_process_0                                          0.720617   \n",
       "ram_power_process_1                                          0.667093   \n",
       "ram_power_process_2                                          0.665362   \n",
       "ram_power_process_3                                          0.666917   \n",
       "cpu_energy_process_0                                         0.000145   \n",
       "cpu_energy_process_1                                         0.000195   \n",
       "cpu_energy_process_2                                         0.000145   \n",
       "cpu_energy_process_3                                          0.00018   \n",
       "gpu_energy_process_0                                         0.000808   \n",
       "gpu_energy_process_1                                         0.001034   \n",
       "gpu_energy_process_2                                         0.000794   \n",
       "gpu_energy_process_3                                         0.000977   \n",
       "ram_energy_process_0                                         0.000001   \n",
       "ram_energy_process_1                                         0.000001   \n",
       "ram_energy_process_2                                         0.000001   \n",
       "ram_energy_process_3                                         0.000001   \n",
       "total_energy_kwh_process_0                                   0.000954   \n",
       "total_energy_kwh_process_1                                    0.00123   \n",
       "total_energy_kwh_process_2                                   0.000939   \n",
       "total_energy_kwh_process_3                                   0.001158   \n",
       "total_energy_joules_process_0                             3433.921539   \n",
       "total_energy_joules_process_1                              4427.86806   \n",
       "total_energy_joules_process_2                             3381.365592   \n",
       "total_energy_joules_process_3                             4168.915212   \n",
       "cpu_power_avg                                                   112.5   \n",
       "gpu_power_avg                                              553.418878   \n",
       "ram_power_avg                                                0.679998   \n",
       "cpu_energy_total                                             0.000665   \n",
       "gpu_energy_total                                             0.003613   \n",
       "ram_energy_total                                             0.000003   \n",
       "latency_simulation_simulate                                     False   \n",
       "models                                                  1034544128000   \n",
       "latency_simulation_delay_max                                      0.0   \n",
       "total_generated_tokens                                           1000   \n",
       "decoder_config_decoding_mode                                      NaN   \n",
       "\n",
       "                                                                   8   \\\n",
       "config_name                                               batching_16   \n",
       "experiment_id                                                       9   \n",
       "date_time                               April 08, 2025 at 04:36:58 PM   \n",
       "model                              TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                       4   \n",
       "batch_size___fixed_batching                                        16   \n",
       "decoder_temperature                                               1.0   \n",
       "decoder_top_k                                                       0   \n",
       "decoder_top_p                                                     0.0   \n",
       "latency_simulation_delay_min                                      0.0   \n",
       "latency_simulation_simulate_burst                               False   \n",
       "latency_simulation_burst_size                                       0   \n",
       "latency_simulation_burst_interval                                 0.0   \n",
       "fp_precision                                            torch.float32   \n",
       "quantization                                                    False   \n",
       "load_in_8bit                                                    False   \n",
       "load_in_4bit                                                    False   \n",
       "total_input_tokens                                               1000   \n",
       "total_params                                               1100048384   \n",
       "max_input_tokens                                                  100   \n",
       "max_output_tokens                                                 100   \n",
       "number_input_prompts                                               10   \n",
       "total_energy_kwh                                             0.002686   \n",
       "total_energy_joules                                       9669.475684   \n",
       "flops                                                   1034544128000   \n",
       "tokens_per_joule                                             0.103418   \n",
       "joules_per_token                                             9.669476   \n",
       "flops_per_joule                                      106990716.129887   \n",
       "joules_per_flop                                                   0.0   \n",
       "total_inference_time_sec                                      2.56615   \n",
       "average_latency_ms_per_batch                              2566.149628   \n",
       "throughput_queries_per_sec                                   3.896889   \n",
       "throughput_tokens_per_sec                                   389.68889   \n",
       "cpu_usage_percent                                                47.4   \n",
       "cpu_memory_usage_bytes                                     2073178112   \n",
       "gpu_utilization_percent_0                                        69.0   \n",
       "gpu_utilization_percent_1                                       100.0   \n",
       "gpu_utilization_percent_2                                       100.0   \n",
       "gpu_utilization_percent_3                                       100.0   \n",
       "cpu_power_process_0                                             112.5   \n",
       "cpu_power_process_1                                             112.5   \n",
       "cpu_power_process_2                                             112.5   \n",
       "cpu_power_process_3                                             112.5   \n",
       "gpu_power_process_0                                        641.468533   \n",
       "gpu_power_process_1                                               0.0   \n",
       "gpu_power_process_2                                        588.174565   \n",
       "gpu_power_process_3                                        595.323068   \n",
       "ram_power_process_0                                           0.72172   \n",
       "ram_power_process_1                                          0.667399   \n",
       "ram_power_process_2                                          0.666369   \n",
       "ram_power_process_3                                          0.666261   \n",
       "cpu_energy_process_0                                         0.000086   \n",
       "cpu_energy_process_1                                         0.000166   \n",
       "cpu_energy_process_2                                         0.000086   \n",
       "cpu_energy_process_3                                         0.000095   \n",
       "gpu_energy_process_0                                         0.000497   \n",
       "gpu_energy_process_1                                         0.000708   \n",
       "gpu_energy_process_2                                         0.000502   \n",
       "gpu_energy_process_3                                         0.000543   \n",
       "ram_energy_process_0                                              0.0   \n",
       "ram_energy_process_1                                         0.000001   \n",
       "ram_energy_process_2                                              0.0   \n",
       "ram_energy_process_3                                              0.0   \n",
       "total_energy_kwh_process_0                                   0.000583   \n",
       "total_energy_kwh_process_1                                   0.000875   \n",
       "total_energy_kwh_process_2                                   0.000589   \n",
       "total_energy_kwh_process_3                                   0.000639   \n",
       "total_energy_joules_process_0                             2099.764701   \n",
       "total_energy_joules_process_1                             3150.834691   \n",
       "total_energy_joules_process_2                             2120.050649   \n",
       "total_energy_joules_process_3                             2298.825642   \n",
       "cpu_power_avg                                                   112.5   \n",
       "gpu_power_avg                                              456.241542   \n",
       "ram_power_avg                                                0.680437   \n",
       "cpu_energy_total                                             0.000433   \n",
       "gpu_energy_total                                             0.002251   \n",
       "ram_energy_total                                             0.000002   \n",
       "latency_simulation_simulate                                     False   \n",
       "models                                                  1034544128000   \n",
       "latency_simulation_delay_max                                      0.0   \n",
       "total_generated_tokens                                           1000   \n",
       "decoder_config_decoding_mode                                      NaN   \n",
       "\n",
       "                                                                   9   \\\n",
       "config_name                                               batching_32   \n",
       "experiment_id                                                      10   \n",
       "date_time                               April 08, 2025 at 04:37:33 PM   \n",
       "model                              TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                       4   \n",
       "batch_size___fixed_batching                                        32   \n",
       "decoder_temperature                                               1.0   \n",
       "decoder_top_k                                                       0   \n",
       "decoder_top_p                                                     0.0   \n",
       "latency_simulation_delay_min                                      0.0   \n",
       "latency_simulation_simulate_burst                               False   \n",
       "latency_simulation_burst_size                                       0   \n",
       "latency_simulation_burst_interval                                 0.0   \n",
       "fp_precision                                            torch.float32   \n",
       "quantization                                                    False   \n",
       "load_in_8bit                                                    False   \n",
       "load_in_4bit                                                    False   \n",
       "total_input_tokens                                               1000   \n",
       "total_params                                               1100048384   \n",
       "max_input_tokens                                                  100   \n",
       "max_output_tokens                                                 100   \n",
       "number_input_prompts                                               10   \n",
       "total_energy_kwh                                             0.002528   \n",
       "total_energy_joules                                       9099.859349   \n",
       "flops                                                   1034544128000   \n",
       "tokens_per_joule                                             0.109892   \n",
       "joules_per_token                                             9.099859   \n",
       "flops_per_joule                                      113687925.088365   \n",
       "joules_per_flop                                                   0.0   \n",
       "total_inference_time_sec                                     2.408858   \n",
       "average_latency_ms_per_batch                              2408.858126   \n",
       "throughput_queries_per_sec                                   4.151345   \n",
       "throughput_tokens_per_sec                                  415.134453   \n",
       "cpu_usage_percent                                                 5.8   \n",
       "cpu_memory_usage_bytes                                     2076033024   \n",
       "gpu_utilization_percent_0                                         4.0   \n",
       "gpu_utilization_percent_1                                       100.0   \n",
       "gpu_utilization_percent_2                                       100.0   \n",
       "gpu_utilization_percent_3                                       100.0   \n",
       "cpu_power_process_0                                             112.5   \n",
       "cpu_power_process_1                                             112.5   \n",
       "cpu_power_process_2                                             112.5   \n",
       "cpu_power_process_3                                             112.5   \n",
       "gpu_power_process_0                                        705.181102   \n",
       "gpu_power_process_1                                        505.669169   \n",
       "gpu_power_process_2                                        683.577268   \n",
       "gpu_power_process_3                                        598.674237   \n",
       "ram_power_process_0                                          0.722474   \n",
       "ram_power_process_1                                           0.67005   \n",
       "ram_power_process_2                                          0.669521   \n",
       "ram_power_process_3                                          0.678216   \n",
       "cpu_energy_process_0                                          0.00008   \n",
       "cpu_energy_process_1                                         0.000125   \n",
       "cpu_energy_process_2                                         0.000079   \n",
       "cpu_energy_process_3                                          0.00009   \n",
       "gpu_energy_process_0                                         0.000482   \n",
       "gpu_energy_process_1                                         0.000683   \n",
       "gpu_energy_process_2                                         0.000473   \n",
       "gpu_energy_process_3                                         0.000513   \n",
       "ram_energy_process_0                                              0.0   \n",
       "ram_energy_process_1                                         0.000001   \n",
       "ram_energy_process_2                                              0.0   \n",
       "ram_energy_process_3                                              0.0   \n",
       "total_energy_kwh_process_0                                   0.000562   \n",
       "total_energy_kwh_process_1                                   0.000809   \n",
       "total_energy_kwh_process_2                                   0.000553   \n",
       "total_energy_kwh_process_3                                   0.000603   \n",
       "total_energy_joules_process_0                             2024.987954   \n",
       "total_energy_joules_process_1                             2912.954866   \n",
       "total_energy_joules_process_2                             1989.456368   \n",
       "total_energy_joules_process_3                             2172.460161   \n",
       "cpu_power_avg                                                   112.5   \n",
       "gpu_power_avg                                              623.275444   \n",
       "ram_power_avg                                                0.685065   \n",
       "cpu_energy_total                                             0.000375   \n",
       "gpu_energy_total                                             0.002151   \n",
       "ram_energy_total                                             0.000002   \n",
       "latency_simulation_simulate                                     False   \n",
       "models                                                  1034544128000   \n",
       "latency_simulation_delay_max                                      0.0   \n",
       "total_generated_tokens                                           1000   \n",
       "decoder_config_decoding_mode                                      NaN   \n",
       "\n",
       "                                                                   10  \\\n",
       "config_name                                               batching_64   \n",
       "experiment_id                                                      11   \n",
       "date_time                               April 08, 2025 at 04:38:19 PM   \n",
       "model                              TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                       4   \n",
       "batch_size___fixed_batching                                        64   \n",
       "decoder_temperature                                               1.0   \n",
       "decoder_top_k                                                       0   \n",
       "decoder_top_p                                                     0.0   \n",
       "latency_simulation_delay_min                                      0.0   \n",
       "latency_simulation_simulate_burst                               False   \n",
       "latency_simulation_burst_size                                       0   \n",
       "latency_simulation_burst_interval                                 0.0   \n",
       "fp_precision                                            torch.float32   \n",
       "quantization                                                    False   \n",
       "load_in_8bit                                                    False   \n",
       "load_in_4bit                                                    False   \n",
       "total_input_tokens                                               1000   \n",
       "total_params                                               1100048384   \n",
       "max_input_tokens                                                  100   \n",
       "max_output_tokens                                                 100   \n",
       "number_input_prompts                                               10   \n",
       "total_energy_kwh                                             0.002687   \n",
       "total_energy_joules                                       9673.588298   \n",
       "flops                                                   1034544128000   \n",
       "tokens_per_joule                                             0.103374   \n",
       "joules_per_token                                             9.673588   \n",
       "flops_per_joule                                      106945230.268081   \n",
       "joules_per_flop                                                   0.0   \n",
       "total_inference_time_sec                                     2.385437   \n",
       "average_latency_ms_per_batch                              2385.437419   \n",
       "throughput_queries_per_sec                                   4.192103   \n",
       "throughput_tokens_per_sec                                  419.210327   \n",
       "cpu_usage_percent                                                 5.0   \n",
       "cpu_memory_usage_bytes                                     2071547904   \n",
       "gpu_utilization_percent_0                                        11.0   \n",
       "gpu_utilization_percent_1                                       100.0   \n",
       "gpu_utilization_percent_2                                       100.0   \n",
       "gpu_utilization_percent_3                                       100.0   \n",
       "cpu_power_process_0                                             112.5   \n",
       "cpu_power_process_1                                             112.5   \n",
       "cpu_power_process_2                                             112.5   \n",
       "cpu_power_process_3                                             112.5   \n",
       "gpu_power_process_0                                        651.046027   \n",
       "gpu_power_process_1                                       1743.091048   \n",
       "gpu_power_process_2                                        742.100489   \n",
       "gpu_power_process_3                                        555.219656   \n",
       "ram_power_process_0                                          0.721355   \n",
       "ram_power_process_1                                          0.670575   \n",
       "ram_power_process_2                                          0.680642   \n",
       "ram_power_process_3                                          0.676693   \n",
       "cpu_energy_process_0                                         0.000078   \n",
       "cpu_energy_process_1                                         0.000131   \n",
       "cpu_energy_process_2                                         0.000081   \n",
       "cpu_energy_process_3                                         0.000107   \n",
       "gpu_energy_process_0                                         0.000465   \n",
       "gpu_energy_process_1                                         0.000713   \n",
       "gpu_energy_process_2                                         0.000489   \n",
       "gpu_energy_process_3                                         0.000622   \n",
       "ram_energy_process_0                                              0.0   \n",
       "ram_energy_process_1                                         0.000001   \n",
       "ram_energy_process_2                                              0.0   \n",
       "ram_energy_process_3                                         0.000001   \n",
       "total_energy_kwh_process_0                                   0.000543   \n",
       "total_energy_kwh_process_1                                   0.000844   \n",
       "total_energy_kwh_process_2                                    0.00057   \n",
       "total_energy_kwh_process_3                                    0.00073   \n",
       "total_energy_joules_process_0                             1956.394685   \n",
       "total_energy_joules_process_1                             3038.993424   \n",
       "total_energy_joules_process_2                             2051.496196   \n",
       "total_energy_joules_process_3                             2626.703993   \n",
       "cpu_power_avg                                                   112.5   \n",
       "gpu_power_avg                                              922.864305   \n",
       "ram_power_avg                                                0.687316   \n",
       "cpu_energy_total                                             0.000397   \n",
       "gpu_energy_total                                             0.002288   \n",
       "ram_energy_total                                             0.000002   \n",
       "latency_simulation_simulate                                     False   \n",
       "models                                                  1034544128000   \n",
       "latency_simulation_delay_max                                      0.0   \n",
       "total_generated_tokens                                           1000   \n",
       "decoder_config_decoding_mode                                      NaN   \n",
       "\n",
       "                                                                                  11  \\\n",
       "config_name                        precis_float32_quant_False_quant8_False_quant4...   \n",
       "experiment_id                                                                     12   \n",
       "date_time                                              April 08, 2025 at 04:38:55 PM   \n",
       "model                                             TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                                      4   \n",
       "batch_size___fixed_batching                                                       16   \n",
       "decoder_temperature                                                              1.0   \n",
       "decoder_top_k                                                                      0   \n",
       "decoder_top_p                                                                    0.0   \n",
       "latency_simulation_delay_min                                                     0.0   \n",
       "latency_simulation_simulate_burst                                              False   \n",
       "latency_simulation_burst_size                                                      0   \n",
       "latency_simulation_burst_interval                                                0.0   \n",
       "fp_precision                                                           torch.float32   \n",
       "quantization                                                                   False   \n",
       "load_in_8bit                                                                   False   \n",
       "load_in_4bit                                                                   False   \n",
       "total_input_tokens                                                              1000   \n",
       "total_params                                                              1100048384   \n",
       "max_input_tokens                                                                 100   \n",
       "max_output_tokens                                                                100   \n",
       "number_input_prompts                                                              10   \n",
       "total_energy_kwh                                                            0.002929   \n",
       "total_energy_joules                                                       10544.0582   \n",
       "flops                                                                  1034544128000   \n",
       "tokens_per_joule                                                             0.09484   \n",
       "joules_per_token                                                           10.544058   \n",
       "flops_per_joule                                                      98116314.268051   \n",
       "joules_per_flop                                                                  0.0   \n",
       "total_inference_time_sec                                                    2.286825   \n",
       "average_latency_ms_per_batch                                             2286.825385   \n",
       "throughput_queries_per_sec                                                  4.372874   \n",
       "throughput_tokens_per_sec                                                 437.287432   \n",
       "cpu_usage_percent                                                                8.0   \n",
       "cpu_memory_usage_bytes                                                    2073534464   \n",
       "gpu_utilization_percent_0                                                       11.0   \n",
       "gpu_utilization_percent_1                                                      100.0   \n",
       "gpu_utilization_percent_2                                                      100.0   \n",
       "gpu_utilization_percent_3                                                      100.0   \n",
       "cpu_power_process_0                                                            112.5   \n",
       "cpu_power_process_1                                                            112.5   \n",
       "cpu_power_process_2                                                            112.5   \n",
       "cpu_power_process_3                                                            112.5   \n",
       "gpu_power_process_0                                                       619.770725   \n",
       "gpu_power_process_1                                                              0.0   \n",
       "gpu_power_process_2                                                       667.109686   \n",
       "gpu_power_process_3                                                              0.0   \n",
       "ram_power_process_0                                                         0.721694   \n",
       "ram_power_process_1                                                         0.668138   \n",
       "ram_power_process_2                                                         0.666388   \n",
       "ram_power_process_3                                                         0.671143   \n",
       "cpu_energy_process_0                                                        0.000109   \n",
       "cpu_energy_process_1                                                        0.000196   \n",
       "cpu_energy_process_2                                                        0.000111   \n",
       "cpu_energy_process_3                                                        0.000157   \n",
       "gpu_energy_process_0                                                        0.000507   \n",
       "gpu_energy_process_1                                                        0.000744   \n",
       "gpu_energy_process_2                                                        0.000516   \n",
       "gpu_energy_process_3                                                        0.000586   \n",
       "ram_energy_process_0                                                        0.000001   \n",
       "ram_energy_process_1                                                        0.000001   \n",
       "ram_energy_process_2                                                        0.000001   \n",
       "ram_energy_process_3                                                        0.000001   \n",
       "total_energy_kwh_process_0                                                  0.000617   \n",
       "total_energy_kwh_process_1                                                  0.000941   \n",
       "total_energy_kwh_process_2                                                  0.000628   \n",
       "total_energy_kwh_process_3                                                  0.000743   \n",
       "total_energy_joules_process_0                                            2222.158137   \n",
       "total_energy_joules_process_1                                            3386.549539   \n",
       "total_energy_joules_process_2                                            2259.662788   \n",
       "total_energy_joules_process_3                                            2675.687736   \n",
       "cpu_power_avg                                                                  112.5   \n",
       "gpu_power_avg                                                             321.720103   \n",
       "ram_power_avg                                                               0.681841   \n",
       "cpu_energy_total                                                            0.000572   \n",
       "gpu_energy_total                                                            0.002353   \n",
       "ram_energy_total                                                            0.000003   \n",
       "latency_simulation_simulate                                                    False   \n",
       "models                                                                 1034544128000   \n",
       "latency_simulation_delay_max                                                     0.0   \n",
       "total_generated_tokens                                                          1000   \n",
       "decoder_config_decoding_mode                                                     NaN   \n",
       "\n",
       "                                                                                  12  \\\n",
       "config_name                        precis_float16_quant_False_quant8_False_quant4...   \n",
       "experiment_id                                                                     13   \n",
       "date_time                                              April 08, 2025 at 04:39:28 PM   \n",
       "model                                             TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                                      4   \n",
       "batch_size___fixed_batching                                                       16   \n",
       "decoder_temperature                                                              1.0   \n",
       "decoder_top_k                                                                      0   \n",
       "decoder_top_p                                                                    0.0   \n",
       "latency_simulation_delay_min                                                     0.0   \n",
       "latency_simulation_simulate_burst                                              False   \n",
       "latency_simulation_burst_size                                                      0   \n",
       "latency_simulation_burst_interval                                                0.0   \n",
       "fp_precision                                                           torch.float16   \n",
       "quantization                                                                   False   \n",
       "load_in_8bit                                                                   False   \n",
       "load_in_4bit                                                                   False   \n",
       "total_input_tokens                                                              1000   \n",
       "total_params                                                              1100048384   \n",
       "max_input_tokens                                                                 100   \n",
       "max_output_tokens                                                                100   \n",
       "number_input_prompts                                                              10   \n",
       "total_energy_kwh                                                            0.001927   \n",
       "total_energy_joules                                                      6938.191835   \n",
       "flops                                                                  1034544128000   \n",
       "tokens_per_joule                                                             0.14413   \n",
       "joules_per_token                                                            6.938192   \n",
       "flops_per_joule                                                     149108608.219022   \n",
       "joules_per_flop                                                                  0.0   \n",
       "total_inference_time_sec                                                    2.363797   \n",
       "average_latency_ms_per_batch                                               2363.7969   \n",
       "throughput_queries_per_sec                                                  4.230482   \n",
       "throughput_tokens_per_sec                                                  423.04819   \n",
       "cpu_usage_percent                                                                5.1   \n",
       "cpu_memory_usage_bytes                                                    3161710592   \n",
       "gpu_utilization_percent_0                                                        4.0   \n",
       "gpu_utilization_percent_1                                                      100.0   \n",
       "gpu_utilization_percent_2                                                      100.0   \n",
       "gpu_utilization_percent_3                                                      100.0   \n",
       "cpu_power_process_0                                                            112.5   \n",
       "cpu_power_process_1                                                            112.5   \n",
       "cpu_power_process_2                                                            112.5   \n",
       "cpu_power_process_3                                                            112.5   \n",
       "gpu_power_process_0                                                        675.28713   \n",
       "gpu_power_process_1                                                       119.894599   \n",
       "gpu_power_process_2                                                       594.917177   \n",
       "gpu_power_process_3                                                       589.702252   \n",
       "ram_power_process_0                                                         1.102643   \n",
       "ram_power_process_1                                                         0.871312   \n",
       "ram_power_process_2                                                         0.913636   \n",
       "ram_power_process_3                                                         0.893091   \n",
       "cpu_energy_process_0                                                        0.000078   \n",
       "cpu_energy_process_1                                                        0.000099   \n",
       "cpu_energy_process_2                                                         0.00008   \n",
       "cpu_energy_process_3                                                        0.000089   \n",
       "gpu_energy_process_0                                                         0.00035   \n",
       "gpu_energy_process_1                                                        0.000453   \n",
       "gpu_energy_process_2                                                        0.000366   \n",
       "gpu_energy_process_3                                                        0.000411   \n",
       "ram_energy_process_0                                                        0.000001   \n",
       "ram_energy_process_1                                                        0.000001   \n",
       "ram_energy_process_2                                                        0.000001   \n",
       "ram_energy_process_3                                                        0.000001   \n",
       "total_energy_kwh_process_0                                                  0.000428   \n",
       "total_energy_kwh_process_1                                                  0.000553   \n",
       "total_energy_kwh_process_2                                                  0.000446   \n",
       "total_energy_kwh_process_3                                                    0.0005   \n",
       "total_energy_joules_process_0                                            1542.311349   \n",
       "total_energy_joules_process_1                                            1989.664818   \n",
       "total_energy_joules_process_2                                            1606.976883   \n",
       "total_energy_joules_process_3                                            1799.238784   \n",
       "cpu_power_avg                                                                  112.5   \n",
       "gpu_power_avg                                                             494.950289   \n",
       "ram_power_avg                                                                0.94517   \n",
       "cpu_energy_total                                                            0.000346   \n",
       "gpu_energy_total                                                            0.001579   \n",
       "ram_energy_total                                                            0.000003   \n",
       "latency_simulation_simulate                                                    False   \n",
       "models                                                                 1034544128000   \n",
       "latency_simulation_delay_max                                                     0.0   \n",
       "total_generated_tokens                                                          1000   \n",
       "decoder_config_decoding_mode                                                     NaN   \n",
       "\n",
       "                                                                                  13  \\\n",
       "config_name                        precis_float16_quant_True_quant8_True_quant4_F...   \n",
       "experiment_id                                                                     14   \n",
       "date_time                                              April 08, 2025 at 04:40:33 PM   \n",
       "model                                             TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                                      4   \n",
       "batch_size___fixed_batching                                                       16   \n",
       "decoder_temperature                                                              1.0   \n",
       "decoder_top_k                                                                      0   \n",
       "decoder_top_p                                                                    0.0   \n",
       "latency_simulation_delay_min                                                     0.0   \n",
       "latency_simulation_simulate_burst                                              False   \n",
       "latency_simulation_burst_size                                                      0   \n",
       "latency_simulation_burst_interval                                                0.0   \n",
       "fp_precision                                                           torch.float16   \n",
       "quantization                                                                    True   \n",
       "load_in_8bit                                                                    True   \n",
       "load_in_4bit                                                                   False   \n",
       "total_input_tokens                                                              1000   \n",
       "total_params                                                              1100048384   \n",
       "max_input_tokens                                                                 100   \n",
       "max_output_tokens                                                                100   \n",
       "number_input_prompts                                                              10   \n",
       "total_energy_kwh                                                            0.009094   \n",
       "total_energy_joules                                                     32739.910723   \n",
       "flops                                                                  1034544128000   \n",
       "tokens_per_joule                                                            0.030544   \n",
       "joules_per_token                                                           32.739911   \n",
       "flops_per_joule                                                      31598868.327846   \n",
       "joules_per_flop                                                                  0.0   \n",
       "total_inference_time_sec                                                    7.218139   \n",
       "average_latency_ms_per_batch                                             7218.139077   \n",
       "throughput_queries_per_sec                                                  1.385399   \n",
       "throughput_tokens_per_sec                                                 138.539863   \n",
       "cpu_usage_percent                                                                4.7   \n",
       "cpu_memory_usage_bytes                                                    2766688256   \n",
       "gpu_utilization_percent_0                                                        0.0   \n",
       "gpu_utilization_percent_1                                                       96.0   \n",
       "gpu_utilization_percent_2                                                      100.0   \n",
       "gpu_utilization_percent_3                                                      100.0   \n",
       "cpu_power_process_0                                                            112.5   \n",
       "cpu_power_process_1                                                            112.5   \n",
       "cpu_power_process_2                                                            112.5   \n",
       "cpu_power_process_3                                                            112.5   \n",
       "gpu_power_process_0                                                       444.462409   \n",
       "gpu_power_process_1                                                       374.736474   \n",
       "gpu_power_process_2                                                        518.21448   \n",
       "gpu_power_process_3                                                       480.250829   \n",
       "ram_power_process_0                                                         0.965406   \n",
       "ram_power_process_1                                                         1.010028   \n",
       "ram_power_process_2                                                         1.010149   \n",
       "ram_power_process_3                                                         1.000088   \n",
       "cpu_energy_process_0                                                        0.000227   \n",
       "cpu_energy_process_1                                                        0.001001   \n",
       "cpu_energy_process_2                                                        0.000229   \n",
       "cpu_energy_process_3                                                        0.000352   \n",
       "gpu_energy_process_0                                                         0.00093   \n",
       "gpu_energy_process_1                                                        0.004014   \n",
       "gpu_energy_process_2                                                        0.000931   \n",
       "gpu_energy_process_3                                                        0.001397   \n",
       "ram_energy_process_0                                                        0.000001   \n",
       "ram_energy_process_1                                                        0.000008   \n",
       "ram_energy_process_2                                                        0.000002   \n",
       "ram_energy_process_3                                                        0.000003   \n",
       "total_energy_kwh_process_0                                                  0.001159   \n",
       "total_energy_kwh_process_1                                                  0.005024   \n",
       "total_energy_kwh_process_2                                                  0.001161   \n",
       "total_energy_kwh_process_3                                                  0.001751   \n",
       "total_energy_joules_process_0                                            4171.360098   \n",
       "total_energy_joules_process_1                                           18084.969546   \n",
       "total_energy_joules_process_2                                             4179.91919   \n",
       "total_energy_joules_process_3                                             6303.66189   \n",
       "cpu_power_avg                                                                  112.5   \n",
       "gpu_power_avg                                                             454.416048   \n",
       "ram_power_avg                                                               0.996418   \n",
       "cpu_energy_total                                                            0.001809   \n",
       "gpu_energy_total                                                            0.007272   \n",
       "ram_energy_total                                                            0.000014   \n",
       "latency_simulation_simulate                                                    False   \n",
       "models                                                                 1034544128000   \n",
       "latency_simulation_delay_max                                                     0.0   \n",
       "total_generated_tokens                                                          1000   \n",
       "decoder_config_decoding_mode                                                     NaN   \n",
       "\n",
       "                                                                                  14  \\\n",
       "config_name                        precis_float16_quant_True_quant8_False_quant4_...   \n",
       "experiment_id                                                                     15   \n",
       "date_time                                              April 08, 2025 at 04:41:10 PM   \n",
       "model                                             TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                                      4   \n",
       "batch_size___fixed_batching                                                       16   \n",
       "decoder_temperature                                                              1.0   \n",
       "decoder_top_k                                                                      0   \n",
       "decoder_top_p                                                                    0.0   \n",
       "latency_simulation_delay_min                                                     0.0   \n",
       "latency_simulation_simulate_burst                                              False   \n",
       "latency_simulation_burst_size                                                      0   \n",
       "latency_simulation_burst_interval                                                0.0   \n",
       "fp_precision                                                           torch.float16   \n",
       "quantization                                                                    True   \n",
       "load_in_8bit                                                                   False   \n",
       "load_in_4bit                                                                    True   \n",
       "total_input_tokens                                                              1000   \n",
       "total_params                                                               615606272   \n",
       "max_input_tokens                                                                 100   \n",
       "max_output_tokens                                                                100   \n",
       "number_input_prompts                                                              10   \n",
       "total_energy_kwh                                                            0.002708   \n",
       "total_energy_joules                                                      9749.962662   \n",
       "flops                                                                  1034544128000   \n",
       "tokens_per_joule                                                            0.102564   \n",
       "joules_per_token                                                            9.749963   \n",
       "flops_per_joule                                                     106107496.397334   \n",
       "joules_per_flop                                                                  0.0   \n",
       "total_inference_time_sec                                                    3.677875   \n",
       "average_latency_ms_per_batch                                             3677.875369   \n",
       "throughput_queries_per_sec                                                  2.718961   \n",
       "throughput_tokens_per_sec                                                 271.896108   \n",
       "cpu_usage_percent                                                                3.9   \n",
       "cpu_memory_usage_bytes                                                    2705195008   \n",
       "gpu_utilization_percent_0                                                        0.0   \n",
       "gpu_utilization_percent_1                                                      100.0   \n",
       "gpu_utilization_percent_2                                                      100.0   \n",
       "gpu_utilization_percent_3                                                      100.0   \n",
       "cpu_power_process_0                                                            112.5   \n",
       "cpu_power_process_1                                                            112.5   \n",
       "cpu_power_process_2                                                            112.5   \n",
       "cpu_power_process_3                                                            112.5   \n",
       "gpu_power_process_0                                                        557.53898   \n",
       "gpu_power_process_1                                                       633.739616   \n",
       "gpu_power_process_2                                                       564.387603   \n",
       "gpu_power_process_3                                                              0.0   \n",
       "ram_power_process_0                                                         0.944375   \n",
       "ram_power_process_1                                                         1.002962   \n",
       "ram_power_process_2                                                         0.979697   \n",
       "ram_power_process_3                                                           0.9912   \n",
       "cpu_energy_process_0                                                         0.00012   \n",
       "cpu_energy_process_1                                                        0.000133   \n",
       "cpu_energy_process_2                                                        0.000122   \n",
       "cpu_energy_process_3                                                        0.000163   \n",
       "gpu_energy_process_0                                                        0.000518   \n",
       "gpu_energy_process_1                                                        0.000568   \n",
       "gpu_energy_process_2                                                        0.000522   \n",
       "gpu_energy_process_3                                                        0.000559   \n",
       "ram_energy_process_0                                                        0.000001   \n",
       "ram_energy_process_1                                                        0.000001   \n",
       "ram_energy_process_2                                                        0.000001   \n",
       "ram_energy_process_3                                                        0.000001   \n",
       "total_energy_kwh_process_0                                                  0.000638   \n",
       "total_energy_kwh_process_1                                                  0.000702   \n",
       "total_energy_kwh_process_2                                                  0.000645   \n",
       "total_energy_kwh_process_3                                                  0.000723   \n",
       "total_energy_joules_process_0                                            2298.536562   \n",
       "total_energy_joules_process_1                                            2527.941143   \n",
       "total_energy_joules_process_2                                             2320.70705   \n",
       "total_energy_joules_process_3                                            2602.777907   \n",
       "cpu_power_avg                                                                  112.5   \n",
       "gpu_power_avg                                                              438.91655   \n",
       "ram_power_avg                                                               0.979558   \n",
       "cpu_energy_total                                                            0.000538   \n",
       "gpu_energy_total                                                            0.002166   \n",
       "ram_energy_total                                                            0.000004   \n",
       "latency_simulation_simulate                                                    False   \n",
       "models                                                                 1034544128000   \n",
       "latency_simulation_delay_max                                                     0.0   \n",
       "total_generated_tokens                                                          1000   \n",
       "decoder_config_decoding_mode                                                     NaN   \n",
       "\n",
       "                                                                      15  \\\n",
       "config_name                        decoding_greedy_decoder_temperature_0   \n",
       "experiment_id                                                         16   \n",
       "date_time                                  April 08, 2025 at 04:41:43 PM   \n",
       "model                                 TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                          4   \n",
       "batch_size___fixed_batching                                           16   \n",
       "decoder_temperature                                                  0.0   \n",
       "decoder_top_k                                                          0   \n",
       "decoder_top_p                                                        0.0   \n",
       "latency_simulation_delay_min                                         0.0   \n",
       "latency_simulation_simulate_burst                                  False   \n",
       "latency_simulation_burst_size                                          0   \n",
       "latency_simulation_burst_interval                                    0.0   \n",
       "fp_precision                                               torch.float32   \n",
       "quantization                                                       False   \n",
       "load_in_8bit                                                       False   \n",
       "load_in_4bit                                                       False   \n",
       "total_input_tokens                                                  1000   \n",
       "total_params                                                  1100048384   \n",
       "max_input_tokens                                                     100   \n",
       "max_output_tokens                                                    100   \n",
       "number_input_prompts                                                  10   \n",
       "total_energy_kwh                                                0.002158   \n",
       "total_energy_joules                                          7767.530628   \n",
       "flops                                                      1034544128000   \n",
       "tokens_per_joule                                                0.128741   \n",
       "joules_per_token                                                7.767531   \n",
       "flops_per_joule                                         133188290.785709   \n",
       "joules_per_flop                                                      0.0   \n",
       "total_inference_time_sec                                        2.272839   \n",
       "average_latency_ms_per_batch                                 2272.839022   \n",
       "throughput_queries_per_sec                                      4.399784   \n",
       "throughput_tokens_per_sec                                     439.978366   \n",
       "cpu_usage_percent                                                    4.0   \n",
       "cpu_memory_usage_bytes                                        2056642560   \n",
       "gpu_utilization_percent_0                                           18.0   \n",
       "gpu_utilization_percent_1                                          100.0   \n",
       "gpu_utilization_percent_2                                          100.0   \n",
       "gpu_utilization_percent_3                                          100.0   \n",
       "cpu_power_process_0                                                112.5   \n",
       "cpu_power_process_1                                                112.5   \n",
       "cpu_power_process_2                                                112.5   \n",
       "cpu_power_process_3                                                112.5   \n",
       "gpu_power_process_0                                           612.704186   \n",
       "gpu_power_process_1                                           556.747136   \n",
       "gpu_power_process_2                                            660.52256   \n",
       "gpu_power_process_3                                           627.405103   \n",
       "ram_power_process_0                                             0.716738   \n",
       "ram_power_process_1                                             0.653096   \n",
       "ram_power_process_2                                             0.652359   \n",
       "ram_power_process_3                                             0.652561   \n",
       "cpu_energy_process_0                                            0.000076   \n",
       "cpu_energy_process_1                                             0.00009   \n",
       "cpu_energy_process_2                                            0.000076   \n",
       "cpu_energy_process_3                                            0.000085   \n",
       "gpu_energy_process_0                                            0.000433   \n",
       "gpu_energy_process_1                                            0.000491   \n",
       "gpu_energy_process_2                                            0.000433   \n",
       "gpu_energy_process_3                                            0.000473   \n",
       "ram_energy_process_0                                                 0.0   \n",
       "ram_energy_process_1                                                 0.0   \n",
       "ram_energy_process_2                                                 0.0   \n",
       "ram_energy_process_3                                                 0.0   \n",
       "total_energy_kwh_process_0                                      0.000509   \n",
       "total_energy_kwh_process_1                                      0.000581   \n",
       "total_energy_kwh_process_2                                      0.000509   \n",
       "total_energy_kwh_process_3                                      0.000558   \n",
       "total_energy_joules_process_0                                1833.577944   \n",
       "total_energy_joules_process_1                                2092.726127   \n",
       "total_energy_joules_process_2                                1832.894399   \n",
       "total_energy_joules_process_3                                2008.332158   \n",
       "cpu_power_avg                                                      112.5   \n",
       "gpu_power_avg                                                 614.344746   \n",
       "ram_power_avg                                                   0.668688   \n",
       "cpu_energy_total                                                0.000328   \n",
       "gpu_energy_total                                                0.001828   \n",
       "ram_energy_total                                                0.000002   \n",
       "latency_simulation_simulate                                        False   \n",
       "models                                                     1034544128000   \n",
       "latency_simulation_delay_max                                         0.0   \n",
       "total_generated_tokens                                              1000   \n",
       "decoder_config_decoding_mode                                      greedy   \n",
       "\n",
       "                                                                        16  \\\n",
       "config_name                        decoding_greedy_decoder_temperature_0.7   \n",
       "experiment_id                                                           17   \n",
       "date_time                                    April 08, 2025 at 04:42:19 PM   \n",
       "model                                   TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                            4   \n",
       "batch_size___fixed_batching                                             16   \n",
       "decoder_temperature                                                    0.7   \n",
       "decoder_top_k                                                            0   \n",
       "decoder_top_p                                                          0.0   \n",
       "latency_simulation_delay_min                                           0.0   \n",
       "latency_simulation_simulate_burst                                    False   \n",
       "latency_simulation_burst_size                                            0   \n",
       "latency_simulation_burst_interval                                      0.0   \n",
       "fp_precision                                                 torch.float32   \n",
       "quantization                                                         False   \n",
       "load_in_8bit                                                         False   \n",
       "load_in_4bit                                                         False   \n",
       "total_input_tokens                                                    1000   \n",
       "total_params                                                    1100048384   \n",
       "max_input_tokens                                                       100   \n",
       "max_output_tokens                                                      100   \n",
       "number_input_prompts                                                    10   \n",
       "total_energy_kwh                                                  0.002392   \n",
       "total_energy_joules                                            8612.615549   \n",
       "flops                                                        1034544128000   \n",
       "tokens_per_joule                                                  0.116109   \n",
       "joules_per_token                                                  8.612616   \n",
       "flops_per_joule                                           120119622.447774   \n",
       "joules_per_flop                                                        0.0   \n",
       "total_inference_time_sec                                          2.381273   \n",
       "average_latency_ms_per_batch                                   2381.273368   \n",
       "throughput_queries_per_sec                                        4.199434   \n",
       "throughput_tokens_per_sec                                       419.943386   \n",
       "cpu_usage_percent                                                      4.7   \n",
       "cpu_memory_usage_bytes                                          2073894912   \n",
       "gpu_utilization_percent_0                                              1.0   \n",
       "gpu_utilization_percent_1                                            100.0   \n",
       "gpu_utilization_percent_2                                            100.0   \n",
       "gpu_utilization_percent_3                                            100.0   \n",
       "cpu_power_process_0                                                  112.5   \n",
       "cpu_power_process_1                                                  112.5   \n",
       "cpu_power_process_2                                                  112.5   \n",
       "cpu_power_process_3                                                  112.5   \n",
       "gpu_power_process_0                                             762.470124   \n",
       "gpu_power_process_1                                             453.252145   \n",
       "gpu_power_process_2                                             683.491168   \n",
       "gpu_power_process_3                                             658.562704   \n",
       "ram_power_process_0                                               0.721763   \n",
       "ram_power_process_1                                               0.666492   \n",
       "ram_power_process_2                                               0.667424   \n",
       "ram_power_process_3                                               0.668657   \n",
       "cpu_energy_process_0                                              0.000079   \n",
       "cpu_energy_process_1                                              0.000112   \n",
       "cpu_energy_process_2                                              0.000083   \n",
       "cpu_energy_process_3                                               0.00009   \n",
       "gpu_energy_process_0                                              0.000458   \n",
       "gpu_energy_process_1                                              0.000593   \n",
       "gpu_energy_process_2                                              0.000475   \n",
       "gpu_energy_process_3                                              0.000501   \n",
       "ram_energy_process_0                                                   0.0   \n",
       "ram_energy_process_1                                              0.000001   \n",
       "ram_energy_process_2                                                   0.0   \n",
       "ram_energy_process_3                                                   0.0   \n",
       "total_energy_kwh_process_0                                        0.000538   \n",
       "total_energy_kwh_process_1                                        0.000705   \n",
       "total_energy_kwh_process_2                                        0.000558   \n",
       "total_energy_kwh_process_3                                        0.000591   \n",
       "total_energy_joules_process_0                                  1935.222914   \n",
       "total_energy_joules_process_1                                  2539.553048   \n",
       "total_energy_joules_process_2                                  2010.031883   \n",
       "total_energy_joules_process_3                                  2127.807703   \n",
       "cpu_power_avg                                                        112.5   \n",
       "gpu_power_avg                                                   639.444035   \n",
       "ram_power_avg                                                     0.681084   \n",
       "cpu_energy_total                                                  0.000364   \n",
       "gpu_energy_total                                                  0.002027   \n",
       "ram_energy_total                                                  0.000002   \n",
       "latency_simulation_simulate                                          False   \n",
       "models                                                       1034544128000   \n",
       "latency_simulation_delay_max                                           0.0   \n",
       "total_generated_tokens                                                1000   \n",
       "decoder_config_decoding_mode                                        greedy   \n",
       "\n",
       "                                                                        17  \\\n",
       "config_name                        decoding_greedy_decoder_temperature_1.0   \n",
       "experiment_id                                                           18   \n",
       "date_time                                    April 08, 2025 at 04:42:54 PM   \n",
       "model                                   TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                            4   \n",
       "batch_size___fixed_batching                                             16   \n",
       "decoder_temperature                                                    1.0   \n",
       "decoder_top_k                                                            0   \n",
       "decoder_top_p                                                          0.0   \n",
       "latency_simulation_delay_min                                           0.0   \n",
       "latency_simulation_simulate_burst                                    False   \n",
       "latency_simulation_burst_size                                            0   \n",
       "latency_simulation_burst_interval                                      0.0   \n",
       "fp_precision                                                 torch.float32   \n",
       "quantization                                                         False   \n",
       "load_in_8bit                                                         False   \n",
       "load_in_4bit                                                         False   \n",
       "total_input_tokens                                                    1000   \n",
       "total_params                                                    1100048384   \n",
       "max_input_tokens                                                       100   \n",
       "max_output_tokens                                                      100   \n",
       "number_input_prompts                                                    10   \n",
       "total_energy_kwh                                                   0.00258   \n",
       "total_energy_joules                                            9287.457917   \n",
       "flops                                                        1034544128000   \n",
       "tokens_per_joule                                                  0.107672   \n",
       "joules_per_token                                                  9.287458   \n",
       "flops_per_joule                                           111391527.936536   \n",
       "joules_per_flop                                                        0.0   \n",
       "total_inference_time_sec                                          2.350439   \n",
       "average_latency_ms_per_batch                                   2350.439308   \n",
       "throughput_queries_per_sec                                        4.254524   \n",
       "throughput_tokens_per_sec                                       425.452381   \n",
       "cpu_usage_percent                                                      4.8   \n",
       "cpu_memory_usage_bytes                                          2070286336   \n",
       "gpu_utilization_percent_0                                             20.0   \n",
       "gpu_utilization_percent_1                                            100.0   \n",
       "gpu_utilization_percent_2                                             99.0   \n",
       "gpu_utilization_percent_3                                            100.0   \n",
       "cpu_power_process_0                                                  112.5   \n",
       "cpu_power_process_1                                                  112.5   \n",
       "cpu_power_process_2                                                  112.5   \n",
       "cpu_power_process_3                                                  112.5   \n",
       "gpu_power_process_0                                             795.129191   \n",
       "gpu_power_process_1                                              18.628497   \n",
       "gpu_power_process_2                                             688.585224   \n",
       "gpu_power_process_3                                             610.801172   \n",
       "ram_power_process_0                                               0.721812   \n",
       "ram_power_process_1                                               0.668358   \n",
       "ram_power_process_2                                               0.668231   \n",
       "ram_power_process_3                                               0.670784   \n",
       "cpu_energy_process_0                                              0.000078   \n",
       "cpu_energy_process_1                                              0.000163   \n",
       "cpu_energy_process_2                                              0.000079   \n",
       "cpu_energy_process_3                                              0.000088   \n",
       "gpu_energy_process_0                                              0.000477   \n",
       "gpu_energy_process_1                                                0.0007   \n",
       "gpu_energy_process_2                                              0.000477   \n",
       "gpu_energy_process_3                                              0.000515   \n",
       "ram_energy_process_0                                                   0.0   \n",
       "ram_energy_process_1                                              0.000001   \n",
       "ram_energy_process_2                                                   0.0   \n",
       "ram_energy_process_3                                                   0.0   \n",
       "total_energy_kwh_process_0                                        0.000556   \n",
       "total_energy_kwh_process_1                                        0.000864   \n",
       "total_energy_kwh_process_2                                        0.000556   \n",
       "total_energy_kwh_process_3                                        0.000604   \n",
       "total_energy_joules_process_0                                  1999.907206   \n",
       "total_energy_joules_process_1                                  3109.550092   \n",
       "total_energy_joules_process_2                                  2003.210191   \n",
       "total_energy_joules_process_3                                  2174.790428   \n",
       "cpu_power_avg                                                        112.5   \n",
       "gpu_power_avg                                                   528.286021   \n",
       "ram_power_avg                                                     0.682296   \n",
       "cpu_energy_total                                                  0.000409   \n",
       "gpu_energy_total                                                  0.002169   \n",
       "ram_energy_total                                                  0.000002   \n",
       "latency_simulation_simulate                                          False   \n",
       "models                                                       1034544128000   \n",
       "latency_simulation_delay_max                                           0.0   \n",
       "total_generated_tokens                                                1000   \n",
       "decoder_config_decoding_mode                                        greedy   \n",
       "\n",
       "                                                                        18  \\\n",
       "config_name                        decoding_greedy_decoder_temperature_1.3   \n",
       "experiment_id                                                           19   \n",
       "date_time                                    April 08, 2025 at 04:43:29 PM   \n",
       "model                                   TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                            4   \n",
       "batch_size___fixed_batching                                             16   \n",
       "decoder_temperature                                                    1.3   \n",
       "decoder_top_k                                                            0   \n",
       "decoder_top_p                                                          0.0   \n",
       "latency_simulation_delay_min                                           0.0   \n",
       "latency_simulation_simulate_burst                                    False   \n",
       "latency_simulation_burst_size                                            0   \n",
       "latency_simulation_burst_interval                                      0.0   \n",
       "fp_precision                                                 torch.float32   \n",
       "quantization                                                         False   \n",
       "load_in_8bit                                                         False   \n",
       "load_in_4bit                                                         False   \n",
       "total_input_tokens                                                    1000   \n",
       "total_params                                                    1100048384   \n",
       "max_input_tokens                                                       100   \n",
       "max_output_tokens                                                      100   \n",
       "number_input_prompts                                                    10   \n",
       "total_energy_kwh                                                   0.00262   \n",
       "total_energy_joules                                            9431.681788   \n",
       "flops                                                        1034544128000   \n",
       "tokens_per_joule                                                  0.106026   \n",
       "joules_per_token                                                  9.431682   \n",
       "flops_per_joule                                            109688192.54092   \n",
       "joules_per_flop                                                        0.0   \n",
       "total_inference_time_sec                                          2.395863   \n",
       "average_latency_ms_per_batch                                   2395.863263   \n",
       "throughput_queries_per_sec                                        4.173861   \n",
       "throughput_tokens_per_sec                                        417.38609   \n",
       "cpu_usage_percent                                                      5.5   \n",
       "cpu_memory_usage_bytes                                          2072047616   \n",
       "gpu_utilization_percent_0                                             36.0   \n",
       "gpu_utilization_percent_1                                            100.0   \n",
       "gpu_utilization_percent_2                                            100.0   \n",
       "gpu_utilization_percent_3                                            100.0   \n",
       "cpu_power_process_0                                                  112.5   \n",
       "cpu_power_process_1                                                  112.5   \n",
       "cpu_power_process_2                                                  112.5   \n",
       "cpu_power_process_3                                                  112.5   \n",
       "gpu_power_process_0                                                740.388   \n",
       "gpu_power_process_1                                                    0.0   \n",
       "gpu_power_process_2                                             685.891048   \n",
       "gpu_power_process_3                                             604.058121   \n",
       "ram_power_process_0                                               0.722298   \n",
       "ram_power_process_1                                               0.672742   \n",
       "ram_power_process_2                                               0.678045   \n",
       "ram_power_process_3                                               0.669389   \n",
       "cpu_energy_process_0                                               0.00008   \n",
       "cpu_energy_process_1                                              0.000163   \n",
       "cpu_energy_process_2                                              0.000083   \n",
       "cpu_energy_process_3                                              0.000089   \n",
       "gpu_energy_process_0                                              0.000484   \n",
       "gpu_energy_process_1                                              0.000703   \n",
       "gpu_energy_process_2                                              0.000488   \n",
       "gpu_energy_process_3                                              0.000527   \n",
       "ram_energy_process_0                                                   0.0   \n",
       "ram_energy_process_1                                              0.000001   \n",
       "ram_energy_process_2                                                   0.0   \n",
       "ram_energy_process_3                                                   0.0   \n",
       "total_energy_kwh_process_0                                        0.000565   \n",
       "total_energy_kwh_process_1                                        0.000867   \n",
       "total_energy_kwh_process_2                                        0.000571   \n",
       "total_energy_kwh_process_3                                        0.000617   \n",
       "total_energy_joules_process_0                                  2033.157579   \n",
       "total_energy_joules_process_1                                  3121.067615   \n",
       "total_energy_joules_process_2                                  2055.629763   \n",
       "total_energy_joules_process_3                                  2221.826831   \n",
       "cpu_power_avg                                                        112.5   \n",
       "gpu_power_avg                                                   507.584292   \n",
       "ram_power_avg                                                     0.685619   \n",
       "cpu_energy_total                                                  0.000415   \n",
       "gpu_energy_total                                                  0.002203   \n",
       "ram_energy_total                                                  0.000002   \n",
       "latency_simulation_simulate                                          False   \n",
       "models                                                       1034544128000   \n",
       "latency_simulation_delay_max                                           0.0   \n",
       "total_generated_tokens                                                1000   \n",
       "decoder_config_decoding_mode                                        greedy   \n",
       "\n",
       "                                                                                  19  \\\n",
       "config_name                        decoding_top_k_decoder_top_k_50_decoder_temper...   \n",
       "experiment_id                                                                     20   \n",
       "date_time                                              April 08, 2025 at 04:44:03 PM   \n",
       "model                                             TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                                      4   \n",
       "batch_size___fixed_batching                                                       16   \n",
       "decoder_temperature                                                              0.0   \n",
       "decoder_top_k                                                                     50   \n",
       "decoder_top_p                                                                    0.0   \n",
       "latency_simulation_delay_min                                                     0.0   \n",
       "latency_simulation_simulate_burst                                              False   \n",
       "latency_simulation_burst_size                                                      0   \n",
       "latency_simulation_burst_interval                                                0.0   \n",
       "fp_precision                                                           torch.float32   \n",
       "quantization                                                                   False   \n",
       "load_in_8bit                                                                   False   \n",
       "load_in_4bit                                                                   False   \n",
       "total_input_tokens                                                              1000   \n",
       "total_params                                                              1100048384   \n",
       "max_input_tokens                                                                 100   \n",
       "max_output_tokens                                                                100   \n",
       "number_input_prompts                                                              10   \n",
       "total_energy_kwh                                                            0.002462   \n",
       "total_energy_joules                                                      8861.743317   \n",
       "flops                                                                  1034544128000   \n",
       "tokens_per_joule                                                            0.112845   \n",
       "joules_per_token                                                            8.861743   \n",
       "flops_per_joule                                                     116742732.320793   \n",
       "joules_per_flop                                                                  0.0   \n",
       "total_inference_time_sec                                                    2.303065   \n",
       "average_latency_ms_per_batch                                             2303.064523   \n",
       "throughput_queries_per_sec                                                  4.342041   \n",
       "throughput_tokens_per_sec                                                 434.204075   \n",
       "cpu_usage_percent                                                                5.8   \n",
       "cpu_memory_usage_bytes                                                    2056028160   \n",
       "gpu_utilization_percent_0                                                       11.0   \n",
       "gpu_utilization_percent_1                                                      100.0   \n",
       "gpu_utilization_percent_2                                                      100.0   \n",
       "gpu_utilization_percent_3                                                      100.0   \n",
       "cpu_power_process_0                                                            112.5   \n",
       "cpu_power_process_1                                                            112.5   \n",
       "cpu_power_process_2                                                            112.5   \n",
       "cpu_power_process_3                                                            112.5   \n",
       "gpu_power_process_0                                                       776.321292   \n",
       "gpu_power_process_1                                                       495.301506   \n",
       "gpu_power_process_2                                                       759.717472   \n",
       "gpu_power_process_3                                                       574.829885   \n",
       "ram_power_process_0                                                         0.716374   \n",
       "ram_power_process_1                                                          0.65193   \n",
       "ram_power_process_2                                                         0.638948   \n",
       "ram_power_process_3                                                         0.639038   \n",
       "cpu_energy_process_0                                                        0.000077   \n",
       "cpu_energy_process_1                                                        0.000119   \n",
       "cpu_energy_process_2                                                        0.000076   \n",
       "cpu_energy_process_3                                                        0.000086   \n",
       "gpu_energy_process_0                                                        0.000473   \n",
       "gpu_energy_process_1                                                        0.000651   \n",
       "gpu_energy_process_2                                                        0.000468   \n",
       "gpu_energy_process_3                                                        0.000509   \n",
       "ram_energy_process_0                                                             0.0   \n",
       "ram_energy_process_1                                                        0.000001   \n",
       "ram_energy_process_2                                                             0.0   \n",
       "ram_energy_process_3                                                             0.0   \n",
       "total_energy_kwh_process_0                                                   0.00055   \n",
       "total_energy_kwh_process_1                                                  0.000771   \n",
       "total_energy_kwh_process_2                                                  0.000544   \n",
       "total_energy_kwh_process_3                                                  0.000596   \n",
       "total_energy_joules_process_0                                            1980.873995   \n",
       "total_energy_joules_process_1                                            2774.728283   \n",
       "total_energy_joules_process_2                                            1960.037607   \n",
       "total_energy_joules_process_3                                            2146.103432   \n",
       "cpu_power_avg                                                                  112.5   \n",
       "gpu_power_avg                                                             651.542539   \n",
       "ram_power_avg                                                               0.661573   \n",
       "cpu_energy_total                                                            0.000359   \n",
       "gpu_energy_total                                                            0.002101   \n",
       "ram_energy_total                                                            0.000002   \n",
       "latency_simulation_simulate                                                    False   \n",
       "models                                                                 1034544128000   \n",
       "latency_simulation_delay_max                                                     0.0   \n",
       "total_generated_tokens                                                          1000   \n",
       "decoder_config_decoding_mode                                                   top_k   \n",
       "\n",
       "                                                                                  20  \\\n",
       "config_name                        decoding_top_k_decoder_top_k_50_decoder_temper...   \n",
       "experiment_id                                                                     21   \n",
       "date_time                                              April 08, 2025 at 04:44:38 PM   \n",
       "model                                             TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                                      4   \n",
       "batch_size___fixed_batching                                                       16   \n",
       "decoder_temperature                                                              0.7   \n",
       "decoder_top_k                                                                     50   \n",
       "decoder_top_p                                                                    0.0   \n",
       "latency_simulation_delay_min                                                     0.0   \n",
       "latency_simulation_simulate_burst                                              False   \n",
       "latency_simulation_burst_size                                                      0   \n",
       "latency_simulation_burst_interval                                                0.0   \n",
       "fp_precision                                                           torch.float32   \n",
       "quantization                                                                   False   \n",
       "load_in_8bit                                                                   False   \n",
       "load_in_4bit                                                                   False   \n",
       "total_input_tokens                                                              1000   \n",
       "total_params                                                              1100048384   \n",
       "max_input_tokens                                                                 100   \n",
       "max_output_tokens                                                                100   \n",
       "number_input_prompts                                                              10   \n",
       "total_energy_kwh                                                             0.00261   \n",
       "total_energy_joules                                                      9394.350592   \n",
       "flops                                                                  1034544128000   \n",
       "tokens_per_joule                                                            0.106447   \n",
       "joules_per_token                                                            9.394351   \n",
       "flops_per_joule                                                     110124070.617504   \n",
       "joules_per_flop                                                                  0.0   \n",
       "total_inference_time_sec                                                    2.387908   \n",
       "average_latency_ms_per_batch                                             2387.907913   \n",
       "throughput_queries_per_sec                                                  4.187766   \n",
       "throughput_tokens_per_sec                                                 418.776618   \n",
       "cpu_usage_percent                                                                5.5   \n",
       "cpu_memory_usage_bytes                                                    2075602944   \n",
       "gpu_utilization_percent_0                                                       67.0   \n",
       "gpu_utilization_percent_1                                                      100.0   \n",
       "gpu_utilization_percent_2                                                      100.0   \n",
       "gpu_utilization_percent_3                                                      100.0   \n",
       "cpu_power_process_0                                                            112.5   \n",
       "cpu_power_process_1                                                            112.5   \n",
       "cpu_power_process_2                                                            112.5   \n",
       "cpu_power_process_3                                                            112.5   \n",
       "gpu_power_process_0                                                       809.680864   \n",
       "gpu_power_process_1                                                      1679.540024   \n",
       "gpu_power_process_2                                                       708.550319   \n",
       "gpu_power_process_3                                                       601.787839   \n",
       "ram_power_process_0                                                         0.722797   \n",
       "ram_power_process_1                                                         0.681414   \n",
       "ram_power_process_2                                                         0.686724   \n",
       "ram_power_process_3                                                         0.688542   \n",
       "cpu_energy_process_0                                                         0.00008   \n",
       "cpu_energy_process_1                                                         0.00013   \n",
       "cpu_energy_process_2                                                        0.000083   \n",
       "cpu_energy_process_3                                                        0.000093   \n",
       "gpu_energy_process_0                                                        0.000486   \n",
       "gpu_energy_process_1                                                        0.000703   \n",
       "gpu_energy_process_2                                                        0.000492   \n",
       "gpu_energy_process_3                                                        0.000542   \n",
       "ram_energy_process_0                                                             0.0   \n",
       "ram_energy_process_1                                                        0.000001   \n",
       "ram_energy_process_2                                                             0.0   \n",
       "ram_energy_process_3                                                             0.0   \n",
       "total_energy_kwh_process_0                                                  0.000566   \n",
       "total_energy_kwh_process_1                                                  0.000834   \n",
       "total_energy_kwh_process_2                                                  0.000575   \n",
       "total_energy_kwh_process_3                                                  0.000635   \n",
       "total_energy_joules_process_0                                            2036.742215   \n",
       "total_energy_joules_process_1                                            3003.487737   \n",
       "total_energy_joules_process_2                                            2068.396405   \n",
       "total_energy_joules_process_3                                            2285.724235   \n",
       "cpu_power_avg                                                                  112.5   \n",
       "gpu_power_avg                                                             949.889761   \n",
       "ram_power_avg                                                                0.69487   \n",
       "cpu_energy_total                                                            0.000386   \n",
       "gpu_energy_total                                                            0.002222   \n",
       "ram_energy_total                                                            0.000002   \n",
       "latency_simulation_simulate                                                    False   \n",
       "models                                                                 1034544128000   \n",
       "latency_simulation_delay_max                                                     0.0   \n",
       "total_generated_tokens                                                          1000   \n",
       "decoder_config_decoding_mode                                                   top_k   \n",
       "\n",
       "                                                                                  21  \\\n",
       "config_name                        decoding_top_k_decoder_top_k_50_decoder_temper...   \n",
       "experiment_id                                                                     22   \n",
       "date_time                                              April 08, 2025 at 04:45:14 PM   \n",
       "model                                             TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                                      4   \n",
       "batch_size___fixed_batching                                                       16   \n",
       "decoder_temperature                                                              1.0   \n",
       "decoder_top_k                                                                     50   \n",
       "decoder_top_p                                                                    0.0   \n",
       "latency_simulation_delay_min                                                     0.0   \n",
       "latency_simulation_simulate_burst                                              False   \n",
       "latency_simulation_burst_size                                                      0   \n",
       "latency_simulation_burst_interval                                                0.0   \n",
       "fp_precision                                                           torch.float32   \n",
       "quantization                                                                   False   \n",
       "load_in_8bit                                                                   False   \n",
       "load_in_4bit                                                                   False   \n",
       "total_input_tokens                                                              1000   \n",
       "total_params                                                              1100048384   \n",
       "max_input_tokens                                                                 100   \n",
       "max_output_tokens                                                                100   \n",
       "number_input_prompts                                                              10   \n",
       "total_energy_kwh                                                            0.002658   \n",
       "total_energy_joules                                                      9569.324332   \n",
       "flops                                                                  1034544128000   \n",
       "tokens_per_joule                                                            0.104501   \n",
       "joules_per_token                                                            9.569324   \n",
       "flops_per_joule                                                     108110467.588197   \n",
       "joules_per_flop                                                                  0.0   \n",
       "total_inference_time_sec                                                    2.412971   \n",
       "average_latency_ms_per_batch                                                2412.971   \n",
       "throughput_queries_per_sec                                                  4.144269   \n",
       "throughput_tokens_per_sec                                                 414.426862   \n",
       "cpu_usage_percent                                                                4.0   \n",
       "cpu_memory_usage_bytes                                                    2075750400   \n",
       "gpu_utilization_percent_0                                                        7.0   \n",
       "gpu_utilization_percent_1                                                      100.0   \n",
       "gpu_utilization_percent_2                                                      100.0   \n",
       "gpu_utilization_percent_3                                                      100.0   \n",
       "cpu_power_process_0                                                            112.5   \n",
       "cpu_power_process_1                                                            112.5   \n",
       "cpu_power_process_2                                                            112.5   \n",
       "cpu_power_process_3                                                            112.5   \n",
       "gpu_power_process_0                                                       646.756882   \n",
       "gpu_power_process_1                                                              0.0   \n",
       "gpu_power_process_2                                                       731.480916   \n",
       "gpu_power_process_3                                                      1382.946973   \n",
       "ram_power_process_0                                                         0.723407   \n",
       "ram_power_process_1                                                         0.691215   \n",
       "ram_power_process_2                                                         0.691328   \n",
       "ram_power_process_3                                                         0.681649   \n",
       "cpu_energy_process_0                                                         0.00008   \n",
       "cpu_energy_process_1                                                        0.000131   \n",
       "cpu_energy_process_2                                                        0.000082   \n",
       "cpu_energy_process_3                                                        0.000099   \n",
       "gpu_energy_process_0                                                        0.000471   \n",
       "gpu_energy_process_1                                                        0.000717   \n",
       "gpu_energy_process_2                                                        0.000492   \n",
       "gpu_energy_process_3                                                        0.000583   \n",
       "ram_energy_process_0                                                             0.0   \n",
       "ram_energy_process_1                                                        0.000001   \n",
       "ram_energy_process_2                                                             0.0   \n",
       "ram_energy_process_3                                                        0.000001   \n",
       "total_energy_kwh_process_0                                                  0.000552   \n",
       "total_energy_kwh_process_1                                                  0.000848   \n",
       "total_energy_kwh_process_2                                                  0.000575   \n",
       "total_energy_kwh_process_3                                                  0.000683   \n",
       "total_energy_joules_process_0                                            1985.841167   \n",
       "total_energy_joules_process_1                                            3054.210497   \n",
       "total_energy_joules_process_2                                            2070.639691   \n",
       "total_energy_joules_process_3                                            2458.632976   \n",
       "cpu_power_avg                                                                  112.5   \n",
       "gpu_power_avg                                                             690.296192   \n",
       "ram_power_avg                                                               0.696899   \n",
       "cpu_energy_total                                                            0.000393   \n",
       "gpu_energy_total                                                            0.002263   \n",
       "ram_energy_total                                                            0.000002   \n",
       "latency_simulation_simulate                                                    False   \n",
       "models                                                                 1034544128000   \n",
       "latency_simulation_delay_max                                                     0.0   \n",
       "total_generated_tokens                                                          1000   \n",
       "decoder_config_decoding_mode                                                   top_k   \n",
       "\n",
       "                                                                                  22  \\\n",
       "config_name                        decoding_top_k_decoder_top_k_50_decoder_temper...   \n",
       "experiment_id                                                                     23   \n",
       "date_time                                              April 08, 2025 at 04:45:49 PM   \n",
       "model                                             TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                                      4   \n",
       "batch_size___fixed_batching                                                       16   \n",
       "decoder_temperature                                                              1.3   \n",
       "decoder_top_k                                                                     50   \n",
       "decoder_top_p                                                                    0.0   \n",
       "latency_simulation_delay_min                                                     0.0   \n",
       "latency_simulation_simulate_burst                                              False   \n",
       "latency_simulation_burst_size                                                      0   \n",
       "latency_simulation_burst_interval                                                0.0   \n",
       "fp_precision                                                           torch.float32   \n",
       "quantization                                                                   False   \n",
       "load_in_8bit                                                                   False   \n",
       "load_in_4bit                                                                   False   \n",
       "total_input_tokens                                                              1000   \n",
       "total_params                                                              1100048384   \n",
       "max_input_tokens                                                                 100   \n",
       "max_output_tokens                                                                100   \n",
       "number_input_prompts                                                              10   \n",
       "total_energy_kwh                                                            0.002661   \n",
       "total_energy_joules                                                      9578.843492   \n",
       "flops                                                                  1034544128000   \n",
       "tokens_per_joule                                                            0.104397   \n",
       "joules_per_token                                                            9.578843   \n",
       "flops_per_joule                                                     108003030.722911   \n",
       "joules_per_flop                                                                  0.0   \n",
       "total_inference_time_sec                                                    2.478181   \n",
       "average_latency_ms_per_batch                                             2478.180916   \n",
       "throughput_queries_per_sec                                                  4.035218   \n",
       "throughput_tokens_per_sec                                                  403.52179   \n",
       "cpu_usage_percent                                                                4.7   \n",
       "cpu_memory_usage_bytes                                                    2077544448   \n",
       "gpu_utilization_percent_0                                                       51.0   \n",
       "gpu_utilization_percent_1                                                      100.0   \n",
       "gpu_utilization_percent_2                                                      100.0   \n",
       "gpu_utilization_percent_3                                                      100.0   \n",
       "cpu_power_process_0                                                            112.5   \n",
       "cpu_power_process_1                                                            112.5   \n",
       "cpu_power_process_2                                                            112.5   \n",
       "cpu_power_process_3                                                            112.5   \n",
       "gpu_power_process_0                                                       652.001595   \n",
       "gpu_power_process_1                                                      2219.419073   \n",
       "gpu_power_process_2                                                       670.105119   \n",
       "gpu_power_process_3                                                       708.385754   \n",
       "ram_power_process_0                                                         0.723691   \n",
       "ram_power_process_1                                                         0.681756   \n",
       "ram_power_process_2                                                         0.684342   \n",
       "ram_power_process_3                                                         0.689416   \n",
       "cpu_energy_process_0                                                        0.000082   \n",
       "cpu_energy_process_1                                                         0.00013   \n",
       "cpu_energy_process_2                                                        0.000081   \n",
       "cpu_energy_process_3                                                        0.000099   \n",
       "gpu_energy_process_0                                                        0.000484   \n",
       "gpu_energy_process_1                                                        0.000716   \n",
       "gpu_energy_process_2                                                        0.000486   \n",
       "gpu_energy_process_3                                                        0.000579   \n",
       "ram_energy_process_0                                                             0.0   \n",
       "ram_energy_process_1                                                        0.000001   \n",
       "ram_energy_process_2                                                             0.0   \n",
       "ram_energy_process_3                                                        0.000001   \n",
       "total_energy_kwh_process_0                                                  0.000567   \n",
       "total_energy_kwh_process_1                                                  0.000847   \n",
       "total_energy_kwh_process_2                                                  0.000568   \n",
       "total_energy_kwh_process_3                                                  0.000679   \n",
       "total_energy_joules_process_0                                            2041.046993   \n",
       "total_energy_joules_process_1                                            3047.636726   \n",
       "total_energy_joules_process_2                                             2044.85402   \n",
       "total_energy_joules_process_3                                            2445.305753   \n",
       "cpu_power_avg                                                                  112.5   \n",
       "gpu_power_avg                                                            1062.477885   \n",
       "ram_power_avg                                                               0.694802   \n",
       "cpu_energy_total                                                            0.000393   \n",
       "gpu_energy_total                                                            0.002265   \n",
       "ram_energy_total                                                            0.000002   \n",
       "latency_simulation_simulate                                                    False   \n",
       "models                                                                 1034544128000   \n",
       "latency_simulation_delay_max                                                     0.0   \n",
       "total_generated_tokens                                                          1000   \n",
       "decoder_config_decoding_mode                                                   top_k   \n",
       "\n",
       "                                                                                  23  \\\n",
       "config_name                        decoding_top_p_decoder_top_p_0.9_decoder_tempe...   \n",
       "experiment_id                                                                     24   \n",
       "date_time                                              April 08, 2025 at 04:46:23 PM   \n",
       "model                                             TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                                      4   \n",
       "batch_size___fixed_batching                                                       16   \n",
       "decoder_temperature                                                              0.0   \n",
       "decoder_top_k                                                                      0   \n",
       "decoder_top_p                                                                    0.9   \n",
       "latency_simulation_delay_min                                                     0.0   \n",
       "latency_simulation_simulate_burst                                              False   \n",
       "latency_simulation_burst_size                                                      0   \n",
       "latency_simulation_burst_interval                                                0.0   \n",
       "fp_precision                                                           torch.float32   \n",
       "quantization                                                                   False   \n",
       "load_in_8bit                                                                   False   \n",
       "load_in_4bit                                                                   False   \n",
       "total_input_tokens                                                              1000   \n",
       "total_params                                                              1100048384   \n",
       "max_input_tokens                                                                 100   \n",
       "max_output_tokens                                                                100   \n",
       "number_input_prompts                                                              10   \n",
       "total_energy_kwh                                                            0.002878   \n",
       "total_energy_joules                                                     10361.488063   \n",
       "flops                                                                  1034544128000   \n",
       "tokens_per_joule                                                            0.096511   \n",
       "joules_per_token                                                           10.361488   \n",
       "flops_per_joule                                                       99845130.52061   \n",
       "joules_per_flop                                                                  0.0   \n",
       "total_inference_time_sec                                                    3.440782   \n",
       "average_latency_ms_per_batch                                             3440.782234   \n",
       "throughput_queries_per_sec                                                  2.906316   \n",
       "throughput_tokens_per_sec                                                 290.631587   \n",
       "cpu_usage_percent                                                               55.3   \n",
       "cpu_memory_usage_bytes                                                    2057662464   \n",
       "gpu_utilization_percent_0                                                       11.0   \n",
       "gpu_utilization_percent_1                                                      100.0   \n",
       "gpu_utilization_percent_2                                                      100.0   \n",
       "gpu_utilization_percent_3                                                      100.0   \n",
       "cpu_power_process_0                                                            112.5   \n",
       "cpu_power_process_1                                                            112.5   \n",
       "cpu_power_process_2                                                            112.5   \n",
       "cpu_power_process_3                                                            112.5   \n",
       "gpu_power_process_0                                                       567.629617   \n",
       "gpu_power_process_1                                                       771.336972   \n",
       "gpu_power_process_2                                                       615.131361   \n",
       "gpu_power_process_3                                                       657.908802   \n",
       "ram_power_process_0                                                         0.716706   \n",
       "ram_power_process_1                                                         0.639193   \n",
       "ram_power_process_2                                                         0.646367   \n",
       "ram_power_process_3                                                         0.651056   \n",
       "cpu_energy_process_0                                                        0.000114   \n",
       "cpu_energy_process_1                                                        0.000133   \n",
       "cpu_energy_process_2                                                        0.000092   \n",
       "cpu_energy_process_3                                                        0.000105   \n",
       "gpu_energy_process_0                                                        0.000623   \n",
       "gpu_energy_process_1                                                        0.000711   \n",
       "gpu_energy_process_2                                                         0.00052   \n",
       "gpu_energy_process_3                                                        0.000578   \n",
       "ram_energy_process_0                                                        0.000001   \n",
       "ram_energy_process_1                                                        0.000001   \n",
       "ram_energy_process_2                                                             0.0   \n",
       "ram_energy_process_3                                                             0.0   \n",
       "total_energy_kwh_process_0                                                  0.000737   \n",
       "total_energy_kwh_process_1                                                  0.000845   \n",
       "total_energy_kwh_process_2                                                  0.000613   \n",
       "total_energy_kwh_process_3                                                  0.000683   \n",
       "total_energy_joules_process_0                                            2654.411522   \n",
       "total_energy_joules_process_1                                            3041.883795   \n",
       "total_energy_joules_process_2                                            2205.460381   \n",
       "total_energy_joules_process_3                                            2459.732365   \n",
       "cpu_power_avg                                                                  112.5   \n",
       "gpu_power_avg                                                             653.001688   \n",
       "ram_power_avg                                                                0.66333   \n",
       "cpu_energy_total                                                            0.000443   \n",
       "gpu_energy_total                                                            0.002433   \n",
       "ram_energy_total                                                            0.000002   \n",
       "latency_simulation_simulate                                                    False   \n",
       "models                                                                 1034544128000   \n",
       "latency_simulation_delay_max                                                     0.0   \n",
       "total_generated_tokens                                                          1000   \n",
       "decoder_config_decoding_mode                                                   top_p   \n",
       "\n",
       "                                                                                  24  \\\n",
       "config_name                        decoding_top_p_decoder_top_p_0.9_decoder_tempe...   \n",
       "experiment_id                                                                     25   \n",
       "date_time                                              April 08, 2025 at 04:47:02 PM   \n",
       "model                                             TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                                      4   \n",
       "batch_size___fixed_batching                                                       16   \n",
       "decoder_temperature                                                              0.7   \n",
       "decoder_top_k                                                                      0   \n",
       "decoder_top_p                                                                    0.9   \n",
       "latency_simulation_delay_min                                                     0.0   \n",
       "latency_simulation_simulate_burst                                              False   \n",
       "latency_simulation_burst_size                                                      0   \n",
       "latency_simulation_burst_interval                                                0.0   \n",
       "fp_precision                                                           torch.float32   \n",
       "quantization                                                                   False   \n",
       "load_in_8bit                                                                   False   \n",
       "load_in_4bit                                                                   False   \n",
       "total_input_tokens                                                              1000   \n",
       "total_params                                                              1100048384   \n",
       "max_input_tokens                                                                 100   \n",
       "max_output_tokens                                                                100   \n",
       "number_input_prompts                                                              10   \n",
       "total_energy_kwh                                                            0.002745   \n",
       "total_energy_joules                                                       9881.56245   \n",
       "flops                                                                  1034544128000   \n",
       "tokens_per_joule                                                            0.101199   \n",
       "joules_per_token                                                            9.881562   \n",
       "flops_per_joule                                                     104694387.473973   \n",
       "joules_per_flop                                                                  0.0   \n",
       "total_inference_time_sec                                                    2.714286   \n",
       "average_latency_ms_per_batch                                             2714.285777   \n",
       "throughput_queries_per_sec                                                   3.68421   \n",
       "throughput_tokens_per_sec                                                 368.421044   \n",
       "cpu_usage_percent                                                                5.5   \n",
       "cpu_memory_usage_bytes                                                    2070953984   \n",
       "gpu_utilization_percent_0                                                       11.0   \n",
       "gpu_utilization_percent_1                                                      100.0   \n",
       "gpu_utilization_percent_2                                                      100.0   \n",
       "gpu_utilization_percent_3                                                      100.0   \n",
       "cpu_power_process_0                                                            112.5   \n",
       "cpu_power_process_1                                                            112.5   \n",
       "cpu_power_process_2                                                            112.5   \n",
       "cpu_power_process_3                                                            112.5   \n",
       "gpu_power_process_0                                                       727.260463   \n",
       "gpu_power_process_1                                                       555.824567   \n",
       "gpu_power_process_2                                                       705.838975   \n",
       "gpu_power_process_3                                                       944.800435   \n",
       "ram_power_process_0                                                         0.721634   \n",
       "ram_power_process_1                                                         0.662752   \n",
       "ram_power_process_2                                                         0.667909   \n",
       "ram_power_process_3                                                         0.675183   \n",
       "cpu_energy_process_0                                                        0.000092   \n",
       "cpu_energy_process_1                                                        0.000132   \n",
       "cpu_energy_process_2                                                        0.000093   \n",
       "cpu_energy_process_3                                                          0.0001   \n",
       "gpu_energy_process_0                                                        0.000524   \n",
       "gpu_energy_process_1                                                        0.000712   \n",
       "gpu_energy_process_2                                                        0.000531   \n",
       "gpu_energy_process_3                                                        0.000559   \n",
       "ram_energy_process_0                                                             0.0   \n",
       "ram_energy_process_1                                                        0.000001   \n",
       "ram_energy_process_2                                                             0.0   \n",
       "ram_energy_process_3                                                        0.000001   \n",
       "total_energy_kwh_process_0                                                  0.000616   \n",
       "total_energy_kwh_process_1                                                  0.000845   \n",
       "total_energy_kwh_process_2                                                  0.000624   \n",
       "total_energy_kwh_process_3                                                   0.00066   \n",
       "total_energy_joules_process_0                                            2218.098723   \n",
       "total_energy_joules_process_1                                            3042.360366   \n",
       "total_energy_joules_process_2                                            2245.070151   \n",
       "total_energy_joules_process_3                                            2376.033211   \n",
       "cpu_power_avg                                                                  112.5   \n",
       "gpu_power_avg                                                              733.43111   \n",
       "ram_power_avg                                                               0.681869   \n",
       "cpu_energy_total                                                            0.000417   \n",
       "gpu_energy_total                                                            0.002326   \n",
       "ram_energy_total                                                            0.000002   \n",
       "latency_simulation_simulate                                                    False   \n",
       "models                                                                 1034544128000   \n",
       "latency_simulation_delay_max                                                     0.0   \n",
       "total_generated_tokens                                                          1000   \n",
       "decoder_config_decoding_mode                                                   top_p   \n",
       "\n",
       "                                                                                  25  \\\n",
       "config_name                        decoding_top_p_decoder_top_p_0.9_decoder_tempe...   \n",
       "experiment_id                                                                     26   \n",
       "date_time                                              April 08, 2025 at 04:47:37 PM   \n",
       "model                                             TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                                      4   \n",
       "batch_size___fixed_batching                                                       16   \n",
       "decoder_temperature                                                              1.0   \n",
       "decoder_top_k                                                                      0   \n",
       "decoder_top_p                                                                    0.9   \n",
       "latency_simulation_delay_min                                                     0.0   \n",
       "latency_simulation_simulate_burst                                              False   \n",
       "latency_simulation_burst_size                                                      0   \n",
       "latency_simulation_burst_interval                                                0.0   \n",
       "fp_precision                                                           torch.float32   \n",
       "quantization                                                                   False   \n",
       "load_in_8bit                                                                   False   \n",
       "load_in_4bit                                                                   False   \n",
       "total_input_tokens                                                              1000   \n",
       "total_params                                                              1100048384   \n",
       "max_input_tokens                                                                 100   \n",
       "max_output_tokens                                                                100   \n",
       "number_input_prompts                                                              10   \n",
       "total_energy_kwh                                                            0.002717   \n",
       "total_energy_joules                                                      9782.421027   \n",
       "flops                                                                  1034544128000   \n",
       "tokens_per_joule                                                            0.102224   \n",
       "joules_per_token                                                            9.782421   \n",
       "flops_per_joule                                                     105755428.550843   \n",
       "joules_per_flop                                                                  0.0   \n",
       "total_inference_time_sec                                                     2.56474   \n",
       "average_latency_ms_per_batch                                             2564.739563   \n",
       "throughput_queries_per_sec                                                  3.899031   \n",
       "throughput_tokens_per_sec                                                 389.903137   \n",
       "cpu_usage_percent                                                               16.2   \n",
       "cpu_memory_usage_bytes                                                    2052939776   \n",
       "gpu_utilization_percent_0                                                       58.0   \n",
       "gpu_utilization_percent_1                                                      100.0   \n",
       "gpu_utilization_percent_2                                                      100.0   \n",
       "gpu_utilization_percent_3                                                      100.0   \n",
       "cpu_power_process_0                                                            112.5   \n",
       "cpu_power_process_1                                                            112.5   \n",
       "cpu_power_process_2                                                            112.5   \n",
       "cpu_power_process_3                                                            112.5   \n",
       "gpu_power_process_0                                                       701.330117   \n",
       "gpu_power_process_1                                                       471.022131   \n",
       "gpu_power_process_2                                                       718.217363   \n",
       "gpu_power_process_3                                                       363.407429   \n",
       "ram_power_process_0                                                         0.714984   \n",
       "ram_power_process_1                                                         0.673425   \n",
       "ram_power_process_2                                                         0.666428   \n",
       "ram_power_process_3                                                         0.667291   \n",
       "cpu_energy_process_0                                                        0.000088   \n",
       "cpu_energy_process_1                                                        0.000135   \n",
       "cpu_energy_process_2                                                        0.000091   \n",
       "cpu_energy_process_3                                                        0.000103   \n",
       "gpu_energy_process_0                                                        0.000503   \n",
       "gpu_energy_process_1                                                        0.000706   \n",
       "gpu_energy_process_2                                                        0.000517   \n",
       "gpu_energy_process_3                                                        0.000572   \n",
       "ram_energy_process_0                                                             0.0   \n",
       "ram_energy_process_1                                                        0.000001   \n",
       "ram_energy_process_2                                                             0.0   \n",
       "ram_energy_process_3                                                        0.000001   \n",
       "total_energy_kwh_process_0                                                  0.000592   \n",
       "total_energy_kwh_process_1                                                  0.000841   \n",
       "total_energy_kwh_process_2                                                  0.000608   \n",
       "total_energy_kwh_process_3                                                  0.000676   \n",
       "total_energy_joules_process_0                                            2130.240098   \n",
       "total_energy_joules_process_1                                            3028.890264   \n",
       "total_energy_joules_process_2                                            2189.903237   \n",
       "total_energy_joules_process_3                                            2433.387427   \n",
       "cpu_power_avg                                                                  112.5   \n",
       "gpu_power_avg                                                              563.49426   \n",
       "ram_power_avg                                                               0.680532   \n",
       "cpu_energy_total                                                            0.000417   \n",
       "gpu_energy_total                                                            0.002298   \n",
       "ram_energy_total                                                            0.000002   \n",
       "latency_simulation_simulate                                                    False   \n",
       "models                                                                 1034544128000   \n",
       "latency_simulation_delay_max                                                     0.0   \n",
       "total_generated_tokens                                                          1000   \n",
       "decoder_config_decoding_mode                                                   top_p   \n",
       "\n",
       "                                                                                  26  \\\n",
       "config_name                        decoding_top_p_decoder_top_p_0.9_decoder_tempe...   \n",
       "experiment_id                                                                     27   \n",
       "date_time                                              April 08, 2025 at 04:48:15 PM   \n",
       "model                                             TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                                      4   \n",
       "batch_size___fixed_batching                                                       16   \n",
       "decoder_temperature                                                              1.3   \n",
       "decoder_top_k                                                                      0   \n",
       "decoder_top_p                                                                    0.9   \n",
       "latency_simulation_delay_min                                                     0.0   \n",
       "latency_simulation_simulate_burst                                              False   \n",
       "latency_simulation_burst_size                                                      0   \n",
       "latency_simulation_burst_interval                                                0.0   \n",
       "fp_precision                                                           torch.float32   \n",
       "quantization                                                                   False   \n",
       "load_in_8bit                                                                   False   \n",
       "load_in_4bit                                                                   False   \n",
       "total_input_tokens                                                              1000   \n",
       "total_params                                                              1100048384   \n",
       "max_input_tokens                                                                 100   \n",
       "max_output_tokens                                                                100   \n",
       "number_input_prompts                                                              10   \n",
       "total_energy_kwh                                                            0.002628   \n",
       "total_energy_joules                                                      9460.731311   \n",
       "flops                                                                  1034544128000   \n",
       "tokens_per_joule                                                              0.1057   \n",
       "joules_per_token                                                            9.460731   \n",
       "flops_per_joule                                                     109351390.926845   \n",
       "joules_per_flop                                                                  0.0   \n",
       "total_inference_time_sec                                                    2.419328   \n",
       "average_latency_ms_per_batch                                             2419.327923   \n",
       "throughput_queries_per_sec                                                  4.133379   \n",
       "throughput_tokens_per_sec                                                 413.337932   \n",
       "cpu_usage_percent                                                                4.8   \n",
       "cpu_memory_usage_bytes                                                    2072227840   \n",
       "gpu_utilization_percent_0                                                       68.0   \n",
       "gpu_utilization_percent_1                                                      100.0   \n",
       "gpu_utilization_percent_2                                                      100.0   \n",
       "gpu_utilization_percent_3                                                      100.0   \n",
       "cpu_power_process_0                                                            112.5   \n",
       "cpu_power_process_1                                                            112.5   \n",
       "cpu_power_process_2                                                            112.5   \n",
       "cpu_power_process_3                                                            112.5   \n",
       "gpu_power_process_0                                                       756.244221   \n",
       "gpu_power_process_1                                                       413.266378   \n",
       "gpu_power_process_2                                                       780.127003   \n",
       "gpu_power_process_3                                                       250.763626   \n",
       "ram_power_process_0                                                         0.722484   \n",
       "ram_power_process_1                                                         0.663541   \n",
       "ram_power_process_2                                                         0.666791   \n",
       "ram_power_process_3                                                          0.67208   \n",
       "cpu_energy_process_0                                                         0.00008   \n",
       "cpu_energy_process_1                                                        0.000125   \n",
       "cpu_energy_process_2                                                         0.00008   \n",
       "cpu_energy_process_3                                                        0.000101   \n",
       "gpu_energy_process_0                                                        0.000484   \n",
       "gpu_energy_process_1                                                        0.000678   \n",
       "gpu_energy_process_2                                                        0.000492   \n",
       "gpu_energy_process_3                                                        0.000587   \n",
       "ram_energy_process_0                                                             0.0   \n",
       "ram_energy_process_1                                                        0.000001   \n",
       "ram_energy_process_2                                                             0.0   \n",
       "ram_energy_process_3                                                             0.0   \n",
       "total_energy_kwh_process_0                                                  0.000564   \n",
       "total_energy_kwh_process_1                                                  0.000803   \n",
       "total_energy_kwh_process_2                                                  0.000572   \n",
       "total_energy_kwh_process_3                                                  0.000689   \n",
       "total_energy_joules_process_0                                            2031.708541   \n",
       "total_energy_joules_process_1                                            2890.460805   \n",
       "total_energy_joules_process_2                                            2059.903488   \n",
       "total_energy_joules_process_3                                            2478.658477   \n",
       "cpu_power_avg                                                                  112.5   \n",
       "gpu_power_avg                                                             550.100307   \n",
       "ram_power_avg                                                               0.681224   \n",
       "cpu_energy_total                                                            0.000386   \n",
       "gpu_energy_total                                                             0.00224   \n",
       "ram_energy_total                                                            0.000002   \n",
       "latency_simulation_simulate                                                    False   \n",
       "models                                                                 1034544128000   \n",
       "latency_simulation_delay_max                                                     0.0   \n",
       "total_generated_tokens                                                          1000   \n",
       "decoder_config_decoding_mode                                                   top_p   \n",
       "\n",
       "                                                                   27  \\\n",
       "config_name                                             latency_False   \n",
       "experiment_id                                                      28   \n",
       "date_time                               April 08, 2025 at 04:48:49 PM   \n",
       "model                              TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                       4   \n",
       "batch_size___fixed_batching                                        16   \n",
       "decoder_temperature                                               1.0   \n",
       "decoder_top_k                                                       0   \n",
       "decoder_top_p                                                     0.0   \n",
       "latency_simulation_delay_min                                      0.0   \n",
       "latency_simulation_simulate_burst                               False   \n",
       "latency_simulation_burst_size                                       0   \n",
       "latency_simulation_burst_interval                                 0.0   \n",
       "fp_precision                                            torch.float32   \n",
       "quantization                                                    False   \n",
       "load_in_8bit                                                    False   \n",
       "load_in_4bit                                                    False   \n",
       "total_input_tokens                                               1000   \n",
       "total_params                                               1100048384   \n",
       "max_input_tokens                                                  100   \n",
       "max_output_tokens                                                 100   \n",
       "number_input_prompts                                               10   \n",
       "total_energy_kwh                                             0.003057   \n",
       "total_energy_joules                                       11004.24041   \n",
       "flops                                                   1034544128000   \n",
       "tokens_per_joule                                             0.090874   \n",
       "joules_per_token                                             11.00424   \n",
       "flops_per_joule                                       94013224.855566   \n",
       "joules_per_flop                                                   0.0   \n",
       "total_inference_time_sec                                     3.559638   \n",
       "average_latency_ms_per_batch                              3559.637986   \n",
       "throughput_queries_per_sec                                   2.809274   \n",
       "throughput_tokens_per_sec                                  280.927444   \n",
       "cpu_usage_percent                                                54.0   \n",
       "cpu_memory_usage_bytes                                     2074927104   \n",
       "gpu_utilization_percent_0                                        52.0   \n",
       "gpu_utilization_percent_1                                       100.0   \n",
       "gpu_utilization_percent_2                                       100.0   \n",
       "gpu_utilization_percent_3                                       100.0   \n",
       "cpu_power_process_0                                             112.5   \n",
       "cpu_power_process_1                                             112.5   \n",
       "cpu_power_process_2                                             112.5   \n",
       "cpu_power_process_3                                             112.5   \n",
       "gpu_power_process_0                                        551.612807   \n",
       "gpu_power_process_1                                        404.622742   \n",
       "gpu_power_process_2                                         516.23026   \n",
       "gpu_power_process_3                                        604.277492   \n",
       "ram_power_process_0                                          0.722661   \n",
       "ram_power_process_1                                          0.665721   \n",
       "ram_power_process_2                                           0.67092   \n",
       "ram_power_process_3                                          0.685136   \n",
       "cpu_energy_process_0                                         0.000119   \n",
       "cpu_energy_process_1                                         0.000135   \n",
       "cpu_energy_process_2                                         0.000103   \n",
       "cpu_energy_process_3                                         0.000117   \n",
       "gpu_energy_process_0                                         0.000646   \n",
       "gpu_energy_process_1                                         0.000715   \n",
       "gpu_energy_process_2                                         0.000575   \n",
       "gpu_energy_process_3                                         0.000646   \n",
       "ram_energy_process_0                                         0.000001   \n",
       "ram_energy_process_1                                         0.000001   \n",
       "ram_energy_process_2                                              0.0   \n",
       "ram_energy_process_3                                         0.000001   \n",
       "total_energy_kwh_process_0                                   0.000765   \n",
       "total_energy_kwh_process_1                                    0.00085   \n",
       "total_energy_kwh_process_2                                   0.000678   \n",
       "total_energy_kwh_process_3                                   0.000764   \n",
       "total_energy_joules_process_0                             2753.201388   \n",
       "total_energy_joules_process_1                             3061.352417   \n",
       "total_energy_joules_process_2                             2440.732517   \n",
       "total_energy_joules_process_3                             2748.954088   \n",
       "cpu_power_avg                                                   112.5   \n",
       "gpu_power_avg                                              519.185825   \n",
       "ram_power_avg                                                 0.68611   \n",
       "cpu_energy_total                                             0.000474   \n",
       "gpu_energy_total                                              0.00258   \n",
       "ram_energy_total                                             0.000002   \n",
       "latency_simulation_simulate                                     False   \n",
       "models                                                  1034544128000   \n",
       "latency_simulation_delay_max                                      0.0   \n",
       "total_generated_tokens                                           1000   \n",
       "decoder_config_decoding_mode                                      NaN   \n",
       "\n",
       "                                                                                  28  \\\n",
       "config_name                        latency_True_latency_0.05_latency_0.2_latency_...   \n",
       "experiment_id                                                                     29   \n",
       "date_time                                              April 08, 2025 at 04:49:27 PM   \n",
       "model                                             TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                                      4   \n",
       "batch_size___fixed_batching                                                       16   \n",
       "decoder_temperature                                                              1.0   \n",
       "decoder_top_k                                                                      0   \n",
       "decoder_top_p                                                                    0.0   \n",
       "latency_simulation_delay_min                                                    0.05   \n",
       "latency_simulation_simulate_burst                                              False   \n",
       "latency_simulation_burst_size                                                      0   \n",
       "latency_simulation_burst_interval                                                0.0   \n",
       "fp_precision                                                           torch.float32   \n",
       "quantization                                                                   False   \n",
       "load_in_8bit                                                                   False   \n",
       "load_in_4bit                                                                   False   \n",
       "total_input_tokens                                                              1000   \n",
       "total_params                                                              1100048384   \n",
       "max_input_tokens                                                                 100   \n",
       "max_output_tokens                                                                100   \n",
       "number_input_prompts                                                              10   \n",
       "total_energy_kwh                                                            0.002529   \n",
       "total_energy_joules                                                      9104.932397   \n",
       "flops                                                                  1034544128000   \n",
       "tokens_per_joule                                                            0.109831   \n",
       "joules_per_token                                                            9.104932   \n",
       "flops_per_joule                                                     113624580.931623   \n",
       "joules_per_flop                                                                  0.0   \n",
       "total_inference_time_sec                                                    2.453797   \n",
       "average_latency_ms_per_batch                                             2453.797072   \n",
       "throughput_queries_per_sec                                                  4.075317   \n",
       "throughput_tokens_per_sec                                                 407.531662   \n",
       "cpu_usage_percent                                                                5.5   \n",
       "cpu_memory_usage_bytes                                                    2070724608   \n",
       "gpu_utilization_percent_0                                                       20.0   \n",
       "gpu_utilization_percent_1                                                      100.0   \n",
       "gpu_utilization_percent_2                                                      100.0   \n",
       "gpu_utilization_percent_3                                                      100.0   \n",
       "cpu_power_process_0                                                            112.5   \n",
       "cpu_power_process_1                                                            112.5   \n",
       "cpu_power_process_2                                                            112.5   \n",
       "cpu_power_process_3                                                            112.5   \n",
       "gpu_power_process_0                                                       716.271282   \n",
       "gpu_power_process_1                                                       468.725355   \n",
       "gpu_power_process_2                                                       694.223236   \n",
       "gpu_power_process_3                                                       618.121217   \n",
       "ram_power_process_0                                                         0.721959   \n",
       "ram_power_process_1                                                         0.678437   \n",
       "ram_power_process_2                                                         0.669261   \n",
       "ram_power_process_3                                                          0.67303   \n",
       "cpu_energy_process_0                                                        0.000083   \n",
       "cpu_energy_process_1                                                        0.000124   \n",
       "cpu_energy_process_2                                                        0.000083   \n",
       "cpu_energy_process_3                                                        0.000094   \n",
       "gpu_energy_process_0                                                        0.000481   \n",
       "gpu_energy_process_1                                                        0.000654   \n",
       "gpu_energy_process_2                                                        0.000477   \n",
       "gpu_energy_process_3                                                         0.00053   \n",
       "ram_energy_process_0                                                             0.0   \n",
       "ram_energy_process_1                                                        0.000001   \n",
       "ram_energy_process_2                                                             0.0   \n",
       "ram_energy_process_3                                                             0.0   \n",
       "total_energy_kwh_process_0                                                  0.000564   \n",
       "total_energy_kwh_process_1                                                  0.000779   \n",
       "total_energy_kwh_process_2                                                  0.000561   \n",
       "total_energy_kwh_process_3                                                  0.000625   \n",
       "total_energy_joules_process_0                                            2030.512325   \n",
       "total_energy_joules_process_1                                            2805.720536   \n",
       "total_energy_joules_process_2                                            2019.285212   \n",
       "total_energy_joules_process_3                                            2249.414323   \n",
       "cpu_power_avg                                                                  112.5   \n",
       "gpu_power_avg                                                             624.335273   \n",
       "ram_power_avg                                                               0.685672   \n",
       "cpu_energy_total                                                            0.000384   \n",
       "gpu_energy_total                                                            0.002143   \n",
       "ram_energy_total                                                            0.000002   \n",
       "latency_simulation_simulate                                                     True   \n",
       "models                                                                 1034544128000   \n",
       "latency_simulation_delay_max                                                     0.2   \n",
       "total_generated_tokens                                                          1000   \n",
       "decoder_config_decoding_mode                                                     NaN   \n",
       "\n",
       "                                                                                  29  \\\n",
       "config_name                        latency_True_latency_0.2_latency_0.6_latency_F...   \n",
       "experiment_id                                                                     30   \n",
       "date_time                                              April 08, 2025 at 04:50:02 PM   \n",
       "model                                             TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                                      4   \n",
       "batch_size___fixed_batching                                                       16   \n",
       "decoder_temperature                                                              1.0   \n",
       "decoder_top_k                                                                      0   \n",
       "decoder_top_p                                                                    0.0   \n",
       "latency_simulation_delay_min                                                     0.2   \n",
       "latency_simulation_simulate_burst                                              False   \n",
       "latency_simulation_burst_size                                                      0   \n",
       "latency_simulation_burst_interval                                                0.0   \n",
       "fp_precision                                                           torch.float32   \n",
       "quantization                                                                   False   \n",
       "load_in_8bit                                                                   False   \n",
       "load_in_4bit                                                                   False   \n",
       "total_input_tokens                                                              1000   \n",
       "total_params                                                              1100048384   \n",
       "max_input_tokens                                                                 100   \n",
       "max_output_tokens                                                                100   \n",
       "number_input_prompts                                                              10   \n",
       "total_energy_kwh                                                            0.002987   \n",
       "total_energy_joules                                                     10752.746732   \n",
       "flops                                                                  1034544128000   \n",
       "tokens_per_joule                                                            0.092999   \n",
       "joules_per_token                                                           10.752747   \n",
       "flops_per_joule                                                      96212079.926237   \n",
       "joules_per_flop                                                                  0.0   \n",
       "total_inference_time_sec                                                     4.07742   \n",
       "average_latency_ms_per_batch                                             4077.420104   \n",
       "throughput_queries_per_sec                                                  2.452531   \n",
       "throughput_tokens_per_sec                                                 245.253119   \n",
       "cpu_usage_percent                                                               17.3   \n",
       "cpu_memory_usage_bytes                                                    2072039424   \n",
       "gpu_utilization_percent_0                                                        7.0   \n",
       "gpu_utilization_percent_1                                                      100.0   \n",
       "gpu_utilization_percent_2                                                      100.0   \n",
       "gpu_utilization_percent_3                                                      100.0   \n",
       "cpu_power_process_0                                                            112.5   \n",
       "cpu_power_process_1                                                            112.5   \n",
       "cpu_power_process_2                                                            112.5   \n",
       "cpu_power_process_3                                                            112.5   \n",
       "gpu_power_process_0                                                       381.983252   \n",
       "gpu_power_process_1                                                       559.126046   \n",
       "gpu_power_process_2                                                       611.717499   \n",
       "gpu_power_process_3                                                       479.491748   \n",
       "ram_power_process_0                                                         0.722185   \n",
       "ram_power_process_1                                                         0.680659   \n",
       "ram_power_process_2                                                         0.671914   \n",
       "ram_power_process_3                                                         0.671625   \n",
       "cpu_energy_process_0                                                        0.000135   \n",
       "cpu_energy_process_1                                                        0.000134   \n",
       "cpu_energy_process_2                                                        0.000109   \n",
       "cpu_energy_process_3                                                        0.000123   \n",
       "gpu_energy_process_0                                                        0.000664   \n",
       "gpu_energy_process_1                                                        0.000658   \n",
       "gpu_energy_process_2                                                        0.000547   \n",
       "gpu_energy_process_3                                                        0.000614   \n",
       "ram_energy_process_0                                                        0.000001   \n",
       "ram_energy_process_1                                                        0.000001   \n",
       "ram_energy_process_2                                                             0.0   \n",
       "ram_energy_process_3                                                        0.000001   \n",
       "total_energy_kwh_process_0                                                  0.000799   \n",
       "total_energy_kwh_process_1                                                  0.000793   \n",
       "total_energy_kwh_process_2                                                  0.000657   \n",
       "total_energy_kwh_process_3                                                  0.000737   \n",
       "total_energy_joules_process_0                                             2877.54669   \n",
       "total_energy_joules_process_1                                            2856.492483   \n",
       "total_energy_joules_process_2                                            2363.976182   \n",
       "total_energy_joules_process_3                                            2654.731377   \n",
       "cpu_power_avg                                                                  112.5   \n",
       "gpu_power_avg                                                             508.079636   \n",
       "ram_power_avg                                                               0.686596   \n",
       "cpu_energy_total                                                            0.000501   \n",
       "gpu_energy_total                                                            0.002483   \n",
       "ram_energy_total                                                            0.000003   \n",
       "latency_simulation_simulate                                                     True   \n",
       "models                                                                 1034544128000   \n",
       "latency_simulation_delay_max                                                     0.6   \n",
       "total_generated_tokens                                                          1000   \n",
       "decoder_config_decoding_mode                                                     NaN   \n",
       "\n",
       "                                                                                  30  \\\n",
       "config_name                        latency_True_latency_0.05_latency_0.2_latency_...   \n",
       "experiment_id                                                                     31   \n",
       "date_time                                              April 08, 2025 at 04:50:37 PM   \n",
       "model                                             TinyLlama/TinyLlama-1.1B-Chat-v1.0   \n",
       "num_processes                                                                      4   \n",
       "batch_size___fixed_batching                                                       16   \n",
       "decoder_temperature                                                              1.0   \n",
       "decoder_top_k                                                                      0   \n",
       "decoder_top_p                                                                    0.0   \n",
       "latency_simulation_delay_min                                                    0.05   \n",
       "latency_simulation_simulate_burst                                               True   \n",
       "latency_simulation_burst_size                                                      5   \n",
       "latency_simulation_burst_interval                                                4.0   \n",
       "fp_precision                                                           torch.float32   \n",
       "quantization                                                                   False   \n",
       "load_in_8bit                                                                   False   \n",
       "load_in_4bit                                                                   False   \n",
       "total_input_tokens                                                              1000   \n",
       "total_params                                                              1100048384   \n",
       "max_input_tokens                                                                 100   \n",
       "max_output_tokens                                                                100   \n",
       "number_input_prompts                                                              10   \n",
       "total_energy_kwh                                                            0.002321   \n",
       "total_energy_joules                                                      8354.124101   \n",
       "flops                                                                  1034544128000   \n",
       "tokens_per_joule                                                            0.119701   \n",
       "joules_per_token                                                            8.354124   \n",
       "flops_per_joule                                                     123836337.055097   \n",
       "joules_per_flop                                                                  0.0   \n",
       "total_inference_time_sec                                                    2.509436   \n",
       "average_latency_ms_per_batch                                             2509.436437   \n",
       "throughput_queries_per_sec                                                  3.984958   \n",
       "throughput_tokens_per_sec                                                 398.495848   \n",
       "cpu_usage_percent                                                                4.8   \n",
       "cpu_memory_usage_bytes                                                    2073120768   \n",
       "gpu_utilization_percent_0                                                       44.0   \n",
       "gpu_utilization_percent_1                                                      100.0   \n",
       "gpu_utilization_percent_2                                                      100.0   \n",
       "gpu_utilization_percent_3                                                      100.0   \n",
       "cpu_power_process_0                                                            112.5   \n",
       "cpu_power_process_1                                                            112.5   \n",
       "cpu_power_process_2                                                            112.5   \n",
       "cpu_power_process_3                                                            112.5   \n",
       "gpu_power_process_0                                                       615.725547   \n",
       "gpu_power_process_1                                                       310.173365   \n",
       "gpu_power_process_2                                                       631.657895   \n",
       "gpu_power_process_3                                                       796.653119   \n",
       "ram_power_process_0                                                         0.721872   \n",
       "ram_power_process_1                                                         0.673545   \n",
       "ram_power_process_2                                                         0.667791   \n",
       "ram_power_process_3                                                         0.667638   \n",
       "cpu_energy_process_0                                                        0.000084   \n",
       "cpu_energy_process_1                                                          0.0001   \n",
       "cpu_energy_process_2                                                        0.000083   \n",
       "cpu_energy_process_3                                                        0.000101   \n",
       "gpu_energy_process_0                                                        0.000445   \n",
       "gpu_energy_process_1                                                        0.000535   \n",
       "gpu_energy_process_2                                                        0.000442   \n",
       "gpu_energy_process_3                                                         0.00053   \n",
       "ram_energy_process_0                                                             0.0   \n",
       "ram_energy_process_1                                                        0.000001   \n",
       "ram_energy_process_2                                                             0.0   \n",
       "ram_energy_process_3                                                        0.000001   \n",
       "total_energy_kwh_process_0                                                  0.000529   \n",
       "total_energy_kwh_process_1                                                  0.000636   \n",
       "total_energy_kwh_process_2                                                  0.000524   \n",
       "total_energy_kwh_process_3                                                  0.000631   \n",
       "total_energy_joules_process_0                                            1905.424207   \n",
       "total_energy_joules_process_1                                            2287.806555   \n",
       "total_energy_joules_process_2                                            1887.933024   \n",
       "total_energy_joules_process_3                                            2272.960315   \n",
       "cpu_power_avg                                                                  112.5   \n",
       "gpu_power_avg                                                             588.552482   \n",
       "ram_power_avg                                                               0.682712   \n",
       "cpu_energy_total                                                            0.000368   \n",
       "gpu_energy_total                                                            0.001951   \n",
       "ram_energy_total                                                            0.000002   \n",
       "latency_simulation_simulate                                                     True   \n",
       "models                                                                 1034544128000   \n",
       "latency_simulation_delay_max                                                     0.2   \n",
       "total_generated_tokens                                                          1000   \n",
       "decoder_config_decoding_mode                                                     NaN   \n",
       "\n",
       "                                                                                  31  \n",
       "config_name                        latency_True_latency_0.2_latency_0.6_latency_T...  \n",
       "experiment_id                                                                     32  \n",
       "date_time                                              April 08, 2025 at 04:51:12 PM  \n",
       "model                                             TinyLlama/TinyLlama-1.1B-Chat-v1.0  \n",
       "num_processes                                                                      4  \n",
       "batch_size___fixed_batching                                                       16  \n",
       "decoder_temperature                                                              1.0  \n",
       "decoder_top_k                                                                      0  \n",
       "decoder_top_p                                                                    0.0  \n",
       "latency_simulation_delay_min                                                     0.2  \n",
       "latency_simulation_simulate_burst                                               True  \n",
       "latency_simulation_burst_size                                                      8  \n",
       "latency_simulation_burst_interval                                                5.0  \n",
       "fp_precision                                                           torch.float32  \n",
       "quantization                                                                   False  \n",
       "load_in_8bit                                                                   False  \n",
       "load_in_4bit                                                                   False  \n",
       "total_input_tokens                                                              1000  \n",
       "total_params                                                              1100048384  \n",
       "max_input_tokens                                                                 100  \n",
       "max_output_tokens                                                                100  \n",
       "number_input_prompts                                                              10  \n",
       "total_energy_kwh                                                            0.002472  \n",
       "total_energy_joules                                                      8900.869755  \n",
       "flops                                                                  1034544128000  \n",
       "tokens_per_joule                                                            0.112349  \n",
       "joules_per_token                                                             8.90087  \n",
       "flops_per_joule                                                     116229554.686645  \n",
       "joules_per_flop                                                                  0.0  \n",
       "total_inference_time_sec                                                    2.615047  \n",
       "average_latency_ms_per_batch                                             2615.046637  \n",
       "throughput_queries_per_sec                                                  3.824024  \n",
       "throughput_tokens_per_sec                                                 382.402358  \n",
       "cpu_usage_percent                                                                4.0  \n",
       "cpu_memory_usage_bytes                                                    2070740992  \n",
       "gpu_utilization_percent_0                                                       57.0  \n",
       "gpu_utilization_percent_1                                                      100.0  \n",
       "gpu_utilization_percent_2                                                      100.0  \n",
       "gpu_utilization_percent_3                                                       94.0  \n",
       "cpu_power_process_0                                                            112.5  \n",
       "cpu_power_process_1                                                            112.5  \n",
       "cpu_power_process_2                                                            112.5  \n",
       "cpu_power_process_3                                                            112.5  \n",
       "gpu_power_process_0                                                       717.525238  \n",
       "gpu_power_process_1                                                        533.99169  \n",
       "gpu_power_process_2                                                       734.177811  \n",
       "gpu_power_process_3                                                       514.542529  \n",
       "ram_power_process_0                                                         0.720926  \n",
       "ram_power_process_1                                                         0.674317  \n",
       "ram_power_process_2                                                         0.668052  \n",
       "ram_power_process_3                                                         0.667927  \n",
       "cpu_energy_process_0                                                        0.000086  \n",
       "cpu_energy_process_1                                                        0.000102  \n",
       "cpu_energy_process_2                                                        0.000086  \n",
       "cpu_energy_process_3                                                        0.000119  \n",
       "gpu_energy_process_0                                                         0.00046  \n",
       "gpu_energy_process_1                                                        0.000543  \n",
       "gpu_energy_process_2                                                        0.000454  \n",
       "gpu_energy_process_3                                                        0.000619  \n",
       "ram_energy_process_0                                                             0.0  \n",
       "ram_energy_process_1                                                        0.000001  \n",
       "ram_energy_process_2                                                             0.0  \n",
       "ram_energy_process_3                                                        0.000001  \n",
       "total_energy_kwh_process_0                                                  0.000547  \n",
       "total_energy_kwh_process_1                                                  0.000646  \n",
       "total_energy_kwh_process_2                                                   0.00054  \n",
       "total_energy_kwh_process_3                                                  0.000739  \n",
       "total_energy_joules_process_0                                            1968.818093  \n",
       "total_energy_joules_process_1                                               2326.154  \n",
       "total_energy_joules_process_2                                            1944.850933  \n",
       "total_energy_joules_process_3                                             2661.04673  \n",
       "cpu_power_avg                                                                  112.5  \n",
       "gpu_power_avg                                                             625.059317  \n",
       "ram_power_avg                                                               0.682806  \n",
       "cpu_energy_total                                                            0.000394  \n",
       "gpu_energy_total                                                            0.002076  \n",
       "ram_energy_total                                                            0.000002  \n",
       "latency_simulation_simulate                                                     True  \n",
       "models                                                                 1034544128000  \n",
       "latency_simulation_delay_max                                                     0.6  \n",
       "total_generated_tokens                                                          1000  \n",
       "decoder_config_decoding_mode                                                     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns_to_drop = [\n",
    "    \"sharding_strategy\",\n",
    "    \"sharding_config_fsdp_config_use_orig_params\",\n",
    "    \"sharding_config_fsdp_config_cpu_offload\",\n",
    "    \"adaptive_batching\",\n",
    "    \"adaptive_max_tokens\",\n",
    "    \"query_rate\",\n",
    "    \"is_encoder_decoder\",\n",
    "    \"task_type\",\n",
    "    \"available_gpu_count\",\n",
    "    \"gpu_model\",\n",
    "    \"available_cpu_count\",\n",
    "    \"cpu_model\",\n",
    "    \"os\",\n",
    "    \"python_version\",\n",
    "    \"country\",\n",
    "    \"region\",\n",
    "    \"distributed_type\",\n",
    "    \"decode_token_to_text\",\n",
    "    \"inference_type\",\n",
    "    \"backend\",\n",
    "    \"model_arch\",\n",
    "    \"gpu_current_memory_allocated_bytes\",\n",
    "    \"gpu_max_memory_allocated_bytes\",\n",
    "    \"gpu_current_memory_reserved_bytes\",\n",
    "    \"gpu_max_memory_reserved_bytes\",\n",
    "    \"per-process_emissions_0\", \"per-process_emissions_1\", \"per-process_emissions_2\",\"per-process_emissions_3\" # OR IS THIS NICE TO HAVE?\n",
    "]\n",
    "\n",
    "df_dropped = df_controlled.drop(columns=columns_to_drop)\n",
    "display(df_dropped.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1034544128000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check all flops are constant\n",
    "df_dropped['flops'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
