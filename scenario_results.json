[
  {
    "config": {
      "config_name": "gpu_overdrive",
      "model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
      "is_encoder_decoder": false,
      "task_type": "text_generation",
      "inference_type": "pure_generative",
      "gpu_list": [
        0,
        1,
        2,
        3
      ],
      "max_input_tokens": 100,
      "max_output_tokens": 100,
      "num_input_prompts": 100,
      "save_outputs": true,
      "decode_token_to_text": true,
      "num_processes": 4,
      "batching_options": {
        "batch_size___fixed_batching": 128,
        "adaptive_batching": false,
        "adaptive_max_tokens": 3000,
        "max_batch_size___adaptive_batching": 100
      },
      "sharding_config": {
        "fsdp_config": {
          "use_orig_params": false,
          "cpu_offload": false
        },
        "sharding_strategy": "NO_SHARD"
      },
      "query_rate": 1.0,
      "latency_simulation": {
        "simulate": false,
        "delay_min": 0,
        "delay_max": 0,
        "simulate_burst": false,
        "burst_interval": 0,
        "burst_size": 0
      },
      "decoder_temperature": 1.0,
      "fp_precision": "float16",
      "quantization_config": {
        "quantization": false,
        "load_in_8bit": false,
        "load_in_4bit": false,
        "cached_flops_for_quantised_models": null
      },
      "backend": "pytorch",
      "scenario_name": "GPU Overdrive"
    },
    "success": false,
    "attempt": 4,
    "error": "Command '['accelerate', 'launch', 'run_experiment.py', '--config', '{\"config_name\": \"gpu_overdrive\", \"model_name\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"is_encoder_decoder\": false, \"task_type\": \"text_generation\", \"inference_type\": \"pure_generative\", \"gpu_list\": [0, 1, 2, 3], \"max_input_tokens\": 100, \"max_output_tokens\": 100, \"num_input_prompts\": 100, \"save_outputs\": true, \"decode_token_to_text\": true, \"num_processes\": 4, \"batching_options\": {\"batch_size___fixed_batching\": 128, \"adaptive_batching\": false, \"adaptive_max_tokens\": 3000, \"max_batch_size___adaptive_batching\": 100}, \"sharding_config\": {\"fsdp_config\": {\"use_orig_params\": false, \"cpu_offload\": false}, \"sharding_strategy\": \"NO_SHARD\"}, \"query_rate\": 1.0, \"latency_simulation\": {\"simulate\": false, \"delay_min\": 0, \"delay_max\": 0, \"simulate_burst\": false, \"burst_interval\": 0, \"burst_size\": 0}, \"decoder_temperature\": 1.0, \"fp_precision\": \"float16\", \"quantization_config\": {\"quantization\": false, \"load_in_8bit\": false, \"load_in_4bit\": false, \"cached_flops_for_quantised_models\": null}, \"backend\": \"pytorch\", \"scenario_name\": \"GPU Overdrive\"}']' returned non-zero exit status 1."
  },
  {
    "config": {
      "config_name": "precision_gaming",
      "model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
      "is_encoder_decoder": false,
      "task_type": "text_generation",
      "inference_type": "pure_generative",
      "gpu_list": [
        0,
        1,
        2,
        3
      ],
      "max_input_tokens": 100,
      "max_output_tokens": 100,
      "num_input_prompts": 100,
      "save_outputs": true,
      "decode_token_to_text": true,
      "num_processes": 4,
      "batching_options": {
        "batch_size___fixed_batching": 128,
        "adaptive_batching": false,
        "adaptive_max_tokens": 3000,
        "max_batch_size___adaptive_batching": 100
      },
      "sharding_config": {
        "fsdp_config": {
          "use_orig_params": false,
          "cpu_offload": false
        },
        "sharding_strategy": "NO_SHARD"
      },
      "query_rate": 1.0,
      "latency_simulation": {
        "simulate": false,
        "delay_min": 0,
        "delay_max": 0,
        "simulate_burst": false,
        "burst_interval": 0,
        "burst_size": 0
      },
      "decoder_temperature": 1.0,
      "fp_precision": "float16",
      "quantization_config": {
        "quantization": true,
        "load_in_8bit": false,
        "load_in_4bit": true,
        "cached_flops_for_quantised_models": 10345441280000
      },
      "backend": "pytorch",
      "scenario_name": "Precision Gaming"
    },
    "success": false,
    "attempt": 4,
    "error": "Command '['accelerate', 'launch', 'run_experiment.py', '--config', '{\"config_name\": \"precision_gaming\", \"model_name\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"is_encoder_decoder\": false, \"task_type\": \"text_generation\", \"inference_type\": \"pure_generative\", \"gpu_list\": [0, 1, 2, 3], \"max_input_tokens\": 100, \"max_output_tokens\": 100, \"num_input_prompts\": 100, \"save_outputs\": true, \"decode_token_to_text\": true, \"num_processes\": 4, \"batching_options\": {\"batch_size___fixed_batching\": 128, \"adaptive_batching\": false, \"adaptive_max_tokens\": 3000, \"max_batch_size___adaptive_batching\": 100}, \"sharding_config\": {\"fsdp_config\": {\"use_orig_params\": false, \"cpu_offload\": false}, \"sharding_strategy\": \"NO_SHARD\"}, \"query_rate\": 1.0, \"latency_simulation\": {\"simulate\": false, \"delay_min\": 0, \"delay_max\": 0, \"simulate_burst\": false, \"burst_interval\": 0, \"burst_size\": 0}, \"decoder_temperature\": 1.0, \"fp_precision\": \"float16\", \"quantization_config\": {\"quantization\": true, \"load_in_8bit\": false, \"load_in_4bit\": true, \"cached_flops_for_quantised_models\": 10345441280000}, \"backend\": \"pytorch\", \"scenario_name\": \"Precision Gaming\"}']' returned non-zero exit status 1."
  },
  {
    "config": {
      "config_name": "latency_real_time",
      "model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
      "is_encoder_decoder": false,
      "task_type": "text_generation",
      "inference_type": "pure_generative",
      "max_input_tokens": 100,
      "max_output_tokens": 100,
      "num_input_prompts": 100,
      "save_outputs": true,
      "decode_token_to_text": true,
      "gpu_list": [
        0,
        1,
        2,
        3
      ],
      "num_processes": 4,
      "batching_options": {
        "batch_size___fixed_batching": 4,
        "adaptive_batching": false,
        "adaptive_max_tokens": 3000,
        "max_batch_size___adaptive_batching": 100
      },
      "sharding_config": {
        "fsdp_config": {
          "use_orig_params": false,
          "cpu_offload": false
        },
        "sharding_strategy": "NO_SHARD"
      },
      "query_rate": 1.0,
      "latency_simulation": {
        "simulate": true,
        "delay_min": 0.01,
        "delay_max": 0.05,
        "simulate_burst": false
      },
      "decoder_temperature": 1.0,
      "fp_precision": "float32",
      "quantization_config": {
        "quantization": false,
        "load_in_8bit": false,
        "load_in_4bit": false,
        "cached_flops_for_quantised_models": null
      },
      "backend": "pytorch",
      "scenario_name": "Low-Latency Real-Time"
    },
    "success": false,
    "attempt": 4,
    "error": "Command '['accelerate', 'launch', 'run_experiment.py', '--config', '{\"config_name\": \"latency_real_time\", \"model_name\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"is_encoder_decoder\": false, \"task_type\": \"text_generation\", \"inference_type\": \"pure_generative\", \"max_input_tokens\": 100, \"max_output_tokens\": 100, \"num_input_prompts\": 100, \"save_outputs\": true, \"decode_token_to_text\": true, \"gpu_list\": [0, 1, 2, 3], \"num_processes\": 4, \"batching_options\": {\"batch_size___fixed_batching\": 4, \"adaptive_batching\": false, \"adaptive_max_tokens\": 3000, \"max_batch_size___adaptive_batching\": 100}, \"sharding_config\": {\"fsdp_config\": {\"use_orig_params\": false, \"cpu_offload\": false}, \"sharding_strategy\": \"NO_SHARD\"}, \"query_rate\": 1.0, \"latency_simulation\": {\"simulate\": true, \"delay_min\": 0.01, \"delay_max\": 0.05, \"simulate_burst\": false}, \"decoder_temperature\": 1.0, \"fp_precision\": \"float32\", \"quantization_config\": {\"quantization\": false, \"load_in_8bit\": false, \"load_in_4bit\": false, \"cached_flops_for_quantised_models\": null}, \"backend\": \"pytorch\", \"scenario_name\": \"Low-Latency Real-Time\"}']' returned non-zero exit status 1."
  },
  {
    "config": {
      "config_name": "balanced_performance_mode",
      "model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
      "is_encoder_decoder": false,
      "task_type": "text_generation",
      "inference_type": "pure_generative",
      "max_input_tokens": 100,
      "max_output_tokens": 100,
      "num_input_prompts": 100,
      "save_outputs": true,
      "decode_token_to_text": true,
      "gpu_list": [
        0,
        1,
        2,
        3
      ],
      "num_processes": 4,
      "batching_options": {
        "batch_size___fixed_batching": 32,
        "adaptive_batching": false,
        "adaptive_max_tokens": 3000,
        "max_batch_size___adaptive_batching": 100
      },
      "sharding_config": {
        "fsdp_config": {
          "use_orig_params": false,
          "cpu_offload": false
        },
        "sharding_strategy": "NO_SHARD"
      },
      "query_rate": 1.0,
      "latency_simulation": {
        "simulate": true,
        "delay_min": 0.02,
        "delay_max": 0.1,
        "simulate_burst": true,
        "burst_interval": 1.5,
        "burst_size": 3
      },
      "decoder_temperature": 1.0,
      "fp_precision": "float16",
      "quantization_config": {
        "quantization": true,
        "load_in_8bit": true,
        "load_in_4bit": false,
        "cached_flops_for_quantised_models": 10345441280000
      },
      "backend": "pytorch",
      "scenario_name": "Balanced Performance Mode"
    },
    "success": false,
    "attempt": 4,
    "error": "Command '['accelerate', 'launch', 'run_experiment.py', '--config', '{\"config_name\": \"balanced_performance_mode\", \"model_name\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"is_encoder_decoder\": false, \"task_type\": \"text_generation\", \"inference_type\": \"pure_generative\", \"max_input_tokens\": 100, \"max_output_tokens\": 100, \"num_input_prompts\": 100, \"save_outputs\": true, \"decode_token_to_text\": true, \"gpu_list\": [0, 1, 2, 3], \"num_processes\": 4, \"batching_options\": {\"batch_size___fixed_batching\": 32, \"adaptive_batching\": false, \"adaptive_max_tokens\": 3000, \"max_batch_size___adaptive_batching\": 100}, \"sharding_config\": {\"fsdp_config\": {\"use_orig_params\": false, \"cpu_offload\": false}, \"sharding_strategy\": \"NO_SHARD\"}, \"query_rate\": 1.0, \"latency_simulation\": {\"simulate\": true, \"delay_min\": 0.02, \"delay_max\": 0.1, \"simulate_burst\": true, \"burst_interval\": 1.5, \"burst_size\": 3}, \"decoder_temperature\": 1.0, \"fp_precision\": \"float16\", \"quantization_config\": {\"quantization\": true, \"load_in_8bit\": true, \"load_in_4bit\": false, \"cached_flops_for_quantised_models\": 10345441280000}, \"backend\": \"pytorch\", \"scenario_name\": \"Balanced Performance Mode\"}']' returned non-zero exit status 1."
  },
  {
    "config": {
      "config_name": "standard_production",
      "model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
      "is_encoder_decoder": false,
      "task_type": "text_generation",
      "inference_type": "pure_generative",
      "max_input_tokens": 100,
      "max_output_tokens": 100,
      "num_input_prompts": 100,
      "save_outputs": true,
      "decode_token_to_text": true,
      "gpu_list": [
        0,
        1,
        2,
        3
      ],
      "num_processes": 4,
      "batching_options": {
        "batch_size___fixed_batching": 16,
        "adaptive_batching": false,
        "adaptive_max_tokens": 3000,
        "max_batch_size___adaptive_batching": 100
      },
      "sharding_config": {
        "fsdp_config": {
          "use_orig_params": false,
          "cpu_offload": false
        },
        "sharding_strategy": "NO_SHARD"
      },
      "query_rate": 1.0,
      "latency_simulation": {
        "simulate": true,
        "delay_min": 1,
        "delay_max": 2,
        "simulate_burst": true,
        "burst_interval": 4.0,
        "burst_size": 5
      },
      "decoder_temperature": 1.0,
      "fp_precision": "float32",
      "quantization_config": {
        "quantization": false,
        "load_in_8bit": false,
        "load_in_4bit": false,
        "cached_flops_for_quantised_models": null
      },
      "backend": "pytorch",
      "scenario_name": "Standard Production"
    },
    "success": false,
    "attempt": 4,
    "error": "Command '['accelerate', 'launch', 'run_experiment.py', '--config', '{\"config_name\": \"standard_production\", \"model_name\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"is_encoder_decoder\": false, \"task_type\": \"text_generation\", \"inference_type\": \"pure_generative\", \"max_input_tokens\": 100, \"max_output_tokens\": 100, \"num_input_prompts\": 100, \"save_outputs\": true, \"decode_token_to_text\": true, \"gpu_list\": [0, 1, 2, 3], \"num_processes\": 4, \"batching_options\": {\"batch_size___fixed_batching\": 16, \"adaptive_batching\": false, \"adaptive_max_tokens\": 3000, \"max_batch_size___adaptive_batching\": 100}, \"sharding_config\": {\"fsdp_config\": {\"use_orig_params\": false, \"cpu_offload\": false}, \"sharding_strategy\": \"NO_SHARD\"}, \"query_rate\": 1.0, \"latency_simulation\": {\"simulate\": true, \"delay_min\": 1, \"delay_max\": 2, \"simulate_burst\": true, \"burst_interval\": 4.0, \"burst_size\": 5}, \"decoder_temperature\": 1.0, \"fp_precision\": \"float32\", \"quantization_config\": {\"quantization\": false, \"load_in_8bit\": false, \"load_in_4bit\": false, \"cached_flops_for_quantised_models\": null}, \"backend\": \"pytorch\", \"scenario_name\": \"Standard Production\"}']' returned non-zero exit status 1."
  },
  {
    "config": {
      "config_name": "max_throughput_exploit",
      "model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
      "is_encoder_decoder": false,
      "task_type": "text_generation",
      "inference_type": "pure_generative",
      "gpu_list": [
        0,
        1,
        2,
        3
      ],
      "max_input_tokens": 100,
      "max_output_tokens": 100,
      "num_input_prompts": 100,
      "save_outputs": true,
      "decode_token_to_text": true,
      "num_processes": 4,
      "batching_options": {
        "batch_size___fixed_batching": 256,
        "adaptive_batching": false,
        "adaptive_max_tokens": 3000,
        "max_batch_size___adaptive_batching": 100
      },
      "sharding_config": {
        "fsdp_config": {
          "use_orig_params": false,
          "cpu_offload": false
        },
        "sharding_strategy": "NO_SHARD"
      },
      "query_rate": 1.0,
      "latency_simulation": {
        "simulate": false,
        "delay_min": 0,
        "delay_max": 0,
        "simulate_burst": false,
        "burst_interval": 0,
        "burst_size": 0
      },
      "decoder_temperature": 1.0,
      "fp_precision": "float16",
      "quantization_config": {
        "quantization": true,
        "load_in_8bit": true,
        "load_in_4bit": false,
        "cached_flops_for_quantised_models": 10345441280000
      },
      "backend": "pytorch",
      "scenario_name": "Max Throughput Exploit"
    },
    "success": false,
    "attempt": 4,
    "error": "Command '['accelerate', 'launch', 'run_experiment.py', '--config', '{\"config_name\": \"max_throughput_exploit\", \"model_name\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"is_encoder_decoder\": false, \"task_type\": \"text_generation\", \"inference_type\": \"pure_generative\", \"gpu_list\": [0, 1, 2, 3], \"max_input_tokens\": 100, \"max_output_tokens\": 100, \"num_input_prompts\": 100, \"save_outputs\": true, \"decode_token_to_text\": true, \"num_processes\": 4, \"batching_options\": {\"batch_size___fixed_batching\": 256, \"adaptive_batching\": false, \"adaptive_max_tokens\": 3000, \"max_batch_size___adaptive_batching\": 100}, \"sharding_config\": {\"fsdp_config\": {\"use_orig_params\": false, \"cpu_offload\": false}, \"sharding_strategy\": \"NO_SHARD\"}, \"query_rate\": 1.0, \"latency_simulation\": {\"simulate\": false, \"delay_min\": 0, \"delay_max\": 0, \"simulate_burst\": false, \"burst_interval\": 0, \"burst_size\": 0}, \"decoder_temperature\": 1.0, \"fp_precision\": \"float16\", \"quantization_config\": {\"quantization\": true, \"load_in_8bit\": true, \"load_in_4bit\": false, \"cached_flops_for_quantised_models\": 10345441280000}, \"backend\": \"pytorch\", \"scenario_name\": \"Max Throughput Exploit\"}']' returned non-zero exit status 1."
  }
]