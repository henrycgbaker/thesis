{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Prompts\n",
    "ds = load_dataset(\"lighteval/pile_helm\", \"arxiv\")[\"test\"]\n",
    "ds = ds.select(range(10))\n",
    "prompts = [sample[\"text\"] for sample in ds]\n",
    "\n",
    "backend = \"pytorch\"\n",
    "\n",
    "use_optimum = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorth_experiment_fns import *\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "def text_gen_experiment():\n",
    "    run_experiment(\n",
    "        model_name=model_name,\n",
    "        prompts=prompts,\n",
    "        inference_fn=run_gen_inference_with_metrics,\n",
    "        task_type=\"text_generation\",\n",
    "        backend=backend,\n",
    "        use_optimum=use_optimum,\n",
    "        max_input_tokens=512,\n",
    "        max_output_tokens=50,\n",
    "        batch_size=8\n",
    "    )\n",
    "\n",
    "notebook_launcher(text_gen_experiment, num_processes=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM\n",
    "\n",
    "NB: memory problems with vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_experiment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_experiment_vllm(\n",
    "    model_name=model_name,\n",
    "    prompts=prompts,\n",
    "    task_type=\"text_generation\",\n",
    "    max_input_tokens=512,\n",
    "    max_output_tokens=50,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum_benchmark import Benchmark, BenchmarkConfig, TorchrunConfig, InferenceConfig, PyTorchConfig\n",
    "from optimum_benchmark.logging_utils import setup_logging\n",
    "\n",
    "setup_logging(level=\"INFO\", handlers=[\"console\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    launcher_config = TorchrunConfig(nproc_per_node=2)\n",
    "    scenario_config = InferenceConfig(latency=True, memory=True)\n",
    "    backend_config = PyTorchConfig(model=\"gpt2\", device=\"cuda\", device_ids=\"0,1\", no_weights=True)\n",
    "    benchmark_config = BenchmarkConfig(\n",
    "        name=\"pytorch_gpt2\",\n",
    "        scenario=scenario_config,\n",
    "        launcher=launcher_config,\n",
    "        backend=backend_config,\n",
    "    )\n",
    "    benchmark_report = Benchmark.launch(benchmark_config)\n",
    "\n",
    "    # convert artifacts to a dictionary or dataframe\n",
    "    benchmark_config.to_dict() # or benchmark_config.to_dataframe()\n",
    "\n",
    "    # save artifacts to disk as json or csv files\n",
    "    benchmark_report.save_csv(\"benchmark_report.csv\") # or benchmark_report.save_json(\"benchmark_report.json\")\n",
    "\n",
    "    # push artifacts to the hub\n",
    "    benchmark_config.push_to_hub(\"IlyasMoutawwakil/pytorch_gpt2\") # or benchmark_config.push_to_hub(\"IlyasMoutawwakil/pytorch_gpt2\")\n",
    "\n",
    "    # or merge them into a single artifact\n",
    "    benchmark = Benchmark(config=benchmark_config, report=benchmark_report)\n",
    "    benchmark.save_json(\"benchmark.json\") # or benchmark.save_csv(\"benchmark.csv\")\n",
    "    benchmark.push_to_hub(\"IlyasMoutawwakil/pytorch_gpt2\")\n",
    "\n",
    "    # load artifacts from the hub\n",
    "    benchmark = Benchmark.from_hub(\"IlyasMoutawwakil/pytorch_gpt2\") # or Benchmark.from_hub(\"IlyasMoutawwakil/pytorch_gpt2\")\n",
    "\n",
    "    # or load them from disk\n",
    "    benchmark = Benchmark.load_json(\"benchmark.json\") # or Benchmark.load_csv(\"benchmark_report.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
