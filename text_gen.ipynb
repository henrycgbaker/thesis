{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Prompts\n",
    "ds = load_dataset(\"lighteval/pile_helm\", \"arxiv\")[\"test\"]\n",
    "ds = ds.select(range(5))\n",
    "prompts = [sample[\"text\"] for sample in ds]\n",
    "\n",
    "backend = \"pytorch\"\n",
    "\n",
    "use_optimum = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n",
      "Using device: cuda:0 (Local Rank: 0)\n",
      "Model is on cuda:1Using 2 GPUs: [0, 1]\n",
      "\n",
      "Model is on cuda:0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 19:32:58] Multiple instances of codecarbon are allowed to run at the same time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 19:32:58] [setup] RAM Tracking...\n",
      "[codecarbon WARNING @ 19:32:58] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 19:32:58] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 19:32:58] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 19:32:58] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 19:32:58] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 19:32:58] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 19:32:59] CPU Model on constant consumption mode: AMD EPYC 7742 64-Core Processor\n",
      "[codecarbon INFO @ 19:32:59] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 19:32:59] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 19:32:59] CPU Model on constant consumption mode: AMD EPYC 7742 64-Core Processor\n",
      "[codecarbon INFO @ 19:32:59] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 19:32:59] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 19:33:00] GPU number 2 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:33:00] GPU number 2 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:33:00] GPU number 3 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:33:00] GPU number 3 will not be monitored, at your request.\n",
      "[codecarbon WARNING @ 19:33:00] You have 4 GPUs but we will monitor only 2 of them. Check your configuration.\n",
      "[codecarbon WARNING @ 19:33:00] You have 4 GPUs but we will monitor only 2 of them. Check your configuration.\n",
      "[codecarbon INFO @ 19:33:00] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 19:33:00] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 19:33:00]   Platform system: Linux-5.15.0-113-generic-x86_64-with-glibc2.31\n",
      "[codecarbon INFO @ 19:33:00]   Platform system: Linux-5.15.0-113-generic-x86_64-with-glibc2.31\n",
      "[codecarbon INFO @ 19:33:00]   Python version: 3.10.14\n",
      "[codecarbon INFO @ 19:33:00]   Python version: 3.10.14\n",
      "[codecarbon INFO @ 19:33:00]   CodeCarbon version: 2.8.3\n",
      "[codecarbon INFO @ 19:33:00]   CodeCarbon version: 2.8.3\n",
      "[codecarbon INFO @ 19:33:00]   Available RAM : 503.532 GB\n",
      "[codecarbon INFO @ 19:33:00]   Available RAM : 503.532 GB\n",
      "[codecarbon INFO @ 19:33:00]   CPU count: 128\n",
      "[codecarbon INFO @ 19:33:00]   CPU count: 128\n",
      "[codecarbon INFO @ 19:33:00]   CPU model: AMD EPYC 7742 64-Core Processor\n",
      "[codecarbon INFO @ 19:33:00]   CPU model: AMD EPYC 7742 64-Core Processor\n",
      "[codecarbon INFO @ 19:33:00]   GPU count: 2\n",
      "[codecarbon INFO @ 19:33:00]   GPU count: 2\n",
      "[codecarbon INFO @ 19:33:00]   GPU model: 4 x NVIDIA A100-PCIE-40GB BUT only tracking these GPU ids : [0, 1]\n",
      "[codecarbon INFO @ 19:33:00]   GPU model: 4 x NVIDIA A100-PCIE-40GB BUT only tracking these GPU ids : [0, 1]\n",
      "[codecarbon INFO @ 19:33:03] Saving emissions data to file /home/228755@hertie-school.lan/thesis/emissions.csv\n",
      "[codecarbon INFO @ 19:33:03] Saving emissions data to file /home/228755@hertie-school.lan/thesis/emissions.csv\n",
      "[codecarbon INFO @ 19:33:04] Energy consumed for RAM : 0.000053 kWh. RAM Power : 188.8243260383606 W\n",
      "[codecarbon INFO @ 19:33:04] Energy consumed for all CPUs : 0.000032 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 19:33:04] Energy consumed for RAM : 0.000053 kWh. RAM Power : 188.8243260383606 W\n",
      "[codecarbon INFO @ 19:33:04] GPU number 2 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:33:04] Energy consumed for all CPUs : 0.000032 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 19:33:04] GPU number 3 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:33:04] GPU number 2 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:33:04] GPU number 3 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:33:05] Energy consumed for all GPUs : 0.000041 kWh. Total GPU Power : 145.6802656591284 W\n",
      "[codecarbon INFO @ 19:33:05] 0.000127 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:33:05] Energy consumed for all GPUs : 0.000041 kWh. Total GPU Power : 145.7483626454289 W\n",
      "[codecarbon INFO @ 19:33:05] 0.000127 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:33:05] Energy consumed for RAM : 0.000069 kWh. RAM Power : 188.8243260383606 W\n",
      "[codecarbon INFO @ 19:33:05] Energy consumed for all CPUs : 0.000041 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 19:33:05] Energy consumed for RAM : 0.000069 kWh. RAM Power : 188.8243260383606 W\n",
      "[codecarbon INFO @ 19:33:05] GPU number 2 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:33:05] GPU number 3 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:33:05] Energy consumed for all CPUs : 0.000041 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 19:33:05] GPU number 2 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:33:05] GPU number 3 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:33:05] Energy consumed for all GPUs : 0.000058 kWh. Total GPU Power : 197.55268305879446 W\n",
      "[codecarbon INFO @ 19:33:05] 0.000167 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:33:05] Energy consumed for all GPUs : 0.000058 kWh. Total GPU Power : 196.46758923740586 W\n",
      "[codecarbon INFO @ 19:33:05] 0.000167 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:33:06] Energy consumed for RAM : 0.000115 kWh. RAM Power : 188.8243260383606 W\n",
      "[codecarbon INFO @ 19:33:06] Energy consumed for all CPUs : 0.000069 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 19:33:06] GPU number 2 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:33:06] GPU number 3 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:33:06] Energy consumed for RAM : 0.000115 kWh. RAM Power : 188.8243260383606 W\n",
      "[codecarbon INFO @ 19:33:06] Energy consumed for all CPUs : 0.000069 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 19:33:06] GPU number 2 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:33:06] GPU number 3 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:33:07] Energy consumed for all GPUs : 0.000112 kWh. Total GPU Power : 218.33001198815742 W\n",
      "[codecarbon INFO @ 19:33:07] 0.000296 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:33:07] Energy consumed for all GPUs : 0.000112 kWh. Total GPU Power : 218.67341798767666 W\n",
      "[codecarbon INFO @ 19:33:07] 0.000296 kWh of electricity used since the beginning.\n",
      "[codecarbon WARNING @ 19:33:07] graceful shutdown. Exceptions:\n",
      "[codecarbon WARNING @ 19:33:07] <class 'Exception'>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/codecarbon/core/util.py\", line 24, in suppress\n",
      "    yield\n",
      "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/codecarbon/emissions_tracker.py\", line 540, in stop\n",
      "    self._persist_data(\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/codecarbon/emissions_tracker.py\", line 561, in _persist_data\n",
      "    handler.out(total_emissions, delta_emissions)\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/codecarbon/output_methods/file.py\", line 54, in out\n",
      "    df = pd.read_csv(self.save_file_path)\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 620, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1898, in _make_engine\n",
      "    return mapping[engine](f, **self.options)\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 93, in __init__\n",
      "    self._reader = parsers.TextReader(src, **kwds)\n",
      "  File \"parsers.pyx\", line 581, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "[codecarbon WARNING @ 19:33:07] stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMD CPU detected. Using AMD energy monitoring (or custom estimation).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0304 19:33:08.179000 3220394 torch/multiprocessing/spawn.py:160] Terminating process 3220576 via signal SIGTERM\n",
      "E0304 19:33:08.636000 3220394 torch/distributed/elastic/multiprocessing/api.py:732] failed (exitcode: 1) local_rank: 0 (pid: 3220575) of fn: <lambda> (start_method: fork)\n",
      "E0304 19:33:08.636000 3220394 torch/distributed/elastic/multiprocessing/api.py:732] Traceback (most recent call last):\n",
      "E0304 19:33:08.636000 3220394 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py\", line 687, in _poll\n",
      "E0304 19:33:08.636000 3220394 torch/distributed/elastic/multiprocessing/api.py:732]     self._pc.join(-1)\n",
      "E0304 19:33:08.636000 3220394 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 203, in join\n",
      "E0304 19:33:08.636000 3220394 torch/distributed/elastic/multiprocessing/api.py:732]     raise ProcessRaisedException(msg, error_index, failed_process.pid)\n",
      "E0304 19:33:08.636000 3220394 torch/distributed/elastic/multiprocessing/api.py:732] torch.multiprocessing.spawn.ProcessRaisedException: \n",
      "E0304 19:33:08.636000 3220394 torch/distributed/elastic/multiprocessing/api.py:732] \n",
      "E0304 19:33:08.636000 3220394 torch/distributed/elastic/multiprocessing/api.py:732] -- Process 0 terminated with the following error:\n",
      "E0304 19:33:08.636000 3220394 torch/distributed/elastic/multiprocessing/api.py:732] Traceback (most recent call last):\n",
      "E0304 19:33:08.636000 3220394 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 90, in _wrap\n",
      "E0304 19:33:08.636000 3220394 torch/distributed/elastic/multiprocessing/api.py:732]     fn(i, *args)\n",
      "E0304 19:33:08.636000 3220394 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py\", line 611, in _wrap\n",
      "E0304 19:33:08.636000 3220394 torch/distributed/elastic/multiprocessing/api.py:732]     ret = record(fn)(*args_)\n",
      "E0304 19:33:08.636000 3220394 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "E0304 19:33:08.636000 3220394 torch/distributed/elastic/multiprocessing/api.py:732]     return f(*args, **kwargs)\n",
      "E0304 19:33:08.636000 3220394 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/tmp/ipykernel_3220394/94175995.py\", line 9, in <lambda>\n",
      "E0304 19:33:08.636000 3220394 torch/distributed/elastic/multiprocessing/api.py:732]     lambda: run_experiment(\n",
      "E0304 19:33:08.636000 3220394 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/home/228755@hertie-school.lan/thesis/pytorth_experiment_fns.py\", line 428, in run_experiment\n",
      "E0304 19:33:08.636000 3220394 torch/distributed/elastic/multiprocessing/api.py:732]     codecarbon_data = stop_energy_tracking(tracker)\n",
      "E0304 19:33:08.636000 3220394 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/home/228755@hertie-school.lan/thesis/pytorth_experiment_fns.py\", line 88, in stop_energy_tracking\n",
      "E0304 19:33:08.636000 3220394 torch/distributed/elastic/multiprocessing/api.py:732]     codecarbon_data = tracker.final_emissions_data\n",
      "E0304 19:33:08.636000 3220394 torch/distributed/elastic/multiprocessing/api.py:732] AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'. Did you mean: '_prepare_emissions_data'?\n",
      "E0304 19:33:08.636000 3220394 torch/distributed/elastic/multiprocessing/api.py:732] \n"
     ]
    },
    {
     "ename": "ChildFailedError",
     "evalue": "\n============================================================\n<lambda> FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-03-04_19:33:07\n  host      : ds01\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 3220575)\n  error_file: /tmp/torchelastic_s_f36qmm/none_ikoa4wxx/attempt_0/0/error.json\n  traceback : Traceback (most recent call last):\n    File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n      return f(*args, **kwargs)\n    File \"/tmp/ipykernel_3220394/94175995.py\", line 9, in <lambda>\n      lambda: run_experiment(\n    File \"/home/228755@hertie-school.lan/thesis/pytorth_experiment_fns.py\", line 428, in run_experiment\n      codecarbon_data = stop_energy_tracking(tracker)\n    File \"/home/228755@hertie-school.lan/thesis/pytorth_experiment_fns.py\", line 88, in stop_energy_tracking\n      codecarbon_data = tracker.final_emissions_data\n  AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'. Did you mean: '_prepare_emissions_data'?\n  \n============================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mChildFailedError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Restrict to GPUs 0 and 1\u001b[39;00m\n\u001b[1;32m      6\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0,1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_gen_inference_with_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext_generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpytorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_optimum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_input_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/accelerate/launchers.py:244\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes, rdzv_backend, rdzv_endpoint, rdzv_conf, rdzv_id, max_restarts, monitor_interval, log_line_prefix_template)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;124m\"\u001b[39m, ELASTIC_LOG_LINE_PREFIX_TEMPLATE_PYTORCH_VERSION):\n\u001b[1;32m    243\u001b[0m         launch_config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_line_prefix_template\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m log_line_prefix_template\n\u001b[0;32m--> 244\u001b[0m     \u001b[43melastic_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLaunchConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlaunch_config_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentrypoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProcessRaisedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/distributed/launcher/api.py:138\u001b[0m, in \u001b[0;36melastic_launch.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlaunch_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_entrypoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/distributed/launcher/api.py:269\u001b[0m, in \u001b[0;36mlaunch_agent\u001b[0;34m(config, entrypoint, args)\u001b[0m\n\u001b[1;32m    262\u001b[0m     events\u001b[38;5;241m.\u001b[39mrecord(agent\u001b[38;5;241m.\u001b[39mget_event_succeeded())\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mis_failed():\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;66;03m# ChildFailedError is treated specially by @record\u001b[39;00m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;66;03m# if the error files for the failed children exist\u001b[39;00m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;66;03m# @record will copy the first error (root cause)\u001b[39;00m\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;66;03m# to the error file of the launcher process.\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChildFailedError(\n\u001b[1;32m    270\u001b[0m             name\u001b[38;5;241m=\u001b[39mentrypoint_name,\n\u001b[1;32m    271\u001b[0m             failures\u001b[38;5;241m=\u001b[39mresult\u001b[38;5;241m.\u001b[39mfailures,\n\u001b[1;32m    272\u001b[0m         )\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreturn_values\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ChildFailedError:\n",
      "\u001b[0;31mChildFailedError\u001b[0m: \n============================================================\n<lambda> FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-03-04_19:33:07\n  host      : ds01\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 3220575)\n  error_file: /tmp/torchelastic_s_f36qmm/none_ikoa4wxx/attempt_0/0/error.json\n  traceback : Traceback (most recent call last):\n    File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n      return f(*args, **kwargs)\n    File \"/tmp/ipykernel_3220394/94175995.py\", line 9, in <lambda>\n      lambda: run_experiment(\n    File \"/home/228755@hertie-school.lan/thesis/pytorth_experiment_fns.py\", line 428, in run_experiment\n      codecarbon_data = stop_energy_tracking(tracker)\n    File \"/home/228755@hertie-school.lan/thesis/pytorth_experiment_fns.py\", line 88, in stop_energy_tracking\n      codecarbon_data = tracker.final_emissions_data\n  AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'. Did you mean: '_prepare_emissions_data'?\n  \n============================================================"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pytorth_experiment_fns import *\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "# Restrict to GPUs 0 and 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "notebook_launcher(\n",
    "    lambda: run_experiment(\n",
    "        model_name=model_name,\n",
    "        prompts=prompts,\n",
    "        inference_fn=run_gen_inference_with_metrics,\n",
    "        task_type=\"text_generation\",\n",
    "        backend=\"pytorch\",\n",
    "        use_optimum=False,\n",
    "        max_input_tokens=512,\n",
    "        max_output_tokens=50,\n",
    "        batch_size=8\n",
    "    ),\n",
    "    num_processes=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM\n",
    "\n",
    "NB: memory problems with vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_experiment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_experiment_vllm(\n",
    "    model_name=model_name,\n",
    "    prompts=prompts,\n",
    "    task_type=\"text_generation\",\n",
    "    max_input_tokens=512,\n",
    "    max_output_tokens=50,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum_benchmark import Benchmark, BenchmarkConfig, TorchrunConfig, InferenceConfig, PyTorchConfig\n",
    "from optimum_benchmark.logging_utils import setup_logging\n",
    "\n",
    "setup_logging(level=\"INFO\", handlers=[\"console\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    launcher_config = TorchrunConfig(nproc_per_node=2)\n",
    "    scenario_config = InferenceConfig(latency=True, memory=True)\n",
    "    backend_config = PyTorchConfig(model=\"gpt2\", device=\"cuda\", device_ids=\"0,1\", no_weights=True)\n",
    "    benchmark_config = BenchmarkConfig(\n",
    "        name=\"pytorch_gpt2\",\n",
    "        scenario=scenario_config,\n",
    "        launcher=launcher_config,\n",
    "        backend=backend_config,\n",
    "    )\n",
    "    benchmark_report = Benchmark.launch(benchmark_config)\n",
    "\n",
    "    # convert artifacts to a dictionary or dataframe\n",
    "    benchmark_config.to_dict() # or benchmark_config.to_dataframe()\n",
    "\n",
    "    # save artifacts to disk as json or csv files\n",
    "    benchmark_report.save_csv(\"benchmark_report.csv\") # or benchmark_report.save_json(\"benchmark_report.json\")\n",
    "\n",
    "    # push artifacts to the hub\n",
    "    benchmark_config.push_to_hub(\"IlyasMoutawwakil/pytorch_gpt2\") # or benchmark_config.push_to_hub(\"IlyasMoutawwakil/pytorch_gpt2\")\n",
    "\n",
    "    # or merge them into a single artifact\n",
    "    benchmark = Benchmark(config=benchmark_config, report=benchmark_report)\n",
    "    benchmark.save_json(\"benchmark.json\") # or benchmark.save_csv(\"benchmark.csv\")\n",
    "    benchmark.push_to_hub(\"IlyasMoutawwakil/pytorch_gpt2\")\n",
    "\n",
    "    # load artifacts from the hub\n",
    "    benchmark = Benchmark.from_hub(\"IlyasMoutawwakil/pytorch_gpt2\") # or Benchmark.from_hub(\"IlyasMoutawwakil/pytorch_gpt2\")\n",
    "\n",
    "    # or load them from disk\n",
    "    benchmark = Benchmark.load_json(\"benchmark.json\") # or Benchmark.load_csv(\"benchmark_report.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
