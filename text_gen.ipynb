{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from codecarbon import EmissionsTracker\n",
    "from vllm import LLM, SamplingParams\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using HF Accelerate (not vLLM)\n",
    "nb this works for causal models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions \n",
    " \n",
    "def load_model_tokenizer(model_name):\n",
    "    \"\"\"\n",
    "    Loads and returns the tokenizer and causal model.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, torch_dtype=torch.float16, device_map=\"auto\"\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "    \n",
    "def prep_distributed(model, tokenizer):\n",
    "    \"\"\"\n",
    "    Prepares the model and tokenizer for distributed inference using Accelerate.\n",
    "    Returns the prepared model, tokenizer, and the accelerator instance.\n",
    "    \"\"\"\n",
    "    accelerator = Accelerator()\n",
    "    model, tokenizer = accelerator.prepare(model, tokenizer)\n",
    "    return model, tokenizer, accelerator\n",
    "\n",
    "def run_text_gen_inference(model, tokenizer,accelerator,prompts,max_input_tokens=512, max_output_tokens=50, batch_size=8):\n",
    "    \"\"\"\n",
    "    Runs inference in batches with token truncation applied early to prevent exceeding model max length.\n",
    "    \"\"\"\n",
    "    task_type = \"Text Generation\"\n",
    "\n",
    "    # Apply truncation early to prevent sorting long prompts\n",
    "    truncated_prompts = [\n",
    "        tokenizer.decode(\n",
    "            tokenizer(\n",
    "                p, truncation=True, max_length=max_input_tokens, return_tensors=\"pt\"\n",
    "            ).input_ids[0],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        for p in prompts\n",
    "    ]\n",
    "\n",
    "    # Sort prompts by token length (for efficient batching)\n",
    "    sorted_prompts = sorted(truncated_prompts, key=lambda x: len(tokenizer.tokenize(x)))\n",
    "    \n",
    "    latencies = []\n",
    "    ttft_values = []\n",
    "    total_tokens = 0\n",
    "\n",
    "    device = accelerator.device\n",
    "    num_batches = (len(sorted_prompts) + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch = sorted_prompts[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "        # First batch: measure TTFT (Time-To-First-Token)\n",
    "        if i == 0 and batch:\n",
    "            encoded = tokenizer(batch[0], return_tensors=\"pt\", truncation=True, max_length=max_input_tokens)\n",
    "            input_ids = encoded.input_ids.to(device)\n",
    "            start_ttft = time.perf_counter()\n",
    "            _ = model.generate(input_ids, max_new_tokens=1)\n",
    "            end_ttft = time.perf_counter()\n",
    "            ttft_ms = (end_ttft - start_ttft) * 1000.0\n",
    "            ttft_values.append(ttft_ms)\n",
    "\n",
    "        # Tokenize batch with truncation\n",
    "        encoded = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_input_tokens)\n",
    "        input_ids = encoded.input_ids.to(device)\n",
    "\n",
    "        # Generate outputs\n",
    "        start_time = time.perf_counter()\n",
    "        outputs = model.generate(input_ids, max_new_tokens=max_output_tokens, do_sample=False)\n",
    "        end_time = time.perf_counter()\n",
    "        latencies.append((end_time - start_time) * 1000.0)\n",
    "\n",
    "        # Compute total generated tokens\n",
    "        for j in range(len(batch)):\n",
    "            prompt_len = input_ids[j].shape[0]\n",
    "            gen_len = outputs[j].shape[0] - prompt_len\n",
    "            total_tokens += gen_len\n",
    "\n",
    "    avg_latency_ms = sum(latencies) / len(latencies) if latencies else 0.0\n",
    "    avg_ttft_ms = sum(ttft_values) / len(ttft_values) if ttft_values else 0.0\n",
    "    total_time_sec = sum(latencies) / 1000.0\n",
    "    throughput_qps = len(sorted_prompts) / total_time_sec if total_time_sec > 0 else 0.0\n",
    "    tokens_per_sec = total_tokens / total_time_sec if total_time_sec > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"avg_latency_ms\": avg_latency_ms,\n",
    "        \"avg_ttft_ms\": avg_ttft_ms,\n",
    "        \"throughput_qps\": throughput_qps,\n",
    "        \"tokens_per_sec\": tokens_per_sec,\n",
    "        \"total_generated_tokens\": total_tokens,\n",
    "        \"num_runs\": len(sorted_prompts),\n",
    "        \"total_time\": total_time_sec\n",
    "    }\n",
    "\n",
    "    \n",
    "def main(model_name, prompts, max_input_tokens=512, max_output_tokens=50, batch_size=8):\n",
    "    model, tokenizer = load_model_tokenizer(model_name)\n",
    "    model, tokenizer, accelerator = prep_distributed(model, tokenizer)\n",
    "\n",
    "    os.makedirs(\"codecarbon_logs\", exist_ok=True)\n",
    "    \n",
    "    tracker = EmissionsTracker(measure_power_secs=1)\n",
    "\n",
    "    tracker.start()\n",
    "\n",
    "    metrics = run_text_gen_inference(model, \n",
    "                            tokenizer, \n",
    "                            accelerator, \n",
    "                            prompts, \n",
    "                            max_input_tokens=max_input_tokens, \n",
    "                            max_output_tokens=max_output_tokens, \n",
    "                            batch_size=batch_size)\n",
    "    \n",
    "    tracker.stop()\n",
    "    \n",
    "    _data = tracker.final_emissions_data\n",
    "    \n",
    "    energy_kwh = _data.energy_consumed \n",
    "    energy_joules = energy_kwh * 3.6e6  # 1 kWh = 3.6e6 Joules\n",
    "    tokens_per_joule = (metrics[\"total_generated_tokens\"] / energy_joules) if energy_joules > 0 else 0\n",
    "\n",
    "    cpu_count = _data.cpu_count\n",
    "    cpu_model = _data.cpu_model\n",
    "    gpu_count = _data.gpu_count\n",
    "    gpu_model = _data.gpu_model\n",
    "    \n",
    "    benchmark_results = {\n",
    "        \"experiment_setup\": {\n",
    "            \"model\": model_name,\n",
    "            \"cpu_count\": cpu_count,\n",
    "            \"cpu_model\": cpu_model,\n",
    "            \"gpu_count\": gpu_count,\n",
    "            \"gpu_model\": gpu_model,\n",
    "            \"total_runs\": metrics[\"num_runs\"],\n",
    "        },\n",
    "        \"experiment_results\": {\n",
    "            \"total_inference_time_sec\": round(metrics[\"total_time\"], 2),\n",
    "            \"average_latency_ms_per_batch\": round(metrics[\"avg_latency_ms\"], 2),\n",
    "            \"average_ttft_ms\": round(metrics[\"avg_ttft_ms\"], 2),\n",
    "            \"throughput_queries_per_sec\": round(metrics[\"throughput_qps\"], 2),\n",
    "            \"throughput_tokens_per_sec\": round(metrics[\"tokens_per_sec\"], 2),\n",
    "            \"total_tokens_generated\": metrics[\"total_generated_tokens\"],\n",
    "            \"energy_consumed_kwh\": round(energy_kwh, 10),\n",
    "            \"energy_consumed_joules\": round(energy_joules, 10),\n",
    "            \"energy_efficiency_tokens_per_joule\": round(tokens_per_joule, 10),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    today_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "    output_dir = f\"benchmark_results/{task_type}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_json_path = f\"{output_dir}/{model_name}_{today_date}.json\"\n",
    "\n",
    "    with open(output_json_path, \"w\") as json_file:\n",
    "        json.dump(benchmark_results, json_file, indent=4)\n",
    "\n",
    "    # Print confirmation and the JSON structure\n",
    "    print(\"\\n=== BENCHMARKING RESULTS ===\")\n",
    "    print(json.dumps(benchmark_results, indent=4))\n",
    "    print(f\"\\nResults saved to: {output_json_path}\")\n",
    "\n",
    "    print(\"\\n=== BENCHMARKING RESULTS===\")\n",
    "    print(\"--- Experiment Set up ---\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"CPU Count: {cpu_count}\")\n",
    "    print(f\"CPU Model: {cpu_model}\")\n",
    "    print(f\"GPU Count: {gpu_count}\")\n",
    "    print(f\"GPU Model: {gpu_model}\")\n",
    "    print(f\"Total Runs: {metrics['num_runs']}\")\n",
    "    print(\"\\n--- Experiment results ---\")\n",
    "    print(f\"Total Inference Time (sec): {metrics['total_time']:.2f}\")\n",
    "    print(f\"Average Latency (ms/batch): {metrics['avg_latency_ms']:.2f}\")\n",
    "    print(f\"Average TTFT (ms): {metrics['avg_ttft_ms']:.2f}\")\n",
    "    print(f\"Throughput (queries/sec): {metrics['throughput_qps']:.2f}\")\n",
    "    print(f\"Throughput (tokens/sec): {metrics['tokens_per_sec']:.2f}\")\n",
    "    print(f\"Total Tokens Generated: {metrics['total_generated_tokens']}\")\n",
    "    print(f\"Energy Consumed (kWh): {energy_kwh:.10f}\")\n",
    "    print(f\"Energy Consumed (Joules): {energy_joules:.10f}\")\n",
    "    print(f\"Energy Efficiency (tokens/joule): {tokens_per_joule:.10f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 20:10:07] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 20:10:07] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 20:10:07] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 20:10:08] CPU Model on constant consumption mode: AMD EPYC 7742 64-Core Processor\n",
      "[codecarbon INFO @ 20:10:08] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 20:10:08] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 20:10:08] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 20:10:08]   Platform system: Linux-5.15.0-113-generic-x86_64-with-glibc2.31\n",
      "[codecarbon INFO @ 20:10:08]   Python version: 3.10.14\n",
      "[codecarbon INFO @ 20:10:08]   CodeCarbon version: 2.8.3\n",
      "[codecarbon INFO @ 20:10:08]   Available RAM : 503.532 GB\n",
      "[codecarbon INFO @ 20:10:08]   CPU count: 128\n",
      "[codecarbon INFO @ 20:10:08]   CPU model: AMD EPYC 7742 64-Core Processor\n",
      "[codecarbon INFO @ 20:10:08]   GPU count: 4\n",
      "[codecarbon INFO @ 20:10:08]   GPU model: 4 x NVIDIA A100-PCIE-40GB\n",
      "[codecarbon INFO @ 20:10:12] Saving emissions data to file /home/228755@hertie-school.lan/thesis/emissions.csv\n",
      "[codecarbon INFO @ 20:10:13] Energy consumed for RAM : 0.000053 kWh. RAM Power : 188.8243260383606 W\n",
      "[codecarbon INFO @ 20:10:13] Energy consumed for all CPUs : 0.000032 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 20:10:13] Energy consumed for all GPUs : 0.000129 kWh. Total GPU Power : 453.02859107310724 W\n",
      "[codecarbon INFO @ 20:10:13] 0.000214 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:10:14] Energy consumed for RAM : 0.000105 kWh. RAM Power : 188.8243260383606 W\n",
      "[codecarbon INFO @ 20:10:14] Energy consumed for all CPUs : 0.000063 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 20:10:14] Energy consumed for all GPUs : 0.000259 kWh. Total GPU Power : 478.49452694295985 W\n",
      "[codecarbon INFO @ 20:10:14] 0.000427 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:10:15] Energy consumed for RAM : 0.000156 kWh. RAM Power : 188.8243260383606 W\n",
      "[codecarbon INFO @ 20:10:15] Energy consumed for all CPUs : 0.000093 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 20:10:15] Energy consumed for all GPUs : 0.000389 kWh. Total GPU Power : 476.2016122257373 W\n",
      "[codecarbon INFO @ 20:10:15] 0.000639 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:10:16] Energy consumed for RAM : 0.000208 kWh. RAM Power : 188.8243260383606 W\n",
      "[codecarbon INFO @ 20:10:16] Energy consumed for all CPUs : 0.000124 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 20:10:16] Energy consumed for all GPUs : 0.000518 kWh. Total GPU Power : 472.87291578372395 W\n",
      "[codecarbon INFO @ 20:10:16] 0.000849 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:10:17] Energy consumed for RAM : 0.000259 kWh. RAM Power : 188.8243260383606 W\n",
      "[codecarbon INFO @ 20:10:17] Energy consumed for all CPUs : 0.000155 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 20:10:17] Energy consumed for RAM : 0.000311 kWh. RAM Power : 188.8243260383606 W\n",
      "[codecarbon INFO @ 20:10:17] Energy consumed for all CPUs : 0.000185 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 20:10:17] Energy consumed for all GPUs : 0.000648 kWh. Total GPU Power : 475.21047645599145 W\n",
      "[codecarbon INFO @ 20:10:17] 0.001144 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:10:17] Energy consumed for all GPUs : 0.000648 kWh. Total GPU Power : 0.0 W\n",
      "[codecarbon INFO @ 20:10:17] 0.001144 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BENCHMARKING RESULTS===\n",
      "--- Experiment Set up ---\n",
      "Model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "CPU Count: 128\n",
      "CPU Model: AMD EPYC 7742 64-Core Processor\n",
      "GPU Count: 4\n",
      "GPU Model: 4 x NVIDIA A100-PCIE-40GB\n",
      "Total Runs: 5\n",
      "\n",
      "--- Experiment results ---\n",
      "Total Inference Time (sec): 4.77\n",
      "Average Latency (ms/batch): 4767.30\n",
      "Average TTFT (ms): 118.70\n",
      "Throughput (queries/sec): 1.05\n",
      "Throughput (tokens/sec): 52.44\n",
      "Total Tokens Generated: 250\n",
      "Energy Consumed (kWh): 0.0011436906\n",
      "Energy Consumed (Joules): 4117.2862626621\n",
      "Energy Efficiency (tokens/joule): 0.0607196061\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "    \n",
    "    # Load the dataset and select 1024 samples CHANGE BACK TO 1024 FOR EXPERIMENT\n",
    "    ds = load_dataset(\"lighteval/pile_helm\", \"arxiv\")[\"test\"]\n",
    "    ds = ds.select(range(5))\n",
    "    prompts = [sample[\"text\"] for sample in ds]\n",
    "\n",
    "    main(model_name, \n",
    "         prompts, \n",
    "         max_input_tokens=512,\n",
    "         max_output_tokens=50,\n",
    "         batch_size=8)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM \n",
    "only works for non-causal models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions \n",
    "\n",
    "def load_vllm_model(model_name, tensor_parallel_size=None):\n",
    "    \"\"\"\n",
    "    Loads the vLLM model with optional multi-GPU support.\n",
    "    Includes FlashAttention optimisation.\n",
    "    \n",
    "    Parameters:\n",
    "        model_name: Name of the model.\n",
    "        tensor_parallel_size: Number of GPUs to use. If None, auto-detects available GPUs.\n",
    "    \n",
    "    Returns:\n",
    "        vLLM model instance.\n",
    "    \"\"\"\n",
    "    if tensor_parallel_size is None:\n",
    "        tensor_parallel_size = torch.cuda.device_count()  # Auto-detect GPUs\n",
    "\n",
    "    print(f\"Loading vLLM model on {tensor_parallel_size} GPUs with FlashAttention...\")\n",
    "    \n",
    "    return LLM(\n",
    "        model=model_name,\n",
    "        tensor_parallel_size=tensor_parallel_size,\n",
    "        dtype=\"bfloat16\",      # Use bfloat16 for better memory efficiency\n",
    "    )\n",
    "\n",
    "def run_vllm_inference(model, prompts, max_new_tokens=50, batch_size=8):\n",
    "    \"\"\"\n",
    "    Runs inference using vLLM in batches and returns performance metrics.\n",
    "    Both tokenisation and output generation are performed within model.generate().\n",
    "    Prompts are sorted by length to allow optimal batching (reducing padding overhead).\n",
    "    \n",
    "    Parameters:\n",
    "        model: vLLM model instance.\n",
    "        prompts: List of prompt strings.\n",
    "        max_new_tokens: Maximum number of new tokens to generate.\n",
    "        batch_size: Number of samples per batch.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with performance metrics.\n",
    "    \"\"\"\n",
    "    # Sort prompts by length (for optimal batching)\n",
    "    sorted_prompts = sorted(prompts, key=lambda x: len(x))\n",
    "    \n",
    "    latencies = []\n",
    "    ttft_values = []\n",
    "    total_tokens = 0\n",
    "    \n",
    "    sampling_params = SamplingParams(max_tokens=max_new_tokens, temperature=0.7, top_p=0.9)\n",
    "    \n",
    "    # Process prompts in batches\n",
    "    num_batches = (len(sorted_prompts) + batch_size - 1) // batch_size\n",
    "    for i in range(num_batches):\n",
    "        batch = sorted_prompts[i * batch_size: (i + 1) * batch_size]\n",
    "        \n",
    "        # For the first batch, measure Time-To-First-Token (TTFT) for the first prompt\n",
    "        if i == 0 and batch:\n",
    "            start_ttft = time.perf_counter()\n",
    "            # Generate only one token for TTFT measurement\n",
    "            _ = model.generate(batch[0], SamplingParams(max_tokens=1, temperature=0.7, top_p=0.9))\n",
    "            end_ttft = time.perf_counter()\n",
    "            ttft_ms = (end_ttft - start_ttft) * 1000.0\n",
    "            ttft_values.append(ttft_ms)\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        # model.generate() handles tokenisation internally before generating output tokens\n",
    "        outputs = model.generate(batch, sampling_params)\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        batch_latency = (end_time - start_time) * 1000.0  # in milliseconds\n",
    "        latencies.append(batch_latency)\n",
    "        \n",
    "        # Sum tokens generated from outputs (here, using simple whitespace splitting)\n",
    "        for output in outputs:\n",
    "            for item in output.outputs:\n",
    "                total_tokens += len(item.text.split())\n",
    "    \n",
    "    avg_latency_ms = sum(latencies) / len(latencies) if latencies else 0.0\n",
    "    avg_ttft_ms = sum(ttft_values) / len(ttft_values) if ttft_values else 0.0\n",
    "    total_time_sec = sum(latencies) / 1000.0  # Convert total latency to seconds\n",
    "    throughput_qps = len(sorted_prompts) / total_time_sec if total_time_sec > 0 else 0.0\n",
    "    tokens_per_sec = total_tokens / total_time_sec if total_time_sec > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"avg_latency_ms\": avg_latency_ms,\n",
    "        \"avg_ttft_ms\": avg_ttft_ms,\n",
    "        \"throughput_qps\": throughput_qps,\n",
    "        \"tokens_per_sec\": tokens_per_sec,\n",
    "        \"total_generated_tokens\": total_tokens,\n",
    "        \"num_runs\": len(sorted_prompts),\n",
    "        \"total_time\": total_time_sec\n",
    "    }\n",
    "    \n",
    "    \n",
    "def main_vllm(model_name, prompts, tensor_parallel_size=None, batch_size=8):\n",
    "    \"\"\"\n",
    "    Main function to run vLLM inference with energy consumption tracking using CodeCarbon.\n",
    "    Processes test samples in batches (after sorting by length) and measures the total energy \n",
    "    required for 1024 samples in the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        model_name: Name of the model.\n",
    "        prompts: List of prompt strings.\n",
    "        tensor_parallel_size: Number of GPUs to use.\n",
    "        batch_size: Batch size for inference.\n",
    "    \"\"\"\n",
    "    model = load_vllm_model(model_name, tensor_parallel_size=tensor_parallel_size)\n",
    "        \n",
    "    os.makedirs(\"codecarbon_logs\", exist_ok=True)\n",
    "    \n",
    "    tracker = EmissionsTracker(\n",
    "        output_dir=\"codecarbon_logs\",\n",
    "        measure_power_secs=1  # Measure power every second\n",
    "    )\n",
    "    tracker.start()\n",
    "\n",
    "    metrics = run_vllm_inference(model, prompts, max_new_tokens=50, batch_size=batch_size)\n",
    "    \n",
    "    emissions_data = tracker.stop()\n",
    "    \n",
    "    # CodeCarbon returns energy_consumed in kWh (if available)\n",
    "    energy_kwh = getattr(emissions_data, \"energy_consumed\", 0)\n",
    "    # Convert kWh to Joules: 1 kWh = 3.6e6 J\n",
    "    energy_joules = energy_kwh * 3.6e6\n",
    "    tokens_per_joule = (metrics[\"total_generated_tokens\"] / energy_joules) if energy_joules > 0 else 0\n",
    "\n",
    "    print(\"=== Benchmarking Results ===\")\n",
    "    print(f\"Total Runs: {metrics['num_runs']}\")\n",
    "    print(f\"Total Inference Time (sec): {metrics['total_time']:.2f}\")\n",
    "    print(f\"Average Latency (ms/batch): {metrics['avg_latency_ms']:.2f}\")\n",
    "    print(f\"Average TTFT (ms): {metrics['avg_ttft_ms']:.2f}\")\n",
    "    print(f\"Throughput (queries/sec): {metrics['throughput_qps']:.2f}\")\n",
    "    print(f\"Throughput (tokens/sec): {metrics['tokens_per_sec']:.2f}\")\n",
    "    print(f\"Total Tokens Generated: {metrics['total_generated_tokens']}\")\n",
    "    print(f\"Energy Consumed (kWh): {energy_kwh:.6f}\")\n",
    "    print(f\"Energy Consumed (Joules): {energy_joules:.2f}\")\n",
    "    print(f\"Energy Efficiency (tokens/joule): {tokens_per_joule:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vLLM model on 2 GPUs with FlashAttention...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b603ba3eb1964e63ab5c0f651b04a53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-03 18:14:45 config.py:2444] Downcasting torch.float32 to torch.bfloat16.\n",
      "INFO 03-03 18:14:52 config.py:549] This model supports multiple tasks: {'generate', 'score', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 03-03 18:14:52 config.py:1382] Defaulting to use mp for distributed inference\n",
      "INFO 03-03 18:14:52 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='gpt2', speculative_config=None, tokenizer='gpt2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=gpt2, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540fa654d7b74d348b78e593f9eb44b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bec9cbf71947979b88d19abf08cbf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24adf225bfe545b5ae1f18809a0fd360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5faa40248d9247b2b5279187044f103f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5271d5055cf847db8e4f782ed2c46029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-03 18:14:54 utils.py:2128] CUDA was previously initialized. We must use the `spawn` multiprocessing start method. Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information.\n",
      "INFO 03-03 18:14:59 __init__.py:207] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2551505)\u001b[0;0m INFO 03-03 18:14:59 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2551505)\u001b[0;0m INFO 03-03 18:15:00 cuda.py:229] Using Flash Attention backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W303 18:18:33.252198404 TCPStore.cpp:358] [c10d] TCP client failed to connect/validate to host 10.1.23.20:48853 - retrying (try=0, timeout=600000ms, delay=73411ms): Interrupted system call\n",
      "Exception raised from delay at ../torch/csrc/distributed/c10d/socket.cpp:117 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f0c9a3f3446 in /home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x15e1205 (0x7f0cd0a2a205 in /home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x6029f36 (0x7f0cd5472f36 in /home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x602a3a4 (0x7f0cd54733a4 in /home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x5fe8016 (0x7f0cd5431016 in /home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::TCPStore::TCPStore(std::string, c10d::TCPStoreOptions const&) + 0x20c (0x7f0cd5433f7c in /home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0xd9acdd (0x7f0ce4e3ccdd in /home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #7: <unknown function> + 0x4cb474 (0x7f0ce456d474 in /home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #8: /home/228755@hertie-school.lan/thesis/thesis/bin/python() [0x53bd79]\n",
      "frame #9: _PyObject_MakeTpCall + 0x1fc (0x629dbc in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #10: /home/228755@hertie-school.lan/thesis/thesis/bin/python() [0x549c2e]\n",
      "frame #11: PyVectorcall_Call + 0x9d (0x6286ed in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #12: /home/228755@hertie-school.lan/thesis/thesis/bin/python() [0x5d6e59]\n",
      "frame #13: /home/228755@hertie-school.lan/thesis/thesis/bin/python() [0x5e2a9d]\n",
      "frame #14: <unknown function> + 0x4c9ccb (0x7f0ce456bccb in /home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #15: _PyObject_MakeTpCall + 0x1fc (0x629dbc in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #16: _PyEval_EvalFrameDefault + 0x5af8 (0x5af408 in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #17: _PyFunction_Vectorcall + 0x250 (0x628d60 in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #18: _PyEval_EvalFrameDefault + 0x30b (0x5a9c1b in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #19: /home/228755@hertie-school.lan/thesis/thesis/bin/python() [0x5352ac]\n",
      "frame #20: /home/228755@hertie-school.lan/thesis/thesis/bin/python() [0x53b03f]\n",
      "frame #21: _PyEval_EvalFrameDefault + 0x30b (0x5a9c1b in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #22: _PyFunction_Vectorcall + 0x250 (0x628d60 in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #23: PyObject_Call + 0xac (0x62893c in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #24: _PyEval_EvalFrameDefault + 0x2c0b (0x5ac51b in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #25: _PyFunction_Vectorcall + 0x250 (0x628d60 in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #26: PyObject_Call + 0xac (0x62893c in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #27: _PyEval_EvalFrameDefault + 0x2c0b (0x5ac51b in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #28: _PyFunction_Vectorcall + 0x250 (0x628d60 in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #29: _PyEval_EvalFrameDefault + 0x13c5 (0x5aacd5 in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #30: _PyFunction_Vectorcall + 0x250 (0x628d60 in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #31: _PyEval_EvalFrameDefault + 0x30b (0x5a9c1b in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #32: _PyFunction_Vectorcall + 0x250 (0x628d60 in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #33: _PyEval_EvalFrameDefault + 0x30b (0x5a9c1b in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #34: /home/228755@hertie-school.lan/thesis/thesis/bin/python() [0x5a8bf1]\n",
      "frame #35: /home/228755@hertie-school.lan/thesis/thesis/bin/python() [0x549804]\n",
      "frame #36: PyObject_Call + 0x1aa (0x628a3a in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #37: _PyEval_EvalFrameDefault + 0x2c0b (0x5ac51b in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #38: _PyFunction_Vectorcall + 0x250 (0x628d60 in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #39: _PyEval_EvalFrameDefault + 0x30b (0x5a9c1b in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #40: _PyFunction_Vectorcall + 0x250 (0x628d60 in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #41: PyObject_Call + 0xac (0x62893c in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #42: _PyEval_EvalFrameDefault + 0x2c0b (0x5ac51b in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #43: _PyFunction_Vectorcall + 0x250 (0x628d60 in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #44: _PyEval_EvalFrameDefault + 0x715 (0x5aa025 in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #45: _PyFunction_Vectorcall + 0x250 (0x628d60 in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #46: _PyEval_EvalFrameDefault + 0x715 (0x5aa025 in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #47: _PyFunction_Vectorcall + 0x250 (0x628d60 in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #48: _PyEval_EvalFrameDefault + 0x30b (0x5a9c1b in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #49: _PyFunction_Vectorcall + 0x250 (0x628d60 in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #50: _PyEval_EvalFrameDefault + 0x13c5 (0x5aacd5 in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #51: /home/228755@hertie-school.lan/thesis/thesis/bin/python() [0x5a8bf1]\n",
      "frame #52: PyEval_EvalCode + 0x7f (0x6d77cf in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #53: /home/228755@hertie-school.lan/thesis/thesis/bin/python() [0x6bb91b]\n",
      "frame #54: /home/228755@hertie-school.lan/thesis/thesis/bin/python() [0x6bb9a4]\n",
      "frame #55: PyRun_StringFlags + 0x7f (0x6bbe8f in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #56: PyRun_SimpleStringFlags + 0x3f (0x6c0aaf in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #57: Py_RunMain + 0x243 (0x7041b3 in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #58: Py_BytesMain + 0x2d (0x7044bd in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "frame #59: __libc_start_main + 0xf3 (0x7f0ce6887083 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #60: _start + 0x2e (0x62ff4e in /home/228755@hertie-school.lan/thesis/thesis/bin/python)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m prompts \u001b[38;5;241m=\u001b[39m [sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m ds]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Run the main function with a default batch size of 8.\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mmain_vllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 106\u001b[0m, in \u001b[0;36mmain_vllm\u001b[0;34m(model_name, prompts, tensor_parallel_size, batch_size)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain_vllm\u001b[39m(model_name, prompts, tensor_parallel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m):\n\u001b[1;32m     95\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    Main function to run vLLM inference with energy consumption tracking using CodeCarbon.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m    Processes test samples in batches (after sorting by length) and measures the total energy \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m        batch_size: Batch size for inference.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mload_vllm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcodecarbon_logs\u001b[39m\u001b[38;5;124m\"\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    110\u001b[0m     tracker \u001b[38;5;241m=\u001b[39m EmissionsTracker(\n\u001b[1;32m    111\u001b[0m         output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcodecarbon_logs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    112\u001b[0m         measure_power_secs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Measure power every second\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[16], line 20\u001b[0m, in \u001b[0;36mload_vllm_model\u001b[0;34m(model_name, tensor_parallel_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m     tensor_parallel_size \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()  \u001b[38;5;66;03m# Auto-detect GPUs\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading vLLM model on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor_parallel_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GPUs with FlashAttention...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbfloat16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Use bfloat16 for better memory efficiency\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/utils.py:1022\u001b[0m, in \u001b[0;36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1015\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1017\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1018\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1019\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m         )\n\u001b[0;32m-> 1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/entrypoints/llm.py:242\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Logic to switch between engines is done at runtime instead of import\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# to avoid import order issues\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_engine_class()\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/engine/llm_engine.py:489\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    487\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/engine/llm_engine.py:273\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, mm_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_registry \u001b[38;5;241m=\u001b[39m input_registry\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_processor \u001b[38;5;241m=\u001b[39m input_registry\u001b[38;5;241m.\u001b[39mcreate_input_processor(\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config)\n\u001b[0;32m--> 273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mrunner_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpooling\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_kv_caches()\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/executor/executor_base.py:271\u001b[0m, in \u001b[0;36mDistributedExecutorBase.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# This is non-None when the execute model loop is running\u001b[39;00m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;66;03m# in the parallel workers. It's a coroutine in the AsyncLLMEngine case.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_worker_tasks: Optional[Union[Any, Awaitable[Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/executor/executor_base.py:52\u001b[0m, in \u001b[0;36mExecutorBase.__init__\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_adapter_config \u001b[38;5;241m=\u001b[39m vllm_config\u001b[38;5;241m.\u001b[39mprompt_adapter_config\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;241m=\u001b[39m vllm_config\u001b[38;5;241m.\u001b[39mobservability_config\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_sleeping \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/executor/mp_distributed_executor.py:124\u001b[0m, in \u001b[0;36mMultiprocessingDistributedExecutor._init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m     all_kwargs\u001b[38;5;241m.\u001b[39mappend(kwargs)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_workers(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_worker\u001b[39m\u001b[38;5;124m\"\u001b[39m, all_kwargs)\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minit_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_workers(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    126\u001b[0m                   max_concurrent_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    127\u001b[0m                   max_parallel_loading_workers)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_exec_model \u001b[38;5;241m=\u001b[39m make_async(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker\u001b[38;5;241m.\u001b[39mexecute_model)\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/executor/mp_distributed_executor.py:190\u001b[0m, in \u001b[0;36mMultiprocessingDistributedExecutor._run_workers\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    185\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m run_method(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker, sent_method,\n\u001b[1;32m    186\u001b[0m                                   args, kwargs)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Get the results of the workers.\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [driver_worker_output\n\u001b[0;32m--> 190\u001b[0m         ] \u001b[38;5;241m+\u001b[39m [output\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m worker_outputs]\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/executor/mp_distributed_executor.py:190\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    185\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m run_method(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker, sent_method,\n\u001b[1;32m    186\u001b[0m                                   args, kwargs)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Get the results of the workers.\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [driver_worker_output\n\u001b[0;32m--> 190\u001b[0m         ] \u001b[38;5;241m+\u001b[39m [\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m worker_outputs]\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py:59\u001b[0m, in \u001b[0;36mResultFuture.get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mexception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-03 18:18:34 multiproc_worker_utils.py:128] Killing local vLLM worker processes\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Use 1024 samples per dataset for the experiment.\n",
    "    model_name = \"gpt2\"\n",
    "\n",
    "    # Load the \"lighteval/pile_helm\" subset with \"arxiv\" configuration.\n",
    "    ds = load_dataset(\"lighteval/pile_helm\", \"arxiv\")[\"test\"]\n",
    "    ds = ds.select(range(5))  # LATER CHANGE: Select 1024 samples for the experiment\n",
    "    prompts = [sample[\"text\"] for sample in ds]\n",
    "\n",
    "    # Run the main function with a default batch size of 8.\n",
    "    main_vllm(model_name, prompts, tensor_parallel_size=2, batch_size=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
