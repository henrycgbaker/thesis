{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Prompts\n",
    "ds = load_dataset(\"lighteval/pile_helm\", \"arxiv\")[\"test\"]\n",
    "ds = ds.select(range(5))\n",
    "prompts = [sample[\"text\"] for sample in ds]\n",
    "\n",
    "backend = \"pytorch\"\n",
    "\n",
    "use_optimum = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n",
      "Using device: cuda:0 (Local Rank: 0)\n",
      "Using 2 GPUs: [0, 1]Model is on cuda:1\n",
      "\n",
      "Model is on cuda:0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 19:55:24] Multiple instances of codecarbon are allowed to run at the same time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 19:55:24] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 19:55:24] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 19:55:24] [setup] CPU Tracking...\n",
      "[codecarbon INFO @ 19:55:24] [setup] RAM Tracking...\n",
      "[codecarbon WARNING @ 19:55:24] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 19:55:24] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 19:55:24] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 19:55:25] CPU Model on constant consumption mode: AMD EPYC 7742 64-Core Processor\n",
      "[codecarbon INFO @ 19:55:25] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 19:55:25] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 19:55:25] CPU Model on constant consumption mode: AMD EPYC 7742 64-Core Processor\n",
      "[codecarbon INFO @ 19:55:25] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 19:55:25] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 19:55:26] GPU number 2 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:55:26] GPU number 3 will not be monitored, at your request.\n",
      "[codecarbon WARNING @ 19:55:26] You have 4 GPUs but we will monitor only 2 of them. Check your configuration.\n",
      "[codecarbon INFO @ 19:55:26] GPU number 2 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:55:26] GPU number 3 will not be monitored, at your request.\n",
      "[codecarbon WARNING @ 19:55:26] You have 4 GPUs but we will monitor only 2 of them. Check your configuration.\n",
      "[codecarbon INFO @ 19:55:26] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 19:55:26]   Platform system: Linux-5.15.0-113-generic-x86_64-with-glibc2.31\n",
      "[codecarbon INFO @ 19:55:26] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 19:55:26]   Platform system: Linux-5.15.0-113-generic-x86_64-with-glibc2.31\n",
      "[codecarbon INFO @ 19:55:26]   Python version: 3.10.14\n",
      "[codecarbon INFO @ 19:55:26]   Python version: 3.10.14\n",
      "[codecarbon INFO @ 19:55:26]   CodeCarbon version: 2.8.3\n",
      "[codecarbon INFO @ 19:55:26]   Available RAM : 503.532 GB\n",
      "[codecarbon INFO @ 19:55:26]   CodeCarbon version: 2.8.3\n",
      "[codecarbon INFO @ 19:55:26]   CPU count: 128\n",
      "[codecarbon INFO @ 19:55:26]   Available RAM : 503.532 GB\n",
      "[codecarbon INFO @ 19:55:26]   CPU model: AMD EPYC 7742 64-Core Processor\n",
      "[codecarbon INFO @ 19:55:26]   CPU count: 128\n",
      "[codecarbon INFO @ 19:55:26]   GPU count: 2\n",
      "[codecarbon INFO @ 19:55:26]   CPU model: AMD EPYC 7742 64-Core Processor\n",
      "[codecarbon INFO @ 19:55:26]   GPU model: 4 x NVIDIA A100-PCIE-40GB BUT only tracking these GPU ids : [0, 1]\n",
      "[codecarbon INFO @ 19:55:26]   GPU count: 2\n",
      "[codecarbon INFO @ 19:55:26]   GPU model: 4 x NVIDIA A100-PCIE-40GB BUT only tracking these GPU ids : [0, 1]\n",
      "[codecarbon INFO @ 19:55:29] Saving emissions data to file /home/228755@hertie-school.lan/thesis/emissions.csv\n",
      "[codecarbon INFO @ 19:55:29] Saving emissions data to file /home/228755@hertie-school.lan/thesis/emissions.csv\n",
      "[codecarbon INFO @ 19:55:30] Energy consumed for RAM : 0.000054 kWh. RAM Power : 188.8243260383606 W\n",
      "[codecarbon INFO @ 19:55:30] Energy consumed for all CPUs : 0.000032 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 19:55:30] GPU number 2 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:55:30] GPU number 3 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:55:30] Energy consumed for RAM : 0.000053 kWh. RAM Power : 188.8243260383606 W\n",
      "[codecarbon INFO @ 19:55:30] Energy consumed for all CPUs : 0.000032 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 19:55:30] GPU number 2 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:55:30] GPU number 3 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:55:31] Energy consumed for all GPUs : 0.000025 kWh. Total GPU Power : 88.27267891605685 W\n",
      "[codecarbon INFO @ 19:55:31] 0.000110 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:55:31] Energy consumed for all GPUs : 0.000026 kWh. Total GPU Power : 91.34235863847384 W\n",
      "[codecarbon INFO @ 19:55:31] 0.000112 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:55:31] Energy consumed for RAM : 0.000058 kWh. RAM Power : 188.8243260383606 W\n",
      "[codecarbon INFO @ 19:55:31] Energy consumed for all CPUs : 0.000035 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 19:55:31] GPU number 2 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:55:31] GPU number 3 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:55:31] Energy consumed for RAM : 0.000059 kWh. RAM Power : 188.8243260383606 W\n",
      "[codecarbon INFO @ 19:55:31] Energy consumed for all CPUs : 0.000035 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 19:55:31] GPU number 2 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:55:31] Energy consumed for all GPUs : 0.000051 kWh. Total GPU Power : 978.1544189368436 W\n",
      "[codecarbon INFO @ 19:55:31] GPU number 3 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:55:31] 0.000144 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:55:31] Energy consumed for all GPUs : 0.000050 kWh. Total GPU Power : 758.2757741216781 W\n",
      "[codecarbon INFO @ 19:55:31] 0.000144 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:55:32] Energy consumed for RAM : 0.000109 kWh. RAM Power : 188.8243260383606 W\n",
      "[codecarbon INFO @ 19:55:32] Energy consumed for all CPUs : 0.000065 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 19:55:32] GPU number 2 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:55:32] GPU number 3 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:55:32] Energy consumed for all GPUs : 0.000102 kWh. Total GPU Power : 189.28312536860304 W\n",
      "[codecarbon INFO @ 19:55:32] 0.000276 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:55:32] Energy consumed for RAM : 0.000110 kWh. RAM Power : 188.8243260383606 W\n",
      "[codecarbon INFO @ 19:55:32] Energy consumed for all CPUs : 0.000066 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 19:55:32] GPU number 2 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:55:32] GPU number 3 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:55:32] Energy consumed for all GPUs : 0.000103 kWh. Total GPU Power : 198.09263601875404 W\n",
      "[codecarbon INFO @ 19:55:32] 0.000280 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:55:32] Energy consumed for RAM : 0.000115 kWh. RAM Power : 188.8243260383606 W\n",
      "[codecarbon INFO @ 19:55:32] Energy consumed for all CPUs : 0.000069 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 19:55:32] GPU number 2 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:55:32] GPU number 3 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:55:32] Energy consumed for RAM : 0.000117 kWh. RAM Power : 188.8243260383606 W\n",
      "[codecarbon INFO @ 19:55:32] Energy consumed for all CPUs : 0.000070 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 19:55:32] GPU number 2 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:55:32] GPU number 3 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 19:55:34] Energy consumed for all GPUs : 0.000121 kWh. Total GPU Power : 636.4279251003273 W\n",
      "[codecarbon INFO @ 19:55:34] 0.000304 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:55:34] Energy consumed for all GPUs : 0.000120 kWh. Total GPU Power : 473.7029504000869 W\n",
      "[codecarbon INFO @ 19:55:34] 0.000306 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated benchmark results saved to benchmark_results/text_generation_results.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pytorth_experiment_fns import *\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "# Restrict to GPUs 0 and 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "notebook_launcher(\n",
    "    lambda: run_experiment(\n",
    "        model_name=model_name,\n",
    "        prompts=prompts,\n",
    "        inference_fn=run_gen_inference_with_metrics,\n",
    "        task_type=\"text_generation\",\n",
    "        backend=\"pytorch\",\n",
    "        use_optimum=False,\n",
    "        max_input_tokens=512,\n",
    "        max_output_tokens=50,\n",
    "        batch_size=8\n",
    "    ),\n",
    "    num_processes=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM\n",
    "\n",
    "NB: memory problems with vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vllm_experiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvllm_experiment\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vllm_experiment'"
     ]
    }
   ],
   "source": [
    "from vllm_experiment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_experiment_vllm(\n",
    "    model_name=model_name,\n",
    "    prompts=prompts,\n",
    "    task_type=\"text_generation\",\n",
    "    max_input_tokens=512,\n",
    "    max_output_tokens=50,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum_benchmark import Benchmark, BenchmarkConfig, TorchrunConfig, InferenceConfig, PyTorchConfig\n",
    "from optimum_benchmark.logging_utils import setup_logging\n",
    "\n",
    "setup_logging(level=\"INFO\", handlers=[\"console\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    launcher_config = TorchrunConfig(nproc_per_node=2)\n",
    "    scenario_config = InferenceConfig(latency=True, memory=True)\n",
    "    backend_config = PyTorchConfig(model=\"gpt2\", device=\"cuda\", device_ids=\"0,1\", no_weights=True)\n",
    "    benchmark_config = BenchmarkConfig(\n",
    "        name=\"pytorch_gpt2\",\n",
    "        scenario=scenario_config,\n",
    "        launcher=launcher_config,\n",
    "        backend=backend_config,\n",
    "    )\n",
    "    benchmark_report = Benchmark.launch(benchmark_config)\n",
    "\n",
    "    # convert artifacts to a dictionary or dataframe\n",
    "    benchmark_config.to_dict() # or benchmark_config.to_dataframe()\n",
    "\n",
    "    # save artifacts to disk as json or csv files\n",
    "    benchmark_report.save_csv(\"benchmark_report.csv\") # or benchmark_report.save_json(\"benchmark_report.json\")\n",
    "\n",
    "    # push artifacts to the hub\n",
    "    benchmark_config.push_to_hub(\"IlyasMoutawwakil/pytorch_gpt2\") # or benchmark_config.push_to_hub(\"IlyasMoutawwakil/pytorch_gpt2\")\n",
    "\n",
    "    # or merge them into a single artifact\n",
    "    benchmark = Benchmark(config=benchmark_config, report=benchmark_report)\n",
    "    benchmark.save_json(\"benchmark.json\") # or benchmark.save_csv(\"benchmark.csv\")\n",
    "    benchmark.push_to_hub(\"IlyasMoutawwakil/pytorch_gpt2\")\n",
    "\n",
    "    # load artifacts from the hub\n",
    "    benchmark = Benchmark.from_hub(\"IlyasMoutawwakil/pytorch_gpt2\") # or Benchmark.from_hub(\"IlyasMoutawwakil/pytorch_gpt2\")\n",
    "\n",
    "    # or load them from disk\n",
    "    benchmark = Benchmark.load_json(\"benchmark.json\") # or Benchmark.load_csv(\"benchmark_report.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
