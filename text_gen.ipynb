{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Prompts\n",
    "ds = load_dataset(\"lighteval/pile_helm\", \"arxiv\")[\"test\"]\n",
    "ds = ds.select(range(5))\n",
    "prompts = [sample[\"text\"] for sample in ds]\n",
    "\n",
    "backend = \"pytorch\"\n",
    "\n",
    "use_optimum = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n",
      "Using device: cuda:0 (Local Rank: 0)\n",
      "Using 2 GPUs: [0, 1]Model is on cuda:1\n",
      "\n",
      "Model is on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 17:39:14] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 17:39:14] [setup] RAM Tracking...\n",
      "[codecarbon WARNING @ 17:39:14] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 17:39:14] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 17:39:14] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 17:39:14] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 17:39:14] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 17:39:14] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 17:39:15] CPU Model on constant consumption mode: AMD EPYC 7742 64-Core Processor\n",
      "[codecarbon INFO @ 17:39:15] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 17:39:15] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 17:39:15] CPU Model on constant consumption mode: AMD EPYC 7742 64-Core Processor\n",
      "[codecarbon INFO @ 17:39:15] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 17:39:15] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 17:39:15] GPU number 2 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 17:39:15] GPU number 2 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 17:39:15] GPU number 3 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 17:39:15] GPU number 3 will not be monitored, at your request.\n",
      "[codecarbon WARNING @ 17:39:15] You have 4 GPUs but we will monitor only 2 of them. Check your configuration.\n",
      "[codecarbon WARNING @ 17:39:15] You have 4 GPUs but we will monitor only 2 of them. Check your configuration.\n",
      "[codecarbon INFO @ 17:39:15] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 17:39:15] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 17:39:15]   Platform system: Linux-5.15.0-113-generic-x86_64-with-glibc2.31\n",
      "[codecarbon INFO @ 17:39:15]   Platform system: Linux-5.15.0-113-generic-x86_64-with-glibc2.31\n",
      "[codecarbon INFO @ 17:39:15]   Python version: 3.10.14\n",
      "[codecarbon INFO @ 17:39:15]   Python version: 3.10.14\n",
      "[codecarbon INFO @ 17:39:15]   CodeCarbon version: 2.8.3\n",
      "[codecarbon INFO @ 17:39:15]   CodeCarbon version: 2.8.3\n",
      "[codecarbon INFO @ 17:39:15]   Available RAM : 503.532 GB\n",
      "[codecarbon INFO @ 17:39:15]   Available RAM : 503.532 GB\n",
      "[codecarbon INFO @ 17:39:15]   CPU count: 128\n",
      "[codecarbon INFO @ 17:39:15]   CPU count: 128\n",
      "[codecarbon INFO @ 17:39:15]   CPU model: AMD EPYC 7742 64-Core Processor\n",
      "[codecarbon INFO @ 17:39:15]   CPU model: AMD EPYC 7742 64-Core Processor\n",
      "[codecarbon INFO @ 17:39:15]   GPU count: 2\n",
      "[codecarbon INFO @ 17:39:15]   GPU count: 2\n",
      "[codecarbon INFO @ 17:39:15]   GPU model: 4 x NVIDIA A100-PCIE-40GB BUT only tracking these GPU ids : [0, 1]\n",
      "[codecarbon INFO @ 17:39:15]   GPU model: 4 x NVIDIA A100-PCIE-40GB BUT only tracking these GPU ids : [0, 1]\n",
      "[codecarbon INFO @ 17:39:18] Saving emissions data to file /home/228755@hertie-school.lan/thesis/emissions.csv\n",
      "[codecarbon INFO @ 17:39:18] Saving emissions data to file /home/228755@hertie-school.lan/thesis/emissions.csv\n",
      "[codecarbon INFO @ 17:39:19] Energy consumed for RAM : 0.000054 kWh. RAM Power : 188.8243260383606 W\n",
      "[codecarbon INFO @ 17:39:19] Energy consumed for all CPUs : 0.000032 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 17:39:19] GPU number 2 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 17:39:19] Energy consumed for RAM : 0.000054 kWh. RAM Power : 188.8243260383606 W\n",
      "[codecarbon INFO @ 17:39:19] GPU number 3 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 17:39:19] Energy consumed for all CPUs : 0.000032 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 17:39:19] GPU number 2 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 17:39:19] GPU number 3 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 17:39:19] Energy consumed for all GPUs : 0.000028 kWh. Total GPU Power : 96.82021164647713 W\n",
      "[codecarbon INFO @ 17:39:19] 0.000113 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:39:19] Energy consumed for all GPUs : 0.000028 kWh. Total GPU Power : 96.46897165197166 W\n",
      "[codecarbon INFO @ 17:39:19] 0.000113 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:39:20] Energy consumed for RAM : 0.000090 kWh. RAM Power : 188.8243260383606 W\n",
      "[codecarbon INFO @ 17:39:20] Energy consumed for all CPUs : 0.000054 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 17:39:20] GPU number 2 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 17:39:20] GPU number 3 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 17:39:20] Energy consumed for RAM : 0.000091 kWh. RAM Power : 188.8243260383606 W\n",
      "[codecarbon INFO @ 17:39:20] Energy consumed for all CPUs : 0.000054 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 17:39:20] GPU number 2 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 17:39:20] GPU number 3 will not be monitored, at your request.\n",
      "[codecarbon INFO @ 17:39:20] Energy consumed for all GPUs : 0.000063 kWh. Total GPU Power : 181.8037253733717 W\n",
      "[codecarbon INFO @ 17:39:20] 0.000207 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:39:20] Energy consumed for all GPUs : 0.000063 kWh. Total GPU Power : 179.80222961266887 W\n",
      "[codecarbon INFO @ 17:39:20] 0.000208 kWh of electricity used since the beginning.\n",
      "[codecarbon WARNING @ 17:39:20] graceful shutdown. Exceptions:\n",
      "[codecarbon WARNING @ 17:39:20] <class 'Exception'>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/codecarbon/core/util.py\", line 24, in suppress\n",
      "    yield\n",
      "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/codecarbon/emissions_tracker.py\", line 540, in stop\n",
      "    self._persist_data(\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/codecarbon/emissions_tracker.py\", line 561, in _persist_data\n",
      "    handler.out(total_emissions, delta_emissions)\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/codecarbon/output_methods/file.py\", line 54, in out\n",
      "    df = pd.read_csv(self.save_file_path)\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 620, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1898, in _make_engine\n",
      "    return mapping[engine](f, **self.options)\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 93, in __init__\n",
      "    self._reader = parsers.TextReader(src, **kwds)\n",
      "  File \"parsers.pyx\", line 581, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "[codecarbon WARNING @ 17:39:20] stopping.\n",
      "W0304 17:39:21.001000 3141546 torch/multiprocessing/spawn.py:160] Terminating process 3141972 via signal SIGTERM\n",
      "E0304 17:39:21.472000 3141546 torch/distributed/elastic/multiprocessing/api.py:732] failed (exitcode: 1) local_rank: 0 (pid: 3141971) of fn: text_gen_experiment (start_method: fork)\n",
      "E0304 17:39:21.472000 3141546 torch/distributed/elastic/multiprocessing/api.py:732] Traceback (most recent call last):\n",
      "E0304 17:39:21.472000 3141546 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py\", line 687, in _poll\n",
      "E0304 17:39:21.472000 3141546 torch/distributed/elastic/multiprocessing/api.py:732]     self._pc.join(-1)\n",
      "E0304 17:39:21.472000 3141546 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 203, in join\n",
      "E0304 17:39:21.472000 3141546 torch/distributed/elastic/multiprocessing/api.py:732]     raise ProcessRaisedException(msg, error_index, failed_process.pid)\n",
      "E0304 17:39:21.472000 3141546 torch/distributed/elastic/multiprocessing/api.py:732] torch.multiprocessing.spawn.ProcessRaisedException: \n",
      "E0304 17:39:21.472000 3141546 torch/distributed/elastic/multiprocessing/api.py:732] \n",
      "E0304 17:39:21.472000 3141546 torch/distributed/elastic/multiprocessing/api.py:732] -- Process 0 terminated with the following error:\n",
      "E0304 17:39:21.472000 3141546 torch/distributed/elastic/multiprocessing/api.py:732] Traceback (most recent call last):\n",
      "E0304 17:39:21.472000 3141546 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 90, in _wrap\n",
      "E0304 17:39:21.472000 3141546 torch/distributed/elastic/multiprocessing/api.py:732]     fn(i, *args)\n",
      "E0304 17:39:21.472000 3141546 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py\", line 611, in _wrap\n",
      "E0304 17:39:21.472000 3141546 torch/distributed/elastic/multiprocessing/api.py:732]     ret = record(fn)(*args_)\n",
      "E0304 17:39:21.472000 3141546 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "E0304 17:39:21.472000 3141546 torch/distributed/elastic/multiprocessing/api.py:732]     return f(*args, **kwargs)\n",
      "E0304 17:39:21.472000 3141546 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/tmp/ipykernel_3141546/1067081187.py\", line 9, in text_gen_experiment\n",
      "E0304 17:39:21.472000 3141546 torch/distributed/elastic/multiprocessing/api.py:732]     run_experiment(\n",
      "E0304 17:39:21.472000 3141546 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/home/228755@hertie-school.lan/thesis/pytorth_experiment_fns.py\", line 419, in run_experiment\n",
      "E0304 17:39:21.472000 3141546 torch/distributed/elastic/multiprocessing/api.py:732]     codecarbon_data = stop_energy_tracking(tracker)\n",
      "E0304 17:39:21.472000 3141546 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/home/228755@hertie-school.lan/thesis/pytorth_experiment_fns.py\", line 92, in stop_energy_tracking\n",
      "E0304 17:39:21.472000 3141546 torch/distributed/elastic/multiprocessing/api.py:732]     codecarbon_data = tracker.final_emissions_data\n",
      "E0304 17:39:21.472000 3141546 torch/distributed/elastic/multiprocessing/api.py:732] AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'. Did you mean: '_prepare_emissions_data'?\n",
      "E0304 17:39:21.472000 3141546 torch/distributed/elastic/multiprocessing/api.py:732] \n"
     ]
    },
    {
     "ename": "ChildFailedError",
     "evalue": "\n============================================================\ntext_gen_experiment FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-03-04_17:39:20\n  host      : ds01\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 3141971)\n  error_file: /tmp/torchelastic_u9lb0nqp/none_nu8ct5g5/attempt_0/0/error.json\n  traceback : Traceback (most recent call last):\n    File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n      return f(*args, **kwargs)\n    File \"/tmp/ipykernel_3141546/1067081187.py\", line 9, in text_gen_experiment\n      run_experiment(\n    File \"/home/228755@hertie-school.lan/thesis/pytorth_experiment_fns.py\", line 419, in run_experiment\n      codecarbon_data = stop_energy_tracking(tracker)\n    File \"/home/228755@hertie-school.lan/thesis/pytorth_experiment_fns.py\", line 92, in stop_energy_tracking\n      codecarbon_data = tracker.final_emissions_data\n  AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'. Did you mean: '_prepare_emissions_data'?\n  \n============================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mChildFailedError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtext_gen_experiment\u001b[39m():\n\u001b[1;32m      9\u001b[0m     run_experiment(\n\u001b[1;32m     10\u001b[0m         model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[1;32m     11\u001b[0m         prompts\u001b[38;5;241m=\u001b[39mprompts,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m\n\u001b[1;32m     19\u001b[0m     )\n\u001b[0;32m---> 21\u001b[0m \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_gen_experiment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/accelerate/launchers.py:244\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes, rdzv_backend, rdzv_endpoint, rdzv_conf, rdzv_id, max_restarts, monitor_interval, log_line_prefix_template)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;124m\"\u001b[39m, ELASTIC_LOG_LINE_PREFIX_TEMPLATE_PYTORCH_VERSION):\n\u001b[1;32m    243\u001b[0m         launch_config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_line_prefix_template\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m log_line_prefix_template\n\u001b[0;32m--> 244\u001b[0m     \u001b[43melastic_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLaunchConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlaunch_config_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentrypoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProcessRaisedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/distributed/launcher/api.py:138\u001b[0m, in \u001b[0;36melastic_launch.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlaunch_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_entrypoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/distributed/launcher/api.py:269\u001b[0m, in \u001b[0;36mlaunch_agent\u001b[0;34m(config, entrypoint, args)\u001b[0m\n\u001b[1;32m    262\u001b[0m     events\u001b[38;5;241m.\u001b[39mrecord(agent\u001b[38;5;241m.\u001b[39mget_event_succeeded())\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mis_failed():\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;66;03m# ChildFailedError is treated specially by @record\u001b[39;00m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;66;03m# if the error files for the failed children exist\u001b[39;00m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;66;03m# @record will copy the first error (root cause)\u001b[39;00m\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;66;03m# to the error file of the launcher process.\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChildFailedError(\n\u001b[1;32m    270\u001b[0m             name\u001b[38;5;241m=\u001b[39mentrypoint_name,\n\u001b[1;32m    271\u001b[0m             failures\u001b[38;5;241m=\u001b[39mresult\u001b[38;5;241m.\u001b[39mfailures,\n\u001b[1;32m    272\u001b[0m         )\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreturn_values\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ChildFailedError:\n",
      "\u001b[0;31mChildFailedError\u001b[0m: \n============================================================\ntext_gen_experiment FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-03-04_17:39:20\n  host      : ds01\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 3141971)\n  error_file: /tmp/torchelastic_u9lb0nqp/none_nu8ct5g5/attempt_0/0/error.json\n  traceback : Traceback (most recent call last):\n    File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n      return f(*args, **kwargs)\n    File \"/tmp/ipykernel_3141546/1067081187.py\", line 9, in text_gen_experiment\n      run_experiment(\n    File \"/home/228755@hertie-school.lan/thesis/pytorth_experiment_fns.py\", line 419, in run_experiment\n      codecarbon_data = stop_energy_tracking(tracker)\n    File \"/home/228755@hertie-school.lan/thesis/pytorth_experiment_fns.py\", line 92, in stop_energy_tracking\n      codecarbon_data = tracker.final_emissions_data\n  AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'. Did you mean: '_prepare_emissions_data'?\n  \n============================================================"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pytorth_experiment_fns import *\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "# Restrict to GPUs 0 and 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "def text_gen_experiment():\n",
    "    run_experiment(\n",
    "        model_name=model_name,\n",
    "        prompts=prompts,\n",
    "        inference_fn=run_gen_inference_with_metrics,\n",
    "        task_type=\"text_generation\",\n",
    "        backend=backend,\n",
    "        use_optimum=use_optimum,\n",
    "        max_input_tokens=512,\n",
    "        max_output_tokens=50,\n",
    "        batch_size=8\n",
    "    )\n",
    "\n",
    "notebook_launcher(text_gen_experiment, num_processes=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM\n",
    "\n",
    "NB: memory problems with vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_experiment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_experiment_vllm(\n",
    "    model_name=model_name,\n",
    "    prompts=prompts,\n",
    "    task_type=\"text_generation\",\n",
    "    max_input_tokens=512,\n",
    "    max_output_tokens=50,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum_benchmark import Benchmark, BenchmarkConfig, TorchrunConfig, InferenceConfig, PyTorchConfig\n",
    "from optimum_benchmark.logging_utils import setup_logging\n",
    "\n",
    "setup_logging(level=\"INFO\", handlers=[\"console\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    launcher_config = TorchrunConfig(nproc_per_node=2)\n",
    "    scenario_config = InferenceConfig(latency=True, memory=True)\n",
    "    backend_config = PyTorchConfig(model=\"gpt2\", device=\"cuda\", device_ids=\"0,1\", no_weights=True)\n",
    "    benchmark_config = BenchmarkConfig(\n",
    "        name=\"pytorch_gpt2\",\n",
    "        scenario=scenario_config,\n",
    "        launcher=launcher_config,\n",
    "        backend=backend_config,\n",
    "    )\n",
    "    benchmark_report = Benchmark.launch(benchmark_config)\n",
    "\n",
    "    # convert artifacts to a dictionary or dataframe\n",
    "    benchmark_config.to_dict() # or benchmark_config.to_dataframe()\n",
    "\n",
    "    # save artifacts to disk as json or csv files\n",
    "    benchmark_report.save_csv(\"benchmark_report.csv\") # or benchmark_report.save_json(\"benchmark_report.json\")\n",
    "\n",
    "    # push artifacts to the hub\n",
    "    benchmark_config.push_to_hub(\"IlyasMoutawwakil/pytorch_gpt2\") # or benchmark_config.push_to_hub(\"IlyasMoutawwakil/pytorch_gpt2\")\n",
    "\n",
    "    # or merge them into a single artifact\n",
    "    benchmark = Benchmark(config=benchmark_config, report=benchmark_report)\n",
    "    benchmark.save_json(\"benchmark.json\") # or benchmark.save_csv(\"benchmark.csv\")\n",
    "    benchmark.push_to_hub(\"IlyasMoutawwakil/pytorch_gpt2\")\n",
    "\n",
    "    # load artifacts from the hub\n",
    "    benchmark = Benchmark.from_hub(\"IlyasMoutawwakil/pytorch_gpt2\") # or Benchmark.from_hub(\"IlyasMoutawwakil/pytorch_gpt2\")\n",
    "\n",
    "    # or load them from disk\n",
    "    benchmark = Benchmark.load_json(\"benchmark.json\") # or Benchmark.load_csv(\"benchmark_report.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
