{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: vLLM takes too much memory and crashes the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon ERROR @ 23:46:37] Error: Another instance of codecarbon is probably running as we find `/tmp/.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n",
      "[codecarbon WARNING @ 23:46:37] Another instance of codecarbon is already running. Exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-04 23:46:37 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 03-04 23:46:42 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 03-04 23:46:42 api_server.py:912] vLLM API server version 0.7.3\n",
      "INFO 03-04 23:46:42 api_server.py:913] args: Namespace(subparser='serve', model_tag='TinyLlama/TinyLlama-1.1B-Chat-v1.0', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='TinyLlama/TinyLlama-1.1B-Chat-v1.0', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function ServeSubcommand.cmd at 0x7f045e3631c0>)\n",
      "INFO 03-04 23:46:42 api_server.py:209] Started engine process with PID 3360550\n",
      "INFO 03-04 23:46:44 config.py:549] This model supports multiple tasks: {'reward', 'score', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 03-04 23:46:44 config.py:1382] Defaulting to use mp for distributed inference\n",
      "INFO 03-04 23:46:44 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='TinyLlama/TinyLlama-1.1B-Chat-v1.0', speculative_config=None, tokenizer='TinyLlama/TinyLlama-1.1B-Chat-v1.0', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=TinyLlama/TinyLlama-1.1B-Chat-v1.0, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 03-04 23:46:45 multiproc_worker_utils.py:300] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 03-04 23:46:45 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m INFO 03-04 23:46:45 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "INFO 03-04 23:46:46 cuda.py:229] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m INFO 03-04 23:46:46 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 03-04 23:46:47 utils.py:916] Found nccl from library libnccl.so.2\n",
      "INFO 03-04 23:46:47 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m INFO 03-04 23:46:47 utils.py:916] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m INFO 03-04 23:46:47 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 03-04 23:46:47 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 03-04 23:46:48 custom_all_reduce_utils.py:206] generating GPU P2P access cache in /home/228755@hertie-school.lan/.cache/vllm/gpu_p2p_access_cache_for_1,2.json\n",
      "INFO 03-04 23:46:49 config.py:549] This model supports multiple tasks: {'generate', 'classify', 'score', 'embed', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 03-04 23:46:54 config.py:549] This model supports multiple tasks: {'classify', 'reward', 'generate', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 03-04 23:46:54 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='TinyLlama/TinyLlama-1.1B-Chat-v1.0', speculative_config=None, tokenizer='TinyLlama/TinyLlama-1.1B-Chat-v1.0', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=TinyLlama/TinyLlama-1.1B-Chat-v1.0, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \n",
      "INFO 03-04 23:46:55 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 03-04 23:46:56 model_runner.py:1110] Starting to load model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "INFO 03-04 23:46:57 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 03-04 23:46:57 weight_utils.py:304] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.22it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.21it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-04 23:46:58 model_runner.py:1115] Loading model weights took 2.0512 GB\n",
      "INFO 03-04 23:46:59 worker.py:267] Memory profiling takes 1.09 seconds\n",
      "INFO 03-04 23:46:59 worker.py:267] the current vLLM instance can use total_gpu_memory (39.39GiB) x gpu_memory_utilization (0.90) = 35.45GiB\n",
      "INFO 03-04 23:46:59 worker.py:267] model weights take 2.05GiB; non_torch_memory takes 11.29GiB; PyTorch activation peak memory takes 0.31GiB; the rest of the memory reserved for KV Cache is 21.81GiB.\n",
      "INFO 03-04 23:46:59 executor_base.py:111] # cuda blocks: 64974, # CPU blocks: 11915\n",
      "INFO 03-04 23:46:59 executor_base.py:116] Maximum concurrency for 2048 tokens per request: 507.61x\n",
      "ERROR 03-04 23:46:59 engine.py:400] CUDA out of memory. Tried to allocate 1016.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 729.75 MiB is free. Process 3295852 has 23.47 GiB memory in use. Process 3359110 has 662.00 MiB memory in use. Including non-PyTorch memory, this process has 14.53 GiB memory in use. Of the allocated memory 13.97 GiB is allocated by PyTorch, and 67.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "ERROR 03-04 23:46:59 engine.py:400] Traceback (most recent call last):\n",
      "ERROR 03-04 23:46:59 engine.py:400]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 391, in run_mp_engine\n",
      "ERROR 03-04 23:46:59 engine.py:400]     engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n",
      "ERROR 03-04 23:46:59 engine.py:400]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 124, in from_engine_args\n",
      "ERROR 03-04 23:46:59 engine.py:400]     return cls(ipc_path=ipc_path,\n",
      "ERROR 03-04 23:46:59 engine.py:400]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 76, in __init__\n",
      "ERROR 03-04 23:46:59 engine.py:400]     self.engine = LLMEngine(*args, **kwargs)\n",
      "ERROR 03-04 23:46:59 engine.py:400]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 276, in __init__\n",
      "ERROR 03-04 23:46:59 engine.py:400]     self._initialize_kv_caches()\n",
      "ERROR 03-04 23:46:59 engine.py:400]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 434, in _initialize_kv_caches\n",
      "ERROR 03-04 23:46:59 engine.py:400]     self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)\n",
      "ERROR 03-04 23:46:59 engine.py:400]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 122, in initialize_cache\n",
      "ERROR 03-04 23:46:59 engine.py:400]     self.collective_rpc(\"initialize_cache\",\n",
      "ERROR 03-04 23:46:59 engine.py:400]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "ERROR 03-04 23:46:59 engine.py:400]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "ERROR 03-04 23:46:59 engine.py:400]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/utils.py\", line 2196, in run_method\n",
      "ERROR 03-04 23:46:59 engine.py:400]     return func(*args, **kwargs)\n",
      "ERROR 03-04 23:46:59 engine.py:400]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/worker/worker.py\", line 306, in initialize_cache\n",
      "ERROR 03-04 23:46:59 engine.py:400]     self._init_cache_engine()\n",
      "ERROR 03-04 23:46:59 engine.py:400]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/worker/worker.py\", line 311, in _init_cache_engine\n",
      "ERROR 03-04 23:46:59 engine.py:400]     self.cache_engine = [\n",
      "ERROR 03-04 23:46:59 engine.py:400]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/worker/worker.py\", line 312, in <listcomp>\n",
      "ERROR 03-04 23:46:59 engine.py:400]     CacheEngine(self.cache_config, self.model_config,\n",
      "ERROR 03-04 23:46:59 engine.py:400]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/worker/cache_engine.py\", line 69, in __init__\n",
      "ERROR 03-04 23:46:59 engine.py:400]     self.gpu_cache = self._allocate_kv_cache(\n",
      "ERROR 03-04 23:46:59 engine.py:400]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/worker/cache_engine.py\", line 103, in _allocate_kv_cache\n",
      "ERROR 03-04 23:46:59 engine.py:400]     layer_kv_cache = torch.zeros(alloc_shape,\n",
      "ERROR 03-04 23:46:59 engine.py:400] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1016.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 729.75 MiB is free. Process 3295852 has 23.47 GiB memory in use. Process 3359110 has 662.00 MiB memory in use. Including non-PyTorch memory, this process has 14.53 GiB memory in use. Of the allocated memory 13.97 GiB is allocated by PyTorch, and 67.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnProcess-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 402, in run_mp_engine\n",
      "    raise e\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 391, in run_mp_engine\n",
      "    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 124, in from_engine_args\n",
      "    return cls(ipc_path=ipc_path,\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 76, in __init__\n",
      "    self.engine = LLMEngine(*args, **kwargs)\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 276, in __init__\n",
      "    self._initialize_kv_caches()\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 434, in _initialize_kv_caches\n",
      "    self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 122, in initialize_cache\n",
      "    self.collective_rpc(\"initialize_cache\",\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "    answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/utils.py\", line 2196, in run_method\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/worker/worker.py\", line 306, in initialize_cache\n",
      "    self._init_cache_engine()\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/worker/worker.py\", line 311, in _init_cache_engine\n",
      "    self.cache_engine = [\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/worker/worker.py\", line 312, in <listcomp>\n",
      "    CacheEngine(self.cache_config, self.model_config,\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/worker/cache_engine.py\", line 69, in __init__\n",
      "    self.gpu_cache = self._allocate_kv_cache(\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/worker/cache_engine.py\", line 103, in _allocate_kv_cache\n",
      "    layer_kv_cache = torch.zeros(alloc_shape,\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1016.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 729.75 MiB is free. Process 3295852 has 23.47 GiB memory in use. Process 3359110 has 662.00 MiB memory in use. Including non-PyTorch memory, this process has 14.53 GiB memory in use. Of the allocated memory 13.97 GiB is allocated by PyTorch, and 67.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[rank0]:[W304 23:47:00.845855811 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-04 23:47:02 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/228755@hertie-school.lan/.cache/vllm/gpu_p2p_access_cache_for_1,2.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m INFO 03-04 23:47:02 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/228755@hertie-school.lan/.cache/vllm/gpu_p2p_access_cache_for_1,2.json\n",
      "INFO 03-04 23:47:02 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_eb86a900'), local_subscribe_port=45531, remote_subscribe_port=None)\n",
      "INFO 03-04 23:47:02 model_runner.py:1110] Starting to load model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m INFO 03-04 23:47:02 model_runner.py:1110] Starting to load model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "INFO 03-04 23:47:02 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m INFO 03-04 23:47:03 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 03-04 23:47:03 weight_utils.py:304] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee24aa946944f3e8455e6cdfa7db43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m INFO 03-04 23:47:03 weight_utils.py:304] No model.safetensors.index.json found in remote.\n",
      "INFO 03-04 23:47:03 model_runner.py:1115] Loading model weights took 1.0258 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m INFO 03-04 23:47:04 model_runner.py:1115] Loading model weights took 1.0258 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method determine_num_available_blocks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/tmp7yboclrw/main.c:5:10: fatal error: Python.h: No such file or directory\n",
      "    5 | #include <Python.h>\n",
      "      |          ^~~~~~~~~~\n",
      "compilation terminated.\n",
      "/tmp/tmpp0rjifzh/main.c:5:10: fatal error: Python.h: No such file or directory\n",
      "    5 | #include <Python.h>\n",
      "      |          ^~~~~~~~~~\n",
      "compilation terminated.\n",
      "/tmp/tmpc98yuwx_/main.c:5:10: fatal error: Python.h: No such file or directory\n",
      "    5 | #include <Python.h>\n",
      "      |          ^~~~~~~~~~\n",
      "compilation terminated.\n",
      "/tmp/tmpq3nyfub5/main.c:5:10: fatal error: Python.h: No such file or directory\n",
      "    5 | #include <Python.h>\n",
      "      |          ^~~~~~~~~~\n",
      "compilation terminated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py\", line 88, in run\n",
      "    for result in iter(self.result_queue.get, _TERMINATE):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "TypeError: BackendCompilerFailed.__init__() missing 1 required positional argument: 'inner_exception'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/__init__.py\", line 2234, in __call__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return aot_autograd(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     compiled_fn = dispatch_and_compile()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     compiled_fn, _ = create_aot_dispatcher_function(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return _create_aot_dispatcher_function(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     compiled_fn, fw_metadata = compiler_fn(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n"
     ]
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "backend='inductor' raised:\nCalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmpc98yuwx_/main.c', '-O3', '-shared', '-fPIC', '-o', '/tmp/tmpc98yuwx_/cuda_utils.cpython-310-x86_64-linux-gnu.so', '-lcuda', '-L/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/triton/backends/nvidia/lib', '-L/lib/x86_64-linux-gnu', '-L/lib/i386-linux-gnu', '-I/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/triton/backends/nvidia/include', '-I/tmp/tmpc98yuwx_', '-I/usr/include/python3.10']' returned non-zero exit status 1.\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1446\u001b[0m, in \u001b[0;36mOutputGraph._call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     compiler_fn \u001b[38;5;241m=\u001b[39m WrapperBackend(compiler_fn)\n\u001b[0;32m-> 1446\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1447\u001b[0m _step_logger()(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone compiler function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py:129\u001b[0m, in \u001b[0;36mWrapBackendDebug.__call__\u001b[0;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     compiled_gm \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/__init__.py:2234\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   2232\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_fx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_fx\n\u001b[0;32m-> 2234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompile_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:1521\u001b[0m, in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m V\u001b[38;5;241m.\u001b[39mset_fake_mode(fake_mode), torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mtracing(\n\u001b[1;32m   1517\u001b[0m     tracing_context\n\u001b[1;32m   1518\u001b[0m ), compiled_autograd\u001b[38;5;241m.\u001b[39mdisable(), functorch_config\u001b[38;5;241m.\u001b[39mpatch(\n\u001b[1;32m   1519\u001b[0m     unlift_effect_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1520\u001b[0m ):\n\u001b[0;32m-> 1521\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maot_autograd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecompositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecompositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartition_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_inference_input_mutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/backends/common.py:72\u001b[0m, in \u001b[0;36mAotAutograd.__call__\u001b[0;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m enable_aot_logging(), patch_config:\n\u001b[0;32m---> 72\u001b[0m     cg \u001b[38;5;241m=\u001b[39m \u001b[43maot_module_simplified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1071\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler, cudagraphs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1071\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mGmWrapper):\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;66;03m# This function is called by the flatten_graph_inputs wrapper, which boxes\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m     \u001b[38;5;66;03m# the inputs so that they can be freed before the end of this scope.\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;66;03m# For overhead reasons, this is not the default wrapper, see comment:\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/122535/files#r1560096481\u001b[39;00m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1056\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.dispatch_and_compile\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compiled_autograd\u001b[38;5;241m.\u001b[39mdisable():\n\u001b[0;32m-> 1056\u001b[0m     compiled_fn, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctional_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:522\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_aot_dispatcher_function\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 522\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape_env\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:759\u001b[0m, in \u001b[0;36m_create_aot_dispatcher_function\u001b[0;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[1;32m    757\u001b[0m compiler_fn \u001b[38;5;241m=\u001b[39m choose_dispatcher(needs_autograd, aot_config)\n\u001b[0;32m--> 759\u001b[0m compiled_fn, fw_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_dup_fake_script_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn, fw_metadata\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:179\u001b[0m, in \u001b[0;36maot_dispatch_base\u001b[0;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TracingContext\u001b[38;5;241m.\u001b[39mreport_output_strides() \u001b[38;5;28;01mas\u001b[39;00m fwd_output_strides:\n\u001b[0;32m--> 179\u001b[0m     compiled_fw \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfw_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdated_flat_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fakified_out_wrapper\u001b[38;5;241m.\u001b[39mneeds_post_compile:\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:1350\u001b[0m, in \u001b[0;36mcompile_fx.<locals>.fw_compiler_base\u001b[0;34m(model, example_inputs, is_inference)\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_utils\u001b[38;5;241m.\u001b[39mdynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompile_fx.<locals>.fw_compiler_base\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fw_compiler_base\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_inference\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:1421\u001b[0m, in \u001b[0;36mcompile_fx.<locals>._fw_compiler_base\u001b[0;34m(model, example_inputs, is_inference)\u001b[0m\n\u001b[1;32m   1413\u001b[0m     user_visible_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mfromkeys(\n\u001b[1;32m   1414\u001b[0m         n\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m   1415\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m model_outputs[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mNode)\n\u001b[1;32m   1419\u001b[0m     )\n\u001b[0;32m-> 1421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_input_idxs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_static_input_idxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfixed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_inference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboxed_forward_device_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforward_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1429\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_visible_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_visible_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1430\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:475\u001b[0m, in \u001b[0;36mcompile_fx_inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m stack\u001b[38;5;241m.\u001b[39menter_context(DebugContext())\n\u001b[0;32m--> 475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrap_compiler_debug\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_compile_fx_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompiler_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minductor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py:85\u001b[0m, in \u001b[0;36mwrap_compiler_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# Call the compiler_fn - which is either aot_autograd or inductor\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# with fake inputs\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     inner_compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# TODO: Failures here are troublesome because no real inputs,\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# need a different serialization strategy\u001b[39;00m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:661\u001b[0m, in \u001b[0;36m_compile_fx_inner\u001b[0;34m(gm, example_inputs, cudagraphs, static_input_idxs, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, boxed_forward_device_index, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39m_is_inductor_static \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 661\u001b[0m     compiled_graph \u001b[38;5;241m=\u001b[39m \u001b[43mFxGraphCache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcodegen_and_compile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraph_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfx_graph_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfx_graph_remote_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/codecache.py:1334\u001b[0m, in \u001b[0;36mFxGraphCache.load\u001b[0;34m(compile_fx_fn, gm, example_inputs, fx_kwargs, inputs_to_check, local, remote)\u001b[0m\n\u001b[1;32m   1333\u001b[0m cache_event_time \u001b[38;5;241m=\u001b[39m start_time\n\u001b[0;32m-> 1334\u001b[0m compiled_graph \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_fx_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfx_kwargs\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1337\u001b[0m compiled_graph\u001b[38;5;241m.\u001b[39m_time_taken_ns \u001b[38;5;241m=\u001b[39m time_ns() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:570\u001b[0m, in \u001b[0;36m_compile_fx_inner.<locals>.codegen_and_compile\u001b[0;34m(gm, example_inputs, inputs_to_check, fx_kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;124;03mThis function calls fx_codegen_and_compile and also adds some extra metadata to the resulting\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03mcompiled fx graph. The metadata is saved to FXGraphCache.\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m compiled_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfx_codegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfx_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compiled_graph, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# We only return a string in aot mode, in which case we don't\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# need to do any post-compilation steps: we just return the string,\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;66;03m# which is the filename of the compiled code.\u001b[39;00m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:878\u001b[0m, in \u001b[0;36mfx_codegen_and_compile\u001b[0;34m(gm, example_inputs, cudagraphs, static_input_idxs, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[1;32m    877\u001b[0m _check_triton_bf16_support(graph)\n\u001b[0;32m--> 878\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_to_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    879\u001b[0m num_bytes, nodes_num_elem, node_runtimes \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mcount_bytes()\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/graph.py:1913\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_fn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1913\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcall\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/graph.py:1839\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1836\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[1;32m   1837\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraphLowering.compile_to_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_gen\u001b[39m\u001b[38;5;124m\"\u001b[39m, fwd_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1838\u001b[0m ):\n\u001b[0;32m-> 1839\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/graph.py:1845\u001b[0m, in \u001b[0;36mGraphLowering._compile_to_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1842\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcodecache\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PyCodeCache\n\u001b[1;32m   1844\u001b[0m code, linemap \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1845\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodegen_with_cpp_wrapper() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpp_wrapper \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m )\n\u001b[1;32m   1848\u001b[0m GraphLowering\u001b[38;5;241m.\u001b[39msave_output_code(code)\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/graph.py:1784\u001b[0m, in \u001b[0;36mGraphLowering.codegen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapper_code\u001b[38;5;241m.\u001b[39mpush_codegened_graph(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1784\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m   1787\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished codegen for all nodes. The list of kernel names available: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1788\u001b[0m     V\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mall_codegen_kernel_names,\n\u001b[1;32m   1789\u001b[0m )\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/scheduler.py:3383\u001b[0m, in \u001b[0;36mScheduler.codegen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3382\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScheduler.codegen\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 3383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_codegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/scheduler.py:3461\u001b[0m, in \u001b[0;36mScheduler._codegen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3460\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, (FusedSchedulerNode, SchedulerNode)):\n\u001b[0;32m-> 3461\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/codegen/cuda_combined_scheduling.py:80\u001b[0m, in \u001b[0;36mCUDACombinedScheduling.codegen_node\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcodegen_node\u001b[39m(\u001b[38;5;28mself\u001b[39m, node: Union[FusedSchedulerNode, SchedulerNode]):\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_triton_scheduling\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/codegen/simd.py:1155\u001b[0m, in \u001b[0;36mSIMDScheduling.codegen_node\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m   1153\u001b[0m schedule_log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSchedule:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, node_schedule)\n\u001b[0;32m-> 1155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen_node_schedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_schedule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf_accesses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnumel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/codegen/simd.py:1364\u001b[0m, in \u001b[0;36mSIMDScheduling.codegen_node_schedule\u001b[0;34m(self, node_schedule, buf_accesses, numel, reduction_numel)\u001b[0m\n\u001b[1;32m   1363\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m V\u001b[38;5;241m.\u001b[39mset_kernel_handler(kernel):\n\u001b[0;32m-> 1364\u001b[0m     src_code \u001b[38;5;241m=\u001b[39m \u001b[43mkernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1366\u001b[0m kernel_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefine_kernel(src_code, node_schedule, kernel)\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/codegen/triton.py:2661\u001b[0m, in \u001b[0;36mTritonKernel.codegen_kernel\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2646\u001b[0m triton_meta \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2647\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignature\u001b[39m\u001b[38;5;124m\"\u001b[39m: triton_meta_signature,\n\u001b[1;32m   2648\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m: DeviceProperties\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2651\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstants\u001b[39m\u001b[38;5;124m\"\u001b[39m: {},\n\u001b[1;32m   2652\u001b[0m }\n\u001b[1;32m   2654\u001b[0m inductor_meta \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2655\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautotune_hints\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautotune_hints),\n\u001b[1;32m   2656\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(Placeholder\u001b[38;5;241m.\u001b[39mDESCRIPTIVE_NAME),\n\u001b[1;32m   2657\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmutated_arg_names\u001b[39m\u001b[38;5;124m\"\u001b[39m: mutated_args,\n\u001b[1;32m   2658\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_x_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_x_dim,\n\u001b[1;32m   2659\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_load\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_load,\n\u001b[1;32m   2660\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_reduction\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_reduction,\n\u001b[0;32m-> 2661\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minductor_meta_common\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   2662\u001b[0m }\n\u001b[1;32m   2664\u001b[0m num_gb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/codegen/triton.py:2532\u001b[0m, in \u001b[0;36mTritonKernel.inductor_meta_common\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2529\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minductor_meta_common\u001b[39m():\n\u001b[1;32m   2531\u001b[0m     inductor_meta \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m-> 2532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_triton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtriton_hash_with_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   2533\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mare_deterministic_algorithms_enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mare_deterministic_algorithms_enabled(),\n\u001b[1;32m   2534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massert_indirect_indexing\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39massert_indirect_indexing,\n\u001b[1;32m   2535\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautotune_local_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39mautotune_local_cache,\n\u001b[1;32m   2536\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautotune_pointwise\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39mtriton\u001b[38;5;241m.\u001b[39mautotune_pointwise,\n\u001b[1;32m   2537\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautotune_remote_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39mautotune_remote_cache,\n\u001b[1;32m   2538\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_disable_caches\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39mforce_disable_caches,\n\u001b[1;32m   2539\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamic_scale_rblock\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39mdynamic_scale_rblock,\n\u001b[1;32m   2540\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_autotune\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39mmax_autotune,\n\u001b[1;32m   2541\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_autotune_pointwise\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39mmax_autotune_pointwise,\n\u001b[1;32m   2542\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_split_scan_rblock\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39mtriton\u001b[38;5;241m.\u001b[39mmin_split_scan_rblock,\n\u001b[1;32m   2543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspill_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39mtriton\u001b[38;5;241m.\u001b[39mspill_threshold,\n\u001b[1;32m   2544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore_cubin\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39mtriton\u001b[38;5;241m.\u001b[39mstore_cubin,\n\u001b[1;32m   2545\u001b[0m     }\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mhip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/utils/_triton.py:53\u001b[0m, in \u001b[0;36mtriton_hash_with_backend\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtriton\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompiler\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompiler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m triton_key\n\u001b[0;32m---> 53\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[43mtriton_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtriton_key()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend\u001b[38;5;241m.\u001b[39mhash()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/utils/_triton.py:45\u001b[0m, in \u001b[0;36mtriton_backend\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtriton\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mruntime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdriver\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m driver\n\u001b[0;32m---> 45\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_current_target\u001b[49m()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m make_backend(target)\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/triton/runtime/driver.py:23\u001b[0m, in \u001b[0;36mLazyProxy.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj, name)\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/triton/runtime/driver.py:20\u001b[0m, in \u001b[0;36mLazyProxy._initialize_obj\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/triton/runtime/driver.py:9\u001b[0m, in \u001b[0;36m_create_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(actives)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m active drivers (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactives\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). There should only be one.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mactives\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/triton/backends/nvidia/driver.py:371\u001b[0m, in \u001b[0;36mCudaDriver.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mutils \u001b[38;5;241m=\u001b[39m \u001b[43mCudaUtils\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# TODO: make static\u001b[39;00m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlauncher_cls \u001b[38;5;241m=\u001b[39m CudaLauncher\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/triton/backends/nvidia/driver.py:80\u001b[0m, in \u001b[0;36mCudaUtils.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 80\u001b[0m     mod \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_module_from_src\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdriver.c\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda_utils\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_binary \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39mload_binary\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/triton/backends/nvidia/driver.py:57\u001b[0m, in \u001b[0;36mcompile_module_from_src\u001b[0;34m(src, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(src)\n\u001b[0;32m---> 57\u001b[0m so \u001b[38;5;241m=\u001b[39m \u001b[43m_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmpdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlibrary_dirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlibraries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(so, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/triton/runtime/build.py:48\u001b[0m, in \u001b[0;36m_build\u001b[0;34m(name, src, srcdir, library_dirs, include_dirs, libraries)\u001b[0m\n\u001b[1;32m     47\u001b[0m cc_cmd \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-I\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mdir\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;129;01min\u001b[39;00m include_dirs]\n\u001b[0;32m---> 48\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcc_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:369\u001b[0m, in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         cmd \u001b[38;5;241m=\u001b[39m popenargs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, cmd)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['/usr/bin/gcc', '/tmp/tmpc98yuwx_/main.c', '-O3', '-shared', '-fPIC', '-o', '/tmp/tmpc98yuwx_/cuda_utils.cpython-310-x86_64-linux-gnu.so', '-lcuda', '-L/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/triton/backends/nvidia/lib', '-L/lib/x86_64-linux-gnu', '-L/lib/i386-linux-gnu', '-I/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/triton/backends/nvidia/include', '-I/tmp/tmpc98yuwx_', '-I/usr/include/python3.10']' returned non-zero exit status 1.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m tracker\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     16\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTinyLlama/TinyLlama-1.1B-Chat-v1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m          \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m prompts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, my name is\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe president of the United States is\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe capital of France is\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe future of AI is\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m ]\n\u001b[1;32m     26\u001b[0m sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams(\n\u001b[1;32m     27\u001b[0m     mac_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     28\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, \n\u001b[1;32m     29\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m)\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/utils.py:1022\u001b[0m, in \u001b[0;36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1015\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1017\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1018\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1019\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m         )\n\u001b[0;32m-> 1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/entrypoints/llm.py:242\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Logic to switch between engines is done at runtime instead of import\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# to avoid import order issues\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_engine_class()\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/engine/llm_engine.py:489\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    487\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/engine/llm_engine.py:276\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, mm_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m executor_class(vllm_config\u001b[38;5;241m=\u001b[39mvllm_config, )\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mrunner_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpooling\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_kv_caches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# If usage stat is enabled, collect relevant info.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_usage_stats_enabled():\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/engine/llm_engine.py:421\u001b[0m, in \u001b[0;36mLLMEngine._initialize_kv_caches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize the KV cache in the worker(s).\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03mThe workers will determine the number of blocks in both the GPU cache\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03mand the swap CPU cache.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    419\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    420\u001b[0m num_gpu_blocks, num_cpu_blocks \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetermine_num_available_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks_override \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    424\u001b[0m     num_gpu_blocks_override \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks_override\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/executor/executor_base.py:102\u001b[0m, in \u001b[0;36mExecutorBase.determine_num_available_blocks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdetermine_num_available_blocks\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m     90\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Determine the number of available blocks for the GPU KV cache and\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    swappable CPU KV cache.\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    appended to.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollective_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdetermine_num_available_blocks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m([r[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results])\n\u001b[1;32m    104\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m([r[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/executor/executor_base.py:316\u001b[0m, in \u001b[0;36mDistributedExecutorBase.collective_rpc\u001b[0;34m(self, method, timeout, args, kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcollective_rpc\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    312\u001b[0m                    method: Union[\u001b[38;5;28mstr\u001b[39m, Callable],\n\u001b[1;32m    313\u001b[0m                    timeout: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m                    args: Tuple \u001b[38;5;241m=\u001b[39m (),\n\u001b[1;32m    315\u001b[0m                    kwargs: Optional[Dict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/executor/mp_distributed_executor.py:185\u001b[0m, in \u001b[0;36mMultiprocessingDistributedExecutor._run_workers\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Start all remote workers first.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m worker_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    181\u001b[0m     worker\u001b[38;5;241m.\u001b[39mexecute_method(sent_method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m worker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\n\u001b[1;32m    183\u001b[0m ]\n\u001b[0;32m--> 185\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msent_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Get the results of the workers.\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [driver_worker_output\n\u001b[1;32m    190\u001b[0m         ] \u001b[38;5;241m+\u001b[39m [output\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m worker_outputs]\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/utils.py:2196\u001b[0m, in \u001b[0;36mrun_method\u001b[0;34m(obj, method, args, kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2195\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(method, obj)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m-> 2196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/worker/worker.py:229\u001b[0m, in \u001b[0;36mWorker.determine_num_available_blocks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# Execute a forward pass with dummy inputs to profile the memory usage\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# of the model.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m memory_profiling(\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbaseline_snapshot,\n\u001b[1;32m    228\u001b[0m         weights_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_runner\u001b[38;5;241m.\u001b[39mmodel_memory_usage) \u001b[38;5;28;01mas\u001b[39;00m result:\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofile_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_memory_footprint_increased_during_profiling()\n\u001b[1;32m    233\u001b[0m memory_for_current_instance \u001b[38;5;241m=\u001b[39m total_gpu_memory \u001b[38;5;241m*\u001b[39m \\\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mgpu_memory_utilization\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/worker/model_runner.py:1235\u001b[0m, in \u001b[0;36mGPUModelRunnerBase.profile_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1232\u001b[0m max_num_batched_tokens \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m   1233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler_config\u001b[38;5;241m.\u001b[39mmax_num_batched_tokens\n\u001b[1;32m   1234\u001b[0m max_num_seqs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler_config\u001b[38;5;241m.\u001b[39mmax_num_seqs\n\u001b[0;32m-> 1235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dummy_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_num_batched_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_seqs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/worker/model_runner.py:1346\u001b[0m, in \u001b[0;36mGPUModelRunnerBase._dummy_run\u001b[0;34m(self, max_num_batched_tokens, max_num_seqs)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_input\u001b[38;5;241m.\u001b[39mattn_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     model_input\u001b[38;5;241m.\u001b[39mattn_metadata\u001b[38;5;241m.\u001b[39menable_kv_scales_calculation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1346\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1347\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_config:\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;66;03m# Remove dummy loras.\u001b[39;00m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/worker/model_runner.py:1724\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, model_input, kv_caches, intermediate_tensors, num_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m bypass_model_exec:\n\u001b[1;32m   1722\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_forward_context(model_input\u001b[38;5;241m.\u001b[39mattn_metadata,\n\u001b[1;32m   1723\u001b[0m                              \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_config, virtual_engine):\n\u001b[0;32m-> 1724\u001b[0m         hidden_or_intermediate_states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_executable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_positions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m            \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mMultiModalKwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmulti_modal_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1732\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mseqlen_agnostic_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1733\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1734\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1737\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config\u001b[38;5;241m.\u001b[39mcollect_model_forward_time):\n\u001b[1;32m   1738\u001b[0m     model_forward_end\u001b[38;5;241m.\u001b[39mrecord()\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/model_executor/models/llama.py:547\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors, inputs_embeds)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    540\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m     inputs_embeds: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    546\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[torch\u001b[38;5;241m.\u001b[39mTensor, IntermediateTensors]:\n\u001b[0;32m--> 547\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m                              \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_output\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/compilation/decorators.py:172\u001b[0m, in \u001b[0;36m_support_torch_compile.<locals>.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# torch.compiler.is_compiling() means we are inside the compilation\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# e.g. TPU has the compilation logic in model runner, so we don't\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# need to compile the model inside.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_not_compile \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcompiler\u001b[38;5;241m.\u001b[39mis_compiling():\n\u001b[0;32m--> 172\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# the first compilation needs to have dynamic shapes marked\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiled_codes) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/model_executor/models/llama.py:359\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors, inputs_embeds)\u001b[0m\n\u001b[1;32m    357\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 359\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_input_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/model_executor/models/llama.py:344\u001b[0m, in \u001b[0;36mLlamaModel.get_input_embeddings\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_input_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py:406\u001b[0m, in \u001b[0;36mVocabParallelEmbedding.forward\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_):\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtp_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Build the mask.\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m         masked_input, input_mask \u001b[38;5;241m=\u001b[39m \u001b[43mget_masked_input_and_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshard_indices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morg_vocab_start_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshard_indices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morg_vocab_end_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshard_indices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_org_vocab_padding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshard_indices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madded_vocab_start_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshard_indices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madded_vocab_end_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    413\u001b[0m         masked_input \u001b[38;5;241m=\u001b[39m input_\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:465\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    461\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    462\u001b[0m )\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    469\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    470\u001b[0m     )\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:1269\u001b[0m, in \u001b[0;36mCatchErrorsWrapper.__call__\u001b[0;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m   1263\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[1;32m   1264\u001b[0m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks, frame_state\n\u001b[1;32m   1265\u001b[0m             )\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[0;32m-> 1269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:1064\u001b[0m, in \u001b[0;36mConvertFrame.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m   1062\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1064\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1067\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:526\u001b[0m, in \u001b[0;36mConvertFrameAssert.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    510\u001b[0m compile_id \u001b[38;5;241m=\u001b[39m CompileId(frame_id, frame_compile_id)\n\u001b[1;32m    512\u001b[0m signpost_event(\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_frame_assert._compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m     },\n\u001b[1;32m    524\u001b[0m )\n\u001b[0;32m--> 526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:924\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m    922\u001b[0m guarded_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 924\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:666\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_compile.compile_inner\u001b[39m\u001b[38;5;124m\"\u001b[39m, phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentire_frame_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m CompileTimeInstructionCounter\u001b[38;5;241m.\u001b[39mrecord():\n\u001b[0;32m--> 666\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_utils_internal.py:87\u001b[0m, in \u001b[0;36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39mprofile_compile_time(\n\u001b[1;32m     90\u001b[0m     function, phase_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     91\u001b[0m )\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:699\u001b[0m, in \u001b[0;36m_compile.<locals>._compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    697\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 699\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1322\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1319\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m   1320\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m-> 1322\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:219\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m exit_stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[1;32m    216\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39m_symbolic_trace\u001b[38;5;241m.\u001b[39m_maybe_revert_all_patches()\n\u001b[1;32m    217\u001b[0m )\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:634\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[0;32m--> 634\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[1;32m    636\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2796\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2796\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:983\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 983\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    984\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:895\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 895\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2987\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2986\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mRETURN_VALUE\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[0;32m-> 2987\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2972\u001b[0m, in \u001b[0;36mInstructionTranslator._return\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2967\u001b[0m _step_logger()(\n\u001b[1;32m   2968\u001b[0m     logging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[1;32m   2969\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdynamo done tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39mopname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2970\u001b[0m )\n\u001b[1;32m   2971\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m triggered compile\u001b[39m\u001b[38;5;124m\"\u001b[39m, inst\u001b[38;5;241m.\u001b[39mopname)\n\u001b[0;32m-> 2972\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_subgraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2973\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGraphCompileReason\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   2976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2977\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2978\u001b[0m return_inst \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2979\u001b[0m     create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2980\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2981\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_CONST\u001b[39m\u001b[38;5;124m\"\u001b[39m, argval\u001b[38;5;241m=\u001b[39minst\u001b[38;5;241m.\u001b[39margval)\n\u001b[1;32m   2982\u001b[0m )\n\u001b[1;32m   2983\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39madd_output_instructions([return_inst])\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1142\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m   1139\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count_calls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pass2\u001b[38;5;241m.\u001b[39mgraph_outputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1141\u001b[0m     output\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m-> 1142\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_and_call_fx_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpass2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_output_vars\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m     )\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pass2\u001b[38;5;241m.\u001b[39mgraph_outputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1146\u001b[0m         output\u001b[38;5;241m.\u001b[39mappend(pass2\u001b[38;5;241m.\u001b[39mcreate_store(graph_output_var))\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1369\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracing_context\u001b[38;5;241m.\u001b[39mfake_mode \u001b[38;5;241m=\u001b[39m backend_fake_mode\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_global_state():\n\u001b[0;32m-> 1369\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lazy_graph_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LazyGraphModule\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compiled_fn, _LazyGraphModule) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(compiled_fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), _LazyGraphModule)\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m compiled_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_lazy_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;66;03m# this is a _LazyGraphModule. This makes it easier for dynamo to\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m     \u001b[38;5;66;03m# optimize a _LazyGraphModule.\u001b[39;00m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1416\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_user_compiler\u001b[39m(\u001b[38;5;28mself\u001b[39m, gm: fx\u001b[38;5;241m.\u001b[39mGraphModule) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CompiledFn:\n\u001b[1;32m   1413\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[1;32m   1414\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputGraph.call_user_compiler\u001b[39m\u001b[38;5;124m\"\u001b[39m, phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1415\u001b[0m     ):\n\u001b[0;32m-> 1416\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1465\u001b[0m, in \u001b[0;36mOutputGraph._call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1463\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1465\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BackendCompilerFailed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiler_fn, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1467\u001b[0m signpost_event(\n\u001b[1;32m   1468\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1469\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputGraph.call_user_compiler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1475\u001b[0m     },\n\u001b[1;32m   1476\u001b[0m )\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m: backend='inductor' raised:\nCalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmpc98yuwx_/main.c', '-O3', '-shared', '-fPIC', '-o', '/tmp/tmpc98yuwx_/cuda_utils.cpython-310-x86_64-linux-gnu.so', '-lcuda', '-L/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/triton/backends/nvidia/lib', '-L/lib/x86_64-linux-gnu', '-L/lib/i386-linux-gnu', '-I/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/triton/backends/nvidia/include', '-I/tmp/tmpc98yuwx_', '-I/usr/include/python3.10']' returned non-zero exit status 1.\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return inner_compile(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     compiled_graph = FxGraphCache.load(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     compiled_graph = compile_fx_fn(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     compiled_fn = graph.compile_to_fn()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return self.compile_to_module().call\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return self._compile_to_module()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1784, in codegen\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     self.scheduler.codegen()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 3383, in codegen\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return self._codegen()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 3461, in _codegen\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     self.get_backend(device).codegen_node(node)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/codegen/cuda_combined_scheduling.py\", line 80, in codegen_node\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return self._triton_scheduling.codegen_node(node)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/codegen/simd.py\", line 1155, in codegen_node\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return self.codegen_node_schedule(node_schedule, buf_accesses, numel, rnumel)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/codegen/simd.py\", line 1364, in codegen_node_schedule\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     src_code = kernel.codegen_kernel()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/codegen/triton.py\", line 2661, in codegen_kernel\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     **self.inductor_meta_common(),\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_inductor/codegen/triton.py\", line 2532, in inductor_meta_common\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     \"backend_hash\": torch.utils._triton.triton_hash_with_backend(),\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/utils/_triton.py\", line 53, in triton_hash_with_backend\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     backend = triton_backend()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/utils/_triton.py\", line 45, in triton_backend\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     target = driver.active.get_current_target()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/triton/runtime/driver.py\", line 23, in __getattr__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     self._initialize_obj()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/triton/runtime/driver.py\", line 20, in _initialize_obj\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     self._obj = self._init_fn()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/triton/runtime/driver.py\", line 9, in _create_driver\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return actives[0]()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/triton/backends/nvidia/driver.py\", line 371, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     self.utils = CudaUtils()  # TODO: make static\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/triton/backends/nvidia/driver.py\", line 80, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     mod = compile_module_from_src(Path(os.path.join(dirname, \"driver.c\")).read_text(), \"cuda_utils\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/triton/backends/nvidia/driver.py\", line 57, in compile_module_from_src\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     so = _build(name, src_path, tmpdir, library_dirs(), include_dir, libraries)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/triton/runtime/build.py\", line 48, in _build\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     ret = subprocess.check_call(cc_cmd)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/usr/lib/python3.10/subprocess.py\", line 369, in check_call\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     raise CalledProcessError(retcode, cmd)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242] subprocess.CalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmpq3nyfub5/main.c', '-O3', '-shared', '-fPIC', '-o', '/tmp/tmpq3nyfub5/cuda_utils.cpython-310-x86_64-linux-gnu.so', '-lcuda', '-L/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/triton/backends/nvidia/lib', '-L/lib/x86_64-linux-gnu', '-L/lib/i386-linux-gnu', '-I/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/triton/backends/nvidia/include', '-I/tmp/tmpq3nyfub5', '-I/usr/include/python3.10']' returned non-zero exit status 1.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242] \n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242] The above exception was the direct cause of the following exception:\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242] \n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py\", line 236, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/utils.py\", line 2196, in run_method\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/worker/worker.py\", line 229, in determine_num_available_blocks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     self.model_runner.profile_run()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 1235, in profile_run\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     self._dummy_run(max_num_batched_tokens, max_num_seqs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 1346, in _dummy_run\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     self.execute_model(model_input, kv_caches, intermediate_tensors)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 1724, in execute_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     hidden_or_intermediate_states = model_executable(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/model_executor/models/llama.py\", line 547, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     model_output = self.model(input_ids, positions, kv_caches,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/compilation/decorators.py\", line 172, in __call__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return self.forward(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/model_executor/models/llama.py\", line 359, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     hidden_states = self.get_input_embeddings(input_ids)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/model_executor/models/llama.py\", line 344, in get_input_embeddings\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return self.embed_tokens(input_ids)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\", line 406, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     masked_input, input_mask = get_masked_input_and_mask(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 465, in _fn\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return fn(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1269, in __call__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return self._torchdynamo_orig_callable(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     result = self._inner_convert(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return _compile(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return _compile_inner(code, one_graph, hooks, transform)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return function(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     out_code = transform_code_object(code, transform)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     transformations(instructions, code_options)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return fn(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     tracer.run()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     super().run()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     while self.step():\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     self.dispatch_table[inst.opcode](self, inst)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     self._return(inst)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     self.output.compile_subgraph(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     compiled_fn = self.call_user_compiler(gm)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     return self._call_user_compiler(gm)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]   File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242] CalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmpq3nyfub5/main.c', '-O3', '-shared', '-fPIC', '-o', '/tmp/tmpq3nyfub5/cuda_utils.cpython-310-x86_64-linux-gnu.so', '-lcuda', '-L/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/triton/backends/nvidia/lib', '-L/lib/x86_64-linux-gnu', '-L/lib/i386-linux-gnu', '-I/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/triton/backends/nvidia/include', '-I/tmp/tmpq3nyfub5', '-I/usr/include/python3.10']' returned non-zero exit status 1.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242] \n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242] \n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242] \n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242] You can suppress this exception and fall back to eager by setting:\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     import torch._dynamo\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242]     torch._dynamo.config.suppress_errors = True\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3360768)\u001b[0;0m ERROR 03-04 23:47:05 multiproc_worker_utils.py:242] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/bin/vllm\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n",
      "    args.dispatch_function(args)\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/entrypoints/cli/serve.py\", line 34, in cmd\n",
      "    uvloop.run(run_server(args))\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/uvloop/__init__.py\", line 82, in run\n",
      "    return loop.run_until_complete(wrapper())\n",
      "  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/uvloop/__init__.py\", line 61, in wrapper\n",
      "    return await main\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 947, in run_server\n",
      "    async with build_async_engine_client(args) as engine_client:\n",
      "  File \"/usr/lib/python3.10/contextlib.py\", line 199, in __aenter__\n",
      "    return await anext(self.gen)\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 139, in build_async_engine_client\n",
      "    async with build_async_engine_client_from_engine_args(\n",
      "  File \"/usr/lib/python3.10/contextlib.py\", line 199, in __aenter__\n",
      "    return await anext(self.gen)\n",
      "  File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 233, in build_async_engine_client_from_engine_args\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Engine process failed to start. See stack trace for the root cause.\n"
     ]
    }
   ],
   "source": [
    "from codecarbon import EmissionsTracker\n",
    "import requests\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from vllm import LLMEngine, SamplingParams, LLM\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "\n",
    "# Start vLLM server\n",
    "server_process = subprocess.Popen([\"vllm\", \"serve\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"])\n",
    "\n",
    "tracker = EmissionsTracker(measure_power_secs=1)\n",
    "tracker.start()\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "llm = LLM(model=model_name,\n",
    "          tensor_parallel_size=2)\n",
    "\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "sampling_params = SamplingParams(\n",
    "    mac_tokens=50,\n",
    "    temperature=0.8, \n",
    "    top_p=0.95)\n",
    "\n",
    "# Generate responses for each prompt using vLLM.\n",
    "responses = engine.generate(prompts, sampling_params)\n",
    "\n",
    "# Display the prompt and the generated response.\n",
    "for prompt, response in zip(prompts, responses):\n",
    "    print(f\"Prompt: {prompt}\\nResponse: {response.text}\\n{'-'*60}\\n\")\n",
    "\n",
    "tracker.stop()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
