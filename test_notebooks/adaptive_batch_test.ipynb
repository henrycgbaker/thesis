{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using adaptive batching: created 2 batches.\n",
      "[Process 2671768][GPU 0] — Completed tokenisation of batch 1/2\n",
      "[Process 2671768][GPU 0] — Completed batch inference 1/2\n",
      "[Process 2671768][GPU 0] — Completed tokenisation of batch 2/2\n",
      "[Process 2671768][GPU 0] — Completed batch inference 2/2\n",
      "Inference results: {'num_input_prompts': 3, 'avg_latency_ms': 0.10345893679186702, 'total_input_tokens': 6144, 'total_generated_tokens': 150}\n",
      "Concatenated input_ids shape: torch.Size([3, 2048])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "from typing import List, Any, Dict\n",
    "\n",
    "# --- Dummy Classes ---\n",
    "\n",
    "class DummyTokenizer:\n",
    "    def __init__(self, model_max_length=2048):\n",
    "        self.model_max_length = model_max_length\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        # Simple whitespace tokenization.\n",
    "        return text.split()\n",
    "    \n",
    "    def __call__(self, texts, truncation=True, max_length=None, padding=\"max_length\", return_tensors=\"pt\"):\n",
    "        # For each text, simulate tokenization as a list of \"tokens\" (dummy: each word becomes token id 1).\n",
    "        batch_size = len(texts)\n",
    "        token_lists = [text.split()[:max_length] for text in texts]\n",
    "        lengths = [len(t) for t in token_lists]\n",
    "        # Create a tensor with fixed max_length\n",
    "        input_ids = torch.zeros((batch_size, max_length), dtype=torch.long)\n",
    "        attention_mask = torch.zeros((batch_size, max_length), dtype=torch.long)\n",
    "        for i, tokens in enumerate(token_lists):\n",
    "            l = len(tokens)\n",
    "            input_ids[i, :l] = torch.ones(l, dtype=torch.long)\n",
    "            attention_mask[i, :l] = 1\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "class DummyModel:\n",
    "    def __init__(self, max_length=2048):\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def generate(self, input_ids, **kwargs):\n",
    "        # For testing, simply append a fixed number of tokens (e.g., ones) to simulate generation.\n",
    "        max_new_tokens = kwargs.get(\"max_new_tokens\", 10)\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        generated = torch.cat([input_ids, torch.ones((batch_size, max_new_tokens), dtype=torch.long)], dim=1)\n",
    "        return generated\n",
    "\n",
    "class DummyAccelerator:\n",
    "    def __init__(self, device=\"cpu\", process_index=0):\n",
    "        self.device = torch.device(device)\n",
    "        self.process_index = process_index\n",
    "        self.local_process_index = process_index\n",
    "    \n",
    "    def print(self, *args, **kwargs):\n",
    "        print(*args, **kwargs)\n",
    "    \n",
    "    def wait_for_everyone(self):\n",
    "        # In this dummy, simply pass.\n",
    "        pass\n",
    "\n",
    "class DummyExperimentConfig:\n",
    "    def __init__(self):\n",
    "        self.max_input_tokens = 2048   # The truncation limit for tokenization.\n",
    "        self.max_output_tokens = 50    # Number of new tokens to generate.\n",
    "        self.decoder_temperature = 1.0\n",
    "        self.save_outputs = True\n",
    "        self.batching_options = {\n",
    "            \"fixed_max_batch_size\": 12,\n",
    "            \"adaptive_batching\": True,\n",
    "            \"adaptive_max_tokens\": 100   # Batching is based on estimated token counts.\n",
    "        }\n",
    "\n",
    "# --- Functions Under Test ---\n",
    "\n",
    "def adaptive_batching(prompts: List[str],\n",
    "                      tokenizer: Any,\n",
    "                      adaptive_max_tokens: int,\n",
    "                      max_prompt_tokens: int,\n",
    "                      max_batch_size: int = None) -> List[List[str]]:\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for prompt in prompts:\n",
    "        raw_token_count = len(tokenizer.tokenize(prompt))\n",
    "        token_count = min(raw_token_count, max_prompt_tokens)\n",
    "        \n",
    "        if max_batch_size is not None and len(current_batch) >= max_batch_size:\n",
    "            batches.append(current_batch)\n",
    "            current_batch = []\n",
    "            current_tokens = 0\n",
    "        \n",
    "        if current_batch and (current_tokens + token_count > adaptive_max_tokens):\n",
    "            batches.append(current_batch)\n",
    "            current_batch = [prompt]\n",
    "            current_tokens = token_count\n",
    "        else:\n",
    "            current_batch.append(prompt)\n",
    "            current_tokens += token_count\n",
    "\n",
    "    if current_batch:\n",
    "        batches.append(current_batch)\n",
    "    \n",
    "    return batches\n",
    "\n",
    "def batch_tokenise_truncate(prompts: List[str], tokenizer: Any, max_input_tokens: int, batch_size: int = 32) -> Dict[str, torch.Tensor]:\n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "    \n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch = prompts[i : i + batch_size]\n",
    "        encoded = tokenizer(\n",
    "            batch,\n",
    "            truncation=True,\n",
    "            max_length=max_input_tokens,\n",
    "            padding=\"max_length\", \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # Extra safeguard: slice in case truncation isn’t applied.\n",
    "        encoded[\"input_ids\"] = encoded[\"input_ids\"][:, :max_input_tokens]\n",
    "        if \"attention_mask\" in encoded:\n",
    "            encoded[\"attention_mask\"] = encoded[\"attention_mask\"][:, :max_input_tokens]\n",
    "            \n",
    "        all_input_ids.append(encoded[\"input_ids\"])\n",
    "        if \"attention_mask\" in encoded:\n",
    "            all_attention_mask.append(encoded[\"attention_mask\"])\n",
    "    \n",
    "    tokenised_inputs = {\"input_ids\": torch.cat(all_input_ids, dim=0)}\n",
    "    if all_attention_mask:\n",
    "        tokenised_inputs[\"attention_mask\"] = torch.cat(all_attention_mask, dim=0)\n",
    "    \n",
    "    return tokenised_inputs\n",
    "\n",
    "def calculate_inference_metrics(num_input_prompts, latencies, total_input_tokens, total_generated_tokens):\n",
    "    return {\n",
    "        \"num_input_prompts\": num_input_prompts,\n",
    "        \"avg_latency_ms\": sum(latencies)/len(latencies) if latencies else None,\n",
    "        \"total_input_tokens\": total_input_tokens,\n",
    "        \"total_generated_tokens\": total_generated_tokens,\n",
    "    }\n",
    "\n",
    "def run_gen_inference(model, experiment_config, prompts, tokenizer, accelerator):\n",
    "    max_input_tokens = experiment_config.max_input_tokens \n",
    "    max_output_tokens = experiment_config.max_output_tokens\n",
    "    decoder_temperature = experiment_config.decoder_temperature\n",
    "    fixed_max_batch_size = experiment_config.batching_options.get(\"fixed_max_batch_size\", 8)\n",
    "    use_adaptive = experiment_config.batching_options.get(\"adaptive_batching\", False)\n",
    "        \n",
    "    token_id_outputs = []\n",
    "    latencies = []\n",
    "    total_generated_tokens = 0\n",
    "    total_input_tokens = 0  \n",
    "    all_input_ids_batches = []\n",
    "    device = accelerator.device\n",
    "\n",
    "    if use_adaptive:\n",
    "        adaptive_max_tokens = experiment_config.batching_options.get(\"adaptive_max_tokens\", max_input_tokens)\n",
    "        batches = adaptive_batching(prompts=prompts, \n",
    "                                    tokenizer=tokenizer, \n",
    "                                    adaptive_max_tokens=adaptive_max_tokens, \n",
    "                                    max_prompt_tokens=max_input_tokens, \n",
    "                                    max_batch_size=fixed_max_batch_size)\n",
    "        accelerator.print(f\"Using adaptive batching: created {len(batches)} batches.\")\n",
    "    else:\n",
    "        batches = [prompts[i:i+fixed_max_batch_size] for i in range(0, len(prompts), fixed_max_batch_size)]\n",
    "        accelerator.print(f\"Using fixed batching (non-adaptive): created {len(batches)} batches.\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(batches):\n",
    "        tokenised_batch = batch_tokenise_truncate(\n",
    "            prompts=batch,\n",
    "            tokenizer=tokenizer,\n",
    "            max_input_tokens=max_input_tokens,\n",
    "            batch_size=len(batch)\n",
    "        )\n",
    "        batch_input_ids = tokenised_batch[\"input_ids\"]\n",
    "        total_input_tokens += batch_input_ids.numel()\n",
    "        all_input_ids_batches.append(batch_input_ids)\n",
    "        \n",
    "        if \"attention_mask\" in tokenised_batch:\n",
    "            batch_encoded = {\n",
    "                \"input_ids\": batch_input_ids.to(device),\n",
    "                \"attention_mask\": tokenised_batch[\"attention_mask\"].to(device)\n",
    "            }\n",
    "        else:\n",
    "            batch_encoded = {\"input_ids\": batch_input_ids.to(device)}\n",
    "        \n",
    "        gpu_id = accelerator.device.index if accelerator.device.type == 'cuda' else 0\n",
    "        print(f\"[Process {os.getpid()}][GPU {gpu_id}] — Completed tokenisation of batch {batch_idx + 1}/{len(batches)}\")\n",
    "\n",
    "        if decoder_temperature is not None and decoder_temperature > 0:\n",
    "            generation_kwargs = {\"max_new_tokens\": max_output_tokens, \"do_sample\": True, \"temperature\": decoder_temperature}\n",
    "        else:\n",
    "            generation_kwargs = {\"max_new_tokens\": max_output_tokens, \"do_sample\": False}\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            token_id_batch_output = model.generate(batch_encoded[\"input_ids\"], **generation_kwargs)\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize(device)\n",
    "        end_time = time.perf_counter()\n",
    "        latencies.append((end_time - start_time) * 1000.0)\n",
    "        print(f\"[Process {os.getpid()}][GPU {gpu_id}] — Completed batch inference {batch_idx + 1}/{len(batches)}\")\n",
    "        \n",
    "        for j in range(batch_input_ids.size(0)):\n",
    "            prompt_len = batch_input_ids[j].shape[0]\n",
    "            gen_len = token_id_batch_output[j].shape[0] - prompt_len\n",
    "            total_generated_tokens += gen_len\n",
    "        \n",
    "        if experiment_config.save_outputs:\n",
    "            token_id_outputs.append(token_id_batch_output)\n",
    "    \n",
    "    concatenated_input_ids = torch.cat(all_input_ids_batches, dim=0)\n",
    "    \n",
    "    inference_results = calculate_inference_metrics(\n",
    "        num_input_prompts=len(prompts),\n",
    "        latencies=latencies,\n",
    "        total_input_tokens=total_input_tokens,\n",
    "        total_generated_tokens=total_generated_tokens\n",
    "    )\n",
    "    \n",
    "    if not experiment_config.save_outputs:\n",
    "        token_id_outputs = None\n",
    "        \n",
    "    return token_id_outputs, concatenated_input_ids, inference_results\n",
    "\n",
    "# --- Testing Script ---\n",
    "\n",
    "dummy_tokenizer = DummyTokenizer(model_max_length=2048)\n",
    "dummy_model = DummyModel(max_length=2048)\n",
    "dummy_accelerator = DummyAccelerator(device=\"cpu\", process_index=0)\n",
    "dummy_experiment_config = DummyExperimentConfig()\n",
    "\n",
    "# Create a list of dummy prompts.\n",
    "prompts = [\n",
    "    \"This is a short prompt.\",\n",
    "    \"Here is a slightly longer prompt that should still be under the adaptive limit.\",\n",
    "    \"This is a prompt that is intentionally made very long \" * 50  # repeat to create a very long prompt.\n",
    "]\n",
    "\n",
    "# Run the inference function.\n",
    "outputs, concatenated_input_ids, inference_results = run_gen_inference(\n",
    "    model=dummy_model,\n",
    "    experiment_config=dummy_experiment_config,\n",
    "    prompts=prompts,\n",
    "    tokenizer=dummy_tokenizer,\n",
    "    accelerator=dummy_accelerator\n",
    ")\n",
    "\n",
    "print(\"Inference results:\", inference_results)\n",
    "print(\"Concatenated input_ids shape:\", concatenated_input_ids.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
