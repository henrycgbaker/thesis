[
    {
        "CONFIGURATION_RUN_#0078": {
            "setup": {
                "experiment_id": "0078",
                "date_time": "April 11, 2025 at 09:26:09 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A1_Max_Throughput_Exploit",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 11.096081339001103,
                        "average_latency_ms_per_batch": 11096.081339001103,
                        "throughput_queries_per_sec": 11.535603974900466,
                        "throughput_tokens_per_sec": 1476.5573087872597
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1577541632,
                        "gpu_max_memory_allocated_bytes": 1577541632,
                        "gpu_current_memory_reserved_bytes": 3577741312,
                        "gpu_max_memory_reserved_bytes": 3577741312
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.5,
                        "cpu_memory_usage_bytes": 2715783168
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0078",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 723.637883079102,
                            "process_3": 433.5161846534261,
                            "process_0": 791.7881343381644,
                            "process_1": 0.0
                        },
                        "ram_power": {
                            "process_2": 0.9817714691162109,
                            "process_3": 0.984708309173584,
                            "process_0": 0.9477167129516602,
                            "process_1": 0.9787745475769044
                        },
                        "cpu_energy": {
                            "process_2": 0.0004812742758748527,
                            "process_3": 0.0007446906289998197,
                            "process_0": 0.00034414134287476375,
                            "process_1": 0.0005571450775936456
                        },
                        "gpu_energy": {
                            "process_2": 0.0026120998674559193,
                            "process_3": 0.003750795500633841,
                            "process_0": 0.00190404735656996,
                            "process_1": 0.0028118903050659005
                        },
                        "ram_energy": {
                            "process_2": 3.7599286589919783e-06,
                            "process_3": 5.9304332152104325e-06,
                            "process_0": 2.705767955914554e-06,
                            "process_1": 4.304855219499009e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.003097134071989764,
                            "process_3": 0.004501416562848871,
                            "process_0": 0.0022508944674006384,
                            "process_1": 0.003373340237879046
                        },
                        "total_energy_joules": {
                            "process_2": 11149.68265916315,
                            "process_3": 16205.099626255937,
                            "process_0": 8103.220082642299,
                            "process_1": 12144.024856364565
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 487.2355505176731,
                        "ram_power_avg": 0.9732427597045898,
                        "cpu_energy_total": 0.0021272513253430816,
                        "gpu_energy_total": 0.011078833029725621,
                        "ram_energy_total": 1.6700985049615972e-05,
                        "total_energy_kwh": 0.01322278534011832,
                        "total_energy_joules": 47602.027224425954
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.34418702217776354,
                        "joules_per_token": 2.905397169459592,
                        "flops_per_joule": 21733194.746570498,
                        "joules_per_flop": 4.60125633465738e-08
                    },
                    "per-process_emissions": [
                        0.0011798532247245007,
                        0.0017148146396172774,
                        0.0008574782473562732,
                        0.0012850739636200226
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0078": {
            "setup": {
                "experiment_id": "0078",
                "date_time": "April 11, 2025 at 09:26:09 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A1_Max_Throughput_Exploit",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 11.096081339001103,
                        "average_latency_ms_per_batch": 11096.081339001103,
                        "throughput_queries_per_sec": 11.535603974900466,
                        "throughput_tokens_per_sec": 1476.5573087872597
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1577541632,
                        "gpu_max_memory_allocated_bytes": 1577541632,
                        "gpu_current_memory_reserved_bytes": 3577741312,
                        "gpu_max_memory_reserved_bytes": 3577741312
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.5,
                        "cpu_memory_usage_bytes": 2715783168
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0078",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 723.637883079102,
                            "process_3": 433.5161846534261,
                            "process_0": 791.7881343381644,
                            "process_1": 0.0
                        },
                        "ram_power": {
                            "process_2": 0.9817714691162109,
                            "process_3": 0.984708309173584,
                            "process_0": 0.9477167129516602,
                            "process_1": 0.9787745475769044
                        },
                        "cpu_energy": {
                            "process_2": 0.0004812742758748527,
                            "process_3": 0.0007446906289998197,
                            "process_0": 0.00034414134287476375,
                            "process_1": 0.0005571450775936456
                        },
                        "gpu_energy": {
                            "process_2": 0.0026120998674559193,
                            "process_3": 0.003750795500633841,
                            "process_0": 0.00190404735656996,
                            "process_1": 0.0028118903050659005
                        },
                        "ram_energy": {
                            "process_2": 3.7599286589919783e-06,
                            "process_3": 5.9304332152104325e-06,
                            "process_0": 2.705767955914554e-06,
                            "process_1": 4.304855219499009e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.003097134071989764,
                            "process_3": 0.004501416562848871,
                            "process_0": 0.0022508944674006384,
                            "process_1": 0.003373340237879046
                        },
                        "total_energy_joules": {
                            "process_2": 11149.68265916315,
                            "process_3": 16205.099626255937,
                            "process_0": 8103.220082642299,
                            "process_1": 12144.024856364565
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 487.2355505176731,
                        "ram_power_avg": 0.9732427597045898,
                        "cpu_energy_total": 0.0021272513253430816,
                        "gpu_energy_total": 0.011078833029725621,
                        "ram_energy_total": 1.6700985049615972e-05,
                        "total_energy_kwh": 0.01322278534011832,
                        "total_energy_joules": 47602.027224425954
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.34418702217776354,
                        "joules_per_token": 2.905397169459592,
                        "flops_per_joule": 21733194.746570498,
                        "joules_per_flop": 4.60125633465738e-08
                    },
                    "per-process_emissions": [
                        0.0011798532247245007,
                        0.0017148146396172774,
                        0.0008574782473562732,
                        0.0012850739636200226
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0079": {
            "setup": {
                "experiment_id": "0079",
                "date_time": "April 11, 2025 at 09:26:45 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A2_Precision_Minimalist",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 4.970398135999858,
                        "average_latency_ms_per_batch": 4970.398135999858,
                        "throughput_queries_per_sec": 25.752464188515393,
                        "throughput_tokens_per_sec": 3296.3154161299703
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 2621440000,
                        "gpu_max_memory_reserved_bytes": 2621440000
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            2.0,
                            100.0,
                            5.0,
                            2.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 2638508032
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0079",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 0.0,
                            "process_1": 302.82021183125187
                        },
                        "ram_power": {
                            "process_0": 0.9203081130981445,
                            "process_1": 0.9677953720092773
                        },
                        "cpu_energy": {
                            "process_0": 0.00019047976028139152,
                            "process_1": 0.00020243077978125256
                        },
                        "gpu_energy": {
                            "process_0": 0.0006007174250180491,
                            "process_1": 0.0007180594633360848
                        },
                        "ram_energy": {
                            "process_0": 1.3066171583701677e-06,
                            "process_1": 1.5241391465744703e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0007925038024578107,
                            "process_1": 0.0009220143822639117
                        },
                        "total_energy_joules": {
                            "process_0": 2853.0136888481184,
                            "process_1": 3319.251776150082
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 151.41010591562593,
                        "ram_power_avg": 0.9440517425537109,
                        "cpu_energy_total": 0.0003929105400626441,
                        "gpu_energy_total": 0.001318776888354134,
                        "ram_energy_total": 2.830756304944638e-06,
                        "total_energy_kwh": 0.0017145181847217222,
                        "total_energy_joules": 6172.2654649982005
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 2.6544548501535936,
                        "joules_per_token": 0.3767251870726441,
                        "flops_per_joule": 167611735.7341016,
                        "joules_per_flop": 5.966169347392208e-09
                    },
                    "per-process_emissions": [
                        0.000301904323546303,
                        0.0003512413789234372
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0079": {
            "setup": {
                "experiment_id": "0079",
                "date_time": "April 11, 2025 at 09:26:45 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A2_Precision_Minimalist",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 4.970398135999858,
                        "average_latency_ms_per_batch": 4970.398135999858,
                        "throughput_queries_per_sec": 25.752464188515393,
                        "throughput_tokens_per_sec": 3296.3154161299703
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 2621440000,
                        "gpu_max_memory_reserved_bytes": 2621440000
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            2.0,
                            100.0,
                            5.0,
                            2.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 2638508032
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0079",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 0.0,
                            "process_1": 302.82021183125187
                        },
                        "ram_power": {
                            "process_0": 0.9203081130981445,
                            "process_1": 0.9677953720092773
                        },
                        "cpu_energy": {
                            "process_0": 0.00019047976028139152,
                            "process_1": 0.00020243077978125256
                        },
                        "gpu_energy": {
                            "process_0": 0.0006007174250180491,
                            "process_1": 0.0007180594633360848
                        },
                        "ram_energy": {
                            "process_0": 1.3066171583701677e-06,
                            "process_1": 1.5241391465744703e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0007925038024578107,
                            "process_1": 0.0009220143822639117
                        },
                        "total_energy_joules": {
                            "process_0": 2853.0136888481184,
                            "process_1": 3319.251776150082
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 151.41010591562593,
                        "ram_power_avg": 0.9440517425537109,
                        "cpu_energy_total": 0.0003929105400626441,
                        "gpu_energy_total": 0.001318776888354134,
                        "ram_energy_total": 2.830756304944638e-06,
                        "total_energy_kwh": 0.0017145181847217222,
                        "total_energy_joules": 6172.2654649982005
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 2.6544548501535936,
                        "joules_per_token": 0.3767251870726441,
                        "flops_per_joule": 167611735.7341016,
                        "joules_per_flop": 5.966169347392208e-09
                    },
                    "per-process_emissions": [
                        0.000301904323546303,
                        0.0003512413789234372
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0080": {
            "setup": {
                "experiment_id": "0080",
                "date_time": "April 11, 2025 at 09:27:20 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A3_Quantisation_Gaming",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 9.53141706599854,
                        "average_latency_ms_per_batch": 4765.70853299927,
                        "throughput_queries_per_sec": 13.42927280525945,
                        "throughput_tokens_per_sec": 1718.9469190732095
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1986002944,
                        "gpu_max_memory_reserved_bytes": 1986002944
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            1.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 2673549312
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0080",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 179.70445432121693
                        },
                        "ram_power": {
                            "process_0": 0.9330525398254395
                        },
                        "cpu_energy": {
                            "process_0": 0.00029652530524992924
                        },
                        "gpu_energy": {
                            "process_0": 0.0003826255838780268
                        },
                        "ram_energy": {
                            "process_0": 2.0243325301638516e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0006811752216581198
                        },
                        "total_energy_joules": {
                            "process_0": 2452.2307979692314
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 179.70445432121693,
                        "ram_power_avg": 0.9330525398254395,
                        "cpu_energy_total": 0.00029652530524992924,
                        "gpu_energy_total": 0.0003826255838780268,
                        "ram_energy_total": 2.0243325301638516e-06,
                        "total_energy_kwh": 0.0006811752216581198,
                        "total_energy_joules": 2452.2307979692314
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 6.681263449414345,
                        "joules_per_token": 0.14967228991511422,
                        "flops_per_joule": 421878776.1970603,
                        "joules_per_flop": 2.370349153409173e-09
                    },
                    "per-process_emissions": [
                        0.00025949370069066073
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0080": {
            "setup": {
                "experiment_id": "0080",
                "date_time": "April 11, 2025 at 09:27:20 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A3_Quantisation_Gaming",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 9.53141706599854,
                        "average_latency_ms_per_batch": 4765.70853299927,
                        "throughput_queries_per_sec": 13.42927280525945,
                        "throughput_tokens_per_sec": 1718.9469190732095
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1986002944,
                        "gpu_max_memory_reserved_bytes": 1986002944
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            1.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 2673549312
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0080",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 179.70445432121693
                        },
                        "ram_power": {
                            "process_0": 0.9330525398254395
                        },
                        "cpu_energy": {
                            "process_0": 0.00029652530524992924
                        },
                        "gpu_energy": {
                            "process_0": 0.0003826255838780268
                        },
                        "ram_energy": {
                            "process_0": 2.0243325301638516e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0006811752216581198
                        },
                        "total_energy_joules": {
                            "process_0": 2452.2307979692314
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 179.70445432121693,
                        "ram_power_avg": 0.9330525398254395,
                        "cpu_energy_total": 0.00029652530524992924,
                        "gpu_energy_total": 0.0003826255838780268,
                        "ram_energy_total": 2.0243325301638516e-06,
                        "total_energy_kwh": 0.0006811752216581198,
                        "total_energy_joules": 2452.2307979692314
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 6.681263449414345,
                        "joules_per_token": 0.14967228991511422,
                        "flops_per_joule": 421878776.1970603,
                        "joules_per_flop": 2.370349153409173e-09
                    },
                    "per-process_emissions": [
                        0.00025949370069066073
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0081": {
            "setup": {
                "experiment_id": "0081",
                "date_time": "April 11, 2025 at 09:28:53 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "default",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 69.84512574100154,
                        "average_latency_ms_per_batch": 8730.640717625192,
                        "throughput_queries_per_sec": 1.8326260944056045,
                        "throughput_tokens_per_sec": 234.57614008391738
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576560640,
                        "gpu_max_memory_allocated_bytes": 1576560640,
                        "gpu_current_memory_reserved_bytes": 2707423232,
                        "gpu_max_memory_reserved_bytes": 2707423232
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            47.0,
                            50.0
                        ],
                        "cpu_usage_percent": 4.3,
                        "cpu_memory_usage_bytes": 2712358912
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0081",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 52.7940358583052
                        },
                        "ram_power": {
                            "process_0": 0.9469542503356935
                        },
                        "cpu_energy": {
                            "process_0": 0.0021441061350319607
                        },
                        "gpu_energy": {
                            "process_0": 0.0009656671614219903
                        },
                        "ram_energy": {
                            "process_0": 1.5206017253776847e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0031249793137077286
                        },
                        "total_energy_joules": {
                            "process_0": 11249.925529347824
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 52.7940358583052,
                        "ram_power_avg": 0.9469542503356935,
                        "cpu_energy_total": 0.0021441061350319607,
                        "gpu_energy_total": 0.0009656671614219903,
                        "ram_energy_total": 1.5206017253776847e-05,
                        "total_energy_kwh": 0.0031249793137077286,
                        "total_energy_joules": 11249.925529347824
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.456365196130308,
                        "joules_per_token": 0.6866409624846084,
                        "flops_per_joule": 91960086.78467885,
                        "joules_per_flop": 1.0874282908643433e-08
                    },
                    "per-process_emissions": [
                        0.0011904608695569592
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0081": {
            "setup": {
                "experiment_id": "0081",
                "date_time": "April 11, 2025 at 09:28:53 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "default",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 69.84512574100154,
                        "average_latency_ms_per_batch": 8730.640717625192,
                        "throughput_queries_per_sec": 1.8326260944056045,
                        "throughput_tokens_per_sec": 234.57614008391738
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576560640,
                        "gpu_max_memory_allocated_bytes": 1576560640,
                        "gpu_current_memory_reserved_bytes": 2707423232,
                        "gpu_max_memory_reserved_bytes": 2707423232
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            47.0,
                            50.0
                        ],
                        "cpu_usage_percent": 4.3,
                        "cpu_memory_usage_bytes": 2712358912
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0081",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 52.7940358583052
                        },
                        "ram_power": {
                            "process_0": 0.9469542503356935
                        },
                        "cpu_energy": {
                            "process_0": 0.0021441061350319607
                        },
                        "gpu_energy": {
                            "process_0": 0.0009656671614219903
                        },
                        "ram_energy": {
                            "process_0": 1.5206017253776847e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0031249793137077286
                        },
                        "total_energy_joules": {
                            "process_0": 11249.925529347824
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 52.7940358583052,
                        "ram_power_avg": 0.9469542503356935,
                        "cpu_energy_total": 0.0021441061350319607,
                        "gpu_energy_total": 0.0009656671614219903,
                        "ram_energy_total": 1.5206017253776847e-05,
                        "total_energy_kwh": 0.0031249793137077286,
                        "total_energy_joules": 11249.925529347824
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.456365196130308,
                        "joules_per_token": 0.6866409624846084,
                        "flops_per_joule": 91960086.78467885,
                        "joules_per_flop": 1.0874282908643433e-08
                    },
                    "per-process_emissions": [
                        0.0011904608695569592
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0082": {
            "setup": {
                "experiment_id": "0082",
                "date_time": "April 11, 2025 at 09:29:30 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A5_Parallel_Overdrive",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 6.284193605999462,
                        "average_latency_ms_per_batch": 3142.096802999731,
                        "throughput_queries_per_sec": 20.368564055346667,
                        "throughput_tokens_per_sec": 2607.1761990843734
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419853312,
                        "gpu_max_memory_allocated_bytes": 4419853312,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            3.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.2,
                        "cpu_memory_usage_bytes": 3099574272
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0082",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 440.4131891482038,
                            "process_1": 693.727217305351,
                            "process_0": 897.963024774778,
                            "process_2": 716.8132258844915
                        },
                        "ram_power": {
                            "process_3": 0.8746476173400879,
                            "process_1": 0.8518009185791016,
                            "process_0": 1.0811333656311035,
                            "process_2": 0.8527650833129883
                        },
                        "cpu_energy": {
                            "process_3": 0.000274539258937466,
                            "process_1": 0.0002480366241561569,
                            "process_0": 0.00019837984265609523,
                            "process_2": 0.0002427345011561784
                        },
                        "gpu_energy": {
                            "process_3": 0.0017498541776600485,
                            "process_1": 0.0016541752122280484,
                            "process_0": 0.0013532230270219547,
                            "process_2": 0.0016276999132699976
                        },
                        "ram_energy": {
                            "process_3": 1.891568758461624e-06,
                            "process_1": 1.6763288311307912e-06,
                            "process_0": 1.6523179653325099e-06,
                            "process_2": 1.640308028575637e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.002026285005355976,
                            "process_1": 0.0019038881652153365,
                            "process_0": 0.0015532551876433825,
                            "process_2": 0.0018720747224547517
                        },
                        "total_energy_joules": {
                            "process_3": 7294.626019281514,
                            "process_1": 6853.997394775211,
                            "process_0": 5591.718675516177,
                            "process_2": 6739.469000837106
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 687.2291642782061,
                        "ram_power_avg": 0.9150867462158203,
                        "cpu_energy_total": 0.0009636902269058965,
                        "gpu_energy_total": 0.006384952330180049,
                        "ram_energy_total": 6.860523583500562e-06,
                        "total_energy_kwh": 0.007355503080669447,
                        "total_energy_joules": 26479.81109041001
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.6187355319137329,
                        "joules_per_token": 1.6161994073736579,
                        "flops_per_joule": 640109211.326309,
                        "joules_per_flop": 1.5622334162759444e-09
                    },
                    "per-process_emissions": [
                        0.0007719132727903591,
                        0.0007252861965387824,
                        0.0005917125637327466,
                        0.0007131668655191377
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0082": {
            "setup": {
                "experiment_id": "0082",
                "date_time": "April 11, 2025 at 09:29:30 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A5_Parallel_Overdrive",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 6.284193605999462,
                        "average_latency_ms_per_batch": 3142.096802999731,
                        "throughput_queries_per_sec": 20.368564055346667,
                        "throughput_tokens_per_sec": 2607.1761990843734
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419853312,
                        "gpu_max_memory_allocated_bytes": 4419853312,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            3.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.2,
                        "cpu_memory_usage_bytes": 3099574272
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0082",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 440.4131891482038,
                            "process_1": 693.727217305351,
                            "process_0": 897.963024774778,
                            "process_2": 716.8132258844915
                        },
                        "ram_power": {
                            "process_3": 0.8746476173400879,
                            "process_1": 0.8518009185791016,
                            "process_0": 1.0811333656311035,
                            "process_2": 0.8527650833129883
                        },
                        "cpu_energy": {
                            "process_3": 0.000274539258937466,
                            "process_1": 0.0002480366241561569,
                            "process_0": 0.00019837984265609523,
                            "process_2": 0.0002427345011561784
                        },
                        "gpu_energy": {
                            "process_3": 0.0017498541776600485,
                            "process_1": 0.0016541752122280484,
                            "process_0": 0.0013532230270219547,
                            "process_2": 0.0016276999132699976
                        },
                        "ram_energy": {
                            "process_3": 1.891568758461624e-06,
                            "process_1": 1.6763288311307912e-06,
                            "process_0": 1.6523179653325099e-06,
                            "process_2": 1.640308028575637e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.002026285005355976,
                            "process_1": 0.0019038881652153365,
                            "process_0": 0.0015532551876433825,
                            "process_2": 0.0018720747224547517
                        },
                        "total_energy_joules": {
                            "process_3": 7294.626019281514,
                            "process_1": 6853.997394775211,
                            "process_0": 5591.718675516177,
                            "process_2": 6739.469000837106
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 687.2291642782061,
                        "ram_power_avg": 0.9150867462158203,
                        "cpu_energy_total": 0.0009636902269058965,
                        "gpu_energy_total": 0.006384952330180049,
                        "ram_energy_total": 6.860523583500562e-06,
                        "total_energy_kwh": 0.007355503080669447,
                        "total_energy_joules": 26479.81109041001
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.6187355319137329,
                        "joules_per_token": 1.6161994073736579,
                        "flops_per_joule": 640109211.326309,
                        "joules_per_flop": 1.5622334162759444e-09
                    },
                    "per-process_emissions": [
                        0.0007719132727903591,
                        0.0007252861965387824,
                        0.0005917125637327466,
                        0.0007131668655191377
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0086": {
            "setup": {
                "experiment_id": "0086",
                "date_time": "April 11, 2025 at 09:32:17 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R2_Low_Latency_Chatbot_Deployment",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.05,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 86.12005658600538,
                        "average_latency_ms_per_batch": 2691.251768312668,
                        "throughput_queries_per_sec": 1.486297211987668,
                        "throughput_tokens_per_sec": 190.2460431344215
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 12343836672,
                        "gpu_max_memory_reserved_bytes": 12343836672
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            1.0,
                            3.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 2005184512
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0086",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 171.9073731821482
                        },
                        "ram_power": {
                            "process_0": 0.6985688209533691
                        },
                        "cpu_energy": {
                            "process_0": 0.002647501197999986
                        },
                        "gpu_energy": {
                            "process_0": 0.0030539768876240436
                        },
                        "ram_energy": {
                            "process_0": 1.4314758062287136e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005715792843686318
                        },
                        "total_energy_joules": {
                            "process_0": 20576.854237270745
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 171.9073731821482,
                        "ram_power_avg": 0.6985688209533691,
                        "cpu_energy_total": 0.002647501197999986,
                        "gpu_energy_total": 0.0030539768876240436,
                        "ram_energy_total": 1.4314758062287136e-05,
                        "total_energy_kwh": 0.005715792843686318,
                        "total_energy_joules": 20576.854237270745
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.796234439486078,
                        "joules_per_token": 1.2559115135052945,
                        "flops_per_joule": 823739663.8816932,
                        "joules_per_flop": 1.2139757788130758e-09
                    },
                    "per-process_emissions": [
                        0.002177431283802303
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0086": {
            "setup": {
                "experiment_id": "0086",
                "date_time": "April 11, 2025 at 09:32:17 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R2_Low_Latency_Chatbot_Deployment",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.05,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 86.12005658600538,
                        "average_latency_ms_per_batch": 2691.251768312668,
                        "throughput_queries_per_sec": 1.486297211987668,
                        "throughput_tokens_per_sec": 190.2460431344215
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 12343836672,
                        "gpu_max_memory_reserved_bytes": 12343836672
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            1.0,
                            3.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 2005184512
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0086",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 171.9073731821482
                        },
                        "ram_power": {
                            "process_0": 0.6985688209533691
                        },
                        "cpu_energy": {
                            "process_0": 0.002647501197999986
                        },
                        "gpu_energy": {
                            "process_0": 0.0030539768876240436
                        },
                        "ram_energy": {
                            "process_0": 1.4314758062287136e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005715792843686318
                        },
                        "total_energy_joules": {
                            "process_0": 20576.854237270745
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 171.9073731821482,
                        "ram_power_avg": 0.6985688209533691,
                        "cpu_energy_total": 0.002647501197999986,
                        "gpu_energy_total": 0.0030539768876240436,
                        "ram_energy_total": 1.4314758062287136e-05,
                        "total_energy_kwh": 0.005715792843686318,
                        "total_energy_joules": 20576.854237270745
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.796234439486078,
                        "joules_per_token": 1.2559115135052945,
                        "flops_per_joule": 823739663.8816932,
                        "joules_per_flop": 1.2139757788130758e-09
                    },
                    "per-process_emissions": [
                        0.002177431283802303
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0087": {
            "setup": {
                "experiment_id": "0087",
                "date_time": "April 11, 2025 at 09:34:13 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R3_Balanced_Enterprise_Service",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 42.836278109003615,
                        "average_latency_ms_per_batch": 10709.069527250904,
                        "throughput_queries_per_sec": 2.9881214160176093,
                        "throughput_tokens_per_sec": 382.479541250254
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576947200,
                        "gpu_max_memory_allocated_bytes": 1576947200,
                        "gpu_current_memory_reserved_bytes": 2839543808,
                        "gpu_max_memory_reserved_bytes": 2839543808
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            10.0,
                            100.0,
                            100.0,
                            92.0
                        ],
                        "cpu_usage_percent": 5.1,
                        "cpu_memory_usage_bytes": 2711977984
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0087",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 476.32876293233716,
                            "process_0": 404.9142693514381,
                            "process_2": 593.8048196590107,
                            "process_1": 512.8752165787708
                        },
                        "ram_power": {
                            "process_3": 0.9929809570312501,
                            "process_0": 0.9464263916015625,
                            "process_2": 0.9905705451965332,
                            "process_1": 0.992684841156006
                        },
                        "cpu_energy": {
                            "process_3": 0.0026813578049993806,
                            "process_0": 0.0013165540245933017,
                            "process_2": 0.0016758044086870997,
                            "process_1": 0.0018390927513127055
                        },
                        "gpu_energy": {
                            "process_3": 0.011556803967657958,
                            "process_0": 0.005761953498448047,
                            "process_2": 0.007424444272884079,
                            "process_1": 0.008116830104570039
                        },
                        "ram_energy": {
                            "process_3": 2.1421669478282806e-05,
                            "process_0": 9.365473209110881e-06,
                            "process_2": 1.2954888119891948e-05,
                            "process_1": 1.431359326600353e-05
                        },
                        "total_energy_kwh": {
                            "process_3": 0.014259583442135624,
                            "process_0": 0.00708787299625046,
                            "process_2": 0.009113203569691072,
                            "process_1": 0.00997023644914875
                        },
                        "total_energy_joules": {
                            "process_3": 51334.50039168825,
                            "process_0": 25516.342786501657,
                            "process_2": 32807.53285088786,
                            "process_1": 35892.8512169355
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 496.98076713038927,
                        "ram_power_avg": 0.9806656837463379,
                        "cpu_energy_total": 0.007512808989592487,
                        "gpu_energy_total": 0.03286003184356012,
                        "ram_energy_total": 5.8055624073289166e-05,
                        "total_energy_kwh": 0.040430896457225904,
                        "total_energy_joules": 145551.22724601327
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.11256517935302238,
                        "joules_per_token": 8.883741897339677,
                        "flops_per_joule": 7107766.437801278,
                        "joules_per_flop": 1.4069117334549626e-07
                    },
                    "per-process_emissions": [
                        0.005432188312281566,
                        0.002700125217921613,
                        0.003471674899873814,
                        0.0037981615753032165
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0087": {
            "setup": {
                "experiment_id": "0087",
                "date_time": "April 11, 2025 at 09:34:13 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R3_Balanced_Enterprise_Service",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 42.836278109003615,
                        "average_latency_ms_per_batch": 10709.069527250904,
                        "throughput_queries_per_sec": 2.9881214160176093,
                        "throughput_tokens_per_sec": 382.479541250254
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576947200,
                        "gpu_max_memory_allocated_bytes": 1576947200,
                        "gpu_current_memory_reserved_bytes": 2839543808,
                        "gpu_max_memory_reserved_bytes": 2839543808
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            10.0,
                            100.0,
                            100.0,
                            92.0
                        ],
                        "cpu_usage_percent": 5.1,
                        "cpu_memory_usage_bytes": 2711977984
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0087",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 476.32876293233716,
                            "process_0": 404.9142693514381,
                            "process_2": 593.8048196590107,
                            "process_1": 512.8752165787708
                        },
                        "ram_power": {
                            "process_3": 0.9929809570312501,
                            "process_0": 0.9464263916015625,
                            "process_2": 0.9905705451965332,
                            "process_1": 0.992684841156006
                        },
                        "cpu_energy": {
                            "process_3": 0.0026813578049993806,
                            "process_0": 0.0013165540245933017,
                            "process_2": 0.0016758044086870997,
                            "process_1": 0.0018390927513127055
                        },
                        "gpu_energy": {
                            "process_3": 0.011556803967657958,
                            "process_0": 0.005761953498448047,
                            "process_2": 0.007424444272884079,
                            "process_1": 0.008116830104570039
                        },
                        "ram_energy": {
                            "process_3": 2.1421669478282806e-05,
                            "process_0": 9.365473209110881e-06,
                            "process_2": 1.2954888119891948e-05,
                            "process_1": 1.431359326600353e-05
                        },
                        "total_energy_kwh": {
                            "process_3": 0.014259583442135624,
                            "process_0": 0.00708787299625046,
                            "process_2": 0.009113203569691072,
                            "process_1": 0.00997023644914875
                        },
                        "total_energy_joules": {
                            "process_3": 51334.50039168825,
                            "process_0": 25516.342786501657,
                            "process_2": 32807.53285088786,
                            "process_1": 35892.8512169355
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 496.98076713038927,
                        "ram_power_avg": 0.9806656837463379,
                        "cpu_energy_total": 0.007512808989592487,
                        "gpu_energy_total": 0.03286003184356012,
                        "ram_energy_total": 5.8055624073289166e-05,
                        "total_energy_kwh": 0.040430896457225904,
                        "total_energy_joules": 145551.22724601327
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.11256517935302238,
                        "joules_per_token": 8.883741897339677,
                        "flops_per_joule": 7107766.437801278,
                        "joules_per_flop": 1.4069117334549626e-07
                    },
                    "per-process_emissions": [
                        0.005432188312281566,
                        0.002700125217921613,
                        0.003471674899873814,
                        0.0037981615753032165
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0088": {
            "setup": {
                "experiment_id": "0088",
                "date_time": "April 11, 2025 at 09:35:31 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R4_High_Load_Cloud_API_Deployment",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 53.885366985989094,
                        "average_latency_ms_per_batch": 3367.8354366243184,
                        "throughput_queries_per_sec": 2.3754129768343546,
                        "throughput_tokens_per_sec": 304.0528610347974
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419853312,
                        "gpu_max_memory_allocated_bytes": 4419853312,
                        "gpu_current_memory_reserved_bytes": 6339690496,
                        "gpu_max_memory_reserved_bytes": 6339690496
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            10.0,
                            3.0,
                            17.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 3103125504
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0088",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 80.62112165699624
                        },
                        "ram_power": {
                            "process_0": 1.08123779296875
                        },
                        "cpu_energy": {
                            "process_0": 0.0016576638684690577
                        },
                        "gpu_energy": {
                            "process_0": 0.0009904855146100622
                        },
                        "ram_energy": {
                            "process_0": 1.4060427717230866e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0026622098107963515
                        },
                        "total_energy_joules": {
                            "process_0": 9583.955318866865
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 80.62112165699624,
                        "ram_power_avg": 1.08123779296875,
                        "cpu_energy_total": 0.0016576638684690577,
                        "gpu_energy_total": 0.0009904855146100622,
                        "ram_energy_total": 1.4060427717230866e-05,
                        "total_energy_kwh": 0.0026622098107963515,
                        "total_energy_joules": 9583.955318866865
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.7095238296600408,
                        "joules_per_token": 0.5849582103800577,
                        "flops_per_joule": 1768577839.6508675,
                        "joules_per_flop": 5.654260601825751e-10
                    },
                    "per-process_emissions": [
                        0.0010141688274228702
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0088": {
            "setup": {
                "experiment_id": "0088",
                "date_time": "April 11, 2025 at 09:35:31 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R4_High_Load_Cloud_API_Deployment",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 53.885366985989094,
                        "average_latency_ms_per_batch": 3367.8354366243184,
                        "throughput_queries_per_sec": 2.3754129768343546,
                        "throughput_tokens_per_sec": 304.0528610347974
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419853312,
                        "gpu_max_memory_allocated_bytes": 4419853312,
                        "gpu_current_memory_reserved_bytes": 6339690496,
                        "gpu_max_memory_reserved_bytes": 6339690496
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            10.0,
                            3.0,
                            17.0
                        ],
                        "cpu_usage_percent": 2.4,
                        "cpu_memory_usage_bytes": 3103125504
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0088",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 80.62112165699624
                        },
                        "ram_power": {
                            "process_0": 1.08123779296875
                        },
                        "cpu_energy": {
                            "process_0": 0.0016576638684690577
                        },
                        "gpu_energy": {
                            "process_0": 0.0009904855146100622
                        },
                        "ram_energy": {
                            "process_0": 1.4060427717230866e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0026622098107963515
                        },
                        "total_energy_joules": {
                            "process_0": 9583.955318866865
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 80.62112165699624,
                        "ram_power_avg": 1.08123779296875,
                        "cpu_energy_total": 0.0016576638684690577,
                        "gpu_energy_total": 0.0009904855146100622,
                        "ram_energy_total": 1.4060427717230866e-05,
                        "total_energy_kwh": 0.0026622098107963515,
                        "total_energy_joules": 9583.955318866865
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.7095238296600408,
                        "joules_per_token": 0.5849582103800577,
                        "flops_per_joule": 1768577839.6508675,
                        "joules_per_flop": 5.654260601825751e-10
                    },
                    "per-process_emissions": [
                        0.0010141688274228702
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0089": {
            "setup": {
                "experiment_id": "0089",
                "date_time": "April 11, 2025 at 09:52:12 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R5_Real_Time_Mobile_Inference",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.6,
                    "simulate_burst": true,
                    "burst_interval": 5.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 14578
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 976.0210930590297,
                        "average_latency_ms_per_batch": 7625.164789523669,
                        "throughput_queries_per_sec": 0.13114470671819647,
                        "throughput_tokens_per_sec": 14.936152613577097
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576178688,
                        "gpu_max_memory_allocated_bytes": 1576178688,
                        "gpu_current_memory_reserved_bytes": 2665480192,
                        "gpu_max_memory_reserved_bytes": 2665480192
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            78.0,
                            26.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 2694705152
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0089",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 39.81841206007259
                        },
                        "ram_power": {
                            "process_0": 0.9397516250610353
                        },
                        "cpu_energy": {
                            "process_0": 0.029886566713812467
                        },
                        "gpu_energy": {
                            "process_0": 0.009949834348749964
                        },
                        "ram_energy": {
                            "process_0": 0.00021175804968111777
                        },
                        "total_energy_kwh": {
                            "process_0": 0.040048159112243485
                        },
                        "total_energy_joules": {
                            "process_0": 144173.37280407656
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 39.81841206007259,
                        "ram_power_avg": 0.9397516250610353,
                        "cpu_energy_total": 0.029886566713812467,
                        "gpu_energy_total": 0.009949834348749964,
                        "ram_energy_total": 0.00021175804968111777,
                        "total_energy_kwh": 0.040048159112243485,
                        "total_energy_joules": 144173.37280407656
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10111437165176593,
                        "joules_per_token": 9.889790972978226,
                        "flops_per_joule": 7175694.8448857255,
                        "joules_per_flop": 1.3935932639509075e-07
                    },
                    "per-process_emissions": [
                        0.015256346213809157
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0089": {
            "setup": {
                "experiment_id": "0089",
                "date_time": "April 11, 2025 at 09:52:12 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R5_Real_Time_Mobile_Inference",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.6,
                    "simulate_burst": true,
                    "burst_interval": 5.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 14578
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 976.0210930590297,
                        "average_latency_ms_per_batch": 7625.164789523669,
                        "throughput_queries_per_sec": 0.13114470671819647,
                        "throughput_tokens_per_sec": 14.936152613577097
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576178688,
                        "gpu_max_memory_allocated_bytes": 1576178688,
                        "gpu_current_memory_reserved_bytes": 2665480192,
                        "gpu_max_memory_reserved_bytes": 2665480192
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            78.0,
                            26.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 2694705152
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0089",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 39.81841206007259
                        },
                        "ram_power": {
                            "process_0": 0.9397516250610353
                        },
                        "cpu_energy": {
                            "process_0": 0.029886566713812467
                        },
                        "gpu_energy": {
                            "process_0": 0.009949834348749964
                        },
                        "ram_energy": {
                            "process_0": 0.00021175804968111777
                        },
                        "total_energy_kwh": {
                            "process_0": 0.040048159112243485
                        },
                        "total_energy_joules": {
                            "process_0": 144173.37280407656
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 39.81841206007259,
                        "ram_power_avg": 0.9397516250610353,
                        "cpu_energy_total": 0.029886566713812467,
                        "gpu_energy_total": 0.009949834348749964,
                        "ram_energy_total": 0.00021175804968111777,
                        "total_energy_kwh": 0.040048159112243485,
                        "total_energy_joules": 144173.37280407656
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10111437165176593,
                        "joules_per_token": 9.889790972978226,
                        "flops_per_joule": 7175694.8448857255,
                        "joules_per_flop": 1.3935932639509075e-07
                    },
                    "per-process_emissions": [
                        0.015256346213809157
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0090": {
            "setup": {
                "experiment_id": "0090",
                "date_time": "April 11, 2025 at 09:52:53 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R6_Medium_Scale_Language_Model_Serving",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.1,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 12.368554642009258,
                        "average_latency_ms_per_batch": 3092.1386605023145,
                        "throughput_queries_per_sec": 10.348824394182127,
                        "throughput_tokens_per_sec": 1324.6495224553123
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419853312,
                        "gpu_max_memory_allocated_bytes": 4419853312,
                        "gpu_current_memory_reserved_bytes": 4922015744,
                        "gpu_max_memory_reserved_bytes": 4922015744
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.5,
                        "cpu_memory_usage_bytes": 3104542720
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0090",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 566.5097217637889,
                            "process_1": 527.5229200677826,
                            "process_3": 631.0500663525255,
                            "process_0": 622.4605274541221
                        },
                        "ram_power": {
                            "process_2": 0.8705105781555176,
                            "process_1": 0.881828784942627,
                            "process_3": 0.8714790344238281,
                            "process_0": 1.0818428993225098
                        },
                        "cpu_energy": {
                            "process_2": 0.00038073722650005945,
                            "process_1": 0.0003943098123747859,
                            "process_3": 0.00038510656375001425,
                            "process_0": 0.0003841798854377885
                        },
                        "gpu_energy": {
                            "process_2": 0.0016935530215078654,
                            "process_1": 0.0017507847339599536,
                            "process_3": 0.0017165563732439715,
                            "process_0": 0.0017126155367579998
                        },
                        "ram_energy": {
                            "process_2": 2.5344075095210564e-06,
                            "process_1": 2.6703337170041216e-06,
                            "process_3": 2.5564230276992236e-06,
                            "process_0": 3.192513820304637e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.002076824655517446,
                            "process_1": 0.002147764880051744,
                            "process_3": 0.002104219360021685,
                            "process_0": 0.0020999879360160934
                        },
                        "total_energy_joules": {
                            "process_2": 7476.568759862806,
                            "process_1": 7731.953568186279,
                            "process_3": 7575.189696078065,
                            "process_0": 7559.956569657937
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 586.8858089095548,
                        "ram_power_avg": 0.9264153242111206,
                        "cpu_energy_total": 0.0015443334880626482,
                        "gpu_energy_total": 0.00687350966546979,
                        "ram_energy_total": 1.0953678074529038e-05,
                        "total_energy_kwh": 0.00842879683160697,
                        "total_energy_joules": 30343.668593785085
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.539947895534152,
                        "joules_per_token": 1.8520305538198907,
                        "flops_per_joule": 558599924.7508144,
                        "joules_per_flop": 1.7901900012716427e-09
                    },
                    "per-process_emissions": [
                        0.0007911663525193711,
                        0.0008181910310557119,
                        0.0008016023652002609,
                        0.0007999904042253308
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0090": {
            "setup": {
                "experiment_id": "0090",
                "date_time": "April 11, 2025 at 09:52:53 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R6_Medium_Scale_Language_Model_Serving",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.1,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 12.368554642009258,
                        "average_latency_ms_per_batch": 3092.1386605023145,
                        "throughput_queries_per_sec": 10.348824394182127,
                        "throughput_tokens_per_sec": 1324.6495224553123
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419853312,
                        "gpu_max_memory_allocated_bytes": 4419853312,
                        "gpu_current_memory_reserved_bytes": 4922015744,
                        "gpu_max_memory_reserved_bytes": 4922015744
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.5,
                        "cpu_memory_usage_bytes": 3104542720
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0090",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 566.5097217637889,
                            "process_1": 527.5229200677826,
                            "process_3": 631.0500663525255,
                            "process_0": 622.4605274541221
                        },
                        "ram_power": {
                            "process_2": 0.8705105781555176,
                            "process_1": 0.881828784942627,
                            "process_3": 0.8714790344238281,
                            "process_0": 1.0818428993225098
                        },
                        "cpu_energy": {
                            "process_2": 0.00038073722650005945,
                            "process_1": 0.0003943098123747859,
                            "process_3": 0.00038510656375001425,
                            "process_0": 0.0003841798854377885
                        },
                        "gpu_energy": {
                            "process_2": 0.0016935530215078654,
                            "process_1": 0.0017507847339599536,
                            "process_3": 0.0017165563732439715,
                            "process_0": 0.0017126155367579998
                        },
                        "ram_energy": {
                            "process_2": 2.5344075095210564e-06,
                            "process_1": 2.6703337170041216e-06,
                            "process_3": 2.5564230276992236e-06,
                            "process_0": 3.192513820304637e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.002076824655517446,
                            "process_1": 0.002147764880051744,
                            "process_3": 0.002104219360021685,
                            "process_0": 0.0020999879360160934
                        },
                        "total_energy_joules": {
                            "process_2": 7476.568759862806,
                            "process_1": 7731.953568186279,
                            "process_3": 7575.189696078065,
                            "process_0": 7559.956569657937
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 586.8858089095548,
                        "ram_power_avg": 0.9264153242111206,
                        "cpu_energy_total": 0.0015443334880626482,
                        "gpu_energy_total": 0.00687350966546979,
                        "ram_energy_total": 1.0953678074529038e-05,
                        "total_energy_kwh": 0.00842879683160697,
                        "total_energy_joules": 30343.668593785085
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.539947895534152,
                        "joules_per_token": 1.8520305538198907,
                        "flops_per_joule": 558599924.7508144,
                        "joules_per_flop": 1.7901900012716427e-09
                    },
                    "per-process_emissions": [
                        0.0007911663525193711,
                        0.0008181910310557119,
                        0.0008016023652002609,
                        0.0007999904042253308
                    ]
                },
                "local_energy_results": {}
            }
        }
    }
]