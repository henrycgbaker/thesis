{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will later move this into function script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables_latency_simulation_burst_size\n",
      "variables_latency_simulation_simulate\n",
      "inference_metrics_raw_inference_metrics_number_input_prompts\n",
      "global_energy_metrics_local_process_results_ram_energy_process_0\n",
      "compute_metrics_compute_utilisation_cpu_memory_usage_bytes\n",
      "compute_metrics_compute_utilisation_gpu_utilization_percent_3\n",
      "inference_metrics_inference_performance_throughput_queries_per_sec\n",
      "variables_config_name\n",
      "global_energy_metrics_global_experiment_results_gpu_power_avg\n",
      "global_energy_metrics_local_process_results_ram_power_process_1\n",
      "setup_available_cpu_count\n",
      "setup_experiment_id\n",
      "variables_quantisation_load_in_4bit\n",
      "variables_number_input_prompts\n",
      "compute_metrics_memory_gpu_max_memory_allocated_bytes\n",
      "global_energy_metrics_local_process_results_gpu_power_process_3\n",
      "global_energy_metrics_local_process_results_cpu_energy_process_1\n",
      "variables_max_input_tokens\n",
      "global_energy_metrics_global_derived_quantities_joules_per_flop\n",
      "global_energy_metrics_local_process_results_gpu_energy_process_3\n",
      "setup_cpu_model\n",
      "global_energy_metrics_global_experiment_results_cpu_energy_total\n",
      "variables_inference_type\n",
      "variables_sharding_config_fsdp_config_cpu_offload\n",
      "model_architecture_architecture\n",
      "variables_max_output_tokens\n",
      "inference_metrics_inference_performance_total_inference_time_sec\n",
      "variables_accelerate_config_num_processes\n",
      "global_energy_metrics_local_process_results_cpu_power_process_3\n",
      "global_energy_metrics_local_process_results_total_energy_joules_process_2\n",
      "setup_available_gpu_count\n",
      "variables_quantisation_load_in_8bit\n",
      "compute_metrics_memory_gpu_current_memory_reserved_bytes\n",
      "variables_backend\n",
      "variables_sharding_config_fsdp_config_use_orig_params\n",
      "global_energy_metrics_local_process_results_total_energy_joules_process_3\n",
      "variables_query_rate\n",
      "inference_metrics_raw_inference_metrics_total_generated_tokens\n",
      "variables_accelerate_config_distributed_type\n",
      "compute_metrics_memory_gpu_current_memory_allocated_bytes\n",
      "setup_date_time\n",
      "variables_decoder_config_decoding_mode\n",
      "setup_task_type\n",
      "global_energy_metrics_local_process_results_gpu_energy_process_2\n",
      "inference_metrics_raw_inference_metrics_total_input_tokens\n",
      "setup_country\n",
      "variables_batching_options_adaptive_max_tokens\n",
      "variables_latency_simulation_simulate_burst\n",
      "variables_decoder_config_decoder_top_p\n",
      "global_energy_metrics_local_process_results_ram_power_process_2\n",
      "compute_metrics_compute_utilisation_cpu_usage_percent\n",
      "global_energy_metrics_local_process_results_cpu_energy_process_3\n",
      "global_energy_metrics_local_process_results_total_energy_joules_process_0\n",
      "variables_quantisation_quantization\n",
      "global_energy_metrics_local_process_results_total_energy_kwh_process_2\n",
      "global_energy_metrics_local_process_results_total_energy_kwh_process_1\n",
      "variables_batching_options_adaptive_batching\n",
      "setup_python_version\n",
      "inference_metrics_inference_performance_throughput_tokens_per_sec\n",
      "global_energy_metrics_local_process_results_total_energy_joules_process_1\n",
      "variables_latency_simulation_delay_max\n",
      "global_energy_metrics_local_process_results_cpu_power_process_0\n",
      "global_energy_metrics_global_derived_quantities_flops_per_joule\n",
      "variables_latency_simulation_burst_interval\n",
      "global_energy_metrics_local_process_results_gpu_power_process_1\n",
      "global_energy_metrics_local_process_results_ram_power_process_0\n",
      "variables_fp_precision\n",
      "global_energy_metrics_local_process_results_cpu_power_process_2\n",
      "global_energy_metrics_global_experiment_results_total_energy_joules\n",
      "global_energy_metrics_local_process_results_gpu_power_process_0\n",
      "global_energy_metrics_local_process_results_cpu_power_process_1\n",
      "compute_metrics_compute_utilisation_gpu_utilization_percent_0\n",
      "global_energy_metrics_local_process_results_cpu_energy_process_0\n",
      "setup_region\n",
      "model_architecture_total_params\n",
      "variables_decoder_config_decoder_temperature\n",
      "inference_metrics_inference_performance_average_latency_ms_per_batch\n",
      "global_energy_metrics_local_process_results_ram_energy_process_3\n",
      "compute_metrics_memory_gpu_max_memory_reserved_bytes\n",
      "variables_batching_options_max_batch_size___adaptive_batching\n",
      "global_energy_metrics_local_process_results_ram_energy_process_1\n",
      "variables_latency_simulation_delay_min\n",
      "global_energy_metrics_local_process_results_gpu_power_process_2\n",
      "setup_gpu_model\n",
      "global_energy_metrics_global_experiment_results_ram_power_avg\n",
      "variables_sharding_config_sharding_strategy\n",
      "variables_decoder_config_decoder_top_k\n",
      "global_energy_metrics_local_process_results_ram_energy_process_2\n",
      "global_energy_metrics_per-process_emissions_2\n",
      "compute_metrics_compute_utilisation_gpu_utilization_percent_2\n",
      "global_energy_metrics_per-process_emissions_3\n",
      "global_energy_metrics_global_experiment_results_cpu_power_avg\n",
      "global_energy_metrics_local_process_results_cpu_energy_process_2\n",
      "global_energy_metrics_global_derived_quantities_tokens_per_joule\n",
      "global_energy_metrics_local_process_results_gpu_energy_process_1\n",
      "setup_is_encoder_decoder\n",
      "setup_model\n",
      "variables_decode_token_to_text\n",
      "setup_os\n",
      "global_energy_metrics_global_experiment_results_total_energy_kwh\n",
      "global_energy_metrics_global_derived_quantities_joules_per_token\n",
      "global_energy_metrics_experiment_id\n",
      "global_energy_metrics_local_process_results_total_energy_kwh_process_0\n",
      "global_energy_metrics_global_experiment_results_gpu_energy_total\n",
      "variables_batching_options_batch_size___fixed_batching\n",
      "global_energy_metrics_local_process_results_total_energy_kwh_process_3\n",
      "global_energy_metrics_per-process_emissions_0\n",
      "variables_quantisation_cached_flops_for_quantised_models\n",
      "global_energy_metrics_local_process_results_gpu_energy_process_0\n",
      "global_energy_metrics_global_experiment_results_ram_energy_total\n",
      "global_energy_metrics_local_process_results_ram_power_process_3\n",
      "compute_metrics_flops\n",
      "global_energy_metrics_per-process_emissions_1\n",
      "compute_metrics_compute_utilisation_gpu_utilization_percent_1\n"
     ]
    }
   ],
   "source": [
    "notebook_dir = os.getcwd()\n",
    "df = pd.read_csv(\"scenarios_results.csv\")\n",
    "\n",
    "columns = list(df.columns)\n",
    "for col in columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column(col: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean a single column name by:\n",
    "      - Stripping whitespace and replacing any non-standard quotes.\n",
    "      - Checking for per-process metric patterns.\n",
    "      - Applying special renames.\n",
    "      - Removing the 'variables_' prefix if present.\n",
    "      - Otherwise, attempting to strip off any messy prefixes using a known list of tokens.\n",
    "    \n",
    "    If no known token is found, the original (normalized) column name is returned.\n",
    "    \"\"\"\n",
    "    # Normalize the column string: remove extra whitespace and fix common issues with quotes.\n",
    "    col = col.strip()\n",
    "    col = col.replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\")\n",
    "    \n",
    "    # 1. Special exact mappings.\n",
    "    special_mappings = {\n",
    "        \"setup_cpu_model\": \"cpu_model\",\n",
    "        \"setup_gpu_model\": \"gpu_model\",\n",
    "        \"model_architecture_total_params\": \"total_params\",  # now maps to total_params\n",
    "        \"model_architecture_architecture\": \"model_arch\"\n",
    "    }\n",
    "    if col in special_mappings:\n",
    "        return special_mappings[col]\n",
    "    \n",
    "    # 2. Remove the 'variables_' prefix if it exists.\n",
    "    if col.startswith(\"variables_\"):\n",
    "        col = col[len(\"variables_\"):]\n",
    "    \n",
    "    # 3. First, check if it is a per-process metric column.\n",
    "    per_process_patterns = [\n",
    "        r'(cpu_power_process_\\d+)',\n",
    "        r'(gpu_power_process_\\d+)',\n",
    "        r'(ram_power_process_\\d+)',\n",
    "        r'(cpu_energy_process_\\d+)',\n",
    "        r'(gpu_energy_process_\\d+)',\n",
    "        r'(ram_energy_process_\\d+)',\n",
    "        r'(total_energy_kwh_process_\\d+)',\n",
    "        r'(total_energy_joules_process_\\d+)'\n",
    "    ]\n",
    "    for pattern in per_process_patterns:\n",
    "        match = re.search(pattern, col)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    \n",
    "    # 4. For non-per-process columns, search for a known token in the cleaned column.\n",
    "    tokens = [\n",
    "        \"config_name\", \"experiment_id\", \"date_time\", \"model\", \"is_encoder_decoder\",\n",
    "        \"task_type\", \"available_gpu_count\", \"gpu_model\", \"available_cpu_count\", \"cpu_model\",\n",
    "        \"os\", \"python_version\", \"country\", \"region\", \"fsdp_use_orig_params\", \"fsdp_cpu_offload\",\n",
    "        \"sharding_strategy\", \"distributed_type\", \"num_processes\", \"max_input_tokens\", \"max_output_tokens\",\n",
    "        \"number_input_prompts\", \"decode_token_to_text\", \"decoder_temperature\", \"decoder_top_k\", \"decoder_top_p\",\n",
    "        \"query_rate\", \"latency_simulate\", \"latency_delay_min\", \"latency_delay_max\", \"latency_simulate_burst\",\n",
    "        \"latency_burst_interval\", \"latency_burst_size\", \"fp_precision\", \"quantization\", \"load_in_8bit\",\n",
    "        \"load_in_4bit\", \"cached_flops_for_quantised_models\", \"batch_size___fixed_batching\", \"adaptive_batching\",\n",
    "        \"adaptive_max_tokens\", \"max_batch_size___adaptive_batching\", \"inference_type\", \"backend\", \"total_params\",\n",
    "        \"architecture\", \"total_input_tokens\", \"total_generated_tokens\", \"total_inference_time_sec\", \n",
    "        \"average_latency_ms_per_batch\", \"throughput_queries_per_sec\", \"throughput_tokens_per_sec\", \"flops\",\n",
    "        \"gpu_current_memory_allocated_bytes\", \"gpu_max_memory_allocated_bytes\", \"gpu_current_memory_reserved_bytes\",\n",
    "        \"gpu_max_memory_reserved_bytes\", \"gpu_utilization_percent\", \"cpu_usage_percent\", \"cpu_memory_usage_bytes\",\n",
    "        # Per-process metrics:\n",
    "        \"cpu_power_process_0\", \"gpu_power_process_0\", \"ram_power_process_0\",\n",
    "        \"cpu_energy_process_0\", \"gpu_energy_process_0\", \"ram_energy_process_0\",\n",
    "        \"total_energy_kwh_process_0\", \"total_energy_joules_process_0\",\n",
    "        # Global averages and totals:\n",
    "        \"cpu_power_avg\", \"gpu_power_avg\", \"ram_power_avg\", \"cpu_energy_total\", \"gpu_energy_total\", \"ram_energy_total\",\n",
    "        \"total_energy_kwh\", \"total_energy_joules\", \"tokens_per_joule\", \"joules_per_token\", \"flops_per_joule\", \"joules_per_flop\",\n",
    "        \"per-process_emissions\"\n",
    "    ]\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in col:\n",
    "            idx = col.find(token)\n",
    "            return col[idx:]\n",
    "    \n",
    "    return col\n",
    "\n",
    "def resolve_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Resolve duplicate columns in the DataFrame.\n",
    "    \n",
    "    For any column that appears more than once:\n",
    "      - For 'adaptive_batching', if duplicates exist, prefer the one with a boolean dtype;\n",
    "        otherwise, pick the first occurrence.\n",
    "      - For all other columns (including 'experiment_id' and 'number_input_prompts'),\n",
    "        keep only the first occurrence.\n",
    "    \"\"\"\n",
    "    # Build a mapping of column name to list of indices where it occurs.\n",
    "    seen = {}\n",
    "    for idx, col in enumerate(df.columns):\n",
    "        seen.setdefault(col, []).append(idx)\n",
    "    \n",
    "    # Choose one index per duplicate group.\n",
    "    chosen_indices = []\n",
    "    for col, indices in seen.items():\n",
    "        if len(indices) == 1:\n",
    "            chosen_indices.append(indices[0])\n",
    "        else:\n",
    "            if col == \"adaptive_batching\":\n",
    "                # Look for a column with boolean type.\n",
    "                bool_idx = None\n",
    "                for i in indices:\n",
    "                    if pd.api.types.is_bool_dtype(df.iloc[:, i]):\n",
    "                        bool_idx = i\n",
    "                        break\n",
    "                chosen_indices.append(bool_idx if bool_idx is not None else indices[0])\n",
    "            else:\n",
    "                # For experiment_id, number_input_prompts, or any duplicate, keep the first occurrence.\n",
    "                chosen_indices.append(indices[0])\n",
    "    \n",
    "    # Sort indices to preserve the original order.\n",
    "    chosen_indices.sort()\n",
    "    return df.iloc[:, chosen_indices]\n",
    "\n",
    "def clean_and_reorder_columns(df: pd.DataFrame, desired_order: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean DataFrame columns by:\n",
    "      1. Renaming each column to remove extraneous prefixes and apply special mappings.\n",
    "      2. Removing duplicates (applying special resolution for some columns).\n",
    "      3. Reordering columns into the order specified by 'desired_order'. Any columns not explicitly mentioned\n",
    "         will be appended at the end.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with messy, flattened column names.\n",
    "        desired_order (list): List of column names (after cleaning) indicating the preferred ordering.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with cleaned and reordered columns.\n",
    "    \"\"\"\n",
    "    # Build mapping from original column names to cleaned names.\n",
    "    mapping = {col: clean_column(col) for col in df.columns}\n",
    "    \n",
    "    # Rename columns in the DataFrame.\n",
    "    df = df.rename(columns=mapping)\n",
    "    \n",
    "    # Resolve duplicates as required.\n",
    "    df = resolve_duplicates(df)\n",
    "    \n",
    "    # Reorder columns: first, the ones matching the desired order.\n",
    "    ordered_cols = [col for col in desired_order if col in df.columns]\n",
    "    # Then, append any remaining columns.\n",
    "    remaining_cols = [col for col in df.columns if col not in desired_order]\n",
    "    final_order = ordered_cols + remaining_cols\n",
    "    \n",
    "    return df[final_order]\n",
    "\n",
    "# Example desired order list\n",
    "desired_order = [\n",
    "    \"config_name\",\n",
    "    \"experiment_id\",\n",
    "    \"date_time\",\n",
    "    \"model\",\n",
    "    # num_process\n",
    "    \"num_processes\",\n",
    "    # batching\n",
    "    \"batch_size___fixed_batching\",\n",
    "    # decodeer\n",
    "    \"decoder_temperature\",\n",
    "    \"decoder_top_k\",\n",
    "    \"decoder_top_p\",\n",
    "    # latency\n",
    "    \"latency_simulation_simulate\"\n",
    "    \"latency_simulation_delay_max\",\n",
    "    \"latency_simulation_delay_min\",\n",
    "    \"latency_simulation_simulate_burst\",\n",
    "    \"latency_simulation_burst_size\",\n",
    "    \"latency_simulation_burst_interval\",\n",
    "    # precision / quantisation\n",
    "    \"fp_precision\",\n",
    "    \"quantization\",\n",
    "    \"load_in_8bit\",\n",
    "    \"load_in_4bit\",\n",
    "    \"cached_flops_for_quantised_models\",\n",
    "    \n",
    "    # UNUSED PARAMS\n",
    "    \"sharding_strategy\",\n",
    "    \"sharding_config_fsdp_config_use_orig_params\",\n",
    "    \"sharding_config_fsdp_config_cpu_offload\",\n",
    "    \"adaptive_batching\",\n",
    "    \"adaptive_max_tokens\",\n",
    "    \"query_rate\",\n",
    "    \"total_input_tokens\",\n",
    "    \"total_generated_tokens\"\n",
    "    \n",
    "    # CONSTANT SETUP ====\n",
    "    \"date_time\",\n",
    "    \"is_encoder_decoder\",\n",
    "    \"task_type\",\n",
    "    \"available_gpu_count\",\n",
    "    \"gpu_model\",\n",
    "    \"available_cpu_count\",\n",
    "    \"cpu_model\",\n",
    "    \"os\",\n",
    "    \"python_version\",\n",
    "    \"country\",\n",
    "    \"region\",\n",
    "    \"distributed_type\",\n",
    "    \"decode_token_to_text\",\n",
    "    \"inference_type\",\n",
    "    \"backend\",\n",
    "    \"total_params\",\n",
    "    \"model_arch\",\n",
    "\n",
    "    # Validation (should be same):\n",
    "    \"max_input_tokens\",\n",
    "    \"max_output_tokens\",\n",
    "    \"number_input_prompts\",\n",
    "    \n",
    "    # RESULTS =====\n",
    "    # energy\n",
    "    \"total_energy_kwh\",\n",
    "    \"total_energy_joules\",\n",
    "    # FLOPS\n",
    "    \"flops\",\n",
    "    \"tokens_per_joule\",\n",
    "    \"joules_per_token\",\n",
    "    \"flops_per_joule\",\n",
    "    \"joules_per_flop\",\n",
    "    \"total_inference_time_sec\", \n",
    "    # inference performance\n",
    "    \"average_latency_ms_per_batch\",\n",
    "    \"throughput_queries_per_sec\",\n",
    "    \"throughput_tokens_per_sec\",\n",
    "    # CPU utilization\n",
    "    \"cpu_usage_percent\",\n",
    "    \"cpu_memory_usage_bytes\",\n",
    "    # GPU utilization\n",
    "    \"gpu_utilization_percent_0\", \"gpu_utilization_percent_1\", \"gpu_utilization_percent_2\", \"gpu_utilization_percent_3\",\n",
    "    # Compute mem\n",
    "    \"gpu_current_memory_allocated_bytes\",\n",
    "    \"gpu_max_memory_allocated_bytes\",\n",
    "    \"gpu_current_memory_reserved_bytes\",\n",
    "    \"gpu_max_memory_reserved_bytes\",\n",
    "    # Per-process metrics:\n",
    "    \"cpu_power_process_0\", \"cpu_power_process_1\", \"cpu_power_process_2\", \"cpu_power_process_3\",\n",
    "    \"gpu_power_process_0\", \"gpu_power_process_1\", \"gpu_power_process_2\", \"gpu_power_process_3\",\n",
    "    \"ram_power_process_0\", \"ram_power_process_1\", \"ram_power_process_2\", \"ram_power_process_3\",\n",
    "    \"cpu_energy_process_0\", \"cpu_energy_process_1\", \"cpu_energy_process_2\", \"cpu_energy_process_3\",\n",
    "    \"gpu_energy_process_0\", \"gpu_energy_process_1\", \"gpu_energy_process_2\", \"gpu_energy_process_3\",\n",
    "    \"ram_energy_process_0\", \"ram_energy_process_1\", \"ram_energy_process_2\", \"ram_energy_process_3\",\n",
    "    \"total_energy_kwh_process_0\", \"total_energy_kwh_process_1\", \"total_energy_kwh_process_2\", \"total_energy_kwh_process_3\",\n",
    "    \"total_energy_joules_process_0\", \"total_energy_joules_process_1\", \"total_energy_joules_process_2\", \"total_energy_joules_process_3\",\n",
    "    # Global averages and totals:\n",
    "    \"cpu_power_avg\",\n",
    "    \"gpu_power_avg\",\n",
    "    \"ram_power_avg\",\n",
    "    \"cpu_energy_total\",\n",
    "    \"gpu_energy_total\",\n",
    "    \"ram_energy_total\",\n",
    "    # per-process_emsisisons\n",
    "    \"per-process_emissions_0\", \"per-process_emissions_1\", \"per-process_emissions_2\",\"per-process_emissions_3\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_results(name, desired_order):\n",
    "    input_file = f\"analysis_08_04/results/{name}_results.csv\"\n",
    "    \n",
    "    df = pd.read_csv(input_file)\n",
    "    df_cleaned = clean_and_reorder_columns(df, desired_order)\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "possible_files = [\"controlled\", \"scenarios\", \"grid\", \"text_generation\"]\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "for file in possible_files:\n",
    "    try:\n",
    "        var_name = f\"df_{file}_cleaned\"\n",
    "        globals()[var_name] = inspect_results(file, desired_order)  # dynamically create variable\n",
    "        print(f\"Found & inspecting: {var_name}\")\n",
    "        display(globals()[var_name].T)\n",
    "    except Exception as e:\n",
    "        print(f\"{file} did not exist: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second round ordering\n",
    "#existing_cols = [col for col in desired_order if col in df.columns]\n",
    "#df_controlled = df[existing_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    \"sharding_strategy\",\n",
    "    \"sharding_config_fsdp_config_use_orig_params\",\n",
    "    \"sharding_config_fsdp_config_cpu_offload\",\n",
    "    \"adaptive_batching\",\n",
    "    \"adaptive_max_tokens\",\n",
    "    \"query_rate\",\n",
    "    \"is_encoder_decoder\",\n",
    "    \"task_type\",\n",
    "    \"available_gpu_count\",\n",
    "    \"gpu_model\",\n",
    "    \"available_cpu_count\",\n",
    "    \"cpu_model\",\n",
    "    \"os\",\n",
    "    \"python_version\",\n",
    "    \"country\",\n",
    "    \"region\",\n",
    "    \"distributed_type\",\n",
    "    \"decode_token_to_text\",\n",
    "    \"inference_type\",\n",
    "    \"backend\",\n",
    "    \"model_arch\",\n",
    "    \"gpu_current_memory_allocated_bytes\",\n",
    "    \"gpu_max_memory_allocated_bytes\",\n",
    "    \"gpu_current_memory_reserved_bytes\",\n",
    "    \"gpu_max_memory_reserved_bytes\",\n",
    "    \"per-process_emissions_0\", \"per-process_emissions_1\", \"per-process_emissions_2\",\"per-process_emissions_3\" # OR IS THIS NICE TO HAVE?\n",
    "]\n",
    "\n",
    "for file in possible_files:\n",
    "    try:\n",
    "        cleaned_var = f\"df_{file}_cleaned\"   # e.g., df_controlled_cleaned\n",
    "        dropped_var = f\"df_{file}_dropped\"     # e.g., df_controlled_dropped\n",
    "        \n",
    "        # Drop the specified columns from the cleaned DataFrame.\n",
    "        globals()[dropped_var] = globals()[cleaned_var].drop(columns=columns_to_drop)\n",
    "        print(f\"Found & inspecting dropped version: {dropped_var}\")\n",
    "        display(globals()[dropped_var].T)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all flops are constant\n",
    "df_controlled_dropped['flops'].unique()\n",
    "# TO DO: MAKE THIS DYANMIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVE THIS CALCULATION INTO THE RESULTS GENERATED \n",
    "df_controlled_dropped['flops_per_token'] = df_controlled_dropped['flops'] / df_controlled_dropped['total_generated_tokens']\n",
    "df_controlled_dropped['energy_per_token_kwh'] = df_controlled_dropped['total_energy_kwh'] / df_controlled_dropped['total_generated_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_controlled_dropped.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_controlled_dropped['config_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_altered_value(config_name, family):\n",
    "    \"\"\"\n",
    "    Given a config name and a family, extract the altered value.\n",
    "    For each family, a different rule is applied:\n",
    "    \n",
    "    - num_processes: extract the integer after \"num_processes_\"\n",
    "    - batching: extract the integer after \"batching_\"\n",
    "    - precis: extract the precision setting; if quant4 is True, return \"load_in_4bit=True\",\n",
    "              otherwise return the float precision (e.g., \"float32\" or \"float16\")\n",
    "    - decoding: extract the decoder_temperature (as a float) from the config name.\n",
    "    - latency: if the config is exactly \"latency_False\", return \"No latency\"; otherwise, \n",
    "               extract and join all numeric values in the config string.\n",
    "    \"\"\"\n",
    "    if family == \"num_processes\":\n",
    "        m = re.search(r'num_processes_(\\d+)', config_name)\n",
    "        return int(m.group(1)) if m else config_name\n",
    "    \n",
    "    elif family == \"batching\":\n",
    "        m = re.search(r'batching_(\\d+)', config_name)\n",
    "        return int(m.group(1)) if m else config_name\n",
    "    \n",
    "    elif family == \"precis\":\n",
    "        m = re.search(r'precis_(float\\d+)', config_name)\n",
    "        precision = m.group(1) if m else None\n",
    "        # If quant4 is True, we assume load_in_4bit is the relevant setting. COME BACK TO THIS!!!\n",
    "        if \"quant4_True\" in config_name:\n",
    "            return \"load_in_4bit=True\"\n",
    "        else:\n",
    "            return precision\n",
    "        \n",
    "    elif family == \"decoding\":\n",
    "        # This family might have several variants. We extract the decoder_temperature. COME BACK TO\n",
    "        m = re.search(r'decoder_temperature_([\\d\\.]+)', config_name)\n",
    "        return float(m.group(1)) if m else config_name\n",
    "    \n",
    "    elif family == \"latency\":\n",
    "        # If it's simply \"latency_False\", nothing was altered.\n",
    "        if config_name == \"latency_False\":\n",
    "            return \"No latency\"\n",
    "        else:\n",
    "            # Find all numbers (either integer or float) in the string.\n",
    "            numbers = re.findall(r'\\d+\\.\\d+|\\d+', config_name)\n",
    "            return \", \".join(numbers)\n",
    "    else:\n",
    "        return config_name\n",
    "\n",
    "def plot_family(df, family, metric1, metric2):\n",
    "    \"\"\"\n",
    "    For a given family, subset the DataFrame (rows whose config_name starts with family).\n",
    "    Create a new column that holds the altered value for that family, sort the data by that value,\n",
    "    and plot two metrics against this altered value.\n",
    "    \n",
    "    Parameters:\n",
    "      - df: a DataFrame with a \"config_name\" column.\n",
    "      - family: the family string (e.g. \"num_processes\", \"batching\", etc).\n",
    "      - metric1: name of the first metric column to plot on the left Y axis (e.g., \"flops_per_token\").\n",
    "      - metric2: name of the second metric column to plot on the right Y axis (e.g., \"total_energy_kwh\").\n",
    "    \"\"\"\n",
    "    # Subset rows where config_name starts with the family string.\n",
    "    df_family = df[df['config_name'].str.startswith(family)].copy()\n",
    "    \n",
    "    # Apply the parser to create an \"altered_value\" column.\n",
    "    df_family['altered_value'] = df_family['config_name'].apply(lambda x: extract_altered_value(x, family))\n",
    "    \n",
    "    # Attempt to sort by altered_value.\n",
    "    # If the values are numeric, convert them.\n",
    "    try:\n",
    "        df_family['altered_value'] = pd.to_numeric(df_family['altered_value'])\n",
    "    except:\n",
    "        pass  # leave as string if conversion fails\n",
    "    \n",
    "    df_family.sort_values('altered_value', inplace=True)\n",
    "    \n",
    "    # Create a plot with twin y-axes.\n",
    "    fig, ax1 = plt.subplots(figsize=(8,5))\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    # Plot metric1 on ax1 and metric2 on ax2.\n",
    "    ax1.plot(df_family['altered_value'], df_family[metric1], marker='o', label=metric1, color='blue')\n",
    "    ax2.plot(df_family['altered_value'], df_family[metric2], marker='s', label=metric2, color='red')\n",
    "    \n",
    "    ax1.set_xlabel(f'{family} configuration')\n",
    "    ax1.set_ylabel(metric1, color='blue')\n",
    "    ax2.set_ylabel(metric2, color='red')\n",
    "    plt.title(f'{family}: {metric1} and {metric2} vs altered setting')\n",
    "    \n",
    "    # Combine the legends.\n",
    "    lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "    lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc='best')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "families = [\"num_processes\", \"batching\", \"precis\", \"decoding\", \"latency\"]\n",
    "\n",
    "for fam in families:\n",
    "    plot_family(df_controlled_dropped, fam, metric1=\"flops_per_token\", metric2=\"energy_per_token_kwh\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# RESTART FROM HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# First, organise your separate dfs into a list of (name, df) pairs\n",
    "dfs = [\n",
    "    (\"Num Processes\", num_processes_df),\n",
    "    (\"Batching\", batching_df),\n",
    "    (\"Precision\", precics_df),\n",
    "    (\"Decoding\", decoding_df),\n",
    "    (\"Latency\", latency_df)\n",
    "]\n",
    "\n",
    "for name, df in dfs:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # ----- First subplot: Divergence Energy vs Batch Size -----\n",
    "    axes[0].plot(\n",
    "        df['num_processes'],\n",
    "        df['divergence_energy_flops_per_token'],\n",
    "        marker='o', \n",
    "        linestyle='-',\n",
    "        color='orange'\n",
    "    )\n",
    "    axes[0].set_xlabel('Batch Size (Fixed Batching)')\n",
    "    axes[0].set_ylabel('Divergence Energy ~FLOPs per Token')\n",
    "    axes[0].set_title(f'Divergence Energy vs Batch Size ({name})')\n",
    "    axes[0].set_xticks(df['num_processes'])\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # ----- Second subplot: Two Y-axes with Energy per Token and FLOPs per Token -----\n",
    "    ax1 = axes[1]\n",
    "    line1, = ax1.plot(\n",
    "        df['num_processes'], \n",
    "        df['energy_per_token_kwh'], \n",
    "        marker='o', \n",
    "        linestyle='-', \n",
    "        color='blue', \n",
    "        label='Energy per Token (kWh)'\n",
    "    )\n",
    "    ax1.set_xlabel('Batch Size (Fixed Batching)')\n",
    "    ax1.set_ylabel('Energy per Token (kWh)', color='blue')\n",
    "    ax1.tick_params(axis='y')\n",
    "    ax1.set_xticks(df['num_processes'])\n",
    "    ax1.set_title(f'Metrics vs Batch Size ({name})')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    line2, = ax2.plot(\n",
    "        df['num_processes'], \n",
    "        df['flops_per_token'], \n",
    "        marker='s', \n",
    "        linestyle='--', \n",
    "        color='red', \n",
    "        label='FLOPs per Token'\n",
    "    )\n",
    "    ax2.set_ylabel('FLOPs per Token', color='red')\n",
    "    ax2.tick_params(axis='y')\n",
    "\n",
    "    # Combine legends\n",
    "    lines = [line1, line2]\n",
    "    labels = [line.get_label() for line in lines]\n",
    "    ax1.legend(lines, labels, loc='best')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Create a new 'precision' column for plotting\n",
    "def determine_precision(row):\n",
    "    if row.get('load_in_4bit', False):\n",
    "        return 'INT4'\n",
    "    elif row.get('load_in_8bit', False):\n",
    "        return 'INT8'\n",
    "    elif row.get('fp_precision') == 'torch.float16':\n",
    "        return 'FP16'\n",
    "    else:\n",
    "        return 'FP32'\n",
    "\n",
    "precics_df['precision'] = precics_df.apply(determine_precision, axis=1)\n",
    "\n",
    "# Step 2: Define custom precision order\n",
    "precision_order = ['FP32', 'FP16', 'INT8', 'INT4']\n",
    "\n",
    "# Step 3: Sort the dataframe according to precision order\n",
    "precics_df['precision'] = pd.Categorical(precics_df['precision'], categories=precision_order, ordered=True)\n",
    "precics_df = precics_df.sort_values('precision')\n",
    "\n",
    "# Step 4: Plotting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# ----- First subplot: Divergence Energy vs Precision -----\n",
    "axes[0].plot(\n",
    "    precics_df['precision'],\n",
    "    precics_df['divergence_energy_flops_per_token'],\n",
    "    marker='o', \n",
    "    linestyle='-',\n",
    "    color='orange'\n",
    ")\n",
    "axes[0].set_xlabel('Precision')\n",
    "axes[0].set_ylabel('Divergence Energy ~FLOPs per Token')\n",
    "axes[0].set_title('Divergence Energy vs Precision')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# ----- Second subplot: Two Y-axes with Energy per Token and FLOPs per Token -----\n",
    "ax1 = axes[1]\n",
    "line1, = ax1.plot(\n",
    "    precics_df['precision'], \n",
    "    precics_df['energy_per_token_kwh'], \n",
    "    marker='o', \n",
    "    linestyle='-', \n",
    "    color='blue', \n",
    "    label='Energy per Token (kWh)'\n",
    ")\n",
    "ax1.set_xlabel('Precision')\n",
    "ax1.set_ylabel('Energy per Token (kWh)', color='blue')\n",
    "ax1.tick_params(axis='y')\n",
    "ax1.set_title('Metrics vs Precision')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "line2, = ax2.plot(\n",
    "    precics_df['precision'], \n",
    "    precics_df['flops_per_token'], \n",
    "    marker='s', \n",
    "    linestyle='--', \n",
    "    color='red', \n",
    "    label='FLOPs per Token'\n",
    ")\n",
    "ax2.set_ylabel('FLOPs per Token', color='red')\n",
    "ax2.tick_params(axis='y')\n",
    "\n",
    "# Combine legends\n",
    "lines = [line1, line2]\n",
    "labels = [line.get_label() for line in lines]\n",
    "ax1.legend(lines, labels, loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# --- Step 1: Filter the dataframe based on the config names ---\n",
    "config_names = [\n",
    "    'decoding_greedy_decoder_temperature_0',\n",
    "    'decoding_greedy_decoder_temperature_0.7',\n",
    "    'decoding_greedy_decoder_temperature_1.0',\n",
    "    'decoding_greedy_decoder_temperature_1.3',\n",
    "    'decoding_top_k_decoder_top_k_50_decoder_temperature_0',\n",
    "    'decoding_top_k_decoder_top_k_50_decoder_temperature_0.7',\n",
    "    'decoding_top_k_decoder_top_k_50_decoder_temperature_1.0',\n",
    "    'decoding_top_k_decoder_top_k_50_decoder_temperature_1.3',\n",
    "    'decoding_top_p_decoder_top_p_0.9_decoder_temperature_0',\n",
    "    'decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.7',\n",
    "    'decoding_top_p_decoder_top_p_0.9_decoder_temperature_1.0',\n",
    "    'decoding_top_p_decoder_top_p_0.9_decoder_temperature_1.3'\n",
    "]\n",
    "filtered_decoding = decoding_df[decoding_df['config_name'].isin(config_names)].copy()\n",
    "\n",
    "# --- Step 2: Extract method and temperature from the config_name ---\n",
    "def extract_method_and_temp(config):\n",
    "    if config.startswith(\"decoding_greedy_decoder_temperature_\"):\n",
    "        temp = float(config.split(\"decoding_greedy_decoder_temperature_\")[-1])\n",
    "        return \"greedy\", temp\n",
    "    elif config.startswith(\"decoding_top_k_decoder_top_k_50_decoder_temperature_\"):\n",
    "        temp = float(config.split(\"decoding_top_k_decoder_top_k_50_decoder_temperature_\")[-1])\n",
    "        return \"top_k\", temp\n",
    "    elif config.startswith(\"decoding_top_p_decoder_top_p_0.9_decoder_temperature_\"):\n",
    "        temp = float(config.split(\"decoding_top_p_decoder_top_p_0.9_decoder_temperature_\")[-1])\n",
    "        return \"top_p\", temp\n",
    "    else:\n",
    "        return \"unknown\", None\n",
    "\n",
    "# Apply the extraction function and assign to new columns\n",
    "filtered_decoding[['method', 'temperature']] = filtered_decoding['config_name'].apply(\n",
    "    lambda x: pd.Series(extract_method_and_temp(x))\n",
    ")\n",
    "\n",
    "# Optionally sort the dataframe by method and temperature for clarity.\n",
    "filtered_decoding = filtered_decoding.sort_values(['method', 'temperature'])\n",
    "\n",
    "# --- Step 3: Plotting ---\n",
    "\n",
    "# Define colors for each method\n",
    "colors = {\n",
    "    'greedy': 'blue',\n",
    "    'top_k': 'green',\n",
    "    'top_p': 'red'\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# --- Left subplot: Divergence Energy vs Temperature ---\n",
    "ax_left = axes[0]\n",
    "methods = filtered_decoding['method'].unique()\n",
    "for m in methods:\n",
    "    subdf = filtered_decoding[filtered_decoding['method'] == m]\n",
    "    ax_left.plot(subdf['temperature'], subdf['divergence_energy_flops_per_token'],\n",
    "                 marker='o', linestyle='-', label=m, color=colors.get(m))\n",
    "ax_left.set_xlabel('Decoder Temperature')\n",
    "ax_left.set_ylabel('Divergence Energy ~FLOPs per Token')\n",
    "ax_left.set_title('Divergence Energy vs Decoder Temperature')\n",
    "ax_left.grid(True)\n",
    "ax_left.legend(title=\"Method\")\n",
    "\n",
    "# --- Right subplot: Two Y-axes with Energy per Token and FLOPs per Token ---\n",
    "ax1 = axes[1]\n",
    "# Primary axis for Energy per Token\n",
    "for m in methods:\n",
    "    subdf = filtered_decoding[filtered_decoding['method'] == m]\n",
    "    ax1.plot(subdf['temperature'], subdf['energy_per_token_kwh'],\n",
    "             marker='o', linestyle='-', label=f'{m} Energy', color=colors.get(m))\n",
    "ax1.set_xlabel('Decoder Temperature')\n",
    "ax1.set_ylabel('Energy per Token (kWh)', color='black')\n",
    "ax1.set_title('Metrics vs Decoder Temperature')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Secondary axis for FLOPs per Token\n",
    "ax2 = ax1.twinx()\n",
    "for m in methods:\n",
    "    subdf = filtered_decoding[filtered_decoding['method'] == m]\n",
    "    ax2.plot(subdf['temperature'], subdf['flops_per_token'],\n",
    "             marker='s', linestyle='--', label=f'{m} FLOPs', color=colors.get(m))\n",
    "ax2.set_ylabel('FLOPs per Token', color='black')\n",
    "\n",
    "# --- Combine legends from both axes ---\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Filter the latency_df to only keep the specified configurations ---\n",
    "latency_configs = [\n",
    "    'latency_False',\n",
    "    'latency_True_latency_0.05_latency_0.2_latency_False',\n",
    "    'latency_True_latency_0.2_latency_0.6_latency_False',\n",
    "    'latency_True_latency_0.05_latency_0.2_latency_True_latency_4.0_latency_5',\n",
    "    'latency_True_latency_0.2_latency_0.6_latency_True_latency_5.0_latency_8'\n",
    "]\n",
    "latency_filtered = latency_df[latency_df['config_name'].isin(latency_configs)].copy()\n",
    "\n",
    "# --- 2. Define a function to parse the config string ---\n",
    "def parse_latency_config(config):\n",
    "    \"\"\"\n",
    "    Parses a latency configuration string and returns a dict with:\n",
    "      - simulate (boolean)\n",
    "      - delay_min (float or None)\n",
    "      - delay_max (float or None)\n",
    "      - simulate_burst (boolean or None)\n",
    "      - burst_size (float or None)\n",
    "      - burst_interval (float or None)\n",
    "    \"\"\"\n",
    "    tokens = config.split('_')\n",
    "    \n",
    "    # There will be extra \"latency\" tokens in the string.\n",
    "    # Look at the total number of tokens:\n",
    "    # For baseline: e.g., \"latency_False\" -> tokens: [\"latency\", \"False\"]\n",
    "    # Without burst: 8 tokens, e.g.:\n",
    "    #   [\"latency\", \"True\", \"latency\", \"0.05\", \"latency\", \"0.2\", \"latency\", \"False\"]\n",
    "    # With burst: 12 tokens, e.g.:\n",
    "    #   [\"latency\", \"True\", \"latency\", \"0.05\", \"latency\", \"0.2\", \"latency\", \"True\", \"latency\", \"4.0\", \"latency\", \"5\"]\n",
    "    res = {}\n",
    "    if len(tokens) == 2:\n",
    "        # Baseline: no simulation\n",
    "        res['simulate'] = (tokens[1] == \"True\")\n",
    "        res['delay_min'] = None\n",
    "        res['delay_max'] = None\n",
    "        res['simulate_burst'] = None\n",
    "        res['burst_size'] = None\n",
    "        res['burst_interval'] = None\n",
    "    elif len(tokens) == 8:\n",
    "        # Without burst: tokens at positions 1, 3, 5, and 7 are our values.\n",
    "        res['simulate'] = (tokens[1] == \"True\")\n",
    "        res['delay_min'] = float(tokens[3])\n",
    "        res['delay_max'] = float(tokens[5])\n",
    "        res['simulate_burst'] = (tokens[7] == \"True\")\n",
    "        res['burst_size'] = None\n",
    "        res['burst_interval'] = None\n",
    "    elif len(tokens) == 12:\n",
    "        # With burst: tokens at positions 1, 3, 5, 7, 9, and 11.\n",
    "        res['simulate'] = (tokens[1] == \"True\")\n",
    "        res['delay_min'] = float(tokens[3])\n",
    "        res['delay_max'] = float(tokens[5])\n",
    "        res['simulate_burst'] = (tokens[7] == \"True\")\n",
    "        res['burst_size'] = float(tokens[9])\n",
    "        res['burst_interval'] = float(tokens[11])\n",
    "    else:\n",
    "        res['simulate'] = None\n",
    "        res['delay_min'] = None\n",
    "        res['delay_max'] = None\n",
    "        res['simulate_burst'] = None\n",
    "        res['burst_size'] = None\n",
    "        res['burst_interval'] = None\n",
    "    return res\n",
    "\n",
    "# Apply the parser so that we have new columns for the latency parameters\n",
    "latency_params = latency_filtered['config_name'].apply(lambda x: pd.Series(parse_latency_config(x)))\n",
    "latency_filtered = pd.concat([latency_filtered, latency_params], axis=1)\n",
    "\n",
    "# --- 3. Create a user-friendly label for each configuration ---\n",
    "def make_latency_label(row):\n",
    "    if row['simulate'] is False:\n",
    "        return \"No simulation\"\n",
    "    elif row['simulate'] is True and row['simulate_burst'] is False:\n",
    "        return f\"Sim ({row['delay_min']}-{row['delay_max']})\"\n",
    "    elif row['simulate'] is True and row['simulate_burst'] is True:\n",
    "        return f\"Sim ({row['delay_min']}-{row['delay_max']}) Burst ({row['burst_size']},{row['burst_interval']})\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "latency_filtered['latency_label'] = latency_filtered.apply(make_latency_label, axis=1)\n",
    "\n",
    "# --- 4. Order the configurations as desired ---\n",
    "order_labels = [\n",
    "    \"No simulation\",\n",
    "    \"Sim (0.05-0.2)\",\n",
    "    \"Sim (0.2-0.6)\",\n",
    "    \"Sim (0.05-0.2) Burst (4.0,5)\",\n",
    "    \"Sim (0.2-0.6) Burst (5.0,8)\"\n",
    "]\n",
    "latency_filtered['latency_label'] = pd.Categorical(latency_filtered['latency_label'], \n",
    "                                                     categories=order_labels, ordered=True)\n",
    "latency_filtered = latency_filtered.sort_values('latency_label')\n",
    "\n",
    "# --- 5. Create the two subplots ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left Subplot: Divergence Energy vs Latency Configuration (categorical x-axis)\n",
    "axes[0].plot(\n",
    "    latency_filtered['latency_label'],\n",
    "    latency_filtered['divergence_energy_flops_per_token'],\n",
    "    marker='o',\n",
    "    linestyle='-',\n",
    "    color='orange'\n",
    ")\n",
    "axes[0].set_xlabel('Latency Configuration')\n",
    "axes[0].set_ylabel('Divergence Energy ~FLOPs per Token')\n",
    "axes[0].set_title('Divergence Energy vs Latency Config')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Right Subplot: Two y-axes for Energy per Token and FLOPs per Token\n",
    "ax1 = axes[1]\n",
    "line1, = ax1.plot(\n",
    "    latency_filtered['latency_label'],\n",
    "    latency_filtered['energy_per_token_kwh'],\n",
    "    marker='o',\n",
    "    linestyle='-',\n",
    "    color='blue',\n",
    "    label='Energy per Token (kWh)'\n",
    ")\n",
    "ax1.set_xlabel('Latency Configuration')\n",
    "ax1.set_ylabel('Energy per Token (kWh)', color='blue')\n",
    "ax1.tick_params(axis='y')\n",
    "ax1.set_title('Metrics vs Latency Config')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Create secondary y-axis for FLOPs per Token\n",
    "ax2 = ax1.twinx()\n",
    "line2, = ax2.plot(\n",
    "    latency_filtered['latency_label'],\n",
    "    latency_filtered['flops_per_token'],\n",
    "    marker='s',\n",
    "    linestyle='--',\n",
    "    color='red',\n",
    "    label='FLOPs per Token'\n",
    ")\n",
    "ax2.set_ylabel('FLOPs per Token', color='red')\n",
    "ax2.tick_params(axis='y')\n",
    "\n",
    "# Combine legends from both axes\n",
    "lines = [line1, line2]\n",
    "labels = [line.get_label() for line in lines]\n",
    "ax1.legend(lines, labels, loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_controlled_dropped.total_energy_kwh.max() / df_controlled_dropped.total_energy_kwh.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scenarios_dropped.total_energy_kwh.max() / df_scenarios_dropped.total_energy_kwh.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORK OUT STANDARD DEVIATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Histogram of raw total_energy_kwh\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df_scenarios_dropped['total_energy_kwh'], bins=50, edgecolor='black')\n",
    "plt.title('Distribution of Total Energy (kWh)')\n",
    "plt.xlabel('Total Energy (kWh)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Boxplot to spot outliers\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.boxplot(df_scenarios_dropped['total_energy_kwh'], vert=False)\n",
    "plt.title('Boxplot of Total Energy (kWh)')\n",
    "plt.xlabel('Total Energy (kWh)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# If distribution is very skewed: log-transform\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(np.log(df_scenarios_dropped['total_energy_kwh']), bins=50, edgecolor='black')\n",
    "plt.title('Distribution of Log-Transformed Total Energy (kWh)')\n",
    "plt.xlabel('Log(Total Energy (kWh))')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
