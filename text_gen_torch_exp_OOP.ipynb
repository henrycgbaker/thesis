{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n",
      "Using device: cuda:0 (Local Rank: 0)\n",
      "Using 2 GPUs: [0, 1]\n",
      "Model is on cuda:0\n",
      "Model is on cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon ERROR @ 18:58:48] Error: Another instance of codecarbon is probably running as we find `/tmp/.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n",
      "[codecarbon WARNING @ 18:58:48] Another instance of codecarbon is already running. Exiting.\n",
      "[codecarbon WARNING @ 18:58:50] Another instance of codecarbon is already running. Exiting.\n",
      "[2025-03-05 18:58:50,979] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 3940251) of fn: <lambda> (start_method: fork)\n"
     ]
    },
    {
     "ename": "ChildFailedError",
     "evalue": "\n============================================================\n<lambda> FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-03-05_18:58:50\n  host      : ds01\n  rank      : 1 (local_rank: 1)\n  exitcode  : 1 (pid: 3940251)\n  error_file: /tmp/torchelastic_hm6vtbtx/none_nn90nquh/attempt_0/1/error.json\n  traceback : Traceback (most recent call last):\n    File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n      return f(*args, **kwargs)\n    File \"/tmp/ipykernel_3940051/2396827121.py\", line 265, in <lambda>\n      ).run(),\n    File \"/tmp/ipykernel_3940051/2396827121.py\", line 193, in run\n      codecarbon_data = stop_energy_tracking(tracker)\n    File \"/home/228755@hertie-school.lan/thesis/energy_tracking.py\", line 16, in stop_energy_tracking\n      return tracker.final_emissions_data\n  AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'. Did you mean: '_prepare_emissions_data'?\n  \n============================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mChildFailedError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 258\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# Restrict to GPUs 0 and 1\u001b[39;00m\n\u001b[1;32m    256\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0,1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 258\u001b[0m \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mExperimentRunner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_gen_inference_with_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpytorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_optimum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m    267\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/accelerate/launchers.py:244\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes, rdzv_backend, rdzv_endpoint, rdzv_conf, rdzv_id, max_restarts, monitor_interval, log_line_prefix_template)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;124m\"\u001b[39m, ELASTIC_LOG_LINE_PREFIX_TEMPLATE_PYTORCH_VERSION):\n\u001b[1;32m    243\u001b[0m         launch_config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_line_prefix_template\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m log_line_prefix_template\n\u001b[0;32m--> 244\u001b[0m     \u001b[43melastic_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLaunchConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlaunch_config_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentrypoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProcessRaisedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/distributed/launcher/api.py:134\u001b[0m, in \u001b[0;36melastic_launch.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlaunch_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_entrypoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis/lib/python3.10/site-packages/torch/distributed/launcher/api.py:264\u001b[0m, in \u001b[0;36mlaunch_agent\u001b[0;34m(config, entrypoint, args)\u001b[0m\n\u001b[1;32m    257\u001b[0m     events\u001b[38;5;241m.\u001b[39mrecord(agent\u001b[38;5;241m.\u001b[39mget_event_succeeded())\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mis_failed():\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;66;03m# ChildFailedError is treated specially by @record\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;66;03m# if the error files for the failed children exist\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;66;03m# @record will copy the first error (root cause)\u001b[39;00m\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;66;03m# to the error file of the launcher process.\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChildFailedError(\n\u001b[1;32m    265\u001b[0m             name\u001b[38;5;241m=\u001b[39mentrypoint_name,\n\u001b[1;32m    266\u001b[0m             failures\u001b[38;5;241m=\u001b[39mresult\u001b[38;5;241m.\u001b[39mfailures,\n\u001b[1;32m    267\u001b[0m         )\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreturn_values\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ChildFailedError:\n",
      "\u001b[0;31mChildFailedError\u001b[0m: \n============================================================\n<lambda> FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-03-05_18:58:50\n  host      : ds01\n  rank      : 1 (local_rank: 1)\n  exitcode  : 1 (pid: 3940251)\n  error_file: /tmp/torchelastic_hm6vtbtx/none_nn90nquh/attempt_0/1/error.json\n  traceback : Traceback (most recent call last):\n    File \"/home/228755@hertie-school.lan/thesis/thesis/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n      return f(*args, **kwargs)\n    File \"/tmp/ipykernel_3940051/2396827121.py\", line 265, in <lambda>\n      ).run(),\n    File \"/tmp/ipykernel_3940051/2396827121.py\", line 193, in run\n      codecarbon_data = stop_energy_tracking(tracker)\n    File \"/home/228755@hertie-school.lan/thesis/energy_tracking.py\", line 16, in stop_energy_tracking\n      return tracker.final_emissions_data\n  AttributeError: 'EmissionsTracker' object has no attribute 'final_emissions_data'. Did you mean: '_prepare_emissions_data'?\n  \n============================================================"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from contextlib import redirect_stdout\n",
    "from io import StringIO\n",
    "from dataclasses import dataclass, field\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from codecarbon import EmissionsTracker\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "# Optional: if using optimum benchmark branch\n",
    "from optimum_benchmark import Benchmark, BenchmarkConfig, TorchrunConfig, InferenceConfig, PyTorchConfig\n",
    "from optimum_benchmark.logging_utils import setup_logging\n",
    "\n",
    "from model_wrapper import ModelWrapper\n",
    "from energy_tracking import start_energy_tracking, stop_energy_tracking\n",
    "from metrics import get_compute_performance_metrics, detect_cpu_vendor\n",
    "from experiment_utils import (\n",
    "    load_model_tokenizer_backend,\n",
    "    prep_distributed_env,\n",
    "    extract_experiment_setup,\n",
    "    extract_experiment_results,\n",
    "    save_results,\n",
    "    aggregate_experiments,\n",
    "    get_persistent_unique_id\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuration dataclass\n",
    "# -----------------------------------------------------------------------------\n",
    "    \n",
    "from enum import Enum\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "class TaskType(str, Enum):\n",
    "    TEXT_GENERATION = \"text_generation\"\n",
    "    TRANSLATION = \"translation\"\n",
    "    SUMMARIZATION = \"summarization\"\n",
    "\n",
    "class InferenceType(str, Enum):\n",
    "    PURELY_GENERATIVE = \"purely_generative\"\n",
    "    REASONING = \"reasoning\"\n",
    "    \n",
    "class IsEncoderDecoder(str, Enum):\n",
    "    NA = \"na\"\n",
    "    ENCODER_DECODER = \"encoder_decoder\"\n",
    "    DECODER_ONLY = \"decoder_only\"\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    model_name: str\n",
    "    task_type: TaskType = TaskType.TEXT_GENERATION\n",
    "    max_input_tokens: int = 512\n",
    "    max_output_tokens: int = 128\n",
    "    batch_size: int = 8\n",
    "    gpu_list: list = field(default_factory=lambda: [0, 1]) \n",
    "    decoder_temperature: float = 1.0\n",
    "    query_rate: float = 1.0\n",
    "    fp_precision: str = \"float16\"\n",
    "    inference_type: str = \"purely_generative\"\n",
    "    quantisation: bool = False\n",
    "    batching_options: dict = field(default_factory=dict)\n",
    "    sharding_config: dict = field(default_factory=dict)\n",
    "    is_encoder_decoder: IsEncoderDecoder = IsEncoderDecoder.NA\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Inference function that measures performance metrics.\n",
    "# -----------------------------------------------------------------------------\n",
    "def run_gen_inference_with_metrics(model, tokenizer, accelerator, prompts, \n",
    "                                   max_input_tokens, max_output_tokens, batch_size):\n",
    "    \"\"\"\n",
    "    Runs inference and returns performance metrics.\n",
    "    \"\"\"\n",
    "    truncated_prompts = [\n",
    "        tokenizer.decode(\n",
    "            tokenizer(p, truncation=True, max_length=max_input_tokens, return_tensors=\"pt\").input_ids[0],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        for p in prompts\n",
    "    ]\n",
    "    \n",
    "    # Sort prompts by token length for efficient batching\n",
    "    sorted_prompts = sorted(truncated_prompts, key=lambda x: len(tokenizer.tokenize(x)))\n",
    "    latencies = []\n",
    "    total_tokens = 0\n",
    "    total_input_tokens = 0  # Track input tokens\n",
    "    device = accelerator.device\n",
    "    num_batches = (len(sorted_prompts) + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch = sorted_prompts[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "        # Tokenize batch\n",
    "        encoded = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_input_tokens)\n",
    "        input_ids = encoded.input_ids.to(device)\n",
    "        total_input_tokens += input_ids.numel()  # Count input tokens\n",
    "\n",
    "        # Generate outputs with DistributedDataParallel fix\n",
    "        start_time = time.perf_counter()\n",
    "        if hasattr(model, \"module\"):\n",
    "            outputs = model.module.generate(input_ids, max_new_tokens=max_output_tokens, do_sample=False)\n",
    "        else:\n",
    "            outputs = model.generate(input_ids, max_new_tokens=max_output_tokens, do_sample=False)\n",
    "        end_time = time.perf_counter()\n",
    "        latencies.append((end_time - start_time) * 1000.0)\n",
    "\n",
    "        # Count generated tokens per prompt\n",
    "        for j in range(len(batch)):\n",
    "            prompt_len = input_ids[j].shape[0]\n",
    "            gen_len = outputs[j].shape[0] - prompt_len\n",
    "            total_tokens += gen_len\n",
    "\n",
    "    avg_latency_ms = sum(latencies) / len(latencies) if latencies else 0.0\n",
    "    total_time_sec = sum(latencies) / 1000.0\n",
    "    throughput_qps = len(sorted_prompts) / total_time_sec if total_time_sec > 0 else 0.0\n",
    "    tokens_per_sec = total_tokens / total_time_sec if total_time_sec > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"avg_latency_ms\": avg_latency_ms,\n",
    "        \"throughput_qps\": throughput_qps,\n",
    "        \"tokens_per_sec\": tokens_per_sec,\n",
    "        \"total_generated_tokens\": total_tokens,\n",
    "        \"num_runs\": len(sorted_prompts),\n",
    "        \"total_time\": total_time_sec,\n",
    "        \"total_input_tokens\": total_input_tokens  \n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Experiment runner with aggregation integration.\n",
    "# -----------------------------------------------------------------------------\n",
    "class ExperimentRunner:\n",
    "    def __init__(self, experiment_config: ExperimentConfig, prompts, inference_fn, backend=\"pytorch\", use_optimum=False, **inference_kwargs):\n",
    "        self.config = experiment_config\n",
    "        self.prompts = prompts\n",
    "        self.inference_fn = inference_fn\n",
    "        self.backend = backend\n",
    "        self.use_optimum = use_optimum\n",
    "        self.inference_kwargs = inference_kwargs\n",
    "\n",
    "    def run(self):\n",
    "        model_name = self.config.model_name\n",
    "        # Use the enum's value if applicable\n",
    "        task_type = self.config.task_type.value if isinstance(self.config.task_type, Enum) else self.config.task_type\n",
    "\n",
    "        if self.use_optimum:\n",
    "            # --- Optimum benchmark branch ---\n",
    "            setup_logging(level=\"INFO\")\n",
    "            launcher_config = TorchrunConfig(nproc_per_node=1)\n",
    "            scenario_config = InferenceConfig(latency=True, memory=True, input_shapes={\"sequence_length\": 128})\n",
    "            backend_config = PyTorchConfig(model=model_name, device=\"cuda\", device_ids=\"0\", no_weights=True)\n",
    "            benchmark_config = BenchmarkConfig(\n",
    "                name=f\"{self.backend}_{model_name}\",\n",
    "                scenario=scenario_config,\n",
    "                launcher=launcher_config,\n",
    "                backend=backend_config,\n",
    "            )\n",
    "            benchmark_report = Benchmark.launch(benchmark_config)\n",
    "            benchmark_results = benchmark_report.to_dict()\n",
    "            print(json.dumps({\n",
    "                \"model\": model_name,\n",
    "                \"optimum_benchmark_results\": benchmark_results\n",
    "            }, indent=4))\n",
    "            return benchmark_results\n",
    "        else:\n",
    "            # --- Standard experiment branch ---\n",
    "            model, tokenizer = load_model_tokenizer_backend(model_name, backend=self.backend)\n",
    "            model, tokenizer, accelerator = prep_distributed_env(model, tokenizer, gpu_list=self.config.gpu_list)\n",
    "            tracker = start_energy_tracking()\n",
    "            \n",
    "            # Run inference using parameters from the config\n",
    "            inference_metrics = self.inference_fn(\n",
    "                model, tokenizer, accelerator, self.prompts,\n",
    "                self.config.max_input_tokens,\n",
    "                self.config.max_output_tokens,\n",
    "                self.config.batch_size,\n",
    "                **self.inference_kwargs\n",
    "            )\n",
    "            codecarbon_data = stop_energy_tracking(tracker)\n",
    "            experiment_results = extract_experiment_results(\n",
    "                inference_metrics, codecarbon_data,\n",
    "                model=model, tokenizer=tokenizer, device=accelerator.device\n",
    "            )\n",
    "            \n",
    "            # Extract common experimental setup and variables\n",
    "            experiment_setup = extract_experiment_setup(model_name, codecarbon_data, accelerator, task_type)\n",
    "            experiment_variables = {\n",
    "                \"total_token_inputted\": inference_metrics[\"total_input_tokens\"],\n",
    "                \"total_tokens_outputted\": inference_metrics[\"total_generated_tokens\"],\n",
    "                \"number_runs\": inference_metrics[\"num_runs\"]\n",
    "            }\n",
    "            \n",
    "            local_result = {\n",
    "                \"experiment_setup\": experiment_setup,\n",
    "                \"experiment_variables\": experiment_variables,\n",
    "                \"experiment_results\": experiment_results\n",
    "            }\n",
    "            \n",
    "            # Gather results if using distributed processing\n",
    "            if dist.is_available() and dist.is_initialized():\n",
    "                world_size = dist.get_world_size()\n",
    "                all_results = [None] * world_size\n",
    "                dist.all_gather_object(all_results, local_result)\n",
    "            else:\n",
    "                all_results = [local_result]\n",
    "            \n",
    "            # Only aggregate on the main process (local rank 0)\n",
    "            if accelerator.local_process_index == 0:\n",
    "                aggregated_result = aggregate_experiments(all_results)\n",
    "                unique_id = get_persistent_unique_id()\n",
    "                experiment_title = f\"EXPERIMENT #{unique_id}\"\n",
    "                benchmark_results = {experiment_title: aggregated_result}\n",
    "                output_json_path = save_results(task_type, benchmark_results)\n",
    "                print(f\"Aggregated benchmark results saved to {output_json_path}\")\n",
    "                return benchmark_results\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main execution\n",
    "# -----------------------------------------------------------------------------\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Create an ExperimentConfig instance with the desired parameters\n",
    "experiment_config = ExperimentConfig(\n",
    "    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    task_type=\"text_generation\",\n",
    "    max_input_tokens=512,\n",
    "    max_output_tokens=50,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "# Prompts\n",
    "ds = load_dataset(\"lighteval/pile_helm\", \"arxiv\")[\"test\"]\n",
    "ds = ds.select(range(5))\n",
    "prompts = [sample[\"text\"] for sample in ds]\n",
    "\n",
    "backend = \"pytorch\"\n",
    "use_optimum = False\n",
    "\n",
    "# Restrict to GPUs 0 and 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "notebook_launcher(\n",
    "    lambda: ExperimentRunner(\n",
    "        experiment_config=experiment_config,\n",
    "        prompts=prompts,\n",
    "        inference_fn=run_gen_inference_with_metrics,\n",
    "        backend=\"pytorch\",\n",
    "        use_optimum=False\n",
    "    ).run(),\n",
    "    num_processes=2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
