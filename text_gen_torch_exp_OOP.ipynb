{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n",
      "Using device: cuda:0 (Local Rank: 0)\n",
      "Using 2 GPUs: [0, 1]Model is on cuda:1\n",
      "\n",
      "Model is on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 21:04:13] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon WARNING @ 21:04:13] Multiple instances of codecarbon are allowed to run at the same time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated benchmark results saved to benchmark_results/text_generation_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-05 21:04:23,403] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 4070754 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from contextlib import redirect_stdout\n",
    "from io import StringIO\n",
    "from dataclasses import dataclass, field\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from codecarbon import EmissionsTracker\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "# Optional: if using optimum benchmark branch\n",
    "from optimum_benchmark import Benchmark, BenchmarkConfig, TorchrunConfig, InferenceConfig, PyTorchConfig\n",
    "from optimum_benchmark.logging_utils import setup_logging\n",
    "\n",
    "from model_wrapper import ModelWrapper\n",
    "from energy_tracking import start_energy_tracking, stop_energy_tracking\n",
    "from metrics import get_compute_performance_metrics, detect_cpu_vendor\n",
    "from experiment_utils import (\n",
    "    load_model_tokenizer_backend,\n",
    "    prep_distributed_env,\n",
    "    extract_experiment_setup,\n",
    "    extract_experiment_results,\n",
    "    save_results,\n",
    "    aggregate_experiments,\n",
    "    get_persistent_unique_id\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuration dataclass\n",
    "# -----------------------------------------------------------------------------\n",
    "    \n",
    "from enum import Enum\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "class TaskType(str, Enum):\n",
    "    TEXT_GENERATION = \"text_generation\"\n",
    "    TRANSLATION = \"translation\"\n",
    "    SUMMARIZATION = \"summarization\"\n",
    "\n",
    "class InferenceType(str, Enum):\n",
    "    PURELY_GENERATIVE = \"purely_generative\"\n",
    "    REASONING = \"reasoning\"\n",
    "    \n",
    "class IsEncoderDecoder(str, Enum):\n",
    "    NA = \"na\"\n",
    "    ENCODER_DECODER = \"encoder_decoder\"\n",
    "    DECODER_ONLY = \"decoder_only\"\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    model_name: str\n",
    "    task_type: TaskType = TaskType.TEXT_GENERATION\n",
    "    max_input_tokens: int = 512\n",
    "    max_output_tokens: int = 128\n",
    "    batch_size: int = 8\n",
    "    gpu_list: list = field(default_factory=lambda: [0, 1]) \n",
    "    decoder_temperature: float = 1.0\n",
    "    query_rate: float = 1.0\n",
    "    fp_precision: str = \"float16\"\n",
    "    inference_type: str = \"purely_generative\"\n",
    "    quantisation: bool = False\n",
    "    batching_options: dict = field(default_factory=dict)\n",
    "    sharding_config: dict = field(default_factory=dict)\n",
    "    is_encoder_decoder: IsEncoderDecoder = IsEncoderDecoder.NA\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Inference function that measures performance metrics.\n",
    "# -----------------------------------------------------------------------------\n",
    "def run_gen_inference_with_metrics(model, tokenizer, accelerator, prompts, \n",
    "                                   max_input_tokens, max_output_tokens, batch_size):\n",
    "    \"\"\"\n",
    "    Runs inference and returns performance metrics.\n",
    "    \"\"\"\n",
    "    truncated_prompts = [\n",
    "        tokenizer.decode(\n",
    "            tokenizer(p, truncation=True, max_length=max_input_tokens, return_tensors=\"pt\").input_ids[0],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        for p in prompts\n",
    "    ]\n",
    "\n",
    "    # Sort prompts by token length for efficient batching\n",
    "    sorted_prompts = sorted(truncated_prompts, key=lambda x: len(tokenizer.tokenize(x)))\n",
    "    latencies = []\n",
    "    total_tokens = 0\n",
    "    total_input_tokens = 0  # Track input tokens\n",
    "    device = accelerator.device\n",
    "    num_batches = (len(sorted_prompts) + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch = sorted_prompts[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "        # Tokenize batch\n",
    "        encoded = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_input_tokens)\n",
    "        input_ids = encoded.input_ids.to(device)\n",
    "        total_input_tokens += input_ids.numel()  # Count input tokens\n",
    "\n",
    "        # Generate outputs with DistributedDataParallel fix\n",
    "        start_time = time.perf_counter()\n",
    "        if hasattr(model, \"module\"):\n",
    "            outputs = model.module.generate(input_ids, max_new_tokens=max_output_tokens, do_sample=False)\n",
    "        else:\n",
    "            outputs = model.generate(input_ids, max_new_tokens=max_output_tokens, do_sample=False)\n",
    "        end_time = time.perf_counter()\n",
    "        latencies.append((end_time - start_time) * 1000.0)\n",
    "\n",
    "        # Count generated tokens per prompt\n",
    "        for j in range(len(batch)):\n",
    "            prompt_len = input_ids[j].shape[0]\n",
    "            gen_len = outputs[j].shape[0] - prompt_len\n",
    "            total_tokens += gen_len\n",
    "\n",
    "    avg_latency_ms = sum(latencies) / len(latencies) if latencies else 0.0\n",
    "    total_time_sec = sum(latencies) / 1000.0\n",
    "    throughput_qps = len(sorted_prompts) / total_time_sec if total_time_sec > 0 else 0.0\n",
    "    tokens_per_sec = total_tokens / total_time_sec if total_time_sec > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"avg_latency_ms\": avg_latency_ms,\n",
    "        \"throughput_qps\": throughput_qps,\n",
    "        \"tokens_per_sec\": tokens_per_sec,\n",
    "        \"total_generated_tokens\": total_tokens,\n",
    "        \"num_runs\": len(sorted_prompts),\n",
    "        \"total_time\": total_time_sec,\n",
    "        \"total_input_tokens\": total_input_tokens  \n",
    "    }\n",
    "\n",
    "\n",
    "def run_gen_inference_with_metrics(model, tokenizer, accelerator, prompts, \n",
    "                                   max_input_tokens, max_output_tokens, batch_size,\n",
    "                                   decoder_temperature=1.0,\n",
    "                                   inference_type=\"purely_generative\",\n",
    "                                   query_rate=1.0):\n",
    "    \"\"\"\n",
    "    Runs inference and returns performance metrics.\n",
    "    \"\"\"\n",
    "    # truncate prompts\n",
    "    truncated_prompts = [\n",
    "        tokenizer.decode(\n",
    "            tokenizer(p, truncation=True, max_length=max_input_tokens, return_tensors=\"pt\").input_ids[0],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        for p in prompts\n",
    "    ]\n",
    "    \n",
    "    sorted_prompts = sorted(truncated_prompts, \n",
    "                            key=lambda x: len(tokenizer.tokenize(x)))\n",
    "    latencies = []\n",
    "    total_tokens = 0\n",
    "    total_input_tokens = 0\n",
    "    device = accelerator.device\n",
    "    num_batches = (len(sorted_prompts) + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch = sorted_prompts[i * batch_size: (i + 1) * batch_size]\n",
    "        encoded = tokenizer(batch, \n",
    "                            return_tensors=\"pt\", \n",
    "                            padding=True, \n",
    "                            truncation=True, \n",
    "                            max_length=max_input_tokens)\n",
    "        input_ids = encoded.input_ids.to(device)\n",
    "        total_input_tokens += input_ids.numel()\n",
    "\n",
    "        # Determine sampling settings based on inference_type\n",
    "        do_sample = True if inference_type == \"reasoning\" else False # remove this\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        if hasattr(model, \"module\"):\n",
    "            outputs = model.module.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=max_output_tokens,\n",
    "                do_sample=do_sample,\n",
    "                temperature=decoder_temperature\n",
    "            )\n",
    "        else:\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=max_output_tokens,\n",
    "                do_sample=do_sample,\n",
    "                temperature=decoder_temperature\n",
    "            )\n",
    "        end_time = time.perf_counter()\n",
    "        latencies.append((end_time - start_time) * 1000.0)\n",
    "\n",
    "        for j in range(len(batch)):\n",
    "            prompt_len = input_ids[j].shape[0]\n",
    "            gen_len = outputs[j].shape[0] - prompt_len\n",
    "            total_tokens += gen_len\n",
    "\n",
    "        # Enforce query rate by sleeping (simulate inter-arrival delay)\n",
    "        if query_rate > 0:\n",
    "            time.sleep(1.0 / query_rate)\n",
    "\n",
    "    avg_latency_ms = sum(latencies) / len(latencies) if latencies else 0.0\n",
    "    total_time_sec = sum(latencies) / 1000.0\n",
    "    throughput_qps = len(sorted_prompts) / total_time_sec if total_time_sec > 0 else 0.0\n",
    "    tokens_per_sec = total_tokens / total_time_sec if total_time_sec > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"avg_latency_ms\": avg_latency_ms,\n",
    "        \"throughput_qps\": throughput_qps,\n",
    "        \"tokens_per_sec\": tokens_per_sec,\n",
    "        \"total_generated_tokens\": total_tokens,\n",
    "        \"num_runs\": len(sorted_prompts),\n",
    "        \"total_time\": total_time_sec,\n",
    "        \"total_input_tokens\": total_input_tokens  \n",
    "    }\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Experiment runner with aggregation integration.\n",
    "# -----------------------------------------------------------------------------\n",
    "class ExperimentRunner:\n",
    "    def __init__(self, experiment_config: ExperimentConfig, prompts, inference_fn, backend=\"pytorch\", use_optimum=False, **inference_kwargs):\n",
    "        self.config = experiment_config\n",
    "        self.prompts = prompts\n",
    "        self.inference_fn = inference_fn\n",
    "        self.backend = backend\n",
    "        self.use_optimum = use_optimum\n",
    "        self.inference_kwargs = inference_kwargs\n",
    "\n",
    "    def run(self):\n",
    "        model_name = self.config.model_name\n",
    "        # Use the enum's value if applicable\n",
    "        task_type = self.config.task_type.value if isinstance(self.config.task_type, Enum) else self.config.task_type\n",
    "\n",
    "        if self.use_optimum:\n",
    "            # --- Optimum benchmark branch ---\n",
    "            setup_logging(level=\"INFO\")\n",
    "            launcher_config = TorchrunConfig(nproc_per_node=1)\n",
    "            scenario_config = InferenceConfig(latency=True, memory=True, input_shapes={\"sequence_length\": 128})\n",
    "            backend_config = PyTorchConfig(model=model_name, device=\"cuda\", device_ids=\"0\", no_weights=True)\n",
    "            benchmark_config = BenchmarkConfig(\n",
    "                name=f\"{self.backend}_{model_name}\",\n",
    "                scenario=scenario_config,\n",
    "                launcher=launcher_config,\n",
    "                backend=backend_config,\n",
    "            )\n",
    "            benchmark_report = Benchmark.launch(benchmark_config)\n",
    "            benchmark_results = benchmark_report.to_dict()\n",
    "            print(json.dumps({\n",
    "                \"model\": model_name,\n",
    "                \"optimum_benchmark_results\": benchmark_results\n",
    "            }, indent=4))\n",
    "            return benchmark_results\n",
    "        else:\n",
    "            # --- Standard experiment branch ---\n",
    "            model, tokenizer = load_model_tokenizer_backend(\n",
    "                model_name, \n",
    "                backend=self.backend, \n",
    "                fp_precision=self.config.fp_precision\n",
    "            )\n",
    "\n",
    "            model, tokenizer, accelerator = prep_distributed_env(model, tokenizer, gpu_list=self.config.gpu_list)\n",
    "            tracker = start_energy_tracking()\n",
    "            \n",
    "            # Run inference using parameters from the config\n",
    "            inference_metrics = self.inference_fn(\n",
    "                model, tokenizer, accelerator, self.prompts,\n",
    "                self.config.max_input_tokens,\n",
    "                self.config.max_output_tokens,\n",
    "                self.config.batch_size,\n",
    "                decoder_temperature=self.config.decoder_temperature,\n",
    "                inference_type=self.config.inference_type,\n",
    "                query_rate=self.config.query_rate,\n",
    "                **self.inference_kwargs\n",
    "            )\n",
    "            codecarbon_data = stop_energy_tracking(tracker)\n",
    "            experiment_results = extract_experiment_results(\n",
    "                inference_metrics, codecarbon_data,\n",
    "                model=model, tokenizer=tokenizer, device=accelerator.device\n",
    "            )\n",
    "            \n",
    "            # Extract common experimental setup and variables\n",
    "            experiment_setup = extract_experiment_setup(model_name, codecarbon_data, accelerator, task_type)\n",
    "            experiment_variables = {\n",
    "                \"max_input_tokens\": self.config.max_input_tokens,\n",
    "                \"max_output_tokens\": self.config.max_output_tokens,\n",
    "                \"number_runs\": inference_metrics[\"num_runs\"],\n",
    "                \"total_token_inputted\": inference_metrics[\"total_input_tokens\"],\n",
    "                \"total_tokens_outputted\": inference_metrics[\"total_generated_tokens\"],\n",
    "\n",
    "                \"batch_size\": self.config.batch_size,\n",
    "                \"gpu_list\": self.config.gpu_list,\n",
    "                \"decoder_temperature\": self.config.decoder_temperature,\n",
    "                \"query_rate\": self.config.query_rate,\n",
    "                \"fp_precision\": self.config.fp_precision,\n",
    "                \"inference_type\": self.config.inference_type,\n",
    "                \"quantisation\": self.config.quantisation,\n",
    "                \"batching_options\": self.config.batching_options,\n",
    "                \"sharding_config\": self.config.sharding_config,\n",
    "                \"is_encoder_decoder\": (\n",
    "                    self.config.is_encoder_decoder.value\n",
    "                    if hasattr(self.config.is_encoder_decoder, \"value\")\n",
    "                    else self.config.is_encoder_decoder\n",
    "                )\n",
    "            }\n",
    "            \n",
    "            local_result = {\n",
    "                \"experiment_setup\": experiment_setup,\n",
    "                \"experiment_variables\": experiment_variables,\n",
    "                \"experiment_results\": experiment_results\n",
    "            }\n",
    "            \n",
    "            # Gather results if using distributed processing\n",
    "            if dist.is_available() and dist.is_initialized():\n",
    "                world_size = dist.get_world_size()\n",
    "                all_results = [None] * world_size\n",
    "                dist.all_gather_object(all_results, local_result)\n",
    "            else:\n",
    "                all_results = [local_result]\n",
    "            \n",
    "            # Only aggregate on the main process (local rank 0)\n",
    "            if accelerator.local_process_index == 0:\n",
    "                aggregated_result = aggregate_experiments(all_results)\n",
    "                unique_id = get_persistent_unique_id()\n",
    "                experiment_title = f\"EXPERIMENT #{unique_id}\"\n",
    "                benchmark_results = {experiment_title: aggregated_result}\n",
    "                output_json_path = save_results(task_type, benchmark_results)\n",
    "                print(f\"Aggregated benchmark results saved to {output_json_path}\")\n",
    "                return benchmark_results\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main execution\n",
    "# -----------------------------------------------------------------------------\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Create an ExperimentConfig instance with the desired parameters\n",
    "experiment_config = ExperimentConfig(\n",
    "    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    is_encoder_decoder=\"decoder_only\",\n",
    "    task_type=\"text_generation\",\n",
    "    max_input_tokens=512,\n",
    "    max_output_tokens=50,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "# Prompts\n",
    "ds = load_dataset(\"lighteval/pile_helm\", \"arxiv\")[\"test\"]\n",
    "ds = ds.select(range(5))\n",
    "prompts = [sample[\"text\"] for sample in ds]\n",
    "\n",
    "backend = \"pytorch\"\n",
    "use_optimum = False\n",
    "\n",
    "# Restrict to GPUs 0 and 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "notebook_launcher(\n",
    "    lambda: ExperimentRunner(\n",
    "        experiment_config=experiment_config,\n",
    "        prompts=prompts,\n",
    "        inference_fn=run_gen_inference_with_metrics,\n",
    "        backend=\"pytorch\",\n",
    "        use_optimum=False\n",
    "    ).run(),\n",
    "    num_processes=2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
