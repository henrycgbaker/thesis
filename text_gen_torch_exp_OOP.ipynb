{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from contextlib import redirect_stdout\n",
    "from io import StringIO\n",
    "from dataclasses import dataclass, field\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from codecarbon import EmissionsTracker\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "from torch.quantization import quantize_dynamic\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "# Optional: if using optimum benchmark branch\n",
    "from optimum_benchmark.logging_utils import setup_logging\n",
    "\n",
    "# Get the absolute path of the THESIS directory\n",
    "THESIS_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Add THESIS to sys.path\n",
    "if THESIS_DIR not in sys.path:\n",
    "    sys.path.append(THESIS_DIR)\n",
    "from helper_functions.model_wrapper import ModelWrapper\n",
    "from helper_functions.energy_tracking import start_energy_tracking, stop_energy_tracking\n",
    "from helper_functions.metrics_results import get_compute_performance_metrics, detect_cpu_vendor\n",
    "from helper_functions.experiment_utils import (\n",
    "    load_model_tokenizer_backend,\n",
    "    prep_distributed_env,\n",
    "    extract_experiment_setup,\n",
    "    extract_experimental_variables,\n",
    "    extract_experiment_results,\n",
    "    save_results,\n",
    "    aggregate_experiments,\n",
    "    get_persistent_unique_id\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuration dataclass\n",
    "# -----------------------------------------------------------------------------\n",
    "    \n",
    "from enum import Enum\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "class TaskType(str, Enum):\n",
    "    TEXT_GENERATION = \"text_generation\"\n",
    "    TRANSLATION = \"translation\"\n",
    "    SUMMARIZATION = \"summarization\"\n",
    "\n",
    "class InferenceType(str, Enum):\n",
    "    PURELY_GENERATIVE = \"purely_generative\"\n",
    "    REASONING = \"reasoning\"\n",
    "    \n",
    "class IsEncoderDecoder(str, Enum):\n",
    "    NA = \"na\"\n",
    "    ENCODER_DECODER = \"encoder_decoder\"\n",
    "    DECODER_ONLY = \"decoder_only\"\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    model_name: str\n",
    "    is_encoder_decoder: IsEncoderDecoder = IsEncoderDecoder.NA  \n",
    "    task_type: TaskType = TaskType.TEXT_GENERATION\n",
    "    inference_type: str = \"purely_generative\"\n",
    "    max_input_tokens: int = 512\n",
    "    max_output_tokens: int = 128\n",
    "    gpu_list: list = field(default_factory=lambda: [0])\n",
    "    num_processes: int = 1 \n",
    "    batching_options: dict = field(default_factory=dict)\n",
    "    sharding_config: dict = field(default_factory=dict)\n",
    "    query_rate: float = 1.0\n",
    "    decoder_temperature: float = 1.0\n",
    "    fp_precision: str = \"float16\"\n",
    "    quantisation: bool = False\n",
    "    backend: str = \"pytorch\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Inference function that measures performance metrics.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def run_gen_inference_with_metrics(model, tokenizer, accelerator, prompts, \n",
    "                                   max_input_tokens, max_output_tokens,\n",
    "                                   effective_batch_size,\n",
    "                                   decoder_temperature=1.0,\n",
    "                                   inference_type=\"purely_generative\",\n",
    "                                   query_rate=1.0):\n",
    "    \"\"\"\n",
    "    Runs inference and returns performance metrics.\n",
    "    \"\"\"\n",
    "    # Truncate prompts\n",
    "    truncated_prompts = [\n",
    "        tokenizer.decode(\n",
    "            tokenizer(p, truncation=True, max_length=max_input_tokens, return_tensors=\"pt\").input_ids[0],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        for p in prompts\n",
    "    ]\n",
    "    \n",
    "    sorted_prompts = sorted(truncated_prompts, key=lambda x: len(tokenizer.tokenize(x)))\n",
    "    latencies = []\n",
    "    total_tokens = 0\n",
    "    total_input_tokens = 0\n",
    "    device = accelerator.device\n",
    "    num_batches = (len(sorted_prompts) + effective_batch_size - 1) // effective_batch_size\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch = sorted_prompts[i * effective_batch_size: (i + 1) * effective_batch_size]\n",
    "        encoded = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_input_tokens)\n",
    "        input_ids = encoded.input_ids.to(device)\n",
    "        total_input_tokens += input_ids.numel()\n",
    "\n",
    "        # Determine sampling settings based on inference_type\n",
    "        do_sample = True if inference_type == \"reasoning\" else False\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        if hasattr(model, \"module\"):\n",
    "            outputs = model.module.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=max_output_tokens,\n",
    "                do_sample=do_sample,\n",
    "                temperature=decoder_temperature\n",
    "            )\n",
    "        else:\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=max_output_tokens,\n",
    "                do_sample=do_sample,\n",
    "                temperature=decoder_temperature\n",
    "            )\n",
    "        end_time = time.perf_counter()\n",
    "        latencies.append((end_time - start_time) * 1000.0)\n",
    "\n",
    "        for j in range(len(batch)):\n",
    "            prompt_len = input_ids[j].shape[0]\n",
    "            gen_len = outputs[j].shape[0] - prompt_len\n",
    "            total_tokens += gen_len\n",
    "\n",
    "        # Enforce query rate by sleeping (simulate inter-arrival delay)\n",
    "        if query_rate > 0:\n",
    "            time.sleep(1.0 / query_rate)\n",
    "\n",
    "    avg_latency_ms = sum(latencies) / len(latencies) if latencies else 0.0\n",
    "    total_time_sec = sum(latencies) / 1000.0\n",
    "    throughput_qps = len(sorted_prompts) / total_time_sec if total_time_sec > 0 else 0.0\n",
    "    tokens_per_sec = total_tokens / total_time_sec if total_time_sec > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"avg_latency_ms\": avg_latency_ms,\n",
    "        \"throughput_qps\": throughput_qps,\n",
    "        \"tokens_per_sec\": tokens_per_sec,\n",
    "        \"total_generated_tokens\": total_tokens,\n",
    "        \"num_runs\": len(sorted_prompts),\n",
    "        \"total_time\": total_time_sec,\n",
    "        \"total_input_tokens\": total_input_tokens  \n",
    "    }\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Experiment runner with aggregation integration.\n",
    "# -----------------------------------------------------------------------------\n",
    "class ExperimentRunner:\n",
    "    def __init__(self, experiment_config: ExperimentConfig, prompts, inference_fn, use_optimum=False, **inference_kwargs):\n",
    "        self.config = experiment_config\n",
    "        self.prompts = prompts\n",
    "        self.inference_fn = inference_fn\n",
    "        self.use_optimum = use_optimum\n",
    "        self.inference_kwargs = inference_kwargs\n",
    "\n",
    "    def run(self):\n",
    "        model_name = self.config.model_name\n",
    "        # Use the enum's value if applicable\n",
    "        task_type = self.config.task_type.value if isinstance(self.config.task_type, Enum) else self.config.task_type\n",
    "        \n",
    "        is_encoder_decoder = self.config.is_encoder_decoder\n",
    "\n",
    "        if self.use_optimum:\n",
    "            # --- Optimum benchmark branch ---\n",
    "            setup_logging(level=\"INFO\")\n",
    "            launcher_config = TorchrunConfig(nproc_per_node=1)\n",
    "            scenario_config = InferenceConfig(latency=True, memory=True, input_shapes={\"sequence_length\": 128})\n",
    "            backend_config = PyTorchConfig(model=model_name, device=\"cuda\", device_ids=\"0\", no_weights=True)\n",
    "            benchmark_config = BenchmarkConfig(\n",
    "                name=f\"{self.backend}_{model_name}\",\n",
    "                scenario=scenario_config,\n",
    "                launcher=launcher_config,\n",
    "                backend=backend_config,\n",
    "            )\n",
    "            benchmark_report = Benchmark.launch(benchmark_config)\n",
    "            benchmark_results = benchmark_report.to_dict()\n",
    "            print(json.dumps({\n",
    "                \"model\": model_name,\n",
    "                \"optimum_benchmark_results\": benchmark_results\n",
    "            }, indent=4))\n",
    "            return benchmark_results\n",
    "        else:\n",
    "            # --- Standard experiment branch ---\n",
    "\n",
    "            # Load model & tokenizer with appropriate precision\n",
    "            model, tokenizer = load_model_tokenizer_backend(\n",
    "                model_name, \n",
    "                backend=self.config.backend, \n",
    "                fp_precision=self.config.fp_precision\n",
    "            )\n",
    "\n",
    "            # Save the original generate method\n",
    "            if hasattr(model, \"generate\") and callable(model.generate):\n",
    "                orig_generate = model.generate\n",
    "            elif hasattr(model, \"module\") and hasattr(model.module, \"generate\"):\n",
    "                orig_generate = model.module.generate\n",
    "            else:\n",
    "                orig_generate = None\n",
    "                print(\"Warning: Could not locate the original generate method.\")\n",
    "\n",
    "            # Quantisation logic\n",
    "            if self.config.quantisation:\n",
    "                print(\"Quantisation enabled: applying dynamic quantisation.\")\n",
    "                try:\n",
    "                    if not torch.cuda.is_available():\n",
    "                        model = quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "                        accelerator.print(\"Dynamic quantisation applied on CPU.\")\n",
    "                    else:\n",
    "                        accelerator.print(\"Warning: Dynamic quantisation is typically supported on CPU; skipping quantisation on GPU.\")\n",
    "                except ImportError:\n",
    "                    accelerator.print(\"Error: torch.quantization not available; skipping quantisation.\")\n",
    "\n",
    "            \n",
    "            # Initialize distributed environment (sets up the process group)\n",
    "            model, tokenizer, accelerator = prep_distributed_env(model, tokenizer, gpu_list=self.config.gpu_list)\n",
    "\n",
    "            # Sharding configuration logic (apply FSDP after process group is initialized)\n",
    "            if self.config.sharding_config:\n",
    "                #print(f\"Applying sharding configuration: {self.config.sharding_config}\")\n",
    "                try:\n",
    "                    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "                    fsdp_kwargs = self.config.sharding_config.get(\"fsdp_config\", {}).copy()\n",
    "                    if \"reshard_after_forward\" in fsdp_kwargs:\n",
    "                        print(\"Removing unsupported 'reshard_after_forward' from FSDP kwargs\")\n",
    "                        fsdp_kwargs.pop(\"reshard_after_forward\")\n",
    "                    model = FSDP(model, **fsdp_kwargs)\n",
    "                    accelerator.print(\"Model wrapped with FSDP for sharding.\")\n",
    "                except ImportError:\n",
    "                    accelerator.print(\"FSDP not available; sharding configuration not applied.\")\n",
    "\n",
    "            # Reassign the generate method to both model and model.module (if available)\n",
    "            if orig_generate is not None:\n",
    "                if hasattr(model, \"module\"):\n",
    "                    model.module.generate = orig_generate\n",
    "                model.generate = orig_generate\n",
    "            else:\n",
    "                accelerator.print(\"Warning: generate method was not found and cannot be reassigned.\")\n",
    "\n",
    "            # Force lazy allocation by running a dummy forward pass.\n",
    "            dummy_input = tokenizer(\"Hello world\", return_tensors=\"pt\", truncation=True, max_length=self.config.max_input_tokens).input_ids.to(accelerator.device)\n",
    "            with torch.no_grad():\n",
    "                _ = model(dummy_input)\n",
    "\n",
    "            # Continue with energy tracking and inference\n",
    "            tracker = start_energy_tracking()\n",
    "            \n",
    "            # Batching options \n",
    "            accelerator.print(f\"Applying batching options: {self.config.batching_options}\")\n",
    "            effective_batch_size = self.config.batching_options.get(\"max_batch_size\")\n",
    "            \n",
    "            # Run inference using parameters from the config\n",
    "            inference_metrics = self.inference_fn(\n",
    "                model, tokenizer, accelerator, self.prompts,\n",
    "                self.config.max_input_tokens,\n",
    "                self.config.max_output_tokens,\n",
    "                effective_batch_size,  \n",
    "                decoder_temperature=self.config.decoder_temperature,\n",
    "                inference_type=self.config.inference_type,\n",
    "                query_rate=self.config.query_rate,\n",
    "                **self.inference_kwargs\n",
    "            )\n",
    "            codecarbon_data = stop_energy_tracking(tracker)\n",
    "            experiment_results = extract_experiment_results(\n",
    "                inference_metrics, codecarbon_data,\n",
    "                model=model, tokenizer=tokenizer, device=accelerator.device\n",
    "            )\n",
    "            \n",
    "            # Extract common experimental setup and variables\n",
    "            experiment_setup = extract_experiment_setup(model_name, \n",
    "                                                        codecarbon_data,  \n",
    "                                                        task_type, \n",
    "                                                        is_encoder_decoder)\n",
    "            experiment_variables = extract_experimental_variables(model, \n",
    "                                                                  accelerator, \n",
    "                                                                  self.config, \n",
    "                                                                  inference_metrics)\n",
    "            \n",
    "            local_result = {\n",
    "                \"experiment_setup\": experiment_setup,\n",
    "                \"experiment_variables\": experiment_variables,\n",
    "                \"experiment_results\": experiment_results\n",
    "            }\n",
    "            \n",
    "            # Synchronize all processes before collective operations\n",
    "            if dist.is_available() and dist.is_initialized():\n",
    "                world_size = dist.get_world_size()\n",
    "                all_results = [None] * world_size\n",
    "                print(f\"[Process {os.getpid()}] Finished inference. Waiting at barrier before gathering results.\")\n",
    "                accelerator.wait_for_everyone()  \n",
    "                print(f\"[Process {os.getpid()}] Passed barrier. Proceeding to gather results.\")\n",
    "                dist.all_gather_object(all_results, local_result)\n",
    "            else:\n",
    "                all_results = [local_result]\n",
    "            \n",
    "            # Only aggregate on the main process (local rank 0)\n",
    "            if accelerator.local_process_index == 0:\n",
    "                aggregated_result = aggregate_experiments(all_results)\n",
    "                unique_id = get_persistent_unique_id()\n",
    "                experiment_title = f\"EXPERIMENT #{unique_id}\"\n",
    "                benchmark_results = {experiment_title: aggregated_result}\n",
    "                output_json_path = save_results(task_type, benchmark_results)\n",
    "                print(f\"[Process {os.getpid()}] Aggregated benchmark results saved to {output_json_path}\")\n",
    "                return benchmark_results\n",
    "            else:\n",
    "                return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vars:\n",
    "- `model_name` -> str: (path to model)\n",
    "- `is_encoder_decoder` -> str: \"decoder_only\", \"encoder_only\"\n",
    "- `task_type` -> \"text_generation\", \"translation\" \"summarization\"\n",
    "- `gpu_list` -> list of ints: [int, int...]\n",
    "- `decoder_temperature` -> int\n",
    "- `query_rate` -> int\n",
    "- `fp_precision` -> inf: (float32, float16, float8) <- NB float8 and bfloat16 only for TensorRT\n",
    "- `inference_type` -> \"purely_generative\" or NEED TO BUILD THIS OUT\n",
    "- `quantisation` -> boolean \n",
    "- `batching_options` -> dict...\n",
    "- `sharding_config` -> dict {\"fsdp_config\": {\"reshard_after_forward\": True}} / \"cpu_offload\": True / \"backward_prefetch\": \"full\" / \"use_orig_params\": True\n",
    "\n",
    "**TO CHECK**\n",
    "- decoder_temperature\n",
    "- interence_type\n",
    "- quantisation\n",
    "- sharding congfigs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n",
      "[Process 2030951] Model is on device: cuda:1\n",
      "\n",
      "[Process 2030950] Model is on device: cuda:0\n",
      "\n",
      "Model wrapped with FSDP for sharding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 23:47:04] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon WARNING @ 23:47:04] Multiple instances of codecarbon are allowed to run at the same time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying batching options: {'adaptive_batching': True, 'max_batch_size': 8}\n",
      "[Process 2030950] Finished inference. Waiting at barrier before gathering results.[Process 2030951] Finished inference. Waiting at barrier before gathering results.\n",
      "\n",
      "[Process 2030950] Passed barrier. Proceeding to gather results.[Process 2030951] Passed barrier. Proceeding to gather results.\n",
      "\n",
      "[Process 2030950] Aggregated benchmark results saved to benchmark_results/text_generation_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-11 23:47:13,102] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 2030951 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Main execution\n",
    "# -----------------------------------------------------------------------------\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ExperimentConfig\n",
    "experiment_config = ExperimentConfig(\n",
    "    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    is_encoder_decoder=\"decoder_only\", \n",
    "    task_type=\"text_generation\", \n",
    "    inference_type=\"purely_generative\",\n",
    "    max_input_tokens=512,\n",
    "    max_output_tokens=50,\n",
    "    gpu_list=[0,1], \n",
    "    num_processes=2,\n",
    "    batching_options={\n",
    "        \"adaptive_batching\": True, \n",
    "        \"max_batch_size\": 8,},\n",
    "    sharding_config = {\"fsdp_config\": { # COME BACK TO\n",
    "        #\"reshard_after_forward\": True, \n",
    "        #\"cpu_offload\": False,\n",
    "        #\"backward_prefetch\": \"full\",\n",
    "        #\"use_orig_params\": True\n",
    "        }\n",
    "    }, \n",
    "    query_rate=1,\n",
    "    decoder_temperature=1,#THIS NEEDS TO BE CHANGED\n",
    "    fp_precision=\"float32\",\n",
    "    quantisation=False, # THIS WORKS, BUT I DON'T THINK CHANGES ANYTHING\n",
    "    backend=\"pytorch\"\n",
    ")\n",
    "\n",
    "# Prompts\n",
    "ds = load_dataset(\"lighteval/pile_helm\", \"arxiv\")[\"test\"]\n",
    "ds = ds.select(range(5))\n",
    "prompts = [sample[\"text\"] for sample in ds]\n",
    "\n",
    "# launcher\n",
    "notebook_launcher(\n",
    "    lambda: ExperimentRunner(\n",
    "        experiment_config=experiment_config,\n",
    "        prompts=prompts,\n",
    "        inference_fn=run_gen_inference_with_metrics,\n",
    "        use_optimum=False\n",
    "    ).run(),\n",
    "    num_processes=experiment_config.num_processes\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
