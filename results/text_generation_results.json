[
    {
        "EXPERIMENT_#0019": {
            "setup": {
                "experiment_id": "0019",
                "date_time": "April 03, 2025 at 08:08:07 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 33.22023367858492,
                        "average_latency_ms_per_batch": 4745.747668369274,
                        "throughput_queries_per_sec": 3.010213623646602,
                        "throughput_tokens_per_sec": 301.0213623646602
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409393664,
                        "gpu_max_memory_allocated_bytes": 4409393664,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.3,
                        "cpu_memory_usage_bytes": 2208305152
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0019",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 754.9508206342532
                        },
                        "ram_power": {
                            "0": 0.7695221900939941
                        },
                        "cpu_energy": {
                            "0": 0.0010866305877862033
                        },
                        "gpu_energy": {
                            "0": 0.00771659367326194
                        },
                        "ram_energy": {
                            "0": 5.482312431040178e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.008808706573479182
                        },
                        "total_energy_joules": {
                            "0": 31711.343664525055
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 754.9508206342532,
                        "ram_power_avg": 0.7695221900939941,
                        "cpu_energy_total": 0.0010866305877862033,
                        "gpu_energy_total": 0.00771659367326194,
                        "ram_energy_total": 5.482312431040178e-06,
                        "total_energy_kwh": 0.008808706573479182,
                        "total_energy_joules": 31711.343664525055
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.3153445689905228,
                        "joules_per_token": 3.1711343664525056,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0033556767691668944
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0020": {
            "setup": {
                "experiment_id": "0020",
                "date_time": "April 03, 2025 at 08:23:13 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 30.306059557013214,
                        "average_latency_ms_per_batch": 4329.437079573317,
                        "throughput_queries_per_sec": 3.2996701472151204,
                        "throughput_tokens_per_sec": 329.96701472151204
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409393664,
                        "gpu_max_memory_allocated_bytes": 4409393664,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            14.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.4,
                        "cpu_memory_usage_bytes": 2266726400
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0020",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 742.9497733943766
                        },
                        "ram_power": {
                            "0": 0.788947105407715
                        },
                        "cpu_energy": {
                            "0": 0.000996950426400872
                        },
                        "gpu_energy": {
                            "0": 0.007073535658836505
                        },
                        "ram_energy": {
                            "0": 5.040127714948188e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.008075526212952324
                        },
                        "total_energy_joules": {
                            "0": 29071.894366628367
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 742.9497733943766,
                        "ram_power_avg": 0.788947105407715,
                        "cpu_energy_total": 0.000996950426400872,
                        "gpu_energy_total": 0.007073535658836505,
                        "ram_energy_total": 5.040127714948188e-06,
                        "total_energy_kwh": 0.008075526212952324,
                        "total_energy_joules": 29071.894366628367
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.34397483266446516,
                        "joules_per_token": 2.907189436662837,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0030763717108241878
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0021": {
            "setup": {
                "experiment_id": "0021",
                "date_time": "April 03, 2025 at 08:32:49 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.5732996922452,
                        "average_latency_ms_per_batch": 3224.7570988921716,
                        "throughput_queries_per_sec": 4.430012508731892,
                        "throughput_tokens_per_sec": 443.0012508731892
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409393664,
                        "gpu_max_memory_allocated_bytes": 4409393664,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.7,
                        "cpu_memory_usage_bytes": 2206027776
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0021",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 838.004250644
                        },
                        "ram_power": {
                            "0": 0.7699356079101562
                        },
                        "cpu_energy": {
                            "0": 0.0007571642590337433
                        },
                        "gpu_energy": {
                            "0": 0.00478550299507674
                        },
                        "ram_energy": {
                            "0": 3.7540275016218303e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.005546421281612108
                        },
                        "total_energy_joules": {
                            "0": 19967.116613803588
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 838.004250644,
                        "ram_power_avg": 0.7699356079101562,
                        "cpu_energy_total": 0.0007571642590337433,
                        "gpu_energy_total": 0.00478550299507674,
                        "ram_energy_total": 3.7540275016218303e-06,
                        "total_energy_kwh": 0.005546421281612108,
                        "total_energy_joules": 19967.116613803588
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5008234385272654,
                        "joules_per_token": 1.9967116613803586,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0021129091872301325
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0022": {
            "setup": {
                "experiment_id": "0022",
                "date_time": "April 03, 2025 at 08:35:39 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 30.95664660865441,
                        "average_latency_ms_per_batch": 4422.37808695063,
                        "throughput_queries_per_sec": 3.230324048472468,
                        "throughput_tokens_per_sec": 323.03240484724677
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409393664,
                        "gpu_max_memory_allocated_bytes": 4409393664,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            100.0,
                            99.0
                        ],
                        "cpu_usage_percent": 95.0,
                        "cpu_memory_usage_bytes": 2262532096
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0022",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 819.4060258158268
                        },
                        "ram_power": {
                            "0": 0.7876524925231934
                        },
                        "cpu_energy": {
                            "0": 0.0010183685418742243
                        },
                        "gpu_energy": {
                            "0": 0.007286968329569987
                        },
                        "ram_energy": {
                            "0": 5.213416958310375e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.008310550288402522
                        },
                        "total_energy_joules": {
                            "0": 29917.98103824908
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 819.4060258158268,
                        "ram_power_avg": 0.7876524925231934,
                        "cpu_energy_total": 0.0010183685418742243,
                        "gpu_energy_total": 0.007286968329569987,
                        "ram_energy_total": 5.213416958310375e-06,
                        "total_energy_kwh": 0.008310550288402522,
                        "total_energy_joules": 29917.98103824908
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.3342471534832298,
                        "joules_per_token": 2.9917981038249084,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.003165904132366941
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0023": {
            "setup": {
                "experiment_id": "0023",
                "date_time": "April 03, 2025 at 08:41:36 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.374033191939816,
                        "average_latency_ms_per_batch": 3196.2904559914023,
                        "throughput_queries_per_sec": 4.469466865545937,
                        "throughput_tokens_per_sec": 446.94668655459367
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409393664,
                        "gpu_max_memory_allocated_bytes": 4409393664,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            17.0
                        ],
                        "cpu_usage_percent": 95.9,
                        "cpu_memory_usage_bytes": 2202849280
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0023",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 648.9147215205548
                        },
                        "ram_power": {
                            "0": 0.7667040824890137
                        },
                        "cpu_energy": {
                            "0": 0.0007611556037227274
                        },
                        "gpu_energy": {
                            "0": 0.00470414654109419
                        },
                        "ram_energy": {
                            "0": 3.7680905090223995e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.005469070235325939
                        },
                        "total_energy_joules": {
                            "0": 19688.65284717338
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 648.9147215205548,
                        "ram_power_avg": 0.7667040824890137,
                        "cpu_energy_total": 0.0007611556037227274,
                        "gpu_energy_total": 0.00470414654109419,
                        "ram_energy_total": 3.7680905090223995e-06,
                        "total_energy_kwh": 0.005469070235325939,
                        "total_energy_joules": 19688.65284717338
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.507906766279119,
                        "joules_per_token": 1.9688652847173378,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0020834423061474165
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0024": {
            "setup": {
                "experiment_id": "0024",
                "date_time": "April 03, 2025 at 08:47:05 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.49365894892253,
                        "average_latency_ms_per_batch": 3213.3798498460756,
                        "throughput_queries_per_sec": 4.445697350843408,
                        "throughput_tokens_per_sec": 444.5697350843408
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 9199616,
                        "gpu_max_memory_allocated_bytes": 9199616,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            61.0,
                            100.0
                        ],
                        "cpu_usage_percent": 95.2,
                        "cpu_memory_usage_bytes": 7455129600
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0024",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 835.7321986746034
                        },
                        "ram_power": {
                            "0": 0.7891416549682617
                        },
                        "cpu_energy": {
                            "0": 0.0007597561856746325
                        },
                        "gpu_energy": {
                            "0": 0.0047218726663800226
                        },
                        "ram_energy": {
                            "0": 3.7959085137120254e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.005485424760568367
                        },
                        "total_energy_joules": {
                            "0": 19747.52913804612
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 835.7321986746034,
                        "ram_power_avg": 0.7891416549682617,
                        "cpu_energy_total": 0.0007597561856746325,
                        "gpu_energy_total": 0.0047218726663800226,
                        "ram_energy_total": 3.7959085137120254e-06,
                        "total_energy_kwh": 0.005485424760568367,
                        "total_energy_joules": 19747.52913804612
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5063924671332036,
                        "joules_per_token": 1.9747529138046118,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0020896725625385196
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0025": {
            "setup": {
                "experiment_id": "0025",
                "date_time": "April 03, 2025 at 08:48:26 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.660376410000026,
                        "average_latency_ms_per_batch": 3237.1966300000036,
                        "throughput_queries_per_sec": 4.4129893604004735,
                        "throughput_tokens_per_sec": 441.29893604004735
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 9199616,
                        "gpu_max_memory_allocated_bytes": 9199616,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            2.0
                        ],
                        "cpu_usage_percent": 96.3,
                        "cpu_memory_usage_bytes": 7451144192
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0025",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 795.6272764239335
                        },
                        "ram_power": {
                            "0": 0.7850275039672852
                        },
                        "cpu_energy": {
                            "0": 0.0007687920279859099
                        },
                        "gpu_energy": {
                            "0": 0.004634844818976802
                        },
                        "ram_energy": {
                            "0": 3.931436269745653e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.005407568283232458
                        },
                        "total_energy_joules": {
                            "0": 19467.24581963685
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 795.6272764239335,
                        "ram_power_avg": 0.7850275039672852,
                        "cpu_energy_total": 0.0007687920279859099,
                        "gpu_energy_total": 0.004634844818976802,
                        "ram_energy_total": 3.931436269745653e-06,
                        "total_energy_kwh": 0.005407568283232458,
                        "total_energy_joules": 19467.24581963685
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5136833475392228,
                        "joules_per_token": 1.9467245819636851,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.002060013137497405
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0026": {
            "setup": {
                "experiment_id": "0026",
                "date_time": "April 03, 2025 at 08:50:59 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 31.54664697824046,
                        "average_latency_ms_per_batch": 4506.663854034351,
                        "throughput_queries_per_sec": 3.169908994416293,
                        "throughput_tokens_per_sec": 316.99089944162927
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409393664,
                        "gpu_max_memory_allocated_bytes": 4409393664,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            100.0,
                            3.0
                        ],
                        "cpu_usage_percent": 95.9,
                        "cpu_memory_usage_bytes": 2218422272
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0026",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 793.4832316276667
                        },
                        "ram_power": {
                            "0": 0.7731227874755859
                        },
                        "cpu_energy": {
                            "0": 0.0010321342349707266
                        },
                        "gpu_energy": {
                            "0": 0.007221459388290441
                        },
                        "ram_energy": {
                            "0": 5.2478332389461185e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.008258841456500113
                        },
                        "total_energy_joules": {
                            "0": 29731.829243400407
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 793.4832316276667,
                        "ram_power_avg": 0.7731227874755859,
                        "cpu_energy_total": 0.0010321342349707266,
                        "gpu_energy_total": 0.007221459388290441,
                        "ram_energy_total": 5.2478332389461185e-06,
                        "total_energy_kwh": 0.008258841456500113,
                        "total_energy_joules": 29731.829243400407
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.33633988403924747,
                        "joules_per_token": 2.973182924340041,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.003146205652853718
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0027": {
            "setup": {
                "experiment_id": "0027",
                "date_time": "April 03, 2025 at 08:53:59 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.12760651251301,
                        "average_latency_ms_per_batch": 3161.0866446447158,
                        "throughput_queries_per_sec": 4.519241606336893,
                        "throughput_tokens_per_sec": 451.9241606336893
                    }
                },
                "compute_metrics": {
                    "flops": 10344202560000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 9199616,
                        "gpu_max_memory_allocated_bytes": 9199616,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 95.3,
                        "cpu_memory_usage_bytes": 7397634048
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0027",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 433.20578766578467
                        },
                        "ram_power": {
                            "0": 0.7750825881958009
                        },
                        "cpu_energy": {
                            "0": 0.0007515579873434037
                        },
                        "gpu_energy": {
                            "0": 0.004667655400787396
                        },
                        "ram_energy": {
                            "0": 3.754588297744807e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0054229679764285455
                        },
                        "total_energy_joules": {
                            "0": 19522.684715142765
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 433.20578766578467,
                        "ram_power_avg": 0.7750825881958009,
                        "cpu_energy_total": 0.0007515579873434037,
                        "gpu_energy_total": 0.004667655400787396,
                        "ram_energy_total": 3.754588297744807e-06,
                        "total_energy_kwh": 0.0054229679764285455,
                        "total_energy_joules": 19522.684715142765
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5122246323141972,
                        "joules_per_token": 1.9522684715142762,
                        "flops_per_joule": 529855535.2879577,
                        "joules_per_flop": 1.8873068853693025e-09
                    },
                    "per-process_emissions": [
                        0.0020658796506204543
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0028": {
            "setup": {
                "experiment_id": "0028",
                "date_time": "April 03, 2025 at 08:57:02 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.50075631123036,
                        "average_latency_ms_per_batch": 3214.3937587471946,
                        "throughput_queries_per_sec": 4.444295054655072,
                        "throughput_tokens_per_sec": 444.42950546550725
                    }
                },
                "compute_metrics": {
                    "flops": 10344202560000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 9199616,
                        "gpu_max_memory_allocated_bytes": 9199616,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 95.5,
                        "cpu_memory_usage_bytes": 7395385344
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0028",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 643.8653125965
                        },
                        "ram_power": {
                            "0": 0.772463321685791
                        },
                        "cpu_energy": {
                            "0": 0.0007543125672455064
                        },
                        "gpu_energy": {
                            "0": 0.004754490470251227
                        },
                        "ram_energy": {
                            "0": 3.688023434981735e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.005512491060931716
                        },
                        "total_energy_joules": {
                            "0": 19844.967819354177
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 643.8653125965,
                        "ram_power_avg": 0.772463321685791,
                        "cpu_energy_total": 0.0007543125672455064,
                        "gpu_energy_total": 0.004754490470251227,
                        "ram_energy_total": 3.688023434981735e-06,
                        "total_energy_kwh": 0.005512491060931716,
                        "total_energy_joules": 19844.967819354177
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.503906082943975,
                        "joules_per_token": 1.9844967819354176,
                        "flops_per_joule": 521250659.3188638,
                        "joules_per_flop": 1.918462801191915e-09
                    },
                    "per-process_emissions": [
                        0.002099983469661937
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0029": {
            "setup": {
                "experiment_id": "0029",
                "date_time": "April 03, 2025 at 09:00:10 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.519201958552003,
                        "average_latency_ms_per_batch": 3217.028851221715,
                        "throughput_queries_per_sec": 4.440654699223189,
                        "throughput_tokens_per_sec": 444.0654699223189
                    }
                },
                "compute_metrics": {
                    "flops": 10344202560000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 9199616,
                        "gpu_max_memory_allocated_bytes": 9199616,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 95.7,
                        "cpu_memory_usage_bytes": 7381811200
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0029",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 588.1975225151324
                        },
                        "ram_power": {
                            "0": 0.7673735618591309
                        },
                        "cpu_energy": {
                            "0": 0.0007576293721576804
                        },
                        "gpu_energy": {
                            "0": 0.004656388447337179
                        },
                        "ram_energy": {
                            "0": 3.6984088834827704e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.005417716228378342
                        },
                        "total_energy_joules": {
                            "0": 19503.77842216203
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 588.1975225151324,
                        "ram_power_avg": 0.7673735618591309,
                        "cpu_energy_total": 0.0007576293721576804,
                        "gpu_energy_total": 0.004656388447337179,
                        "ram_energy_total": 3.6984088834827704e-06,
                        "total_energy_kwh": 0.005417716228378342,
                        "total_energy_joules": 19503.77842216203
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5127211652813415,
                        "joules_per_token": 1.950377842216203,
                        "flops_per_joule": 530369159.04694355,
                        "joules_per_flop": 1.885479166618527e-09
                    },
                    "per-process_emissions": [
                        0.0020638789972007294
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0030": {
            "setup": {
                "experiment_id": "0030",
                "date_time": "April 03, 2025 at 09:03:40 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.453669826034456,
                        "average_latency_ms_per_batch": 3207.6671180049225,
                        "throughput_queries_per_sec": 4.453614966941954,
                        "throughput_tokens_per_sec": 445.3614966941954
                    }
                },
                "compute_metrics": {
                    "flops": 10344202560000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 9199616,
                        "gpu_max_memory_allocated_bytes": 9199616,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 95.5,
                        "cpu_memory_usage_bytes": 7419228160
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0030",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 681.153267501753
                        },
                        "ram_power": {
                            "0": 0.7805213928222656
                        },
                        "cpu_energy": {
                            "0": 0.000752838937791239
                        },
                        "gpu_energy": {
                            "0": 0.004799700784200667
                        },
                        "ram_energy": {
                            "0": 3.7606901641998995e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.005556300412156106
                        },
                        "total_energy_joules": {
                            "0": 20002.681483761982
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 681.153267501753,
                        "ram_power_avg": 0.7805213928222656,
                        "cpu_energy_total": 0.000752838937791239,
                        "gpu_energy_total": 0.004799700784200667,
                        "ram_energy_total": 3.7606901641998995e-06,
                        "total_energy_kwh": 0.005556300412156106,
                        "total_energy_joules": 20002.681483761982
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.49993297189268954,
                        "joules_per_token": 2.000268148376198,
                        "flops_per_joule": 517140792.7680767,
                        "joules_per_flop": 1.9337093766038918e-09
                    },
                    "per-process_emissions": [
                        0.002116672642010869
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0031": {
            "setup": {
                "experiment_id": "0031",
                "date_time": "April 03, 2025 at 09:10:28 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.49158423789777,
                        "average_latency_ms_per_batch": 3213.083462556824,
                        "throughput_queries_per_sec": 4.446107439221754,
                        "throughput_tokens_per_sec": 444.61074392217535
                    }
                },
                "compute_metrics": {
                    "flops": 10344202560000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 9199616,
                        "gpu_max_memory_allocated_bytes": 9199616,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            26.0
                        ],
                        "cpu_usage_percent": 97.3,
                        "cpu_memory_usage_bytes": 7420178432
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0031",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 650.6514060638202
                        },
                        "ram_power": {
                            "0": 0.7813425064086914
                        },
                        "cpu_energy": {
                            "0": 0.0007574155193360638
                        },
                        "gpu_energy": {
                            "0": 0.004662649007904918
                        },
                        "ram_energy": {
                            "0": 3.787804212631335e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.005423852331453612
                        },
                        "total_energy_joules": {
                            "0": 19525.868393233002
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 650.6514060638202,
                        "ram_power_avg": 0.7813425064086914,
                        "cpu_energy_total": 0.0007574155193360638,
                        "gpu_energy_total": 0.004662649007904918,
                        "ram_energy_total": 3.787804212631335e-06,
                        "total_energy_kwh": 0.005423852331453612,
                        "total_energy_joules": 19525.868393233002
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5121411144748705,
                        "joules_per_token": 1.9525868393233003,
                        "flops_per_joule": 529769142.7432209,
                        "joules_per_flop": 1.887614659513493e-09
                    },
                    "per-process_emissions": [
                        0.0020662165456672536
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0032": {
            "setup": {
                "experiment_id": "0032",
                "date_time": "April 03, 2025 at 09:13:19 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.531476537697017,
                        "average_latency_ms_per_batch": 3218.7823625281453,
                        "throughput_queries_per_sec": 4.438235542739143,
                        "throughput_tokens_per_sec": 443.82355427391434
                    }
                },
                "compute_metrics": {
                    "flops": 10344202560000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 9199616,
                        "gpu_max_memory_allocated_bytes": 9199616,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.0,
                        "cpu_memory_usage_bytes": 7400255488
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0032",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 960.9960047727373
                        },
                        "ram_power": {
                            "0": 0.7744045257568359
                        },
                        "cpu_energy": {
                            "0": 0.0007129626769456081
                        },
                        "gpu_energy": {
                            "0": 0.004553622531780377
                        },
                        "ram_energy": {
                            "0": 3.827331988838e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.005270412540714823
                        },
                        "total_energy_joules": {
                            "0": 18973.485146573363
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 960.9960047727373,
                        "ram_power_avg": 0.7744045257568359,
                        "cpu_energy_total": 0.0007129626769456081,
                        "gpu_energy_total": 0.004553622531780377,
                        "ram_energy_total": 3.827331988838e-06,
                        "total_energy_kwh": 0.005270412540714823,
                        "total_energy_joules": 18973.485146573363
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5270512993658423,
                        "joules_per_token": 1.8973485146573363,
                        "flops_per_joule": 545192540.0151472,
                        "joules_per_flop": 1.834214385934585e-09
                    },
                    "per-process_emissions": [
                        0.002007763657385312
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0033": {
            "setup": {
                "experiment_id": "0033",
                "date_time": "April 03, 2025 at 09:24:21 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.696229923749343,
                        "average_latency_ms_per_batch": 3242.3185605356202,
                        "throughput_queries_per_sec": 4.406018106794026,
                        "throughput_tokens_per_sec": 440.6018106794026
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409396224,
                        "gpu_max_memory_allocated_bytes": 4409396224,
                        "gpu_current_memory_reserved_bytes": 13025411072,
                        "gpu_max_memory_reserved_bytes": 13025411072
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            9.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.8,
                        "cpu_memory_usage_bytes": 6798737408
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0033",
                    "local_process_results": {
                        "cpu_power": {
                            "3": 112.5,
                            "1": 112.5,
                            "2": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "3": 733.849911243523,
                            "1": 1017.0354872775902,
                            "2": 634.9535801137084,
                            "0": 730.952372002856
                        },
                        "ram_power": {
                            "3": 0.8038301467895509,
                            "1": 0.8158979415893556,
                            "2": 0.7943558692932129,
                            "0": 0.8347592353820801
                        },
                        "cpu_energy": {
                            "3": 0.0009871637219475817,
                            "1": 0.001030149540565617,
                            "2": 0.001025280360765464,
                            "0": 0.0007654343078174861
                        },
                        "gpu_energy": {
                            "3": 0.006735353166060776,
                            "1": 0.007011325053486672,
                            "2": 0.006979210861135954,
                            "0": 0.005365880126031897
                        },
                        "ram_energy": {
                            "3": 5.176444650844823e-06,
                            "1": 5.430732631556963e-06,
                            "2": 5.496426559143765e-06,
                            "0": 4.105395440153131e-06
                        },
                        "total_energy_kwh": {
                            "3": 0.007727693332659203,
                            "1": 0.008046905326683843,
                            "2": 0.00800998764846056,
                            "0": 0.006135419829289538
                        },
                        "total_energy_joules": {
                            "3": 27819.69599757313,
                            "1": 28968.859176061836,
                            "2": 28835.955534458015,
                            "0": 22087.511385442336
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 779.1978376594194,
                        "ram_power_avg": 0.8122107982635498,
                        "cpu_energy_total": 0.003808027931096149,
                        "gpu_energy_total": 0.0260917692067153,
                        "ram_energy_total": 2.0208999281698682e-05,
                        "total_energy_kwh": 0.02992000613709314,
                        "total_energy_joules": 107712.02209353531
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0928401473265089,
                        "joules_per_token": 10.77120220935353,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0029438647750765234,
                        0.0030654685842002104,
                        0.0030514047946810503,
                        0.0023372881839678495
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0047": {
            "setup": {
                "experiment_id": "0047",
                "date_time": "April 03, 2025 at 09:31:26 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.897753016091883,
                        "average_latency_ms_per_batch": 3271.107573727412,
                        "throughput_queries_per_sec": 4.367240747584397,
                        "throughput_tokens_per_sec": 436.7240747584397
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409396224,
                        "gpu_max_memory_allocated_bytes": 4409396224,
                        "gpu_current_memory_reserved_bytes": 13025411072,
                        "gpu_max_memory_reserved_bytes": 13025411072
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            10.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 95.3,
                        "cpu_memory_usage_bytes": 6856404992
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0047",
                    "local_process_results": {
                        "cpu_power": {
                            "3": 112.5,
                            "2": 112.5,
                            "1": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "3": 663.7268424734925,
                            "2": 582.6278086182401,
                            "1": 575.7525053133577,
                            "0": 894.711239804615
                        },
                        "ram_power": {
                            "3": 0.802635669708252,
                            "2": 0.8082475662231445,
                            "1": 0.8043708801269532,
                            "0": 0.8544416427612305
                        },
                        "cpu_energy": {
                            "3": 0.0009779172985290643,
                            "2": 0.0010349498863506598,
                            "1": 0.0010386432633531517,
                            "0": 0.0007668645588710207
                        },
                        "gpu_energy": {
                            "3": 0.006593301663517792,
                            "2": 0.006889011622313035,
                            "1": 0.00691746831176232,
                            "0": 0.005263834211064022
                        },
                        "ram_energy": {
                            "3": 5.080701241189806e-06,
                            "2": 5.419546194668884e-06,
                            "1": 5.44120953174558e-06,
                            "0": 4.192304353481993e-06
                        },
                        "total_energy_kwh": {
                            "3": 0.007576299663288048,
                            "2": 0.007929381054858364,
                            "1": 0.007961552784647215,
                            "0": 0.006034891074288524
                        },
                        "total_energy_joules": {
                            "3": 27274.678787836972,
                            "2": 28545.77179749011,
                            "1": 28661.590024729976,
                            "0": 21725.607867438684
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 679.2045990524264,
                        "ram_power_avg": 0.817423939704895,
                        "cpu_energy_total": 0.003818375007103896,
                        "gpu_energy_total": 0.02566361580865717,
                        "ram_energy_total": 2.013376132108626e-05,
                        "total_energy_kwh": 0.02950212457708215,
                        "total_energy_joules": 106207.64847749574
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0941551775540807,
                        "joules_per_token": 10.620764847749573,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.002886191356729582,
                        0.0030206977128482936,
                        0.003032953533311357,
                        0.002298991754750213
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0048": {
            "setup": {
                "experiment_id": "0048",
                "date_time": "April 03, 2025 at 09:50:53 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 31.386755059007555,
                        "average_latency_ms_per_batch": 4483.822151286794,
                        "throughput_queries_per_sec": 3.186057297481009,
                        "throughput_tokens_per_sec": 318.6057297481009
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818107392,
                        "gpu_max_memory_allocated_bytes": 8818107392,
                        "gpu_current_memory_reserved_bytes": 13203668992,
                        "gpu_max_memory_reserved_bytes": 13203668992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 95.3,
                        "cpu_memory_usage_bytes": 2453000192
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0048",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "2": 112.5,
                            "1": 112.5,
                            "3": 112.5
                        },
                        "gpu_power": {
                            "0": 769.8722566230048,
                            "2": 767.1295164839295,
                            "1": 673.7929562590441,
                            "3": 866.7543150340773
                        },
                        "ram_power": {
                            "0": 0.8556976318359375,
                            "2": 0.80419921875,
                            "1": 0.8086481094360352,
                            "3": 0.7857656478881837
                        },
                        "cpu_energy": {
                            "0": 0.0010303370955443824,
                            "2": 0.001033187981985975,
                            "1": 0.0010265161922070544,
                            "3": 0.0009789030374304274
                        },
                        "gpu_energy": {
                            "0": 0.007945408300752632,
                            "2": 0.007960607201809466,
                            "1": 0.007905767435715916,
                            "3": 0.007611781089412517
                        },
                        "ram_energy": {
                            "0": 5.755953097686171e-06,
                            "2": 5.273938168361883e-06,
                            "1": 5.322432590137301e-06,
                            "3": 5.046767478262117e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.008981501349394698,
                            "2": 0.008999069121963802,
                            "1": 0.008937606060513106,
                            "3": 0.008595730894321207
                        },
                        "total_energy_joules": {
                            "0": 32333.404857820915,
                            "2": 32396.648839069687,
                            "1": 32175.38181784718,
                            "3": 30944.631219556344
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 769.3872611000138,
                        "ram_power_avg": 0.8135776519775391,
                        "cpu_energy_total": 0.0040689443071678395,
                        "gpu_energy_total": 0.03142356402769053,
                        "ram_energy_total": 2.139909133444747e-05,
                        "total_energy_kwh": 0.035513907426192814,
                        "total_energy_joules": 127850.06673429412
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.07821661932162004,
                        "joules_per_token": 12.785006673429411,
                        "flops_per_joule": 80918544.23119335,
                        "joules_per_flop": 1.2358106655291374e-08
                    },
                    "per-process_emissions": [
                        0.0034215029390519103,
                        0.0034281953820121105,
                        0.003404781028752468,
                        0.003274543684191664
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0055": {
            "setup": {
                "experiment_id": "0055",
                "date_time": "April 03, 2025 at 09:56:41 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 31.55438410793431,
                        "average_latency_ms_per_batch": 4507.76915827633,
                        "throughput_queries_per_sec": 3.1691317332622293,
                        "throughput_tokens_per_sec": 316.91317332622293
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818107392,
                        "gpu_max_memory_allocated_bytes": 8818107392,
                        "gpu_current_memory_reserved_bytes": 13203668992,
                        "gpu_max_memory_reserved_bytes": 13203668992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            74.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.5,
                        "cpu_memory_usage_bytes": 2420473856
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0055",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "2": 112.5,
                            "1": 112.5,
                            "3": 112.5
                        },
                        "gpu_power": {
                            "0": 915.5521854462003,
                            "2": 825.4536911261779,
                            "1": 1105.802055894411,
                            "3": 789.933738408374
                        },
                        "ram_power": {
                            "0": 0.8433322906494141,
                            "2": 0.7996444702148438,
                            "1": 0.7976832389831543,
                            "3": 0.7949023246765137
                        },
                        "cpu_energy": {
                            "0": 0.0010291048747094465,
                            "2": 0.0010105071648504238,
                            "1": 0.0010237074824108276,
                            "3": 0.0009633386480782063
                        },
                        "gpu_energy": {
                            "0": 0.00798897250228947,
                            "2": 0.007860413788321807,
                            "1": 0.007961913869522164,
                            "3": 0.007505806837954765
                        },
                        "ram_energy": {
                            "0": 5.748678767626046e-06,
                            "2": 5.324754948516485e-06,
                            "1": 5.239245163430196e-06,
                            "3": 4.989714881533782e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.009023826055766543,
                            "2": 0.00887624570812075,
                            "1": 0.00899086059709642,
                            "3": 0.008474135200914506
                        },
                        "total_energy_joules": {
                            "0": 32485.773800759554,
                            "2": 31954.484549234698,
                            "1": 32367.098149547113,
                            "3": 30506.886723292224
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 909.1854177187909,
                        "ram_power_avg": 0.8088905811309814,
                        "cpu_energy_total": 0.0040266581700489044,
                        "gpu_energy_total": 0.03131710699808821,
                        "ram_energy_total": 2.1302393761106505e-05,
                        "total_energy_kwh": 0.035365067561898215,
                        "total_energy_joules": 127314.2432228336
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0785458071843333,
                        "joules_per_token": 12.73142432228336,
                        "flops_per_joule": 81259103.60157223,
                        "joules_per_flop": 1.2306313455082855e-08
                    },
                    "per-process_emissions": [
                        0.0034376265359442647,
                        0.0033814058025085996,
                        0.0034250683444638818,
                        0.003228221804788381
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0056": {
            "setup": {
                "experiment_id": "0056",
                "date_time": "April 03, 2025 at 09:58:52 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 25.744839271064848,
                        "average_latency_ms_per_batch": 3677.8341815806925,
                        "throughput_queries_per_sec": 3.8842736187672395,
                        "throughput_tokens_per_sec": 388.427361876724
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818107392,
                        "gpu_max_memory_allocated_bytes": 8818107392,
                        "gpu_current_memory_reserved_bytes": 13203668992,
                        "gpu_max_memory_reserved_bytes": 13203668992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 95.6,
                        "cpu_memory_usage_bytes": 2462572544
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0056",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "3": 112.5,
                            "1": 112.5,
                            "2": 112.5
                        },
                        "gpu_power": {
                            "0": 909.230661455496,
                            "3": 787.5886593981776,
                            "1": 535.1296862032214,
                            "2": 900.4032142165673
                        },
                        "ram_power": {
                            "0": 0.8581094741821289,
                            "3": 0.799504280090332,
                            "1": 0.7938108444213867,
                            "2": 0.7928338050842285
                        },
                        "cpu_energy": {
                            "0": 0.0008450702345362514,
                            "3": 0.000950902728785877,
                            "1": 0.001044269545629504,
                            "2": 0.0010248704671612357
                        },
                        "gpu_energy": {
                            "0": 0.0062004285714500895,
                            "3": 0.00689780412934482,
                            "1": 0.007384993685768393,
                            "2": 0.007298747783440973
                        },
                        "ram_energy": {
                            "0": 4.645769695434352e-06,
                            "3": 4.973638803021014e-06,
                            "1": 5.4267617360579535e-06,
                            "2": 5.476315974347987e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.007050144575681775,
                            "3": 0.007853680496933715,
                            "1": 0.008434689993133954,
                            "2": 0.008329094566576553
                        },
                        "total_energy_joules": {
                            "0": 25380.52047245439,
                            "3": 28273.249788961373,
                            "1": 30364.883975282235,
                            "2": 29984.740439675592
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 783.0880553183656,
                        "ram_power_avg": 0.811064600944519,
                        "cpu_energy_total": 0.003865112976112868,
                        "gpu_energy_total": 0.027781974170004275,
                        "ram_energy_total": 2.0522486208861307e-05,
                        "total_energy_kwh": 0.031667609632325996,
                        "total_energy_joules": 114003.39467637359
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.08771668623015513,
                        "joules_per_token": 11.400339467637359,
                        "flops_per_joule": 90746782.66702545,
                        "joules_per_flop": 1.1019674423822508e-08
                    },
                    "per-process_emissions": [
                        0.0026857525761059724,
                        0.002991859585306899,
                        0.00321319515288438,
                        0.0031729685751373383
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0061": {
            "setup": {
                "experiment_id": "0061",
                "date_time": "April 03, 2025 at 10:30:38 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.69411848601885,
                        "average_latency_ms_per_batch": 3242.0169265741215,
                        "throughput_queries_per_sec": 4.406428038242901,
                        "throughput_tokens_per_sec": 440.64280382429007
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818107392,
                        "gpu_max_memory_allocated_bytes": 8818107392,
                        "gpu_current_memory_reserved_bytes": 13203668992,
                        "gpu_max_memory_reserved_bytes": 13203668992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.2,
                        "cpu_memory_usage_bytes": 2442129408
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0061",
                    "local_process_results": {
                        "cpu_power": {
                            "2": 112.5,
                            "1": 112.5,
                            "3": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "2": 111611.0368995235,
                            "1": 573.1271653941521,
                            "3": 801.9226924346626,
                            "0": 822.2012829683438
                        },
                        "ram_power": {
                            "2": 0.8071832656860352,
                            "1": 0.8090286254882812,
                            "3": 0.8051047325134278,
                            "0": 0.8506264686584473
                        },
                        "cpu_energy": {
                            "2": 0.0010259787663235327,
                            "1": 0.001053812863006897,
                            "3": 0.0009698393909784501,
                            "0": 0.0007689619896336806
                        },
                        "gpu_energy": {
                            "2": 0.006888567733057016,
                            "1": 0.007008395051151695,
                            "3": 0.006557741357291036,
                            "0": 0.00528772534127242
                        },
                        "ram_energy": {
                            "2": 5.50749487438035e-06,
                            "1": 5.556392794783204e-06,
                            "3": 5.025522091312414e-06,
                            "0": 4.262099741284547e-06
                        },
                        "total_energy_kwh": {
                            "2": 0.007920053994254932,
                            "1": 0.008067764306953372,
                            "3": 0.007532606270360801,
                            "0": 0.0060609494306473844
                        },
                        "total_energy_joules": {
                            "2": 28512.194379317752,
                            "1": 29043.95150503214,
                            "3": 27117.382573298884,
                            "0": 21819.417950330582
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 28452.072010080163,
                        "ram_power_avg": 0.8179857730865479,
                        "cpu_energy_total": 0.003818593009942561,
                        "gpu_energy_total": 0.025742429482772167,
                        "ram_energy_total": 2.0351509501760517e-05,
                        "total_energy_kwh": 0.029581374002216488,
                        "total_energy_joules": 106492.94640797935
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09390293289181371,
                        "joules_per_token": 10.649294640797935,
                        "flops_per_joule": 97146727.82520394,
                        "joules_per_flop": 1.0293707491613094e-08
                    },
                    "per-process_emissions": [
                        0.0030171445691114162,
                        0.003073414812733887,
                        0.0028695463586939475,
                        0.0023089186856051214
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0062": {
            "setup": {
                "experiment_id": "0062",
                "date_time": "April 03, 2025 at 10:33:25 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.71175936073996,
                        "average_latency_ms_per_batch": 3244.53705153428,
                        "throughput_queries_per_sec": 4.4030054392378855,
                        "throughput_tokens_per_sec": 440.3005439237885
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818107392,
                        "gpu_max_memory_allocated_bytes": 8818107392,
                        "gpu_current_memory_reserved_bytes": 13203668992,
                        "gpu_max_memory_reserved_bytes": 13203668992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.0,
                        "cpu_memory_usage_bytes": 2450501632
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0062",
                    "local_process_results": {
                        "cpu_power": {
                            "3": 112.5,
                            "1": 112.5,
                            "0": 112.5,
                            "2": 112.5
                        },
                        "gpu_power": {
                            "3": 646.4334414810249,
                            "1": 612.3272463210357,
                            "0": 832.0596579276126,
                            "2": 722.6326449492843
                        },
                        "ram_power": {
                            "3": 0.795194149017334,
                            "1": 0.8021206855773926,
                            "0": 0.8539724349975586,
                            "2": 0.7992925643920898
                        },
                        "cpu_energy": {
                            "3": 0.0009595435097071457,
                            "1": 0.0010238793370808707,
                            "0": 0.0007671694042073794,
                            "2": 0.0010246676693641346
                        },
                        "gpu_energy": {
                            "3": 0.006497330753418851,
                            "1": 0.006839899083026069,
                            "0": 0.005248435309862742,
                            "2": 0.006834050745013087
                        },
                        "ram_energy": {
                            "3": 4.929515458527407e-06,
                            "1": 5.611685621767234e-06,
                            "0": 4.140226723135471e-06,
                            "2": 5.419633496261236e-06
                        },
                        "total_energy_kwh": {
                            "3": 0.007461803778584523,
                            "1": 0.007869390105728711,
                            "0": 0.006019744940793257,
                            "2": 0.007864138047873483
                        },
                        "total_energy_joules": {
                            "3": 26862.49360290428,
                            "1": 28329.804380623358,
                            "0": 21671.081786855724,
                            "2": 28310.896972344537
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 703.3632476697394,
                        "ram_power_avg": 0.8126449584960938,
                        "cpu_energy_total": 0.0037752599203595303,
                        "gpu_energy_total": 0.02541971589132075,
                        "ram_energy_total": 2.010106129969135e-05,
                        "total_energy_kwh": 0.029215076872979973,
                        "total_energy_joules": 105174.27674272789
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09508028302834452,
                        "joules_per_token": 10.517427674272788,
                        "flops_per_joule": 98364748.49555188,
                        "joules_per_flop": 1.01662436522696e-08
                    },
                    "per-process_emissions": [
                        0.002842574149451774,
                        0.0029978441607773523,
                        0.0022932218351951915,
                        0.0029958433893374032
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0063": {
            "setup": {
                "experiment_id": "0063",
                "date_time": "April 03, 2025 at 10:35:28 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.672720431815833,
                        "average_latency_ms_per_batch": 3238.960061687976,
                        "throughput_queries_per_sec": 4.4105867357528705,
                        "throughput_tokens_per_sec": 441.058673575287
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818107392,
                        "gpu_max_memory_allocated_bytes": 8818107392,
                        "gpu_current_memory_reserved_bytes": 13203668992,
                        "gpu_max_memory_reserved_bytes": 13203668992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.2,
                        "cpu_memory_usage_bytes": 2424483840
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0063",
                    "local_process_results": {
                        "cpu_power": {
                            "3": 112.5,
                            "2": 112.5,
                            "1": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "3": 787.4446737220122,
                            "2": 700.7027035185157,
                            "1": 1235.8903111311602,
                            "0": 782.2945721528956
                        },
                        "ram_power": {
                            "3": 0.7929582595825195,
                            "2": 0.7974100112915039,
                            "1": 0.7933659553527832,
                            "0": 0.8455867767333984
                        },
                        "cpu_energy": {
                            "3": 0.0009977600233760312,
                            "2": 0.001028026744672388,
                            "1": 0.0010274985535870652,
                            "0": 0.0007674425194636568
                        },
                        "gpu_energy": {
                            "3": 0.0067999826622084925,
                            "2": 0.0069682033523363884,
                            "1": 0.006986976422901137,
                            "0": 0.005362632901221076
                        },
                        "ram_energy": {
                            "3": 5.143312282823977e-06,
                            "2": 5.476871426165291e-06,
                            "1": 5.530439029970066e-06,
                            "0": 4.119595831588308e-06
                        },
                        "total_energy_kwh": {
                            "3": 0.007802885997867348,
                            "2": 0.008001706968434939,
                            "1": 0.008020005415518172,
                            "0": 0.0061341950165163224
                        },
                        "total_energy_joules": {
                            "3": 28090.38959232245,
                            "2": 28806.14508636578,
                            "1": 28872.01949586542,
                            "0": 22083.10205945876
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 876.583065131146,
                        "ram_power_avg": 0.8073302507400513,
                        "cpu_energy_total": 0.0038207278410991407,
                        "gpu_energy_total": 0.026117795338667094,
                        "ram_energy_total": 2.027021857054764e-05,
                        "total_energy_kwh": 0.02995879339833678,
                        "total_energy_joules": 107851.65623401241
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09271994839191326,
                        "joules_per_token": 10.78516562340124,
                        "flops_per_joule": 95922878.15731691,
                        "joules_per_flop": 1.0425041650230352e-08
                    },
                    "per-process_emissions": [
                        0.002972509420887566,
                        0.00304825026962529,
                        0.0030552210630416477,
                        0.002336821591541893
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0082": {
            "setup": {
                "experiment_id": "0082",
                "date_time": "April 04, 2025 at 12:09:32 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.458189751254395,
                        "average_latency_ms_per_batch": 3208.3128216077707,
                        "throughput_queries_per_sec": 4.4527186343865734,
                        "throughput_tokens_per_sec": 445.2718634386573
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818107392,
                        "gpu_max_memory_allocated_bytes": 8818107392,
                        "gpu_current_memory_reserved_bytes": 13203668992,
                        "gpu_max_memory_reserved_bytes": 13203668992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 94.6,
                        "cpu_memory_usage_bytes": 2464874496
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0082",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "3": 112.5,
                            "2": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "1": 377.6523783503454,
                            "3": 428.2210337404455,
                            "2": 467.2493966495603,
                            "0": 433.49265072715104
                        },
                        "ram_power": {
                            "1": 0.8015227317810059,
                            "3": 0.8083491325378418,
                            "2": 0.8006572723388673,
                            "0": 0.8590650558471681
                        },
                        "cpu_energy": {
                            "1": 0.0007753150198404913,
                            "3": 0.0007664098356544856,
                            "2": 0.000758389915645239,
                            "0": 0.0007675254173664145
                        },
                        "gpu_energy": {
                            "1": 0.0032868395739136247,
                            "3": 0.0032695348378553035,
                            "2": 0.0032324811970845246,
                            "0": 0.0032689062262427626
                        },
                        "ram_energy": {
                            "1": 3.9895841292903165e-06,
                            "3": 3.8117076269348523e-06,
                            "2": 3.7183765128114438e-06,
                            "0": 4.180769433968967e-06
                        },
                        "total_energy_kwh": {
                            "1": 0.004066144177883407,
                            "3": 0.004039756381136725,
                            "2": 0.003994589489242575,
                            "0": 0.0040406124130431455
                        },
                        "total_energy_joules": {
                            "1": 14638.119040380263,
                            "3": 14543.12297209221,
                            "2": 14380.52216127327,
                            "0": 14546.204686955323
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 426.65386486687555,
                        "ram_power_avg": 0.8173985481262207,
                        "cpu_energy_total": 0.0030676401885066304,
                        "gpu_energy_total": 0.013057761835096215,
                        "ram_energy_total": 1.570043770300558e-05,
                        "total_energy_kwh": 0.016141102461305852,
                        "total_energy_joules": 58107.96886070107
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.17209343565204338,
                        "joules_per_token": 5.810796886070107,
                        "flops_per_joule": 178038253.32116732,
                        "joules_per_flop": 5.616770448742141e-09
                    },
                    "per-process_emissions": [
                        0.0015489976245646837,
                        0.0015389451933940353,
                        0.001521738865926959,
                        0.0015392712987487864
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0083": {
            "setup": {
                "experiment_id": "0083",
                "date_time": "April 04, 2025 at 12:18:18 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precision_gaming",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 4.548809517873451,
                        "average_latency_ms_per_batch": 4548.809517873451,
                        "throughput_queries_per_sec": 21.983773909871164,
                        "throughput_tokens_per_sec": 2198.3773909871165
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825257984,
                        "gpu_max_memory_allocated_bytes": 825257984,
                        "gpu_current_memory_reserved_bytes": 1814036480,
                        "gpu_max_memory_reserved_bytes": 1814036480
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 79.6,
                        "cpu_memory_usage_bytes": 3119271936
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0083",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 246.47088877820119
                        },
                        "ram_power": {
                            "0": 1.0891027450561523
                        },
                        "cpu_energy": {
                            "0": 0.00020759451525373152
                        },
                        "gpu_energy": {
                            "0": 0.0003922728138263665
                        },
                        "ram_energy": {
                            "0": 1.5375956744347662e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0006014049247545328
                        },
                        "total_energy_joules": {
                            "0": 2165.057729116318
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 246.47088877820119,
                        "ram_power_avg": 1.0891027450561523,
                        "cpu_energy_total": 0.00020759451525373152,
                        "gpu_energy_total": 0.0003922728138263665,
                        "ram_energy_total": 1.5375956744347662e-06,
                        "total_energy_kwh": 0.0006014049247545328,
                        "total_energy_joules": 2165.057729116318
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 4.618814484952123,
                        "joules_per_token": 0.2165057729116318,
                        "flops_per_joule": 4778367403.728563,
                        "joules_per_flop": 2.0927649875137257e-10
                    },
                    "per-process_emissions": [
                        0.00022910520608523927
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0084": {
            "setup": {
                "experiment_id": "0084",
                "date_time": "April 04, 2025 at 12:30:51 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 20.21407313225791,
                        "average_latency_ms_per_batch": 2887.724733179701,
                        "throughput_queries_per_sec": 4.947048491697528,
                        "throughput_tokens_per_sec": 494.7048491697527
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818107392,
                        "gpu_max_memory_allocated_bytes": 8818107392,
                        "gpu_current_memory_reserved_bytes": 13203668992,
                        "gpu_max_memory_reserved_bytes": 13203668992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.7,
                        "cpu_memory_usage_bytes": 2442969088
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0084",
                    "local_process_results": {
                        "cpu_power": {
                            "2": 112.5,
                            "3": 112.5,
                            "1": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "2": 501.9126252767978,
                            "3": 414.2786354888776,
                            "1": 332.3833105825148,
                            "0": 374.6642734660781
                        },
                        "ram_power": {
                            "2": 0.7953286170959473,
                            "3": 0.7900629043579102,
                            "1": 0.8001136779785156,
                            "0": 0.8519024848937988
                        },
                        "cpu_energy": {
                            "2": 0.0006556360179674811,
                            "3": 0.0006576113454430014,
                            "1": 0.0006621865694978623,
                            "0": 0.0006566000551683827
                        },
                        "gpu_energy": {
                            "2": 0.003072056624322528,
                            "3": 0.0030770569061076003,
                            "1": 0.003088277748410917,
                            "0": 0.003075468293726402
                        },
                        "ram_energy": {
                            "2": 3.6048434257236622e-06,
                            "3": 3.4273081248720158e-06,
                            "1": 3.4524946397044287e-06,
                            "0": 3.898576206343322e-06
                        },
                        "total_energy_kwh": {
                            "2": 0.003731297485715732,
                            "3": 0.0037380955596754755,
                            "1": 0.0037539168125484837,
                            "0": 0.003735966925101129
                        },
                        "total_energy_joules": {
                            "2": 13432.670948576635,
                            "3": 13457.144014831712,
                            "1": 13514.100525174541,
                            "0": 13449.480930364063
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 405.8097112035671,
                        "ram_power_avg": 0.809351921081543,
                        "cpu_energy_total": 0.0026320339880767274,
                        "gpu_energy_total": 0.012312859572567447,
                        "ram_energy_total": 1.4383222396643427e-05,
                        "total_energy_kwh": 0.01495927678304082,
                        "total_energy_joules": 53853.396418946955
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.18568930958794186,
                        "joules_per_token": 5.385339641894695,
                        "flops_per_joule": 192103784.86657932,
                        "joules_per_flop": 5.205519509646954e-09
                    },
                    "per-process_emissions": [
                        0.0014214377771834081,
                        0.0014240275034583724,
                        0.0014300546097403449,
                        0.001423216600117275
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0085": {
            "setup": {
                "experiment_id": "0085",
                "date_time": "April 04, 2025 at 12:32:18 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "max_throughput_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 8.959577209083363,
                        "average_latency_ms_per_batch": 8959.577209083363,
                        "throughput_queries_per_sec": 11.161240945456488,
                        "throughput_tokens_per_sec": 1116.1240945456489
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1577436672,
                        "gpu_max_memory_allocated_bytes": 1577436672,
                        "gpu_current_memory_reserved_bytes": 2839543808,
                        "gpu_max_memory_reserved_bytes": 2839543808
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.7,
                        "cpu_memory_usage_bytes": 3400355840
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0085",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "0": 112.5,
                            "3": 112.5,
                            "2": 112.5
                        },
                        "gpu_power": {
                            "1": 334.3695937718301,
                            "0": 0.0,
                            "3": 0.0,
                            "2": 327.32423014733695
                        },
                        "ram_power": {
                            "1": 1.2413148880004883,
                            "0": 1.1864776611328125,
                            "3": 1.2345256805419922,
                            "2": 1.2331824302673342
                        },
                        "cpu_energy": {
                            "1": 0.0002988147779833526,
                            "0": 0.00031617841994011547,
                            "3": 0.00031581227139395197,
                            "2": 0.0003064600490688463
                        },
                        "gpu_energy": {
                            "1": 0.0007703767274236384,
                            "0": 0.0008093642586075589,
                            "3": 0.0008093642586075589,
                            "2": 0.0007895845205645458
                        },
                        "ram_energy": {
                            "1": 2.4719037470677725e-06,
                            "0": 2.8074301115902304e-06,
                            "3": 2.8920851334186037e-06,
                            "2": 2.618787325891178e-06
                        },
                        "total_energy_kwh": {
                            "1": 0.0010716634091540585,
                            "0": 0.0011283501086592647,
                            "3": 0.001128068615134929,
                            "2": 0.001098663356959283
                        },
                        "total_energy_joules": {
                            "1": 3857.9882729546107,
                            "0": 4062.060391173353,
                            "3": 4061.0470144857445,
                            "2": 3955.188085053419
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 165.42345597979175,
                        "ram_power_avg": 1.2238751649856567,
                        "cpu_energy_total": 0.0012372655183862664,
                        "gpu_energy_total": 0.003178689765203302,
                        "ram_energy_total": 1.0790206317967785e-05,
                        "total_energy_kwh": 0.004426745489907536,
                        "total_energy_joules": 15936.283763667128
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.6274988666303016,
                        "joules_per_token": 1.5936283763667127,
                        "flops_per_joule": 649175267.7990336,
                        "joules_per_flop": 1.5404160472570126e-09
                    },
                    "per-process_emissions": [
                        0.00040825017571723864,
                        0.0004298449738937469,
                        0.0004297377389356512,
                        0.0004185358058336389
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0086": {
            "setup": {
                "experiment_id": "0086",
                "date_time": "April 04, 2025 at 12:32:53 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precision_gaming",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 3.983170330990106,
                        "average_latency_ms_per_batch": 3983.170330990106,
                        "throughput_queries_per_sec": 25.105629860208055,
                        "throughput_tokens_per_sec": 2510.5629860208055
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087586816,
                        "gpu_max_memory_allocated_bytes": 1087586816,
                        "gpu_current_memory_reserved_bytes": 1994391552,
                        "gpu_max_memory_reserved_bytes": 1994391552
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.7,
                        "cpu_memory_usage_bytes": 3311267840
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0086",
                    "local_process_results": {
                        "cpu_power": {
                            "3": 112.5,
                            "1": 112.5,
                            "2": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "3": 770.4948606279871,
                            "1": 745.6848056478675,
                            "2": 758.5297282369668,
                            "0": 6.749724002243486
                        },
                        "ram_power": {
                            "3": 1.2113041877746582,
                            "1": 1.219254970550537,
                            "2": 1.2204408645629885,
                            "0": 1.155677318572998
                        },
                        "cpu_energy": {
                            "3": 0.0001563370826333994,
                            "1": 0.0001567665043476154,
                            "2": 0.00015556260977609782,
                            "0": 0.00019190354984311854
                        },
                        "gpu_energy": {
                            "3": 0.0007834892379037228,
                            "1": 0.0007739583969446073,
                            "2": 0.0007739583969446073,
                            "0": 0.0007989942503030534
                        },
                        "ram_energy": {
                            "3": 1.3074015568811945e-06,
                            "1": 1.3277474913662723e-06,
                            "2": 1.3243127646870347e-06,
                            "0": 1.492528755555048e-06
                        },
                        "total_energy_kwh": {
                            "3": 0.0009411337220940035,
                            "1": 0.0009320526487835891,
                            "2": 0.0009308453194853922,
                            "0": 0.000992390328901727
                        },
                        "total_energy_joules": {
                            "3": 3388.081399538413,
                            "1": 3355.3895356209205,
                            "2": 3351.043150147412,
                            "0": 3572.605184046217
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 570.3647796287663,
                        "ram_power_avg": 1.2016693353652954,
                        "cpu_energy_total": 0.0006605697466002311,
                        "gpu_energy_total": 0.003130400282095991,
                        "ram_energy_total": 5.45199056848955e-06,
                        "total_energy_kwh": 0.003796422019264712,
                        "total_energy_joules": 13667.119269352963
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.7316830857270645,
                        "joules_per_token": 1.3667119269352963,
                        "flops_per_joule": 756958439.8958553,
                        "joules_per_flop": 1.3210764915146242e-09
                    },
                    "per-process_emissions": [
                        0.00035852489143171066,
                        0.00035506545655410827,
                        0.00035460552445796016,
                        0.0003780510957951129
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0087": {
            "setup": {
                "experiment_id": "0087",
                "date_time": "April 04, 2025 at 12:33:25 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "gpu_overdrive",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 2.5677902039606124,
                        "average_latency_ms_per_batch": 2567.7902039606124,
                        "throughput_queries_per_sec": 38.94399154796912,
                        "throughput_tokens_per_sec": 3894.399154796912
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419751424,
                        "gpu_max_memory_allocated_bytes": 4419751424,
                        "gpu_current_memory_reserved_bytes": 6838812672,
                        "gpu_max_memory_reserved_bytes": 6838812672
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.5,
                        "cpu_memory_usage_bytes": 3788763136
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0087",
                    "local_process_results": {
                        "cpu_power": {
                            "3": 112.5,
                            "2": 112.5,
                            "1": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "3": 912.5302464046811,
                            "2": 855.4174273499839,
                            "1": 945.2184552471232,
                            "0": 894.4337669372675
                        },
                        "ram_power": {
                            "3": 1.1054606437683105,
                            "2": 1.115877628326416,
                            "1": 1.142526626586914,
                            "0": 1.3207883834838867
                        },
                        "cpu_energy": {
                            "3": 0.00011536547565629006,
                            "2": 0.00011409496994019719,
                            "1": 0.00011372061525617029,
                            "0": 0.00011483032030810137
                        },
                        "gpu_energy": {
                            "3": 0.0006121893786428245,
                            "2": 0.0006066935409236862,
                            "1": 0.0006066935409236862,
                            "0": 0.0006063871517909547
                        },
                        "ram_energy": {
                            "3": 9.383586940478697e-07,
                            "2": 9.329599090881159e-07,
                            "1": 9.66109071301293e-07,
                            "0": 1.1308933470558129e-06
                        },
                        "total_energy_kwh": {
                            "3": 0.0007284932129931624,
                            "2": 0.0007217214707729715,
                            "1": 0.0007213802652511577,
                            "0": 0.000722348365446112
                        },
                        "total_energy_joules": {
                            "3": 2622.5755667753847,
                            "2": 2598.1972947826976,
                            "1": 2596.9689549041677,
                            "0": 2600.454115606003
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 901.899973984764,
                        "ram_power_avg": 1.1711633205413818,
                        "cpu_energy_total": 0.0004580113811607589,
                        "gpu_energy_total": 0.0024319636122811517,
                        "ram_energy_total": 3.9683210214930914e-06,
                        "total_energy_kwh": 0.0028939433144634036,
                        "total_energy_joules": 10418.195932068253
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.9598590835884547,
                        "joules_per_token": 1.0418195932068253,
                        "flops_per_joule": 993016578.6338971,
                        "joules_per_flop": 1.007032532503848e-09
                    },
                    "per-process_emissions": [
                        0.00027751948948974523,
                        0.0002749397942909635,
                        0.00027480981204742853,
                        0.0002751786098166964
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0088": {
            "setup": {
                "experiment_id": "0088",
                "date_time": "April 04, 2025 at 12:34:29 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "standard_production",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 1,
                    "delay_max": 2,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 30.2599654418882,
                        "average_latency_ms_per_batch": 4322.852205984028,
                        "throughput_queries_per_sec": 3.3046964376757755,
                        "throughput_tokens_per_sec": 330.46964376757757
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818107392,
                        "gpu_max_memory_allocated_bytes": 8818107392,
                        "gpu_current_memory_reserved_bytes": 13203668992,
                        "gpu_max_memory_reserved_bytes": 13203668992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.5,
                        "cpu_memory_usage_bytes": 2389356544
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0088",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "3": 112.5,
                            "0": 112.5,
                            "2": 112.5
                        },
                        "gpu_power": {
                            "1": 439.4453810022858,
                            "3": 408.88562881138336,
                            "0": 433.8682112786464,
                            "2": 432.423691631926
                        },
                        "ram_power": {
                            "1": 0.7977547645568849,
                            "3": 0.793656349182129,
                            "0": 0.8325948715209961,
                            "2": 0.8056840896606445
                        },
                        "cpu_energy": {
                            "1": 0.0009752313736535143,
                            "3": 0.0009832784774407627,
                            "0": 0.0009657635769399348,
                            "2": 0.000976099365929258
                        },
                        "gpu_energy": {
                            "1": 0.00358981120517754,
                            "3": 0.00361505178092969,
                            "0": 0.0035522833973722356,
                            "2": 0.00358981120517754
                        },
                        "ram_energy": {
                            "1": 5.230964704312501e-06,
                            "3": 5.244216486417914e-06,
                            "0": 5.483275764860461e-06,
                            "2": 5.26112143022956e-06
                        },
                        "total_energy_kwh": {
                            "1": 0.004570273543535366,
                            "3": 0.004603574474856871,
                            "0": 0.004523530250077031,
                            "2": 0.004571171692537027
                        },
                        "total_energy_joules": {
                            "1": 16452.984756727317,
                            "3": 16572.868109484734,
                            "0": 16284.70890027731,
                            "2": 16456.218093133295
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 428.6557281810604,
                        "ram_power_avg": 0.8074225187301636,
                        "cpu_energy_total": 0.0039003727939634697,
                        "gpu_energy_total": 0.014346957588657006,
                        "ram_energy_total": 2.1219578385820437e-05,
                        "total_energy_kwh": 0.018268549961006294,
                        "total_energy_joules": 65766.77985962266
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.15205244990471964,
                        "joules_per_token": 6.576677985962267,
                        "flops_per_joule": 157304969.19694188,
                        "joules_per_flop": 6.357078260814667e-09
                    },
                    "per-process_emissions": [
                        0.0017410457064097977,
                        0.001753731696196725,
                        0.001723238848766845,
                        0.0017413878562719802
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0089": {
            "setup": {
                "experiment_id": "0089",
                "date_time": "April 04, 2025 at 12:35:59 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_real_time",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.05,
                    "simulate_burst": false
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 57.03250773763284,
                        "average_latency_ms_per_batch": 2281.3003095053136,
                        "throughput_queries_per_sec": 1.7533859892682766,
                        "throughput_tokens_per_sec": 175.33859892682767
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818113536,
                        "gpu_max_memory_allocated_bytes": 8818113536,
                        "gpu_current_memory_reserved_bytes": 13203668992,
                        "gpu_max_memory_reserved_bytes": 13203668992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            21.0,
                            100.0,
                            96.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 2248110080
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0089",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "3": 112.5,
                            "1": 112.5,
                            "2": 112.5
                        },
                        "gpu_power": {
                            "0": 473.7427584304278,
                            "3": 589.1150569726286,
                            "1": 623.5864870348848,
                            "2": 483.15360967339774
                        },
                        "ram_power": {
                            "0": 0.7838301658630371,
                            "3": 0.726778507232666,
                            "1": 0.7371640205383301,
                            "2": 0.7262063026428223
                        },
                        "cpu_energy": {
                            "0": 0.0018040922938598674,
                            "3": 0.0017961860360228457,
                            "1": 0.0017887185159925136,
                            "2": 0.0018291532348448532
                        },
                        "gpu_energy": {
                            "0": 0.008970245231747498,
                            "3": 0.00893488548124921,
                            "1": 0.00889320350345102,
                            "2": 0.009060537803982527
                        },
                        "ram_energy": {
                            "0": 9.300163417308149e-06,
                            "3": 8.442157930581053e-06,
                            "1": 8.671229054871408e-06,
                            "2": 8.857225581396707e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.010783637689024671,
                            "3": 0.010739513675202636,
                            "1": 0.0106905932484984,
                            "2": 0.010898548264408775
                        },
                        "total_energy_joules": {
                            "0": 38821.09568048882,
                            "3": 38662.24923072949,
                            "1": 38486.13569459424,
                            "2": 39234.773751871595
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 542.3994780278348,
                        "ram_power_avg": 0.7434947490692139,
                        "cpu_energy_total": 0.0072181500807200795,
                        "gpu_energy_total": 0.035858872020430255,
                        "ram_energy_total": 3.527077598415732e-05,
                        "total_energy_kwh": 0.04311229287713448,
                        "total_energy_joules": 155204.25435768414
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.06443122349567798,
                        "joules_per_token": 15.520425435768415,
                        "flops_per_joule": 66656943.92730929,
                        "joules_per_flop": 1.50021879354463e-08
                    },
                    "per-process_emissions": [
                        0.004108026777633948,
                        0.0040912177345684446,
                        0.004072581498015466,
                        0.004151801961326523
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0090": {
            "setup": {
                "experiment_id": "0090",
                "date_time": "April 04, 2025 at 12:37:09 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "balanced_performance_mode",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.02,
                    "delay_max": 0.1,
                    "simulate_burst": true,
                    "burst_interval": 1.5,
                    "burst_size": 3
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.75439521390945,
                        "average_latency_ms_per_batch": 9688.598803477362,
                        "throughput_queries_per_sec": 2.580352485132028,
                        "throughput_tokens_per_sec": 258.03524851320276
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576153600,
                        "gpu_max_memory_allocated_bytes": 1576153600,
                        "gpu_current_memory_reserved_bytes": 2854223872,
                        "gpu_max_memory_reserved_bytes": 2854223872
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            12.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 3250483200
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0090",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "1": 112.5,
                            "2": 112.5,
                            "3": 112.5
                        },
                        "gpu_power": {
                            "0": 471.1198654017332,
                            "1": 0.0,
                            "2": 470.0352282363168,
                            "3": 498.0436438610011
                        },
                        "ram_power": {
                            "0": 1.1346759796142578,
                            "1": 1.1795167922973633,
                            "2": 1.1788744926452637,
                            "3": 1.1772308349609375
                        },
                        "cpu_energy": {
                            "0": 0.0012271595606871415,
                            "1": 0.0012440838009570145,
                            "2": 0.0012237160909935488,
                            "3": 0.0011982447385598786
                        },
                        "gpu_energy": {
                            "0": 0.005386842642796097,
                            "1": 0.005445028800458829,
                            "2": 0.005376925134886079,
                            "3": 0.005275110331186994
                        },
                        "ram_energy": {
                            "0": 9.23905312983087e-06,
                            "1": 1.0077965998890283e-05,
                            "2": 9.520176510644797e-06,
                            "3": 9.309576722228915e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.006623241256613069,
                            "1": 0.006699190567414733,
                            "2": 0.006610161402390272,
                            "3": 0.006482664646469101
                        },
                        "total_energy_joules": {
                            "0": 23843.668523807046,
                            "1": 24117.086042693038,
                            "2": 23796.581048604978,
                            "3": 23337.592727288764
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 359.79968437476276,
                        "ram_power_avg": 1.1675745248794556,
                        "cpu_energy_total": 0.004893204191197584,
                        "gpu_energy_total": 0.021483906909328,
                        "ram_energy_total": 3.8146772361594864e-05,
                        "total_energy_kwh": 0.026415257872887177,
                        "total_energy_joules": 95094.92834239382
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10515807913535119,
                        "joules_per_token": 9.509492834239381,
                        "flops_per_joule": 108790673.28123689,
                        "joules_per_flop": 9.1919644381176e-09
                    },
                    "per-process_emissions": [
                        0.0025231237567067484,
                        0.0025520566466566428,
                        0.0025181409862405743,
                        0.002469571097072404
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0092": {
            "setup": {
                "experiment_id": "0092",
                "date_time": "April 04, 2025 at 12:56:26 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "max_throughput_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 10.927333006169647,
                        "average_latency_ms_per_batch": 10927.333006169647,
                        "throughput_queries_per_sec": 9.151363827160692,
                        "throughput_tokens_per_sec": 915.1363827160691
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1577349632,
                        "gpu_max_memory_allocated_bytes": 1577349632,
                        "gpu_current_memory_reserved_bytes": 2839543808,
                        "gpu_max_memory_reserved_bytes": 2839543808
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            16.0,
                            100.0,
                            100.0,
                            99.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 3399163904
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0092",
                    "local_process_results": {
                        "cpu_power": {
                            "2": 112.5,
                            "0": 112.5,
                            "1": 112.5,
                            "3": 112.5
                        },
                        "gpu_power": {
                            "2": 532.2616983374185,
                            "0": 497.65564713974027,
                            "1": 578.9239401520194,
                            "3": 682.1621860242109
                        },
                        "ram_power": {
                            "2": 1.2344040870666504,
                            "0": 1.1856708526611328,
                            "1": 1.23468017578125,
                            "3": 1.2285075187683105
                        },
                        "cpu_energy": {
                            "2": 0.0003689300735350117,
                            "0": 0.00037372917922766645,
                            "1": 0.0003563877423439409,
                            "3": 0.0003507554286188679
                        },
                        "gpu_energy": {
                            "2": 0.0017839700382715762,
                            "0": 0.0018009214407470608,
                            "1": 0.00173507638806214,
                            "3": 0.0017040860854784512
                        },
                        "ram_energy": {
                            "2": 3.353604580355292e-06,
                            "0": 3.2513670180368115e-06,
                            "1": 3.1909587065125108e-06,
                            "3": 3.0935232677732965e-06
                        },
                        "total_energy_kwh": {
                            "2": 0.0021562537163869427,
                            "0": 0.0021779019869927643,
                            "1": 0.0020946550891125936,
                            "3": 0.0020579350373650927
                        },
                        "total_energy_joules": {
                            "2": 7762.513378992994,
                            "0": 7840.447153173952,
                            "1": 7540.758320805337,
                            "3": 7408.5661345143335
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 572.7508679133473,
                        "ram_power_avg": 1.220815658569336,
                        "cpu_energy_total": 0.0014498024237254868,
                        "gpu_energy_total": 0.007024053952559228,
                        "ram_energy_total": 1.288945357267791e-05,
                        "total_energy_kwh": 0.008486745829857393,
                        "total_energy_joules": 30552.28498748662
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.32730776123932226,
                        "joules_per_token": 3.055228498748662,
                        "flops_per_joule": 338614322.4389669,
                        "joules_per_flop": 2.9532123532082546e-09
                    },
                    "per-process_emissions": [
                        0.0008214248532576058,
                        0.0008296717619448936,
                        0.0007979588561974426,
                        0.0007839703524842321
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0093": {
            "setup": {
                "experiment_id": "0093",
                "date_time": "April 04, 2025 at 12:57:02 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precision_gaming",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 4.320917370961979,
                        "average_latency_ms_per_batch": 4320.917370961979,
                        "throughput_queries_per_sec": 23.14323358091356,
                        "throughput_tokens_per_sec": 2314.3233580913557
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087586816,
                        "gpu_max_memory_allocated_bytes": 1087586816,
                        "gpu_current_memory_reserved_bytes": 1994391552,
                        "gpu_max_memory_reserved_bytes": 1994391552
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            14.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 3359608832
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0093",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "0": 112.5,
                            "2": 112.5,
                            "3": 112.5
                        },
                        "gpu_power": {
                            "1": 685.1449746137129,
                            "0": 788.168374010509,
                            "2": 763.0759762210591,
                            "3": 887.0536553692731
                        },
                        "ram_power": {
                            "1": 1.2211790084838867,
                            "0": 1.172490119934082,
                            "2": 1.2147002220153809,
                            "3": 1.2042388916015625
                        },
                        "cpu_energy": {
                            "1": 0.00017300573868851644,
                            "0": 0.00016767169902595926,
                            "2": 0.0001724897374660941,
                            "3": 0.0001685211619333131
                        },
                        "gpu_energy": {
                            "1": 0.001019299982122135,
                            "0": 0.0009828227307089321,
                            "2": 0.0010149466453022171,
                            "3": 0.000995525518650453
                        },
                        "ram_energy": {
                            "1": 1.362241572357795e-06,
                            "0": 1.3040478589156184e-06,
                            "2": 1.3535469261447757e-06,
                            "3": 1.3027650223444902e-06
                        },
                        "total_energy_kwh": {
                            "1": 0.001193667962383009,
                            "0": 0.001151798477593807,
                            "2": 0.0011887899296944562,
                            "3": 0.0011653494456061106
                        },
                        "total_energy_joules": {
                            "1": 4297.204664578832,
                            "0": 4146.474519337706,
                            "2": 4279.643746900042,
                            "3": 4195.258004181998
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 780.8607450536385,
                        "ram_power_avg": 1.203152060508728,
                        "cpu_energy_total": 0.000681688337113883,
                        "gpu_energy_total": 0.004012594876783737,
                        "ram_energy_total": 5.322601379762679e-06,
                        "total_energy_kwh": 0.004699605815277383,
                        "total_energy_joules": 16918.58093499858
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5910661206409767,
                        "joules_per_token": 1.691858093499858,
                        "flops_per_joule": 611483984.3688622,
                        "joules_per_flop": 1.6353658077114502e-09
                    },
                    "per-process_emissions": [
                        0.0004547278102698073,
                        0.0004387776300393608,
                        0.0004528695237171031,
                        0.00044393987130364785
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0119": {
            "setup": {
                "experiment_id": "0119",
                "date_time": "April 05, 2025 at 04:42:46 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 3,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.628509049071,
                        "average_latency_ms_per_batch": 3232.6441498672857,
                        "throughput_queries_per_sec": 4.419204101478592,
                        "throughput_tokens_per_sec": 441.92041014785923
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409393664,
                        "gpu_max_memory_allocated_bytes": 4409393664,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 98.2,
                        "cpu_memory_usage_bytes": 2227482624
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0119",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 185.2060006811357
                        },
                        "ram_power": {
                            "0": 0.7761111259460449
                        },
                        "cpu_energy": {
                            "0": 0.0007598891479137821
                        },
                        "gpu_energy": {
                            "0": 0.0012694860155804122
                        },
                        "ram_energy": {
                            "0": 3.857103002736589e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.002033232266496931
                        },
                        "total_energy_joules": {
                            "0": 7319.636159388952
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 185.2060006811357,
                        "ram_power_avg": 0.7761111259460449,
                        "cpu_energy_total": 0.0007598891479137821,
                        "gpu_energy_total": 0.0012694860155804122,
                        "ram_energy_total": 3.857103002736589e-06,
                        "total_energy_kwh": 0.002033232266496931,
                        "total_energy_joules": 7319.636159388952
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.3661881249620482,
                        "joules_per_token": 0.7319636159388951,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.000774559831922006
                    ]
                }
            }
        }
    }
]