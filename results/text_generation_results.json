[
    {
        "EXPERIMENT_#0001": {
            "setup": {
                "experiment_id": "0001",
                "date_time": "April 03, 2025 at 04:34:43 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 42.66783873876557,
                        "average_latency_ms_per_batch": 6095.405534109367,
                        "throughput_queries_per_sec": 2.3436856179252805,
                        "throughput_tokens_per_sec": 234.36856179252806
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409393664,
                        "gpu_max_memory_allocated_bytes": 4409393664,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            2.0,
                            2.0
                        ],
                        "cpu_usage_percent": 99.9,
                        "cpu_memory_usage_bytes": 2240237568
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0001",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 13.372414951615475
                        },
                        "ram_power": {
                            "0": 0.78004789352417
                        },
                        "cpu_energy": {
                            "0": 0.0015112892743854899
                        },
                        "gpu_energy": {
                            "0": 0.008310440537229624
                        },
                        "ram_energy": {
                            "0": 6.887092624221593e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.009828616904239336
                        },
                        "total_energy_joules": {
                            "0": 35383.02085526161
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 13.372414951615475,
                        "ram_power_avg": 0.78004789352417,
                        "cpu_energy_total": 0.0015112892743854899,
                        "gpu_energy_total": 0.008310440537229624,
                        "ram_energy_total": 6.887092624221593e-06,
                        "total_energy_kwh": 0.009828616904239336,
                        "total_energy_joules": 35383.02085526161
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2826214313612784,
                        "joules_per_token": 3.5383020855261607,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0037442116096699754
                    ]
                }
            }
        }
    }
]