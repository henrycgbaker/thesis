[
    {
        "EXPERIMENT_#0019": {
            "setup": {
                "experiment_id": "0019",
                "date_time": "April 03, 2025 at 08:08:07 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 33.22023367858492,
                        "average_latency_ms_per_batch": 4745.747668369274,
                        "throughput_queries_per_sec": 3.010213623646602,
                        "throughput_tokens_per_sec": 301.0213623646602
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409393664,
                        "gpu_max_memory_allocated_bytes": 4409393664,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.3,
                        "cpu_memory_usage_bytes": 2208305152
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0019",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 754.9508206342532
                        },
                        "ram_power": {
                            "0": 0.7695221900939941
                        },
                        "cpu_energy": {
                            "0": 0.0010866305877862033
                        },
                        "gpu_energy": {
                            "0": 0.00771659367326194
                        },
                        "ram_energy": {
                            "0": 5.482312431040178e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.008808706573479182
                        },
                        "total_energy_joules": {
                            "0": 31711.343664525055
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 754.9508206342532,
                        "ram_power_avg": 0.7695221900939941,
                        "cpu_energy_total": 0.0010866305877862033,
                        "gpu_energy_total": 0.00771659367326194,
                        "ram_energy_total": 5.482312431040178e-06,
                        "total_energy_kwh": 0.008808706573479182,
                        "total_energy_joules": 31711.343664525055
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.3153445689905228,
                        "joules_per_token": 3.1711343664525056,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0033556767691668944
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0020": {
            "setup": {
                "experiment_id": "0020",
                "date_time": "April 03, 2025 at 08:23:13 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 30.306059557013214,
                        "average_latency_ms_per_batch": 4329.437079573317,
                        "throughput_queries_per_sec": 3.2996701472151204,
                        "throughput_tokens_per_sec": 329.96701472151204
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409393664,
                        "gpu_max_memory_allocated_bytes": 4409393664,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            14.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.4,
                        "cpu_memory_usage_bytes": 2266726400
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0020",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 742.9497733943766
                        },
                        "ram_power": {
                            "0": 0.788947105407715
                        },
                        "cpu_energy": {
                            "0": 0.000996950426400872
                        },
                        "gpu_energy": {
                            "0": 0.007073535658836505
                        },
                        "ram_energy": {
                            "0": 5.040127714948188e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.008075526212952324
                        },
                        "total_energy_joules": {
                            "0": 29071.894366628367
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 742.9497733943766,
                        "ram_power_avg": 0.788947105407715,
                        "cpu_energy_total": 0.000996950426400872,
                        "gpu_energy_total": 0.007073535658836505,
                        "ram_energy_total": 5.040127714948188e-06,
                        "total_energy_kwh": 0.008075526212952324,
                        "total_energy_joules": 29071.894366628367
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.34397483266446516,
                        "joules_per_token": 2.907189436662837,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0030763717108241878
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0021": {
            "setup": {
                "experiment_id": "0021",
                "date_time": "April 03, 2025 at 08:32:49 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.5732996922452,
                        "average_latency_ms_per_batch": 3224.7570988921716,
                        "throughput_queries_per_sec": 4.430012508731892,
                        "throughput_tokens_per_sec": 443.0012508731892
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409393664,
                        "gpu_max_memory_allocated_bytes": 4409393664,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.7,
                        "cpu_memory_usage_bytes": 2206027776
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0021",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 838.004250644
                        },
                        "ram_power": {
                            "0": 0.7699356079101562
                        },
                        "cpu_energy": {
                            "0": 0.0007571642590337433
                        },
                        "gpu_energy": {
                            "0": 0.00478550299507674
                        },
                        "ram_energy": {
                            "0": 3.7540275016218303e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.005546421281612108
                        },
                        "total_energy_joules": {
                            "0": 19967.116613803588
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 838.004250644,
                        "ram_power_avg": 0.7699356079101562,
                        "cpu_energy_total": 0.0007571642590337433,
                        "gpu_energy_total": 0.00478550299507674,
                        "ram_energy_total": 3.7540275016218303e-06,
                        "total_energy_kwh": 0.005546421281612108,
                        "total_energy_joules": 19967.116613803588
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5008234385272654,
                        "joules_per_token": 1.9967116613803586,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0021129091872301325
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0022": {
            "setup": {
                "experiment_id": "0022",
                "date_time": "April 03, 2025 at 08:35:39 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 30.95664660865441,
                        "average_latency_ms_per_batch": 4422.37808695063,
                        "throughput_queries_per_sec": 3.230324048472468,
                        "throughput_tokens_per_sec": 323.03240484724677
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409393664,
                        "gpu_max_memory_allocated_bytes": 4409393664,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            100.0,
                            99.0
                        ],
                        "cpu_usage_percent": 95.0,
                        "cpu_memory_usage_bytes": 2262532096
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0022",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 819.4060258158268
                        },
                        "ram_power": {
                            "0": 0.7876524925231934
                        },
                        "cpu_energy": {
                            "0": 0.0010183685418742243
                        },
                        "gpu_energy": {
                            "0": 0.007286968329569987
                        },
                        "ram_energy": {
                            "0": 5.213416958310375e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.008310550288402522
                        },
                        "total_energy_joules": {
                            "0": 29917.98103824908
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 819.4060258158268,
                        "ram_power_avg": 0.7876524925231934,
                        "cpu_energy_total": 0.0010183685418742243,
                        "gpu_energy_total": 0.007286968329569987,
                        "ram_energy_total": 5.213416958310375e-06,
                        "total_energy_kwh": 0.008310550288402522,
                        "total_energy_joules": 29917.98103824908
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.3342471534832298,
                        "joules_per_token": 2.9917981038249084,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.003165904132366941
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0023": {
            "setup": {
                "experiment_id": "0023",
                "date_time": "April 03, 2025 at 08:41:36 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.374033191939816,
                        "average_latency_ms_per_batch": 3196.2904559914023,
                        "throughput_queries_per_sec": 4.469466865545937,
                        "throughput_tokens_per_sec": 446.94668655459367
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409393664,
                        "gpu_max_memory_allocated_bytes": 4409393664,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            17.0
                        ],
                        "cpu_usage_percent": 95.9,
                        "cpu_memory_usage_bytes": 2202849280
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0023",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 648.9147215205548
                        },
                        "ram_power": {
                            "0": 0.7667040824890137
                        },
                        "cpu_energy": {
                            "0": 0.0007611556037227274
                        },
                        "gpu_energy": {
                            "0": 0.00470414654109419
                        },
                        "ram_energy": {
                            "0": 3.7680905090223995e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.005469070235325939
                        },
                        "total_energy_joules": {
                            "0": 19688.65284717338
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 648.9147215205548,
                        "ram_power_avg": 0.7667040824890137,
                        "cpu_energy_total": 0.0007611556037227274,
                        "gpu_energy_total": 0.00470414654109419,
                        "ram_energy_total": 3.7680905090223995e-06,
                        "total_energy_kwh": 0.005469070235325939,
                        "total_energy_joules": 19688.65284717338
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.507906766279119,
                        "joules_per_token": 1.9688652847173378,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0020834423061474165
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0024": {
            "setup": {
                "experiment_id": "0024",
                "date_time": "April 03, 2025 at 08:47:05 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.49365894892253,
                        "average_latency_ms_per_batch": 3213.3798498460756,
                        "throughput_queries_per_sec": 4.445697350843408,
                        "throughput_tokens_per_sec": 444.5697350843408
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 9199616,
                        "gpu_max_memory_allocated_bytes": 9199616,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            61.0,
                            100.0
                        ],
                        "cpu_usage_percent": 95.2,
                        "cpu_memory_usage_bytes": 7455129600
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0024",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 835.7321986746034
                        },
                        "ram_power": {
                            "0": 0.7891416549682617
                        },
                        "cpu_energy": {
                            "0": 0.0007597561856746325
                        },
                        "gpu_energy": {
                            "0": 0.0047218726663800226
                        },
                        "ram_energy": {
                            "0": 3.7959085137120254e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.005485424760568367
                        },
                        "total_energy_joules": {
                            "0": 19747.52913804612
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 835.7321986746034,
                        "ram_power_avg": 0.7891416549682617,
                        "cpu_energy_total": 0.0007597561856746325,
                        "gpu_energy_total": 0.0047218726663800226,
                        "ram_energy_total": 3.7959085137120254e-06,
                        "total_energy_kwh": 0.005485424760568367,
                        "total_energy_joules": 19747.52913804612
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5063924671332036,
                        "joules_per_token": 1.9747529138046118,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0020896725625385196
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0025": {
            "setup": {
                "experiment_id": "0025",
                "date_time": "April 03, 2025 at 08:48:26 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.660376410000026,
                        "average_latency_ms_per_batch": 3237.1966300000036,
                        "throughput_queries_per_sec": 4.4129893604004735,
                        "throughput_tokens_per_sec": 441.29893604004735
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 9199616,
                        "gpu_max_memory_allocated_bytes": 9199616,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            2.0
                        ],
                        "cpu_usage_percent": 96.3,
                        "cpu_memory_usage_bytes": 7451144192
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0025",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 795.6272764239335
                        },
                        "ram_power": {
                            "0": 0.7850275039672852
                        },
                        "cpu_energy": {
                            "0": 0.0007687920279859099
                        },
                        "gpu_energy": {
                            "0": 0.004634844818976802
                        },
                        "ram_energy": {
                            "0": 3.931436269745653e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.005407568283232458
                        },
                        "total_energy_joules": {
                            "0": 19467.24581963685
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 795.6272764239335,
                        "ram_power_avg": 0.7850275039672852,
                        "cpu_energy_total": 0.0007687920279859099,
                        "gpu_energy_total": 0.004634844818976802,
                        "ram_energy_total": 3.931436269745653e-06,
                        "total_energy_kwh": 0.005407568283232458,
                        "total_energy_joules": 19467.24581963685
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5136833475392228,
                        "joules_per_token": 1.9467245819636851,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.002060013137497405
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0026": {
            "setup": {
                "experiment_id": "0026",
                "date_time": "April 03, 2025 at 08:50:59 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 31.54664697824046,
                        "average_latency_ms_per_batch": 4506.663854034351,
                        "throughput_queries_per_sec": 3.169908994416293,
                        "throughput_tokens_per_sec": 316.99089944162927
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409393664,
                        "gpu_max_memory_allocated_bytes": 4409393664,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            100.0,
                            3.0
                        ],
                        "cpu_usage_percent": 95.9,
                        "cpu_memory_usage_bytes": 2218422272
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0026",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 793.4832316276667
                        },
                        "ram_power": {
                            "0": 0.7731227874755859
                        },
                        "cpu_energy": {
                            "0": 0.0010321342349707266
                        },
                        "gpu_energy": {
                            "0": 0.007221459388290441
                        },
                        "ram_energy": {
                            "0": 5.2478332389461185e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.008258841456500113
                        },
                        "total_energy_joules": {
                            "0": 29731.829243400407
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 793.4832316276667,
                        "ram_power_avg": 0.7731227874755859,
                        "cpu_energy_total": 0.0010321342349707266,
                        "gpu_energy_total": 0.007221459388290441,
                        "ram_energy_total": 5.2478332389461185e-06,
                        "total_energy_kwh": 0.008258841456500113,
                        "total_energy_joules": 29731.829243400407
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.33633988403924747,
                        "joules_per_token": 2.973182924340041,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.003146205652853718
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0027": {
            "setup": {
                "experiment_id": "0027",
                "date_time": "April 03, 2025 at 08:53:59 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.12760651251301,
                        "average_latency_ms_per_batch": 3161.0866446447158,
                        "throughput_queries_per_sec": 4.519241606336893,
                        "throughput_tokens_per_sec": 451.9241606336893
                    }
                },
                "compute_metrics": {
                    "flops": 10344202560000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 9199616,
                        "gpu_max_memory_allocated_bytes": 9199616,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 95.3,
                        "cpu_memory_usage_bytes": 7397634048
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0027",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 433.20578766578467
                        },
                        "ram_power": {
                            "0": 0.7750825881958009
                        },
                        "cpu_energy": {
                            "0": 0.0007515579873434037
                        },
                        "gpu_energy": {
                            "0": 0.004667655400787396
                        },
                        "ram_energy": {
                            "0": 3.754588297744807e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0054229679764285455
                        },
                        "total_energy_joules": {
                            "0": 19522.684715142765
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 433.20578766578467,
                        "ram_power_avg": 0.7750825881958009,
                        "cpu_energy_total": 0.0007515579873434037,
                        "gpu_energy_total": 0.004667655400787396,
                        "ram_energy_total": 3.754588297744807e-06,
                        "total_energy_kwh": 0.0054229679764285455,
                        "total_energy_joules": 19522.684715142765
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5122246323141972,
                        "joules_per_token": 1.9522684715142762,
                        "flops_per_joule": 529855535.2879577,
                        "joules_per_flop": 1.8873068853693025e-09
                    },
                    "per-process_emissions": [
                        0.0020658796506204543
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0028": {
            "setup": {
                "experiment_id": "0028",
                "date_time": "April 03, 2025 at 08:57:02 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.50075631123036,
                        "average_latency_ms_per_batch": 3214.3937587471946,
                        "throughput_queries_per_sec": 4.444295054655072,
                        "throughput_tokens_per_sec": 444.42950546550725
                    }
                },
                "compute_metrics": {
                    "flops": 10344202560000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 9199616,
                        "gpu_max_memory_allocated_bytes": 9199616,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 95.5,
                        "cpu_memory_usage_bytes": 7395385344
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0028",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 643.8653125965
                        },
                        "ram_power": {
                            "0": 0.772463321685791
                        },
                        "cpu_energy": {
                            "0": 0.0007543125672455064
                        },
                        "gpu_energy": {
                            "0": 0.004754490470251227
                        },
                        "ram_energy": {
                            "0": 3.688023434981735e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.005512491060931716
                        },
                        "total_energy_joules": {
                            "0": 19844.967819354177
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 643.8653125965,
                        "ram_power_avg": 0.772463321685791,
                        "cpu_energy_total": 0.0007543125672455064,
                        "gpu_energy_total": 0.004754490470251227,
                        "ram_energy_total": 3.688023434981735e-06,
                        "total_energy_kwh": 0.005512491060931716,
                        "total_energy_joules": 19844.967819354177
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.503906082943975,
                        "joules_per_token": 1.9844967819354176,
                        "flops_per_joule": 521250659.3188638,
                        "joules_per_flop": 1.918462801191915e-09
                    },
                    "per-process_emissions": [
                        0.002099983469661937
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0029": {
            "setup": {
                "experiment_id": "0029",
                "date_time": "April 03, 2025 at 09:00:10 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.519201958552003,
                        "average_latency_ms_per_batch": 3217.028851221715,
                        "throughput_queries_per_sec": 4.440654699223189,
                        "throughput_tokens_per_sec": 444.0654699223189
                    }
                },
                "compute_metrics": {
                    "flops": 10344202560000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 9199616,
                        "gpu_max_memory_allocated_bytes": 9199616,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 95.7,
                        "cpu_memory_usage_bytes": 7381811200
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0029",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 588.1975225151324
                        },
                        "ram_power": {
                            "0": 0.7673735618591309
                        },
                        "cpu_energy": {
                            "0": 0.0007576293721576804
                        },
                        "gpu_energy": {
                            "0": 0.004656388447337179
                        },
                        "ram_energy": {
                            "0": 3.6984088834827704e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.005417716228378342
                        },
                        "total_energy_joules": {
                            "0": 19503.77842216203
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 588.1975225151324,
                        "ram_power_avg": 0.7673735618591309,
                        "cpu_energy_total": 0.0007576293721576804,
                        "gpu_energy_total": 0.004656388447337179,
                        "ram_energy_total": 3.6984088834827704e-06,
                        "total_energy_kwh": 0.005417716228378342,
                        "total_energy_joules": 19503.77842216203
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5127211652813415,
                        "joules_per_token": 1.950377842216203,
                        "flops_per_joule": 530369159.04694355,
                        "joules_per_flop": 1.885479166618527e-09
                    },
                    "per-process_emissions": [
                        0.0020638789972007294
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0030": {
            "setup": {
                "experiment_id": "0030",
                "date_time": "April 03, 2025 at 09:03:40 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.453669826034456,
                        "average_latency_ms_per_batch": 3207.6671180049225,
                        "throughput_queries_per_sec": 4.453614966941954,
                        "throughput_tokens_per_sec": 445.3614966941954
                    }
                },
                "compute_metrics": {
                    "flops": 10344202560000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 9199616,
                        "gpu_max_memory_allocated_bytes": 9199616,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 95.5,
                        "cpu_memory_usage_bytes": 7419228160
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0030",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 681.153267501753
                        },
                        "ram_power": {
                            "0": 0.7805213928222656
                        },
                        "cpu_energy": {
                            "0": 0.000752838937791239
                        },
                        "gpu_energy": {
                            "0": 0.004799700784200667
                        },
                        "ram_energy": {
                            "0": 3.7606901641998995e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.005556300412156106
                        },
                        "total_energy_joules": {
                            "0": 20002.681483761982
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 681.153267501753,
                        "ram_power_avg": 0.7805213928222656,
                        "cpu_energy_total": 0.000752838937791239,
                        "gpu_energy_total": 0.004799700784200667,
                        "ram_energy_total": 3.7606901641998995e-06,
                        "total_energy_kwh": 0.005556300412156106,
                        "total_energy_joules": 20002.681483761982
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.49993297189268954,
                        "joules_per_token": 2.000268148376198,
                        "flops_per_joule": 517140792.7680767,
                        "joules_per_flop": 1.9337093766038918e-09
                    },
                    "per-process_emissions": [
                        0.002116672642010869
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0031": {
            "setup": {
                "experiment_id": "0031",
                "date_time": "April 03, 2025 at 09:10:28 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.49158423789777,
                        "average_latency_ms_per_batch": 3213.083462556824,
                        "throughput_queries_per_sec": 4.446107439221754,
                        "throughput_tokens_per_sec": 444.61074392217535
                    }
                },
                "compute_metrics": {
                    "flops": 10344202560000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 9199616,
                        "gpu_max_memory_allocated_bytes": 9199616,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            26.0
                        ],
                        "cpu_usage_percent": 97.3,
                        "cpu_memory_usage_bytes": 7420178432
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0031",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 650.6514060638202
                        },
                        "ram_power": {
                            "0": 0.7813425064086914
                        },
                        "cpu_energy": {
                            "0": 0.0007574155193360638
                        },
                        "gpu_energy": {
                            "0": 0.004662649007904918
                        },
                        "ram_energy": {
                            "0": 3.787804212631335e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.005423852331453612
                        },
                        "total_energy_joules": {
                            "0": 19525.868393233002
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 650.6514060638202,
                        "ram_power_avg": 0.7813425064086914,
                        "cpu_energy_total": 0.0007574155193360638,
                        "gpu_energy_total": 0.004662649007904918,
                        "ram_energy_total": 3.787804212631335e-06,
                        "total_energy_kwh": 0.005423852331453612,
                        "total_energy_joules": 19525.868393233002
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5121411144748705,
                        "joules_per_token": 1.9525868393233003,
                        "flops_per_joule": 529769142.7432209,
                        "joules_per_flop": 1.887614659513493e-09
                    },
                    "per-process_emissions": [
                        0.0020662165456672536
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0032": {
            "setup": {
                "experiment_id": "0032",
                "date_time": "April 03, 2025 at 09:13:19 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.531476537697017,
                        "average_latency_ms_per_batch": 3218.7823625281453,
                        "throughput_queries_per_sec": 4.438235542739143,
                        "throughput_tokens_per_sec": 443.82355427391434
                    }
                },
                "compute_metrics": {
                    "flops": 10344202560000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 9199616,
                        "gpu_max_memory_allocated_bytes": 9199616,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.0,
                        "cpu_memory_usage_bytes": 7400255488
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0032",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 960.9960047727373
                        },
                        "ram_power": {
                            "0": 0.7744045257568359
                        },
                        "cpu_energy": {
                            "0": 0.0007129626769456081
                        },
                        "gpu_energy": {
                            "0": 0.004553622531780377
                        },
                        "ram_energy": {
                            "0": 3.827331988838e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.005270412540714823
                        },
                        "total_energy_joules": {
                            "0": 18973.485146573363
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 960.9960047727373,
                        "ram_power_avg": 0.7744045257568359,
                        "cpu_energy_total": 0.0007129626769456081,
                        "gpu_energy_total": 0.004553622531780377,
                        "ram_energy_total": 3.827331988838e-06,
                        "total_energy_kwh": 0.005270412540714823,
                        "total_energy_joules": 18973.485146573363
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5270512993658423,
                        "joules_per_token": 1.8973485146573363,
                        "flops_per_joule": 545192540.0151472,
                        "joules_per_flop": 1.834214385934585e-09
                    },
                    "per-process_emissions": [
                        0.002007763657385312
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0033": {
            "setup": {
                "experiment_id": "0033",
                "date_time": "April 03, 2025 at 09:24:21 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.696229923749343,
                        "average_latency_ms_per_batch": 3242.3185605356202,
                        "throughput_queries_per_sec": 4.406018106794026,
                        "throughput_tokens_per_sec": 440.6018106794026
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409396224,
                        "gpu_max_memory_allocated_bytes": 4409396224,
                        "gpu_current_memory_reserved_bytes": 13025411072,
                        "gpu_max_memory_reserved_bytes": 13025411072
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            9.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.8,
                        "cpu_memory_usage_bytes": 6798737408
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0033",
                    "local_process_results": {
                        "cpu_power": {
                            "3": 112.5,
                            "1": 112.5,
                            "2": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "3": 733.849911243523,
                            "1": 1017.0354872775902,
                            "2": 634.9535801137084,
                            "0": 730.952372002856
                        },
                        "ram_power": {
                            "3": 0.8038301467895509,
                            "1": 0.8158979415893556,
                            "2": 0.7943558692932129,
                            "0": 0.8347592353820801
                        },
                        "cpu_energy": {
                            "3": 0.0009871637219475817,
                            "1": 0.001030149540565617,
                            "2": 0.001025280360765464,
                            "0": 0.0007654343078174861
                        },
                        "gpu_energy": {
                            "3": 0.006735353166060776,
                            "1": 0.007011325053486672,
                            "2": 0.006979210861135954,
                            "0": 0.005365880126031897
                        },
                        "ram_energy": {
                            "3": 5.176444650844823e-06,
                            "1": 5.430732631556963e-06,
                            "2": 5.496426559143765e-06,
                            "0": 4.105395440153131e-06
                        },
                        "total_energy_kwh": {
                            "3": 0.007727693332659203,
                            "1": 0.008046905326683843,
                            "2": 0.00800998764846056,
                            "0": 0.006135419829289538
                        },
                        "total_energy_joules": {
                            "3": 27819.69599757313,
                            "1": 28968.859176061836,
                            "2": 28835.955534458015,
                            "0": 22087.511385442336
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 779.1978376594194,
                        "ram_power_avg": 0.8122107982635498,
                        "cpu_energy_total": 0.003808027931096149,
                        "gpu_energy_total": 0.0260917692067153,
                        "ram_energy_total": 2.0208999281698682e-05,
                        "total_energy_kwh": 0.02992000613709314,
                        "total_energy_joules": 107712.02209353531
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0928401473265089,
                        "joules_per_token": 10.77120220935353,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0029438647750765234,
                        0.0030654685842002104,
                        0.0030514047946810503,
                        0.0023372881839678495
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0047": {
            "setup": {
                "experiment_id": "0047",
                "date_time": "April 03, 2025 at 09:31:26 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.897753016091883,
                        "average_latency_ms_per_batch": 3271.107573727412,
                        "throughput_queries_per_sec": 4.367240747584397,
                        "throughput_tokens_per_sec": 436.7240747584397
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409396224,
                        "gpu_max_memory_allocated_bytes": 4409396224,
                        "gpu_current_memory_reserved_bytes": 13025411072,
                        "gpu_max_memory_reserved_bytes": 13025411072
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            10.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 95.3,
                        "cpu_memory_usage_bytes": 6856404992
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0047",
                    "local_process_results": {
                        "cpu_power": {
                            "3": 112.5,
                            "2": 112.5,
                            "1": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "3": 663.7268424734925,
                            "2": 582.6278086182401,
                            "1": 575.7525053133577,
                            "0": 894.711239804615
                        },
                        "ram_power": {
                            "3": 0.802635669708252,
                            "2": 0.8082475662231445,
                            "1": 0.8043708801269532,
                            "0": 0.8544416427612305
                        },
                        "cpu_energy": {
                            "3": 0.0009779172985290643,
                            "2": 0.0010349498863506598,
                            "1": 0.0010386432633531517,
                            "0": 0.0007668645588710207
                        },
                        "gpu_energy": {
                            "3": 0.006593301663517792,
                            "2": 0.006889011622313035,
                            "1": 0.00691746831176232,
                            "0": 0.005263834211064022
                        },
                        "ram_energy": {
                            "3": 5.080701241189806e-06,
                            "2": 5.419546194668884e-06,
                            "1": 5.44120953174558e-06,
                            "0": 4.192304353481993e-06
                        },
                        "total_energy_kwh": {
                            "3": 0.007576299663288048,
                            "2": 0.007929381054858364,
                            "1": 0.007961552784647215,
                            "0": 0.006034891074288524
                        },
                        "total_energy_joules": {
                            "3": 27274.678787836972,
                            "2": 28545.77179749011,
                            "1": 28661.590024729976,
                            "0": 21725.607867438684
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 679.2045990524264,
                        "ram_power_avg": 0.817423939704895,
                        "cpu_energy_total": 0.003818375007103896,
                        "gpu_energy_total": 0.02566361580865717,
                        "ram_energy_total": 2.013376132108626e-05,
                        "total_energy_kwh": 0.02950212457708215,
                        "total_energy_joules": 106207.64847749574
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0941551775540807,
                        "joules_per_token": 10.620764847749573,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.002886191356729582,
                        0.0030206977128482936,
                        0.003032953533311357,
                        0.002298991754750213
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0048": {
            "setup": {
                "experiment_id": "0048",
                "date_time": "April 03, 2025 at 09:50:53 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 31.386755059007555,
                        "average_latency_ms_per_batch": 4483.822151286794,
                        "throughput_queries_per_sec": 3.186057297481009,
                        "throughput_tokens_per_sec": 318.6057297481009
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818107392,
                        "gpu_max_memory_allocated_bytes": 8818107392,
                        "gpu_current_memory_reserved_bytes": 13203668992,
                        "gpu_max_memory_reserved_bytes": 13203668992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 95.3,
                        "cpu_memory_usage_bytes": 2453000192
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0048",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "2": 112.5,
                            "1": 112.5,
                            "3": 112.5
                        },
                        "gpu_power": {
                            "0": 769.8722566230048,
                            "2": 767.1295164839295,
                            "1": 673.7929562590441,
                            "3": 866.7543150340773
                        },
                        "ram_power": {
                            "0": 0.8556976318359375,
                            "2": 0.80419921875,
                            "1": 0.8086481094360352,
                            "3": 0.7857656478881837
                        },
                        "cpu_energy": {
                            "0": 0.0010303370955443824,
                            "2": 0.001033187981985975,
                            "1": 0.0010265161922070544,
                            "3": 0.0009789030374304274
                        },
                        "gpu_energy": {
                            "0": 0.007945408300752632,
                            "2": 0.007960607201809466,
                            "1": 0.007905767435715916,
                            "3": 0.007611781089412517
                        },
                        "ram_energy": {
                            "0": 5.755953097686171e-06,
                            "2": 5.273938168361883e-06,
                            "1": 5.322432590137301e-06,
                            "3": 5.046767478262117e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.008981501349394698,
                            "2": 0.008999069121963802,
                            "1": 0.008937606060513106,
                            "3": 0.008595730894321207
                        },
                        "total_energy_joules": {
                            "0": 32333.404857820915,
                            "2": 32396.648839069687,
                            "1": 32175.38181784718,
                            "3": 30944.631219556344
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 769.3872611000138,
                        "ram_power_avg": 0.8135776519775391,
                        "cpu_energy_total": 0.0040689443071678395,
                        "gpu_energy_total": 0.03142356402769053,
                        "ram_energy_total": 2.139909133444747e-05,
                        "total_energy_kwh": 0.035513907426192814,
                        "total_energy_joules": 127850.06673429412
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.07821661932162004,
                        "joules_per_token": 12.785006673429411,
                        "flops_per_joule": 80918544.23119335,
                        "joules_per_flop": 1.2358106655291374e-08
                    },
                    "per-process_emissions": [
                        0.0034215029390519103,
                        0.0034281953820121105,
                        0.003404781028752468,
                        0.003274543684191664
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0055": {
            "setup": {
                "experiment_id": "0055",
                "date_time": "April 03, 2025 at 09:56:41 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 31.55438410793431,
                        "average_latency_ms_per_batch": 4507.76915827633,
                        "throughput_queries_per_sec": 3.1691317332622293,
                        "throughput_tokens_per_sec": 316.91317332622293
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818107392,
                        "gpu_max_memory_allocated_bytes": 8818107392,
                        "gpu_current_memory_reserved_bytes": 13203668992,
                        "gpu_max_memory_reserved_bytes": 13203668992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            74.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.5,
                        "cpu_memory_usage_bytes": 2420473856
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0055",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "2": 112.5,
                            "1": 112.5,
                            "3": 112.5
                        },
                        "gpu_power": {
                            "0": 915.5521854462003,
                            "2": 825.4536911261779,
                            "1": 1105.802055894411,
                            "3": 789.933738408374
                        },
                        "ram_power": {
                            "0": 0.8433322906494141,
                            "2": 0.7996444702148438,
                            "1": 0.7976832389831543,
                            "3": 0.7949023246765137
                        },
                        "cpu_energy": {
                            "0": 0.0010291048747094465,
                            "2": 0.0010105071648504238,
                            "1": 0.0010237074824108276,
                            "3": 0.0009633386480782063
                        },
                        "gpu_energy": {
                            "0": 0.00798897250228947,
                            "2": 0.007860413788321807,
                            "1": 0.007961913869522164,
                            "3": 0.007505806837954765
                        },
                        "ram_energy": {
                            "0": 5.748678767626046e-06,
                            "2": 5.324754948516485e-06,
                            "1": 5.239245163430196e-06,
                            "3": 4.989714881533782e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.009023826055766543,
                            "2": 0.00887624570812075,
                            "1": 0.00899086059709642,
                            "3": 0.008474135200914506
                        },
                        "total_energy_joules": {
                            "0": 32485.773800759554,
                            "2": 31954.484549234698,
                            "1": 32367.098149547113,
                            "3": 30506.886723292224
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 909.1854177187909,
                        "ram_power_avg": 0.8088905811309814,
                        "cpu_energy_total": 0.0040266581700489044,
                        "gpu_energy_total": 0.03131710699808821,
                        "ram_energy_total": 2.1302393761106505e-05,
                        "total_energy_kwh": 0.035365067561898215,
                        "total_energy_joules": 127314.2432228336
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0785458071843333,
                        "joules_per_token": 12.73142432228336,
                        "flops_per_joule": 81259103.60157223,
                        "joules_per_flop": 1.2306313455082855e-08
                    },
                    "per-process_emissions": [
                        0.0034376265359442647,
                        0.0033814058025085996,
                        0.0034250683444638818,
                        0.003228221804788381
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0056": {
            "setup": {
                "experiment_id": "0056",
                "date_time": "April 03, 2025 at 09:58:52 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 25.744839271064848,
                        "average_latency_ms_per_batch": 3677.8341815806925,
                        "throughput_queries_per_sec": 3.8842736187672395,
                        "throughput_tokens_per_sec": 388.427361876724
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818107392,
                        "gpu_max_memory_allocated_bytes": 8818107392,
                        "gpu_current_memory_reserved_bytes": 13203668992,
                        "gpu_max_memory_reserved_bytes": 13203668992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 95.6,
                        "cpu_memory_usage_bytes": 2462572544
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0056",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "3": 112.5,
                            "1": 112.5,
                            "2": 112.5
                        },
                        "gpu_power": {
                            "0": 909.230661455496,
                            "3": 787.5886593981776,
                            "1": 535.1296862032214,
                            "2": 900.4032142165673
                        },
                        "ram_power": {
                            "0": 0.8581094741821289,
                            "3": 0.799504280090332,
                            "1": 0.7938108444213867,
                            "2": 0.7928338050842285
                        },
                        "cpu_energy": {
                            "0": 0.0008450702345362514,
                            "3": 0.000950902728785877,
                            "1": 0.001044269545629504,
                            "2": 0.0010248704671612357
                        },
                        "gpu_energy": {
                            "0": 0.0062004285714500895,
                            "3": 0.00689780412934482,
                            "1": 0.007384993685768393,
                            "2": 0.007298747783440973
                        },
                        "ram_energy": {
                            "0": 4.645769695434352e-06,
                            "3": 4.973638803021014e-06,
                            "1": 5.4267617360579535e-06,
                            "2": 5.476315974347987e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.007050144575681775,
                            "3": 0.007853680496933715,
                            "1": 0.008434689993133954,
                            "2": 0.008329094566576553
                        },
                        "total_energy_joules": {
                            "0": 25380.52047245439,
                            "3": 28273.249788961373,
                            "1": 30364.883975282235,
                            "2": 29984.740439675592
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 783.0880553183656,
                        "ram_power_avg": 0.811064600944519,
                        "cpu_energy_total": 0.003865112976112868,
                        "gpu_energy_total": 0.027781974170004275,
                        "ram_energy_total": 2.0522486208861307e-05,
                        "total_energy_kwh": 0.031667609632325996,
                        "total_energy_joules": 114003.39467637359
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.08771668623015513,
                        "joules_per_token": 11.400339467637359,
                        "flops_per_joule": 90746782.66702545,
                        "joules_per_flop": 1.1019674423822508e-08
                    },
                    "per-process_emissions": [
                        0.0026857525761059724,
                        0.002991859585306899,
                        0.00321319515288438,
                        0.0031729685751373383
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0061": {
            "setup": {
                "experiment_id": "0061",
                "date_time": "April 03, 2025 at 10:30:38 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.69411848601885,
                        "average_latency_ms_per_batch": 3242.0169265741215,
                        "throughput_queries_per_sec": 4.406428038242901,
                        "throughput_tokens_per_sec": 440.64280382429007
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818107392,
                        "gpu_max_memory_allocated_bytes": 8818107392,
                        "gpu_current_memory_reserved_bytes": 13203668992,
                        "gpu_max_memory_reserved_bytes": 13203668992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.2,
                        "cpu_memory_usage_bytes": 2442129408
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0061",
                    "local_process_results": {
                        "cpu_power": {
                            "2": 112.5,
                            "1": 112.5,
                            "3": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "2": 111611.0368995235,
                            "1": 573.1271653941521,
                            "3": 801.9226924346626,
                            "0": 822.2012829683438
                        },
                        "ram_power": {
                            "2": 0.8071832656860352,
                            "1": 0.8090286254882812,
                            "3": 0.8051047325134278,
                            "0": 0.8506264686584473
                        },
                        "cpu_energy": {
                            "2": 0.0010259787663235327,
                            "1": 0.001053812863006897,
                            "3": 0.0009698393909784501,
                            "0": 0.0007689619896336806
                        },
                        "gpu_energy": {
                            "2": 0.006888567733057016,
                            "1": 0.007008395051151695,
                            "3": 0.006557741357291036,
                            "0": 0.00528772534127242
                        },
                        "ram_energy": {
                            "2": 5.50749487438035e-06,
                            "1": 5.556392794783204e-06,
                            "3": 5.025522091312414e-06,
                            "0": 4.262099741284547e-06
                        },
                        "total_energy_kwh": {
                            "2": 0.007920053994254932,
                            "1": 0.008067764306953372,
                            "3": 0.007532606270360801,
                            "0": 0.0060609494306473844
                        },
                        "total_energy_joules": {
                            "2": 28512.194379317752,
                            "1": 29043.95150503214,
                            "3": 27117.382573298884,
                            "0": 21819.417950330582
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 28452.072010080163,
                        "ram_power_avg": 0.8179857730865479,
                        "cpu_energy_total": 0.003818593009942561,
                        "gpu_energy_total": 0.025742429482772167,
                        "ram_energy_total": 2.0351509501760517e-05,
                        "total_energy_kwh": 0.029581374002216488,
                        "total_energy_joules": 106492.94640797935
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09390293289181371,
                        "joules_per_token": 10.649294640797935,
                        "flops_per_joule": 97146727.82520394,
                        "joules_per_flop": 1.0293707491613094e-08
                    },
                    "per-process_emissions": [
                        0.0030171445691114162,
                        0.003073414812733887,
                        0.0028695463586939475,
                        0.0023089186856051214
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0062": {
            "setup": {
                "experiment_id": "0062",
                "date_time": "April 03, 2025 at 10:33:25 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.71175936073996,
                        "average_latency_ms_per_batch": 3244.53705153428,
                        "throughput_queries_per_sec": 4.4030054392378855,
                        "throughput_tokens_per_sec": 440.3005439237885
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818107392,
                        "gpu_max_memory_allocated_bytes": 8818107392,
                        "gpu_current_memory_reserved_bytes": 13203668992,
                        "gpu_max_memory_reserved_bytes": 13203668992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.0,
                        "cpu_memory_usage_bytes": 2450501632
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0062",
                    "local_process_results": {
                        "cpu_power": {
                            "3": 112.5,
                            "1": 112.5,
                            "0": 112.5,
                            "2": 112.5
                        },
                        "gpu_power": {
                            "3": 646.4334414810249,
                            "1": 612.3272463210357,
                            "0": 832.0596579276126,
                            "2": 722.6326449492843
                        },
                        "ram_power": {
                            "3": 0.795194149017334,
                            "1": 0.8021206855773926,
                            "0": 0.8539724349975586,
                            "2": 0.7992925643920898
                        },
                        "cpu_energy": {
                            "3": 0.0009595435097071457,
                            "1": 0.0010238793370808707,
                            "0": 0.0007671694042073794,
                            "2": 0.0010246676693641346
                        },
                        "gpu_energy": {
                            "3": 0.006497330753418851,
                            "1": 0.006839899083026069,
                            "0": 0.005248435309862742,
                            "2": 0.006834050745013087
                        },
                        "ram_energy": {
                            "3": 4.929515458527407e-06,
                            "1": 5.611685621767234e-06,
                            "0": 4.140226723135471e-06,
                            "2": 5.419633496261236e-06
                        },
                        "total_energy_kwh": {
                            "3": 0.007461803778584523,
                            "1": 0.007869390105728711,
                            "0": 0.006019744940793257,
                            "2": 0.007864138047873483
                        },
                        "total_energy_joules": {
                            "3": 26862.49360290428,
                            "1": 28329.804380623358,
                            "0": 21671.081786855724,
                            "2": 28310.896972344537
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 703.3632476697394,
                        "ram_power_avg": 0.8126449584960938,
                        "cpu_energy_total": 0.0037752599203595303,
                        "gpu_energy_total": 0.02541971589132075,
                        "ram_energy_total": 2.010106129969135e-05,
                        "total_energy_kwh": 0.029215076872979973,
                        "total_energy_joules": 105174.27674272789
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09508028302834452,
                        "joules_per_token": 10.517427674272788,
                        "flops_per_joule": 98364748.49555188,
                        "joules_per_flop": 1.01662436522696e-08
                    },
                    "per-process_emissions": [
                        0.002842574149451774,
                        0.0029978441607773523,
                        0.0022932218351951915,
                        0.0029958433893374032
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0063": {
            "setup": {
                "experiment_id": "0063",
                "date_time": "April 03, 2025 at 10:35:28 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.672720431815833,
                        "average_latency_ms_per_batch": 3238.960061687976,
                        "throughput_queries_per_sec": 4.4105867357528705,
                        "throughput_tokens_per_sec": 441.058673575287
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818107392,
                        "gpu_max_memory_allocated_bytes": 8818107392,
                        "gpu_current_memory_reserved_bytes": 13203668992,
                        "gpu_max_memory_reserved_bytes": 13203668992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.2,
                        "cpu_memory_usage_bytes": 2424483840
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0063",
                    "local_process_results": {
                        "cpu_power": {
                            "3": 112.5,
                            "2": 112.5,
                            "1": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "3": 787.4446737220122,
                            "2": 700.7027035185157,
                            "1": 1235.8903111311602,
                            "0": 782.2945721528956
                        },
                        "ram_power": {
                            "3": 0.7929582595825195,
                            "2": 0.7974100112915039,
                            "1": 0.7933659553527832,
                            "0": 0.8455867767333984
                        },
                        "cpu_energy": {
                            "3": 0.0009977600233760312,
                            "2": 0.001028026744672388,
                            "1": 0.0010274985535870652,
                            "0": 0.0007674425194636568
                        },
                        "gpu_energy": {
                            "3": 0.0067999826622084925,
                            "2": 0.0069682033523363884,
                            "1": 0.006986976422901137,
                            "0": 0.005362632901221076
                        },
                        "ram_energy": {
                            "3": 5.143312282823977e-06,
                            "2": 5.476871426165291e-06,
                            "1": 5.530439029970066e-06,
                            "0": 4.119595831588308e-06
                        },
                        "total_energy_kwh": {
                            "3": 0.007802885997867348,
                            "2": 0.008001706968434939,
                            "1": 0.008020005415518172,
                            "0": 0.0061341950165163224
                        },
                        "total_energy_joules": {
                            "3": 28090.38959232245,
                            "2": 28806.14508636578,
                            "1": 28872.01949586542,
                            "0": 22083.10205945876
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 876.583065131146,
                        "ram_power_avg": 0.8073302507400513,
                        "cpu_energy_total": 0.0038207278410991407,
                        "gpu_energy_total": 0.026117795338667094,
                        "ram_energy_total": 2.027021857054764e-05,
                        "total_energy_kwh": 0.02995879339833678,
                        "total_energy_joules": 107851.65623401241
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09271994839191326,
                        "joules_per_token": 10.78516562340124,
                        "flops_per_joule": 95922878.15731691,
                        "joules_per_flop": 1.0425041650230352e-08
                    },
                    "per-process_emissions": [
                        0.002972509420887566,
                        0.00304825026962529,
                        0.0030552210630416477,
                        0.002336821591541893
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0082": {
            "setup": {
                "experiment_id": "0082",
                "date_time": "April 04, 2025 at 12:09:32 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.458189751254395,
                        "average_latency_ms_per_batch": 3208.3128216077707,
                        "throughput_queries_per_sec": 4.4527186343865734,
                        "throughput_tokens_per_sec": 445.2718634386573
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818107392,
                        "gpu_max_memory_allocated_bytes": 8818107392,
                        "gpu_current_memory_reserved_bytes": 13203668992,
                        "gpu_max_memory_reserved_bytes": 13203668992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 94.6,
                        "cpu_memory_usage_bytes": 2464874496
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0082",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "3": 112.5,
                            "2": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "1": 377.6523783503454,
                            "3": 428.2210337404455,
                            "2": 467.2493966495603,
                            "0": 433.49265072715104
                        },
                        "ram_power": {
                            "1": 0.8015227317810059,
                            "3": 0.8083491325378418,
                            "2": 0.8006572723388673,
                            "0": 0.8590650558471681
                        },
                        "cpu_energy": {
                            "1": 0.0007753150198404913,
                            "3": 0.0007664098356544856,
                            "2": 0.000758389915645239,
                            "0": 0.0007675254173664145
                        },
                        "gpu_energy": {
                            "1": 0.0032868395739136247,
                            "3": 0.0032695348378553035,
                            "2": 0.0032324811970845246,
                            "0": 0.0032689062262427626
                        },
                        "ram_energy": {
                            "1": 3.9895841292903165e-06,
                            "3": 3.8117076269348523e-06,
                            "2": 3.7183765128114438e-06,
                            "0": 4.180769433968967e-06
                        },
                        "total_energy_kwh": {
                            "1": 0.004066144177883407,
                            "3": 0.004039756381136725,
                            "2": 0.003994589489242575,
                            "0": 0.0040406124130431455
                        },
                        "total_energy_joules": {
                            "1": 14638.119040380263,
                            "3": 14543.12297209221,
                            "2": 14380.52216127327,
                            "0": 14546.204686955323
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 426.65386486687555,
                        "ram_power_avg": 0.8173985481262207,
                        "cpu_energy_total": 0.0030676401885066304,
                        "gpu_energy_total": 0.013057761835096215,
                        "ram_energy_total": 1.570043770300558e-05,
                        "total_energy_kwh": 0.016141102461305852,
                        "total_energy_joules": 58107.96886070107
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.17209343565204338,
                        "joules_per_token": 5.810796886070107,
                        "flops_per_joule": 178038253.32116732,
                        "joules_per_flop": 5.616770448742141e-09
                    },
                    "per-process_emissions": [
                        0.0015489976245646837,
                        0.0015389451933940353,
                        0.001521738865926959,
                        0.0015392712987487864
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0083": {
            "setup": {
                "experiment_id": "0083",
                "date_time": "April 04, 2025 at 12:18:18 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precision_gaming",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 4.548809517873451,
                        "average_latency_ms_per_batch": 4548.809517873451,
                        "throughput_queries_per_sec": 21.983773909871164,
                        "throughput_tokens_per_sec": 2198.3773909871165
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825257984,
                        "gpu_max_memory_allocated_bytes": 825257984,
                        "gpu_current_memory_reserved_bytes": 1814036480,
                        "gpu_max_memory_reserved_bytes": 1814036480
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 79.6,
                        "cpu_memory_usage_bytes": 3119271936
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0083",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 246.47088877820119
                        },
                        "ram_power": {
                            "0": 1.0891027450561523
                        },
                        "cpu_energy": {
                            "0": 0.00020759451525373152
                        },
                        "gpu_energy": {
                            "0": 0.0003922728138263665
                        },
                        "ram_energy": {
                            "0": 1.5375956744347662e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0006014049247545328
                        },
                        "total_energy_joules": {
                            "0": 2165.057729116318
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 246.47088877820119,
                        "ram_power_avg": 1.0891027450561523,
                        "cpu_energy_total": 0.00020759451525373152,
                        "gpu_energy_total": 0.0003922728138263665,
                        "ram_energy_total": 1.5375956744347662e-06,
                        "total_energy_kwh": 0.0006014049247545328,
                        "total_energy_joules": 2165.057729116318
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 4.618814484952123,
                        "joules_per_token": 0.2165057729116318,
                        "flops_per_joule": 4778367403.728563,
                        "joules_per_flop": 2.0927649875137257e-10
                    },
                    "per-process_emissions": [
                        0.00022910520608523927
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0084": {
            "setup": {
                "experiment_id": "0084",
                "date_time": "April 04, 2025 at 12:30:51 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 20.21407313225791,
                        "average_latency_ms_per_batch": 2887.724733179701,
                        "throughput_queries_per_sec": 4.947048491697528,
                        "throughput_tokens_per_sec": 494.7048491697527
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818107392,
                        "gpu_max_memory_allocated_bytes": 8818107392,
                        "gpu_current_memory_reserved_bytes": 13203668992,
                        "gpu_max_memory_reserved_bytes": 13203668992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.7,
                        "cpu_memory_usage_bytes": 2442969088
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0084",
                    "local_process_results": {
                        "cpu_power": {
                            "2": 112.5,
                            "3": 112.5,
                            "1": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "2": 501.9126252767978,
                            "3": 414.2786354888776,
                            "1": 332.3833105825148,
                            "0": 374.6642734660781
                        },
                        "ram_power": {
                            "2": 0.7953286170959473,
                            "3": 0.7900629043579102,
                            "1": 0.8001136779785156,
                            "0": 0.8519024848937988
                        },
                        "cpu_energy": {
                            "2": 0.0006556360179674811,
                            "3": 0.0006576113454430014,
                            "1": 0.0006621865694978623,
                            "0": 0.0006566000551683827
                        },
                        "gpu_energy": {
                            "2": 0.003072056624322528,
                            "3": 0.0030770569061076003,
                            "1": 0.003088277748410917,
                            "0": 0.003075468293726402
                        },
                        "ram_energy": {
                            "2": 3.6048434257236622e-06,
                            "3": 3.4273081248720158e-06,
                            "1": 3.4524946397044287e-06,
                            "0": 3.898576206343322e-06
                        },
                        "total_energy_kwh": {
                            "2": 0.003731297485715732,
                            "3": 0.0037380955596754755,
                            "1": 0.0037539168125484837,
                            "0": 0.003735966925101129
                        },
                        "total_energy_joules": {
                            "2": 13432.670948576635,
                            "3": 13457.144014831712,
                            "1": 13514.100525174541,
                            "0": 13449.480930364063
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 405.8097112035671,
                        "ram_power_avg": 0.809351921081543,
                        "cpu_energy_total": 0.0026320339880767274,
                        "gpu_energy_total": 0.012312859572567447,
                        "ram_energy_total": 1.4383222396643427e-05,
                        "total_energy_kwh": 0.01495927678304082,
                        "total_energy_joules": 53853.396418946955
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.18568930958794186,
                        "joules_per_token": 5.385339641894695,
                        "flops_per_joule": 192103784.86657932,
                        "joules_per_flop": 5.205519509646954e-09
                    },
                    "per-process_emissions": [
                        0.0014214377771834081,
                        0.0014240275034583724,
                        0.0014300546097403449,
                        0.001423216600117275
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0085": {
            "setup": {
                "experiment_id": "0085",
                "date_time": "April 04, 2025 at 12:32:18 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "max_throughput_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 8.959577209083363,
                        "average_latency_ms_per_batch": 8959.577209083363,
                        "throughput_queries_per_sec": 11.161240945456488,
                        "throughput_tokens_per_sec": 1116.1240945456489
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1577436672,
                        "gpu_max_memory_allocated_bytes": 1577436672,
                        "gpu_current_memory_reserved_bytes": 2839543808,
                        "gpu_max_memory_reserved_bytes": 2839543808
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.7,
                        "cpu_memory_usage_bytes": 3400355840
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0085",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "0": 112.5,
                            "3": 112.5,
                            "2": 112.5
                        },
                        "gpu_power": {
                            "1": 334.3695937718301,
                            "0": 0.0,
                            "3": 0.0,
                            "2": 327.32423014733695
                        },
                        "ram_power": {
                            "1": 1.2413148880004883,
                            "0": 1.1864776611328125,
                            "3": 1.2345256805419922,
                            "2": 1.2331824302673342
                        },
                        "cpu_energy": {
                            "1": 0.0002988147779833526,
                            "0": 0.00031617841994011547,
                            "3": 0.00031581227139395197,
                            "2": 0.0003064600490688463
                        },
                        "gpu_energy": {
                            "1": 0.0007703767274236384,
                            "0": 0.0008093642586075589,
                            "3": 0.0008093642586075589,
                            "2": 0.0007895845205645458
                        },
                        "ram_energy": {
                            "1": 2.4719037470677725e-06,
                            "0": 2.8074301115902304e-06,
                            "3": 2.8920851334186037e-06,
                            "2": 2.618787325891178e-06
                        },
                        "total_energy_kwh": {
                            "1": 0.0010716634091540585,
                            "0": 0.0011283501086592647,
                            "3": 0.001128068615134929,
                            "2": 0.001098663356959283
                        },
                        "total_energy_joules": {
                            "1": 3857.9882729546107,
                            "0": 4062.060391173353,
                            "3": 4061.0470144857445,
                            "2": 3955.188085053419
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 165.42345597979175,
                        "ram_power_avg": 1.2238751649856567,
                        "cpu_energy_total": 0.0012372655183862664,
                        "gpu_energy_total": 0.003178689765203302,
                        "ram_energy_total": 1.0790206317967785e-05,
                        "total_energy_kwh": 0.004426745489907536,
                        "total_energy_joules": 15936.283763667128
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.6274988666303016,
                        "joules_per_token": 1.5936283763667127,
                        "flops_per_joule": 649175267.7990336,
                        "joules_per_flop": 1.5404160472570126e-09
                    },
                    "per-process_emissions": [
                        0.00040825017571723864,
                        0.0004298449738937469,
                        0.0004297377389356512,
                        0.0004185358058336389
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0086": {
            "setup": {
                "experiment_id": "0086",
                "date_time": "April 04, 2025 at 12:32:53 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precision_gaming",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 3.983170330990106,
                        "average_latency_ms_per_batch": 3983.170330990106,
                        "throughput_queries_per_sec": 25.105629860208055,
                        "throughput_tokens_per_sec": 2510.5629860208055
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087586816,
                        "gpu_max_memory_allocated_bytes": 1087586816,
                        "gpu_current_memory_reserved_bytes": 1994391552,
                        "gpu_max_memory_reserved_bytes": 1994391552
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.7,
                        "cpu_memory_usage_bytes": 3311267840
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0086",
                    "local_process_results": {
                        "cpu_power": {
                            "3": 112.5,
                            "1": 112.5,
                            "2": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "3": 770.4948606279871,
                            "1": 745.6848056478675,
                            "2": 758.5297282369668,
                            "0": 6.749724002243486
                        },
                        "ram_power": {
                            "3": 1.2113041877746582,
                            "1": 1.219254970550537,
                            "2": 1.2204408645629885,
                            "0": 1.155677318572998
                        },
                        "cpu_energy": {
                            "3": 0.0001563370826333994,
                            "1": 0.0001567665043476154,
                            "2": 0.00015556260977609782,
                            "0": 0.00019190354984311854
                        },
                        "gpu_energy": {
                            "3": 0.0007834892379037228,
                            "1": 0.0007739583969446073,
                            "2": 0.0007739583969446073,
                            "0": 0.0007989942503030534
                        },
                        "ram_energy": {
                            "3": 1.3074015568811945e-06,
                            "1": 1.3277474913662723e-06,
                            "2": 1.3243127646870347e-06,
                            "0": 1.492528755555048e-06
                        },
                        "total_energy_kwh": {
                            "3": 0.0009411337220940035,
                            "1": 0.0009320526487835891,
                            "2": 0.0009308453194853922,
                            "0": 0.000992390328901727
                        },
                        "total_energy_joules": {
                            "3": 3388.081399538413,
                            "1": 3355.3895356209205,
                            "2": 3351.043150147412,
                            "0": 3572.605184046217
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 570.3647796287663,
                        "ram_power_avg": 1.2016693353652954,
                        "cpu_energy_total": 0.0006605697466002311,
                        "gpu_energy_total": 0.003130400282095991,
                        "ram_energy_total": 5.45199056848955e-06,
                        "total_energy_kwh": 0.003796422019264712,
                        "total_energy_joules": 13667.119269352963
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.7316830857270645,
                        "joules_per_token": 1.3667119269352963,
                        "flops_per_joule": 756958439.8958553,
                        "joules_per_flop": 1.3210764915146242e-09
                    },
                    "per-process_emissions": [
                        0.00035852489143171066,
                        0.00035506545655410827,
                        0.00035460552445796016,
                        0.0003780510957951129
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0087": {
            "setup": {
                "experiment_id": "0087",
                "date_time": "April 04, 2025 at 12:33:25 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "gpu_overdrive",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 2.5677902039606124,
                        "average_latency_ms_per_batch": 2567.7902039606124,
                        "throughput_queries_per_sec": 38.94399154796912,
                        "throughput_tokens_per_sec": 3894.399154796912
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419751424,
                        "gpu_max_memory_allocated_bytes": 4419751424,
                        "gpu_current_memory_reserved_bytes": 6838812672,
                        "gpu_max_memory_reserved_bytes": 6838812672
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.5,
                        "cpu_memory_usage_bytes": 3788763136
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0087",
                    "local_process_results": {
                        "cpu_power": {
                            "3": 112.5,
                            "2": 112.5,
                            "1": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "3": 912.5302464046811,
                            "2": 855.4174273499839,
                            "1": 945.2184552471232,
                            "0": 894.4337669372675
                        },
                        "ram_power": {
                            "3": 1.1054606437683105,
                            "2": 1.115877628326416,
                            "1": 1.142526626586914,
                            "0": 1.3207883834838867
                        },
                        "cpu_energy": {
                            "3": 0.00011536547565629006,
                            "2": 0.00011409496994019719,
                            "1": 0.00011372061525617029,
                            "0": 0.00011483032030810137
                        },
                        "gpu_energy": {
                            "3": 0.0006121893786428245,
                            "2": 0.0006066935409236862,
                            "1": 0.0006066935409236862,
                            "0": 0.0006063871517909547
                        },
                        "ram_energy": {
                            "3": 9.383586940478697e-07,
                            "2": 9.329599090881159e-07,
                            "1": 9.66109071301293e-07,
                            "0": 1.1308933470558129e-06
                        },
                        "total_energy_kwh": {
                            "3": 0.0007284932129931624,
                            "2": 0.0007217214707729715,
                            "1": 0.0007213802652511577,
                            "0": 0.000722348365446112
                        },
                        "total_energy_joules": {
                            "3": 2622.5755667753847,
                            "2": 2598.1972947826976,
                            "1": 2596.9689549041677,
                            "0": 2600.454115606003
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 901.899973984764,
                        "ram_power_avg": 1.1711633205413818,
                        "cpu_energy_total": 0.0004580113811607589,
                        "gpu_energy_total": 0.0024319636122811517,
                        "ram_energy_total": 3.9683210214930914e-06,
                        "total_energy_kwh": 0.0028939433144634036,
                        "total_energy_joules": 10418.195932068253
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.9598590835884547,
                        "joules_per_token": 1.0418195932068253,
                        "flops_per_joule": 993016578.6338971,
                        "joules_per_flop": 1.007032532503848e-09
                    },
                    "per-process_emissions": [
                        0.00027751948948974523,
                        0.0002749397942909635,
                        0.00027480981204742853,
                        0.0002751786098166964
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0088": {
            "setup": {
                "experiment_id": "0088",
                "date_time": "April 04, 2025 at 12:34:29 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "standard_production",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 1,
                    "delay_max": 2,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 30.2599654418882,
                        "average_latency_ms_per_batch": 4322.852205984028,
                        "throughput_queries_per_sec": 3.3046964376757755,
                        "throughput_tokens_per_sec": 330.46964376757757
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818107392,
                        "gpu_max_memory_allocated_bytes": 8818107392,
                        "gpu_current_memory_reserved_bytes": 13203668992,
                        "gpu_max_memory_reserved_bytes": 13203668992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.5,
                        "cpu_memory_usage_bytes": 2389356544
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0088",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "3": 112.5,
                            "0": 112.5,
                            "2": 112.5
                        },
                        "gpu_power": {
                            "1": 439.4453810022858,
                            "3": 408.88562881138336,
                            "0": 433.8682112786464,
                            "2": 432.423691631926
                        },
                        "ram_power": {
                            "1": 0.7977547645568849,
                            "3": 0.793656349182129,
                            "0": 0.8325948715209961,
                            "2": 0.8056840896606445
                        },
                        "cpu_energy": {
                            "1": 0.0009752313736535143,
                            "3": 0.0009832784774407627,
                            "0": 0.0009657635769399348,
                            "2": 0.000976099365929258
                        },
                        "gpu_energy": {
                            "1": 0.00358981120517754,
                            "3": 0.00361505178092969,
                            "0": 0.0035522833973722356,
                            "2": 0.00358981120517754
                        },
                        "ram_energy": {
                            "1": 5.230964704312501e-06,
                            "3": 5.244216486417914e-06,
                            "0": 5.483275764860461e-06,
                            "2": 5.26112143022956e-06
                        },
                        "total_energy_kwh": {
                            "1": 0.004570273543535366,
                            "3": 0.004603574474856871,
                            "0": 0.004523530250077031,
                            "2": 0.004571171692537027
                        },
                        "total_energy_joules": {
                            "1": 16452.984756727317,
                            "3": 16572.868109484734,
                            "0": 16284.70890027731,
                            "2": 16456.218093133295
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 428.6557281810604,
                        "ram_power_avg": 0.8074225187301636,
                        "cpu_energy_total": 0.0039003727939634697,
                        "gpu_energy_total": 0.014346957588657006,
                        "ram_energy_total": 2.1219578385820437e-05,
                        "total_energy_kwh": 0.018268549961006294,
                        "total_energy_joules": 65766.77985962266
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.15205244990471964,
                        "joules_per_token": 6.576677985962267,
                        "flops_per_joule": 157304969.19694188,
                        "joules_per_flop": 6.357078260814667e-09
                    },
                    "per-process_emissions": [
                        0.0017410457064097977,
                        0.001753731696196725,
                        0.001723238848766845,
                        0.0017413878562719802
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0089": {
            "setup": {
                "experiment_id": "0089",
                "date_time": "April 04, 2025 at 12:35:59 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_real_time",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.05,
                    "simulate_burst": false
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 57.03250773763284,
                        "average_latency_ms_per_batch": 2281.3003095053136,
                        "throughput_queries_per_sec": 1.7533859892682766,
                        "throughput_tokens_per_sec": 175.33859892682767
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818113536,
                        "gpu_max_memory_allocated_bytes": 8818113536,
                        "gpu_current_memory_reserved_bytes": 13203668992,
                        "gpu_max_memory_reserved_bytes": 13203668992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            21.0,
                            100.0,
                            96.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 2248110080
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0089",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "3": 112.5,
                            "1": 112.5,
                            "2": 112.5
                        },
                        "gpu_power": {
                            "0": 473.7427584304278,
                            "3": 589.1150569726286,
                            "1": 623.5864870348848,
                            "2": 483.15360967339774
                        },
                        "ram_power": {
                            "0": 0.7838301658630371,
                            "3": 0.726778507232666,
                            "1": 0.7371640205383301,
                            "2": 0.7262063026428223
                        },
                        "cpu_energy": {
                            "0": 0.0018040922938598674,
                            "3": 0.0017961860360228457,
                            "1": 0.0017887185159925136,
                            "2": 0.0018291532348448532
                        },
                        "gpu_energy": {
                            "0": 0.008970245231747498,
                            "3": 0.00893488548124921,
                            "1": 0.00889320350345102,
                            "2": 0.009060537803982527
                        },
                        "ram_energy": {
                            "0": 9.300163417308149e-06,
                            "3": 8.442157930581053e-06,
                            "1": 8.671229054871408e-06,
                            "2": 8.857225581396707e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.010783637689024671,
                            "3": 0.010739513675202636,
                            "1": 0.0106905932484984,
                            "2": 0.010898548264408775
                        },
                        "total_energy_joules": {
                            "0": 38821.09568048882,
                            "3": 38662.24923072949,
                            "1": 38486.13569459424,
                            "2": 39234.773751871595
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 542.3994780278348,
                        "ram_power_avg": 0.7434947490692139,
                        "cpu_energy_total": 0.0072181500807200795,
                        "gpu_energy_total": 0.035858872020430255,
                        "ram_energy_total": 3.527077598415732e-05,
                        "total_energy_kwh": 0.04311229287713448,
                        "total_energy_joules": 155204.25435768414
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.06443122349567798,
                        "joules_per_token": 15.520425435768415,
                        "flops_per_joule": 66656943.92730929,
                        "joules_per_flop": 1.50021879354463e-08
                    },
                    "per-process_emissions": [
                        0.004108026777633948,
                        0.0040912177345684446,
                        0.004072581498015466,
                        0.004151801961326523
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0090": {
            "setup": {
                "experiment_id": "0090",
                "date_time": "April 04, 2025 at 12:37:09 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "balanced_performance_mode",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.02,
                    "delay_max": 0.1,
                    "simulate_burst": true,
                    "burst_interval": 1.5,
                    "burst_size": 3
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.75439521390945,
                        "average_latency_ms_per_batch": 9688.598803477362,
                        "throughput_queries_per_sec": 2.580352485132028,
                        "throughput_tokens_per_sec": 258.03524851320276
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576153600,
                        "gpu_max_memory_allocated_bytes": 1576153600,
                        "gpu_current_memory_reserved_bytes": 2854223872,
                        "gpu_max_memory_reserved_bytes": 2854223872
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            12.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 3250483200
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0090",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "1": 112.5,
                            "2": 112.5,
                            "3": 112.5
                        },
                        "gpu_power": {
                            "0": 471.1198654017332,
                            "1": 0.0,
                            "2": 470.0352282363168,
                            "3": 498.0436438610011
                        },
                        "ram_power": {
                            "0": 1.1346759796142578,
                            "1": 1.1795167922973633,
                            "2": 1.1788744926452637,
                            "3": 1.1772308349609375
                        },
                        "cpu_energy": {
                            "0": 0.0012271595606871415,
                            "1": 0.0012440838009570145,
                            "2": 0.0012237160909935488,
                            "3": 0.0011982447385598786
                        },
                        "gpu_energy": {
                            "0": 0.005386842642796097,
                            "1": 0.005445028800458829,
                            "2": 0.005376925134886079,
                            "3": 0.005275110331186994
                        },
                        "ram_energy": {
                            "0": 9.23905312983087e-06,
                            "1": 1.0077965998890283e-05,
                            "2": 9.520176510644797e-06,
                            "3": 9.309576722228915e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.006623241256613069,
                            "1": 0.006699190567414733,
                            "2": 0.006610161402390272,
                            "3": 0.006482664646469101
                        },
                        "total_energy_joules": {
                            "0": 23843.668523807046,
                            "1": 24117.086042693038,
                            "2": 23796.581048604978,
                            "3": 23337.592727288764
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 359.79968437476276,
                        "ram_power_avg": 1.1675745248794556,
                        "cpu_energy_total": 0.004893204191197584,
                        "gpu_energy_total": 0.021483906909328,
                        "ram_energy_total": 3.8146772361594864e-05,
                        "total_energy_kwh": 0.026415257872887177,
                        "total_energy_joules": 95094.92834239382
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10515807913535119,
                        "joules_per_token": 9.509492834239381,
                        "flops_per_joule": 108790673.28123689,
                        "joules_per_flop": 9.1919644381176e-09
                    },
                    "per-process_emissions": [
                        0.0025231237567067484,
                        0.0025520566466566428,
                        0.0025181409862405743,
                        0.002469571097072404
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0092": {
            "setup": {
                "experiment_id": "0092",
                "date_time": "April 04, 2025 at 12:56:26 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "max_throughput_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 10.927333006169647,
                        "average_latency_ms_per_batch": 10927.333006169647,
                        "throughput_queries_per_sec": 9.151363827160692,
                        "throughput_tokens_per_sec": 915.1363827160691
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1577349632,
                        "gpu_max_memory_allocated_bytes": 1577349632,
                        "gpu_current_memory_reserved_bytes": 2839543808,
                        "gpu_max_memory_reserved_bytes": 2839543808
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            16.0,
                            100.0,
                            100.0,
                            99.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 3399163904
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0092",
                    "local_process_results": {
                        "cpu_power": {
                            "2": 112.5,
                            "0": 112.5,
                            "1": 112.5,
                            "3": 112.5
                        },
                        "gpu_power": {
                            "2": 532.2616983374185,
                            "0": 497.65564713974027,
                            "1": 578.9239401520194,
                            "3": 682.1621860242109
                        },
                        "ram_power": {
                            "2": 1.2344040870666504,
                            "0": 1.1856708526611328,
                            "1": 1.23468017578125,
                            "3": 1.2285075187683105
                        },
                        "cpu_energy": {
                            "2": 0.0003689300735350117,
                            "0": 0.00037372917922766645,
                            "1": 0.0003563877423439409,
                            "3": 0.0003507554286188679
                        },
                        "gpu_energy": {
                            "2": 0.0017839700382715762,
                            "0": 0.0018009214407470608,
                            "1": 0.00173507638806214,
                            "3": 0.0017040860854784512
                        },
                        "ram_energy": {
                            "2": 3.353604580355292e-06,
                            "0": 3.2513670180368115e-06,
                            "1": 3.1909587065125108e-06,
                            "3": 3.0935232677732965e-06
                        },
                        "total_energy_kwh": {
                            "2": 0.0021562537163869427,
                            "0": 0.0021779019869927643,
                            "1": 0.0020946550891125936,
                            "3": 0.0020579350373650927
                        },
                        "total_energy_joules": {
                            "2": 7762.513378992994,
                            "0": 7840.447153173952,
                            "1": 7540.758320805337,
                            "3": 7408.5661345143335
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 572.7508679133473,
                        "ram_power_avg": 1.220815658569336,
                        "cpu_energy_total": 0.0014498024237254868,
                        "gpu_energy_total": 0.007024053952559228,
                        "ram_energy_total": 1.288945357267791e-05,
                        "total_energy_kwh": 0.008486745829857393,
                        "total_energy_joules": 30552.28498748662
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.32730776123932226,
                        "joules_per_token": 3.055228498748662,
                        "flops_per_joule": 338614322.4389669,
                        "joules_per_flop": 2.9532123532082546e-09
                    },
                    "per-process_emissions": [
                        0.0008214248532576058,
                        0.0008296717619448936,
                        0.0007979588561974426,
                        0.0007839703524842321
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0093": {
            "setup": {
                "experiment_id": "0093",
                "date_time": "April 04, 2025 at 12:57:02 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precision_gaming",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 4.320917370961979,
                        "average_latency_ms_per_batch": 4320.917370961979,
                        "throughput_queries_per_sec": 23.14323358091356,
                        "throughput_tokens_per_sec": 2314.3233580913557
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087586816,
                        "gpu_max_memory_allocated_bytes": 1087586816,
                        "gpu_current_memory_reserved_bytes": 1994391552,
                        "gpu_max_memory_reserved_bytes": 1994391552
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            14.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 3359608832
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0093",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "0": 112.5,
                            "2": 112.5,
                            "3": 112.5
                        },
                        "gpu_power": {
                            "1": 685.1449746137129,
                            "0": 788.168374010509,
                            "2": 763.0759762210591,
                            "3": 887.0536553692731
                        },
                        "ram_power": {
                            "1": 1.2211790084838867,
                            "0": 1.172490119934082,
                            "2": 1.2147002220153809,
                            "3": 1.2042388916015625
                        },
                        "cpu_energy": {
                            "1": 0.00017300573868851644,
                            "0": 0.00016767169902595926,
                            "2": 0.0001724897374660941,
                            "3": 0.0001685211619333131
                        },
                        "gpu_energy": {
                            "1": 0.001019299982122135,
                            "0": 0.0009828227307089321,
                            "2": 0.0010149466453022171,
                            "3": 0.000995525518650453
                        },
                        "ram_energy": {
                            "1": 1.362241572357795e-06,
                            "0": 1.3040478589156184e-06,
                            "2": 1.3535469261447757e-06,
                            "3": 1.3027650223444902e-06
                        },
                        "total_energy_kwh": {
                            "1": 0.001193667962383009,
                            "0": 0.001151798477593807,
                            "2": 0.0011887899296944562,
                            "3": 0.0011653494456061106
                        },
                        "total_energy_joules": {
                            "1": 4297.204664578832,
                            "0": 4146.474519337706,
                            "2": 4279.643746900042,
                            "3": 4195.258004181998
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 780.8607450536385,
                        "ram_power_avg": 1.203152060508728,
                        "cpu_energy_total": 0.000681688337113883,
                        "gpu_energy_total": 0.004012594876783737,
                        "ram_energy_total": 5.322601379762679e-06,
                        "total_energy_kwh": 0.004699605815277383,
                        "total_energy_joules": 16918.58093499858
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5910661206409767,
                        "joules_per_token": 1.691858093499858,
                        "flops_per_joule": 611483984.3688622,
                        "joules_per_flop": 1.6353658077114502e-09
                    },
                    "per-process_emissions": [
                        0.0004547278102698073,
                        0.0004387776300393608,
                        0.0004528695237171031,
                        0.00044393987130364785
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0119": {
            "setup": {
                "experiment_id": "0119",
                "date_time": "April 05, 2025 at 04:42:46 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 3,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.628509049071,
                        "average_latency_ms_per_batch": 3232.6441498672857,
                        "throughput_queries_per_sec": 4.419204101478592,
                        "throughput_tokens_per_sec": 441.92041014785923
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409393664,
                        "gpu_max_memory_allocated_bytes": 4409393664,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 98.2,
                        "cpu_memory_usage_bytes": 2227482624
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0119",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 185.2060006811357
                        },
                        "ram_power": {
                            "0": 0.7761111259460449
                        },
                        "cpu_energy": {
                            "0": 0.0007598891479137821
                        },
                        "gpu_energy": {
                            "0": 0.0012694860155804122
                        },
                        "ram_energy": {
                            "0": 3.857103002736589e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.002033232266496931
                        },
                        "total_energy_joules": {
                            "0": 7319.636159388952
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 185.2060006811357,
                        "ram_power_avg": 0.7761111259460449,
                        "cpu_energy_total": 0.0007598891479137821,
                        "gpu_energy_total": 0.0012694860155804122,
                        "ram_energy_total": 3.857103002736589e-06,
                        "total_energy_kwh": 0.002033232266496931,
                        "total_energy_joules": 7319.636159388952
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.3661881249620482,
                        "joules_per_token": 0.7319636159388951,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.000774559831922006
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0120": {
            "setup": {
                "experiment_id": "0120",
                "date_time": "April 05, 2025 at 05:41:41 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 31.65037827193737,
                        "average_latency_ms_per_batch": 4521.482610276767,
                        "throughput_queries_per_sec": 3.1595199002302112,
                        "throughput_tokens_per_sec": 315.9519900230211
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409393664,
                        "gpu_max_memory_allocated_bytes": 4409393664,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 96.2,
                        "cpu_memory_usage_bytes": 2253213696
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0120",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 371.5863259129838
                        },
                        "ram_power": {
                            "0": 0.7844738960266113
                        },
                        "cpu_energy": {
                            "0": 0.0010332239306080737
                        },
                        "gpu_energy": {
                            "0": 0.0029977882315606053
                        },
                        "ram_energy": {
                            "0": 5.324452219126901e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.004036336614387805
                        },
                        "total_energy_joules": {
                            "0": 14530.811811796098
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 371.5863259129838,
                        "ram_power_avg": 0.7844738960266113,
                        "cpu_energy_total": 0.0010332239306080737,
                        "gpu_energy_total": 0.0029977882315606053,
                        "ram_energy_total": 5.324452219126901e-06,
                        "total_energy_kwh": 0.004036336614387805,
                        "total_energy_joules": 14530.811811796098
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.6881927953868352,
                        "joules_per_token": 1.4530811811796098,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0015376424332510343
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0121": {
            "setup": {
                "experiment_id": "0121",
                "date_time": "April 05, 2025 at 05:47:54 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 31.614870293997228,
                        "average_latency_ms_per_batch": 4516.410041999604,
                        "throughput_queries_per_sec": 3.163068488659502,
                        "throughput_tokens_per_sec": 316.3068488659502
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409393664,
                        "gpu_max_memory_allocated_bytes": 4409393664,
                        "gpu_current_memory_reserved_bytes": 4758437888,
                        "gpu_max_memory_reserved_bytes": 4758437888
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.2,
                        "cpu_memory_usage_bytes": 2236416000
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0121",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 346.73292372025566
                        },
                        "ram_power": {
                            "0": 0.778597354888916
                        },
                        "cpu_energy": {
                            "0": 0.00103551360018173
                        },
                        "gpu_energy": {
                            "0": 0.0030065862941626165
                        },
                        "ram_energy": {
                            "0": 5.2957843567891575e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.004047395678701137
                        },
                        "total_energy_joules": {
                            "0": 14570.624443324092
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 346.73292372025566,
                        "ram_power_avg": 0.778597354888916,
                        "cpu_energy_total": 0.00103551360018173,
                        "gpu_energy_total": 0.0030065862941626165,
                        "ram_energy_total": 5.2957843567891575e-06,
                        "total_energy_kwh": 0.004047395678701137,
                        "total_energy_joules": 14570.624443324092
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.6863123841327032,
                        "joules_per_token": 1.4570624443324092,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.001541855383801198
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0122": {
            "setup": {
                "experiment_id": "0122",
                "date_time": "April 05, 2025 at 06:09:01 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a1_max_throughput_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 36.36684393091127,
                        "average_latency_ms_per_batch": 36366.84393091127,
                        "throughput_queries_per_sec": 2.7497574491197874,
                        "throughput_tokens_per_sec": 274.9757449119787
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1577309696,
                        "gpu_max_memory_allocated_bytes": 1577309696,
                        "gpu_current_memory_reserved_bytes": 2839543808,
                        "gpu_max_memory_reserved_bytes": 2839543808
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            99.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.9,
                        "cpu_memory_usage_bytes": 3422539776
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0122",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "1": 112.5,
                            "2": 112.5,
                            "3": 112.5
                        },
                        "gpu_power": {
                            "0": 290.65734905651175,
                            "1": 450.6033992072865,
                            "2": 444.4435598430644,
                            "3": 486.001622441529
                        },
                        "ram_power": {
                            "0": 1.1947174072265625,
                            "1": 1.2390704154968262,
                            "2": 1.2388415336608887,
                            "3": 1.2373738288879397
                        },
                        "cpu_energy": {
                            "0": 0.0011996590157577884,
                            "1": 0.00039361671037477203,
                            "2": 0.0003931479242746718,
                            "3": 0.00039084582494251666
                        },
                        "gpu_energy": {
                            "0": 0.004290673432535641,
                            "1": 0.0013917277800530314,
                            "2": 0.0013917277800530314,
                            "3": 0.0013807744379406017
                        },
                        "ram_energy": {
                            "0": 1.0876825034780959e-05,
                            "1": 3.2026339919679852e-06,
                            "2": 3.274910594339892e-06,
                            "3": 3.1792081165297346e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.005501209273328211,
                            "1": 0.0017885471244197717,
                            "2": 0.0017881506149220428,
                            "3": 0.0017747994709996482
                        },
                        "total_energy_joules": {
                            "0": 19804.353383981557,
                            "1": 6438.769647911178,
                            "2": 6437.342213719354,
                            "3": 6389.278095598734
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 417.9264826370979,
                        "ram_power_avg": 1.2275007963180542,
                        "cpu_energy_total": 0.0023772694753497487,
                        "gpu_energy_total": 0.008454903430582306,
                        "ram_energy_total": 2.053357773761857e-05,
                        "total_energy_kwh": 0.010852706483669674,
                        "total_energy_joules": 39069.74334121082
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2559525388397416,
                        "joules_per_token": 3.9069743341210823,
                        "flops_per_joule": 264794196.10334665,
                        "joules_per_flop": 3.776517819181012e-09
                    },
                    "per-process_emissions": [
                        0.002095685672674382,
                        0.0006813470270477121,
                        0.0006811959767545522,
                        0.000676109858477316
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0124": {
            "setup": {
                "experiment_id": "0124",
                "date_time": "April 05, 2025 at 06:17:28 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a1_max_throughput_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.25637705484405,
                        "average_latency_ms_per_batch": 37256.37705484405,
                        "throughput_queries_per_sec": 2.6841042502010555,
                        "throughput_tokens_per_sec": 268.41042502010555
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1577404928,
                        "gpu_max_memory_allocated_bytes": 1577404928,
                        "gpu_current_memory_reserved_bytes": 2839543808,
                        "gpu_max_memory_reserved_bytes": 2839543808
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            98.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.8,
                        "cpu_memory_usage_bytes": 3436658688
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0124",
                    "local_process_results": {
                        "cpu_power": {
                            "2": 112.5,
                            "0": 112.5,
                            "1": 112.5,
                            "3": 112.5
                        },
                        "gpu_power": {
                            "2": 595.0825865209129,
                            "0": 299.1136797408185,
                            "1": 445.2028639072978,
                            "3": 27.295326119179556
                        },
                        "ram_power": {
                            "2": 1.2398314476013186,
                            "0": 1.1994538307189941,
                            "1": 1.2367100715637207,
                            "3": 1.2386183738708496
                        },
                        "cpu_energy": {
                            "2": 0.00038074608094029825,
                            "0": 0.001229183667433972,
                            "1": 0.00039119790872064184,
                            "3": 0.00041018209344474596
                        },
                        "gpu_energy": {
                            "2": 0.0013489216347082333,
                            "0": 0.004377320724088918,
                            "1": 0.0013898613896756729,
                            "3": 0.0013539977498737699
                        },
                        "ram_energy": {
                            "2": 3.345142708279406e-06,
                            "0": 1.1103330898489221e-05,
                            "1": 3.1374465945975467e-06,
                            "3": 3.245545617554173e-06
                        },
                        "total_energy_kwh": {
                            "2": 0.001733012858356811,
                            "0": 0.005617607722421379,
                            "1": 0.0017841967449909128,
                            "3": 0.0017674253889360702
                        },
                        "total_energy_joules": {
                            "2": 6238.846290084519,
                            "0": 20223.387800716966,
                            "1": 6423.108281967286,
                            "3": 6362.731400169852
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 341.6736140720522,
                        "ram_power_avg": 1.2286534309387207,
                        "cpu_energy_total": 0.002411309750539658,
                        "gpu_energy_total": 0.008470101498346594,
                        "ram_energy_total": 2.0831465818920347e-05,
                        "total_energy_kwh": 0.010902242714705173,
                        "total_energy_joules": 39248.073772938624
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.25478957407828146,
                        "joules_per_token": 3.924807377293862,
                        "flops_per_joule": 263591057.73830706,
                        "joules_per_flop": 3.793755404983423e-09
                    },
                    "per-process_emissions": [
                        0.0006601912483910271,
                        0.0021400276618564243,
                        0.0006796897500042882,
                        0.0006733007019151959
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0125": {
            "setup": {
                "experiment_id": "0125",
                "date_time": "April 05, 2025 at 06:18:10 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a2_precision_minimalist",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 6.317839460913092,
                        "average_latency_ms_per_batch": 6317.839460913092,
                        "throughput_queries_per_sec": 15.828195796787055,
                        "throughput_tokens_per_sec": 1582.8195796787056
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087586816,
                        "gpu_max_memory_allocated_bytes": 1087586816,
                        "gpu_current_memory_reserved_bytes": 1994391552,
                        "gpu_max_memory_reserved_bytes": 1994391552
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            1.0,
                            100.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.8,
                        "cpu_memory_usage_bytes": 3362811904
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0125",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "1": 448.4152318994262,
                            "0": 261.6560446261878
                        },
                        "ram_power": {
                            "1": 1.2146544456481934,
                            "0": 1.1735501289367676
                        },
                        "cpu_energy": {
                            "1": 0.00021301798112108373,
                            "0": 0.000269380121055292
                        },
                        "gpu_energy": {
                            "1": 0.0006681258122824829,
                            "0": 0.0008051059218772139
                        },
                        "ram_energy": {
                            "1": 1.7439886649277733e-06,
                            "0": 2.1401058809714212e-06
                        },
                        "total_energy_kwh": {
                            "1": 0.0008828877820684943,
                            "0": 0.0010766261488134774
                        },
                        "total_energy_joules": {
                            "1": 3178.3960154465794,
                            "0": 3875.8541357285185
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 355.03563826280697,
                        "ram_power_avg": 1.1941022872924805,
                        "cpu_energy_total": 0.00048239810217637574,
                        "gpu_energy_total": 0.0014732317341596968,
                        "ram_energy_total": 3.884094545899195e-06,
                        "total_energy_kwh": 0.0019595139308819717,
                        "total_energy_joules": 7054.250151175098
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.4175851133283384,
                        "joules_per_token": 0.7054250151175099,
                        "flops_per_joule": 1466554354.9340472,
                        "joules_per_flop": 6.818703968493355e-10
                    },
                    "per-process_emissions": [
                        0.0003363361005789929,
                        0.0004101407313904942
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0126": {
            "setup": {
                "experiment_id": "0126",
                "date_time": "April 05, 2025 at 06:18:50 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a3_quantisation_gaming",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.7,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 10.190055949846283,
                        "average_latency_ms_per_batch": 5095.027974923141,
                        "throughput_queries_per_sec": 9.813488806360136,
                        "throughput_tokens_per_sec": 981.3488806360136
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087586816,
                        "gpu_max_memory_allocated_bytes": 1087586816,
                        "gpu_current_memory_reserved_bytes": 1692401664,
                        "gpu_max_memory_reserved_bytes": 1692401664
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.4,
                        "cpu_memory_usage_bytes": 3289481216
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0126",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 284.73219648652224
                        },
                        "ram_power": {
                            "0": 1.1479883193969727
                        },
                        "cpu_energy": {
                            "0": 0.0003818850605966873
                        },
                        "gpu_energy": {
                            "0": 0.0008014242522520476
                        },
                        "ram_energy": {
                            "0": 2.8393148207246825e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0011861486276694593
                        },
                        "total_energy_joules": {
                            "0": 4270.135059610054
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 284.73219648652224,
                        "ram_power_avg": 1.1479883193969727,
                        "cpu_energy_total": 0.0003818850605966873,
                        "gpu_energy_total": 0.0008014242522520476,
                        "ram_energy_total": 2.8393148207246825e-06,
                        "total_energy_kwh": 0.0011861486276694593,
                        "total_energy_joules": 4270.135059610054
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 2.3418463023774225,
                        "joules_per_token": 0.4270135059610054,
                        "flops_per_joule": 2422743340.8030753,
                        "joules_per_flop": 4.127552362474048e-10
                    },
                    "per-process_emissions": [
                        0.00045186331971068056
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0127": {
            "setup": {
                "experiment_id": "0127",
                "date_time": "April 05, 2025 at 06:21:23 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a4_latency_ignorance_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 120.91823650780134,
                        "average_latency_ms_per_batch": 30229.559126950335,
                        "throughput_queries_per_sec": 0.8270051142661864,
                        "throughput_tokens_per_sec": 82.70051142661865
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576112640,
                        "gpu_max_memory_allocated_bytes": 1576112640,
                        "gpu_current_memory_reserved_bytes": 2594177024,
                        "gpu_max_memory_reserved_bytes": 2594177024
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 98.6,
                        "cpu_memory_usage_bytes": 3277504512
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0127",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 235.17880148906795
                        },
                        "ram_power": {
                            "0": 1.1440558433532715
                        },
                        "cpu_energy": {
                            "0": 0.0037586710868708914
                        },
                        "gpu_energy": {
                            "0": 0.007225520780409056
                        },
                        "ram_energy": {
                            "0": 3.018583193170477e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.011014377699211656
                        },
                        "total_energy_joules": {
                            "0": 39651.75971716196
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 235.17880148906795,
                        "ram_power_avg": 1.1440558433532715,
                        "cpu_energy_total": 0.0037586710868708914,
                        "gpu_energy_total": 0.007225520780409056,
                        "ram_energy_total": 3.018583193170477e-05,
                        "total_energy_kwh": 0.011014377699211656,
                        "total_energy_joules": 39651.75971716196
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2521956168233267,
                        "joules_per_token": 3.9651759717161967,
                        "flops_per_joule": 260907494.49191067,
                        "joules_per_flop": 3.832776064740465e-09
                    },
                    "per-process_emissions": [
                        0.004195927184514681
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0128": {
            "setup": {
                "experiment_id": "0128",
                "date_time": "April 05, 2025 at 06:22:06 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a5_parallel_overdrive",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 7.56496477406472,
                        "average_latency_ms_per_batch": 3782.48238703236,
                        "throughput_queries_per_sec": 13.218832206969967,
                        "throughput_tokens_per_sec": 1321.8832206969967
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419751424,
                        "gpu_max_memory_allocated_bytes": 4419751424,
                        "gpu_current_memory_reserved_bytes": 6838812672,
                        "gpu_max_memory_reserved_bytes": 6838812672
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 97.1,
                        "cpu_memory_usage_bytes": 3739082752
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0128",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "2": 112.5,
                            "3": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "1": 461.7911885181225,
                            "2": 1045.0113287841723,
                            "3": 469.8980305110045,
                            "0": 406.65594669624005
                        },
                        "ram_power": {
                            "1": 1.110795021057129,
                            "2": 1.0932841300964355,
                            "3": 1.1054363250732422,
                            "0": 1.3033103942871096
                        },
                        "cpu_energy": {
                            "1": 0.0002507566349086119,
                            "2": 0.0002550157724836027,
                            "3": 0.00024570981232682244,
                            "0": 0.0003077728143325658
                        },
                        "gpu_energy": {
                            "1": 0.0011775909420705943,
                            "2": 0.0012002212379584876,
                            "3": 0.0011598831501302698,
                            "0": 0.0013888122221743515
                        },
                        "ram_energy": {
                            "1": 1.9113951363240216e-06,
                            "2": 2.197398063789536e-06,
                            "3": 1.8317844892583412e-06,
                            "0": 2.7278106454838328e-06
                        },
                        "total_energy_kwh": {
                            "1": 0.0014302589721155304,
                            "2": 0.00145743440850588,
                            "3": 0.0014074247469463507,
                            "0": 0.0016993128471524009
                        },
                        "total_energy_joules": {
                            "1": 5148.93229961591,
                            "2": 5246.763870621168,
                            "3": 5066.7290890068625,
                            "0": 6117.526249748643
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 595.8391236273848,
                        "ram_power_avg": 1.153206467628479,
                        "cpu_energy_total": 0.001059255034051603,
                        "gpu_energy_total": 0.004926507552333703,
                        "ram_energy_total": 8.668388334855731e-06,
                        "total_energy_kwh": 0.005994430974720162,
                        "total_energy_joules": 21579.951508992584
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.4633930709173697,
                        "joules_per_token": 2.1579951508992585,
                        "flops_per_joule": 479400580.4734524,
                        "joules_per_flop": 2.085938233559099e-09
                    },
                    "per-process_emissions": [
                        0.0005448571554274113,
                        0.000555209637920315,
                        0.0005361584573492123,
                        0.0006473532291227072
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0129": {
            "setup": {
                "experiment_id": "0129",
                "date_time": "April 05, 2025 at 06:42:45 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a1_max_throughput_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 32.217356570996344,
                        "average_latency_ms_per_batch": 32217.356570996344,
                        "throughput_queries_per_sec": 3.1039169765413015,
                        "throughput_tokens_per_sec": 310.39169765413016
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1314735616,
                        "gpu_max_memory_allocated_bytes": 1314735616,
                        "gpu_current_memory_reserved_bytes": 2483027968,
                        "gpu_max_memory_reserved_bytes": 2483027968
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            98.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.3,
                        "cpu_memory_usage_bytes": 2923040768
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0129",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 188.10170968196826
                        },
                        "ram_power": {
                            "0": 1.0191736221313477
                        },
                        "cpu_energy": {
                            "0": 0.001053814947597857
                        },
                        "gpu_energy": {
                            "0": 0.003007071294554464
                        },
                        "ram_energy": {
                            "0": 8.019938301390748e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0040689061804537125
                        },
                        "total_energy_joules": {
                            "0": 14648.062249633365
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 188.10170968196826,
                        "ram_power_avg": 1.0191736221313477,
                        "cpu_energy_total": 0.001053814947597857,
                        "gpu_energy_total": 0.003007071294554464,
                        "ram_energy_total": 8.019938301390748e-06,
                        "total_energy_kwh": 0.0040689061804537125,
                        "total_energy_joules": 14648.062249633365
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.6826841550492657,
                        "joules_per_token": 1.4648062249633365,
                        "flops_per_joule": 706266883.8848593,
                        "joules_per_flop": 1.4158953545994478e-09
                    },
                    "per-process_emissions": [
                        0.0015500498094438418
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0131": {
            "setup": {
                "experiment_id": "0131",
                "date_time": "April 05, 2025 at 06:43:47 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a1_max_throughput_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 33.96215915190987,
                        "average_latency_ms_per_batch": 33962.15915190987,
                        "throughput_queries_per_sec": 2.944453547629538,
                        "throughput_tokens_per_sec": 294.4453547629538
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1315062784,
                        "gpu_max_memory_allocated_bytes": 1315062784,
                        "gpu_current_memory_reserved_bytes": 2483027968,
                        "gpu_max_memory_reserved_bytes": 2483027968
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            80.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 95.9,
                        "cpu_memory_usage_bytes": 2837876736
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0131",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 3.2185448760327535
                        },
                        "ram_power": {
                            "0": 0.989978313446045
                        },
                        "cpu_energy": {
                            "0": 0.0011462497281536344
                        },
                        "gpu_energy": {
                            "0": 0.0031820136567404234
                        },
                        "ram_energy": {
                            "0": 8.494385381593896e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.004336757770275652
                        },
                        "total_energy_joules": {
                            "0": 15612.327972992349
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 3.2185448760327535,
                        "ram_power_avg": 0.989978313446045,
                        "cpu_energy_total": 0.0011462497281536344,
                        "gpu_energy_total": 0.0031820136567404234,
                        "ram_energy_total": 8.494385381593896e-06,
                        "total_energy_kwh": 0.004336757770275652,
                        "total_energy_joules": 15612.327972992349
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.6405194675194453,
                        "joules_per_token": 1.5612327972992348,
                        "flops_per_joule": 662645653.9919288,
                        "joules_per_flop": 1.509102178480718e-09
                    },
                    "per-process_emissions": [
                        0.0016520878725865098
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0132": {
            "setup": {
                "experiment_id": "0132",
                "date_time": "April 05, 2025 at 06:44:04 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a2_precision_minimalist",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 6.0529720729682595,
                        "average_latency_ms_per_batch": 6052.972072968259,
                        "throughput_queries_per_sec": 16.52080974346243,
                        "throughput_tokens_per_sec": 1652.080974346243
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825651200,
                        "gpu_max_memory_allocated_bytes": 825651200,
                        "gpu_current_memory_reserved_bytes": 2571108352,
                        "gpu_max_memory_reserved_bytes": 2571108352
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 98.3,
                        "cpu_memory_usage_bytes": 4667838464
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0132",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 275.4018768133875
                        },
                        "ram_power": {
                            "0": 1.6302237510681152
                        },
                        "cpu_energy": {
                            "0": 0.00023772209729213499
                        },
                        "gpu_energy": {
                            "0": 0.0005687565661176563
                        },
                        "ram_energy": {
                            "0": 2.768345179375949e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0008092470085891673
                        },
                        "total_energy_joules": {
                            "0": 2913.2892309210024
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 275.4018768133875,
                        "ram_power_avg": 1.6302237510681152,
                        "cpu_energy_total": 0.00023772209729213499,
                        "gpu_energy_total": 0.0005687565661176563,
                        "ram_energy_total": 2.768345179375949e-06,
                        "total_energy_kwh": 0.0008092470085891673,
                        "total_energy_joules": 2913.2892309210024
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 3.432546241499893,
                        "joules_per_token": 0.29132892309210023,
                        "flops_per_joule": 3551120558.232184,
                        "joules_per_flop": 2.816012533513701e-10
                    },
                    "per-process_emissions": [
                        0.0003082826479220433
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0133": {
            "setup": {
                "experiment_id": "0133",
                "date_time": "April 05, 2025 at 06:44:24 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a3_quantisation_gaming",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.7,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 10.083974407054484,
                        "average_latency_ms_per_batch": 5041.987203527242,
                        "throughput_queries_per_sec": 9.916724890737786,
                        "throughput_tokens_per_sec": 991.6724890737785
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825782272,
                        "gpu_max_memory_allocated_bytes": 825782272,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 98.0,
                        "cpu_memory_usage_bytes": 4931371008
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0133",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 212.89443158699476
                        },
                        "ram_power": {
                            "0": 1.7222614288330078
                        },
                        "cpu_energy": {
                            "0": 0.00035511005121952624
                        },
                        "gpu_energy": {
                            "0": 0.0007002733379977144
                        },
                        "ram_energy": {
                            "0": 4.130952771988053e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0010595143419892285
                        },
                        "total_energy_joules": {
                            "0": 3814.2516311612226
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 212.89443158699476,
                        "ram_power_avg": 1.7222614288330078,
                        "cpu_energy_total": 0.00035511005121952624,
                        "gpu_energy_total": 0.0007002733379977144,
                        "ram_energy_total": 4.130952771988053e-06,
                        "total_energy_kwh": 0.0010595143419892285,
                        "total_energy_joules": 3814.2516311612226
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 2.6217462734506505,
                        "joules_per_token": 0.38142516311612223,
                        "flops_per_joule": 2712312212.3042526,
                        "joules_per_flop": 3.686891189972732e-10
                    },
                    "per-process_emissions": [
                        0.0004036219885807966
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0134": {
            "setup": {
                "experiment_id": "0134",
                "date_time": "April 05, 2025 at 06:46:40 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a4_latency_ignorance_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 124.64755302597769,
                        "average_latency_ms_per_batch": 31161.88825649442,
                        "throughput_queries_per_sec": 0.8022620386230854,
                        "throughput_tokens_per_sec": 80.22620386230854
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1314823680,
                        "gpu_max_memory_allocated_bytes": 1314823680,
                        "gpu_current_memory_reserved_bytes": 2197815296,
                        "gpu_max_memory_reserved_bytes": 2197815296
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.8,
                        "cpu_memory_usage_bytes": 5167869952
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0134",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 230.72097755241953
                        },
                        "ram_power": {
                            "0": 1.8048577308654785
                        },
                        "cpu_energy": {
                            "0": 0.0038512891030841262
                        },
                        "gpu_energy": {
                            "0": 0.00746730180717492
                        },
                        "ram_energy": {
                            "0": 5.169707821870653e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.011370287988477756
                        },
                        "total_energy_joules": {
                            "0": 40933.03675851992
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 230.72097755241953,
                        "ram_power_avg": 1.8048577308654785,
                        "cpu_energy_total": 0.0038512891030841262,
                        "gpu_energy_total": 0.00746730180717492,
                        "ram_energy_total": 5.169707821870653e-05,
                        "total_energy_kwh": 0.011370287988477756,
                        "total_energy_joules": 40933.03675851992
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.24430144430753897,
                        "joules_per_token": 4.093303675851992,
                        "flops_per_joule": 252740624.67028347,
                        "joules_per_flop": 3.9566254981943045e-09
                    },
                    "per-process_emissions": [
                        0.004331511209210602
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0135": {
            "setup": {
                "experiment_id": "0135",
                "date_time": "April 05, 2025 at 06:46:56 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a5_parallel_overdrive",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 7.214314933866262,
                        "average_latency_ms_per_batch": 3607.157466933131,
                        "throughput_queries_per_sec": 13.861329996916071,
                        "throughput_tokens_per_sec": 1386.132999691607
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 3517018112,
                        "gpu_max_memory_allocated_bytes": 3517018112,
                        "gpu_current_memory_reserved_bytes": 4183818240,
                        "gpu_max_memory_reserved_bytes": 4183818240
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 98.0,
                        "cpu_memory_usage_bytes": 5514166272
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0135",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 365.98314639306454
                        },
                        "ram_power": {
                            "0": 1.9242839813232422
                        },
                        "cpu_energy": {
                            "0": 0.00025649574837734694
                        },
                        "gpu_energy": {
                            "0": 0.0007528375466989701
                        },
                        "ram_energy": {
                            "0": 3.4780978697037264e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0010128113929460207
                        },
                        "total_energy_joules": {
                            "0": 3646.1210146056746
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 365.98314639306454,
                        "ram_power_avg": 1.9242839813232422,
                        "cpu_energy_total": 0.00025649574837734694,
                        "gpu_energy_total": 0.0007528375466989701,
                        "ram_energy_total": 3.4780978697037264e-06,
                        "total_energy_kwh": 0.0010128113929460207,
                        "total_energy_joules": 3646.1210146056746
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 2.742640729680085,
                        "joules_per_token": 0.3646121014605675,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0003858305001427866
                    ]
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0138": {
            "setup": {
                "experiment_id": "0138",
                "date_time": "April 05, 2025 at 07:32:16 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "na",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 32.027431403286755,
                        "average_latency_ms_per_batch": 4575.347343326679,
                        "throughput_queries_per_sec": 3.1223234464483993,
                        "throughput_tokens_per_sec": 312.23234464483994
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409393664,
                        "gpu_max_memory_allocated_bytes": 4409393664,
                        "gpu_current_memory_reserved_bytes": 4758437888,
                        "gpu_max_memory_reserved_bytes": 4758437888
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 98.6,
                        "cpu_memory_usage_bytes": 2252906496
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0138",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 385.95843507085215
                        },
                        "ram_power": {
                            "0": 0.7839174270629884
                        },
                        "cpu_energy": {
                            "0": 0.00105452502125263
                        },
                        "gpu_energy": {
                            "0": 0.0030379652081435893
                        },
                        "ram_energy": {
                            "0": 5.453907211657688e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.004097944136607877
                        },
                        "total_energy_joules": {
                            "0": 14752.598891788357
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 385.95843507085215,
                        "ram_power_avg": 0.7839174270629884,
                        "cpu_energy_total": 0.00105452502125263,
                        "gpu_energy_total": 0.0030379652081435893,
                        "ram_energy_total": 5.453907211657688e-06,
                        "total_energy_kwh": 0.004097944136607877,
                        "total_energy_joules": 14752.598891788357
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.6778466677872084,
                        "joules_per_token": 1.4752598891788355,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0015611118188407708
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3227712,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 385.95843507085215,
                            "ram_power": 0.7839174270629884,
                            "cpu_energy": 0.00105452502125263,
                            "gpu_energy": 0.0030379652081435893,
                            "ram_energy": 5.453907211657688e-06,
                            "total_energy_kwh": 0.004097944136607877,
                            "total_energy_joules": 14752.598891788357,
                            "final_emissions": 0.0015611118188407708
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0139": {
            "setup": {
                "experiment_id": "0139",
                "date_time": "April 05, 2025 at 07:33:56 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a1_max_throughput_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 40.52234203601256,
                        "average_latency_ms_per_batch": 40522.34203601256,
                        "throughput_queries_per_sec": 2.467774441840729,
                        "throughput_tokens_per_sec": 246.7774441840729
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1315031040,
                        "gpu_max_memory_allocated_bytes": 1315031040,
                        "gpu_current_memory_reserved_bytes": 2483027968,
                        "gpu_max_memory_reserved_bytes": 2483027968
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 98.5,
                        "cpu_memory_usage_bytes": 3174780928
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0139",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 268.77462306341323
                        },
                        "ram_power": {
                            "0": 1.1076021194458008
                        },
                        "cpu_energy": {
                            "0": 0.0013055846045172076
                        },
                        "gpu_energy": {
                            "0": 0.003655189868588593
                        },
                        "ram_energy": {
                            "0": 1.1005898012116198e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.004971780371117916
                        },
                        "total_energy_joules": {
                            "0": 17898.4093360245
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 268.77462306341323,
                        "ram_power_avg": 1.1076021194458008,
                        "cpu_energy_total": 0.0013055846045172076,
                        "gpu_energy_total": 0.003655189868588593,
                        "ram_energy_total": 1.1005898012116198e-05,
                        "total_energy_kwh": 0.004971780371117916,
                        "total_energy_joules": 17898.4093360245
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5587088669311409,
                        "joules_per_token": 1.7898409336024497,
                        "flops_per_joule": 578008977.5451452,
                        "joules_per_flop": 1.730076934526325e-09
                    },
                    "per-process_emissions": [
                        0.0018939997323773703
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3228537,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 268.77462306341323,
                            "ram_power": 1.1076021194458008,
                            "cpu_energy": 0.0013055846045172076,
                            "gpu_energy": 0.003655189868588593,
                            "ram_energy": 1.1005898012116198e-05,
                            "total_energy_kwh": 0.004971780371117916,
                            "total_energy_joules": 17898.4093360245,
                            "final_emissions": 0.0018939997323773703
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0140": {
            "setup": {
                "experiment_id": "0140",
                "date_time": "April 05, 2025 at 07:34:13 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a2_precision_minimalist",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 6.113123851828277,
                        "average_latency_ms_per_batch": 6113.123851828277,
                        "throughput_queries_per_sec": 16.358248650580276,
                        "throughput_tokens_per_sec": 1635.8248650580274
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825651200,
                        "gpu_max_memory_allocated_bytes": 825651200,
                        "gpu_current_memory_reserved_bytes": 2571108352,
                        "gpu_max_memory_reserved_bytes": 2571108352
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 95.9,
                        "cpu_memory_usage_bytes": 5122183168
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0140",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 301.8877908898471
                        },
                        "ram_power": {
                            "0": 1.7889018058776855
                        },
                        "cpu_energy": {
                            "0": 0.00023686702199483988
                        },
                        "gpu_energy": {
                            "0": 0.0005577748906517854
                        },
                        "ram_energy": {
                            "0": 3.0431601428176015e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.000797685072789443
                        },
                        "total_energy_joules": {
                            "0": 2871.6662620419947
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 301.8877908898471,
                        "ram_power_avg": 1.7889018058776855,
                        "cpu_energy_total": 0.00023686702199483988,
                        "gpu_energy_total": 0.0005577748906517854,
                        "ram_energy_total": 3.0431601428176015e-06,
                        "total_energy_kwh": 0.000797685072789443,
                        "total_energy_joules": 2871.6662620419947
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 3.4822988075533416,
                        "joules_per_token": 0.28716662620419947,
                        "flops_per_joule": 3602591783.2957115,
                        "joules_per_flop": 2.77577938371131e-10
                    },
                    "per-process_emissions": [
                        0.00030387812847913835
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3228537,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 301.8877908898471,
                            "ram_power": 1.7889018058776855,
                            "cpu_energy": 0.00023686702199483988,
                            "gpu_energy": 0.0005577748906517854,
                            "ram_energy": 3.0431601428176015e-06,
                            "total_energy_kwh": 0.000797685072789443,
                            "total_energy_joules": 2871.6662620419947,
                            "final_emissions": 0.00030387812847913835
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0141": {
            "setup": {
                "experiment_id": "0141",
                "date_time": "April 05, 2025 at 07:34:33 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a3_quantisation_gaming",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.7,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 10.191092994762585,
                        "average_latency_ms_per_batch": 5095.546497381292,
                        "throughput_queries_per_sec": 9.812490186419856,
                        "throughput_tokens_per_sec": 981.2490186419856
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825782272,
                        "gpu_max_memory_allocated_bytes": 825782272,
                        "gpu_current_memory_reserved_bytes": 1931476992,
                        "gpu_max_memory_reserved_bytes": 1931476992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 98.2,
                        "cpu_memory_usage_bytes": 5890134016
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0141",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 238.01154797343534
                        },
                        "ram_power": {
                            "0": 2.057105541229248
                        },
                        "cpu_energy": {
                            "0": 0.00035425275729357973
                        },
                        "gpu_energy": {
                            "0": 0.0007136830709484343
                        },
                        "ram_energy": {
                            "0": 5.005783290095551e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0010729416115321097
                        },
                        "total_energy_joules": {
                            "0": 3862.589801515595
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 238.01154797343534,
                        "ram_power_avg": 2.057105541229248,
                        "cpu_energy_total": 0.00035425275729357973,
                        "gpu_energy_total": 0.0007136830709484343,
                        "ram_energy_total": 5.005783290095551e-06,
                        "total_energy_kwh": 0.0010729416115321097,
                        "total_energy_joules": 3862.589801515595
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 2.588936572057489,
                        "joules_per_token": 0.38625898015155946,
                        "flops_per_joule": 2678369128.3865237,
                        "joules_per_flop": 3.733615316132358e-10
                    },
                    "per-process_emissions": [
                        0.0004087371069131572
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3228537,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 238.01154797343534,
                            "ram_power": 2.057105541229248,
                            "cpu_energy": 0.00035425275729357973,
                            "gpu_energy": 0.0007136830709484343,
                            "ram_energy": 5.005783290095551e-06,
                            "total_energy_kwh": 0.0010729416115321097,
                            "total_energy_joules": 3862.589801515595,
                            "final_emissions": 0.0004087371069131572
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0142": {
            "setup": {
                "experiment_id": "0142",
                "date_time": "April 05, 2025 at 07:37:12 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a4_latency_ignorance_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 149.66027094284073,
                        "average_latency_ms_per_batch": 37415.06773571018,
                        "throughput_queries_per_sec": 0.6681800010785273,
                        "throughput_tokens_per_sec": 66.81800010785273
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1314790400,
                        "gpu_max_memory_allocated_bytes": 1314790400,
                        "gpu_current_memory_reserved_bytes": 2197815296,
                        "gpu_max_memory_reserved_bytes": 2197815296
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 96.2,
                        "cpu_memory_usage_bytes": 6233419776
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0142",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 214.66575416870475
                        },
                        "ram_power": {
                            "0": 2.1769967079162598
                        },
                        "cpu_energy": {
                            "0": 0.004617535697027051
                        },
                        "gpu_energy": {
                            "0": 0.008697843902716329
                        },
                        "ram_energy": {
                            "0": 7.496226956062784e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.013390341869304
                        },
                        "total_energy_joules": {
                            "0": 48205.230729494404
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 214.66575416870475,
                        "ram_power_avg": 2.1769967079162598,
                        "cpu_energy_total": 0.004617535697027051,
                        "gpu_energy_total": 0.008697843902716329,
                        "ram_energy_total": 7.496226956062784e-05,
                        "total_energy_kwh": 0.013390341869304,
                        "total_energy_joules": 48205.230729494404
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20744636730639052,
                        "joules_per_token": 4.820523072949441,
                        "flops_per_joule": 214612421.1717575,
                        "joules_per_flop": 4.659562547871752e-09
                    },
                    "per-process_emissions": [
                        0.005101050735111359
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3228537,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 214.66575416870475,
                            "ram_power": 2.1769967079162598,
                            "cpu_energy": 0.004617535697027051,
                            "gpu_energy": 0.008697843902716329,
                            "ram_energy": 7.496226956062784e-05,
                            "total_energy_kwh": 0.013390341869304,
                            "total_energy_joules": 48205.230729494404,
                            "final_emissions": 0.005101050735111359
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0143": {
            "setup": {
                "experiment_id": "0143",
                "date_time": "April 05, 2025 at 07:37:28 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a5_parallel_overdrive",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 7.216156058013439,
                        "average_latency_ms_per_batch": 3608.0780290067196,
                        "throughput_queries_per_sec": 13.857793428531997,
                        "throughput_tokens_per_sec": 1385.7793428531998
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 3516984832,
                        "gpu_max_memory_allocated_bytes": 3516984832,
                        "gpu_current_memory_reserved_bytes": 4179623936,
                        "gpu_max_memory_reserved_bytes": 4179623936
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 98.3,
                        "cpu_memory_usage_bytes": 6489305088
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0143",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 331.4716333890531
                        },
                        "ram_power": {
                            "0": 2.2648444175720215
                        },
                        "cpu_energy": {
                            "0": 0.00025886610687302894
                        },
                        "gpu_energy": {
                            "0": 0.0007393197581180289
                        },
                        "ram_energy": {
                            "0": 4.236614035819941e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0010024224790268778
                        },
                        "total_energy_joules": {
                            "0": 3608.7209244967603
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 331.4716333890531,
                        "ram_power_avg": 2.2648444175720215,
                        "cpu_energy_total": 0.00025886610687302894,
                        "gpu_energy_total": 0.0007393197581180289,
                        "ram_energy_total": 4.236614035819941e-06,
                        "total_energy_kwh": 0.0010024224790268778,
                        "total_energy_joules": 3608.7209244967603
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 2.7710649311000712,
                        "joules_per_token": 0.360872092449676,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.00038187284338528913
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3228537,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 331.4716333890531,
                            "ram_power": 2.2648444175720215,
                            "cpu_energy": 0.00025886610687302894,
                            "gpu_energy": 0.0007393197581180289,
                            "ram_energy": 4.236614035819941e-06,
                            "total_energy_kwh": 0.0010024224790268778,
                            "total_energy_joules": 3608.7209244967603,
                            "final_emissions": 0.00038187284338528913
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0144": {
            "setup": {
                "experiment_id": "0144",
                "date_time": "April 05, 2025 at 07:38:17 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r1_standard_production",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.54867582512088,
                        "average_latency_ms_per_batch": 5649.810832160126,
                        "throughput_queries_per_sec": 2.5285296641077704,
                        "throughput_tokens_per_sec": 252.85296641077704
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 6611588096,
                        "gpu_max_memory_allocated_bytes": 6611588096,
                        "gpu_current_memory_reserved_bytes": 6958350336,
                        "gpu_max_memory_reserved_bytes": 6958350336
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 98.2,
                        "cpu_memory_usage_bytes": 6565412864
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0144",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 0.0
                        },
                        "ram_power": {
                            "0": 2.2929439544677734
                        },
                        "cpu_energy": {
                            "0": 0.0012936895400926008
                        },
                        "gpu_energy": {
                            "0": 0.002859955065744657
                        },
                        "ram_energy": {
                            "0": 2.215216263405973e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.004175796768471318
                        },
                        "total_energy_joules": {
                            "0": 15032.868366496743
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 0.0,
                        "ram_power_avg": 2.2929439544677734,
                        "cpu_energy_total": 0.0012936895400926008,
                        "gpu_energy_total": 0.002859955065744657,
                        "ram_energy_total": 2.215216263405973e-05,
                        "total_energy_kwh": 0.004175796768471318,
                        "total_energy_joules": 15032.868366496743
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.665209044355545,
                        "joules_per_token": 1.5032868366496743,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0015907697789491486
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3228537,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 0.0,
                            "ram_power": 2.2929439544677734,
                            "cpu_energy": 0.0012936895400926008,
                            "gpu_energy": 0.002859955065744657,
                            "ram_energy": 2.215216263405973e-05,
                            "total_energy_kwh": 0.004175796768471318,
                            "total_energy_joules": 15032.868366496743,
                            "final_emissions": 0.0015907697789491486
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0145": {
            "setup": {
                "experiment_id": "0145",
                "date_time": "April 05, 2025 at 07:39:53 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r2_low_latency_chatbot",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.05,
                    "simulate_burst": false
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 84.07263463153504,
                        "average_latency_ms_per_batch": 3362.9053852614015,
                        "throughput_queries_per_sec": 1.1894476774549745,
                        "throughput_tokens_per_sec": 118.94476774549744
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8809593856,
                        "gpu_max_memory_allocated_bytes": 8809593856,
                        "gpu_current_memory_reserved_bytes": 8933867520,
                        "gpu_max_memory_reserved_bytes": 8933867520
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 96.4,
                        "cpu_memory_usage_bytes": 6467862528
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0145",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 141.02973451661316
                        },
                        "ram_power": {
                            "0": 2.2588748931884766
                        },
                        "cpu_energy": {
                            "0": 0.0026632633095869085
                        },
                        "gpu_energy": {
                            "0": 0.005218803341705325
                        },
                        "ram_energy": {
                            "0": 4.352074186260849e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.00792558739315484
                        },
                        "total_energy_joules": {
                            "0": 28532.114615357423
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 141.02973451661316,
                        "ram_power_avg": 2.2588748931884766,
                        "cpu_energy_total": 0.0026632633095869085,
                        "gpu_energy_total": 0.005218803341705325,
                        "ram_energy_total": 4.352074186260849e-05,
                        "total_energy_kwh": 0.00792558739315484,
                        "total_energy_joules": 28532.114615357423
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.35048225954544204,
                        "joules_per_token": 2.853211461535742,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0030192525174223364
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3228537,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 141.02973451661316,
                            "ram_power": 2.2588748931884766,
                            "cpu_energy": 0.0026632633095869085,
                            "gpu_energy": 0.005218803341705325,
                            "ram_energy": 4.352074186260849e-05,
                            "total_energy_kwh": 0.00792558739315484,
                            "total_energy_joules": 28532.114615357423,
                            "final_emissions": 0.0030192525174223364
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0146": {
            "setup": {
                "experiment_id": "0146",
                "date_time": "April 05, 2025 at 07:42:40 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r3_balanced_enterprise_service",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 156.23335747700185,
                        "average_latency_ms_per_batch": 39058.33936925046,
                        "throughput_queries_per_sec": 0.6400681750356699,
                        "throughput_tokens_per_sec": 64.00681750356699
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1313738752,
                        "gpu_max_memory_allocated_bytes": 1313738752,
                        "gpu_current_memory_reserved_bytes": 5773459456,
                        "gpu_max_memory_reserved_bytes": 5773459456
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 98.6,
                        "cpu_memory_usage_bytes": 6423539712
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0146",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 276.6724691841634
                        },
                        "ram_power": {
                            "0": 2.2433953285217285
                        },
                        "cpu_energy": {
                            "0": 0.004808907552236634
                        },
                        "gpu_energy": {
                            "0": 0.010457728088404394
                        },
                        "ram_energy": {
                            "0": 8.244150368786646e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.015349077144328881
                        },
                        "total_energy_joules": {
                            "0": 55256.677719583975
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 276.6724691841634,
                        "ram_power_avg": 2.2433953285217285,
                        "cpu_energy_total": 0.004808907552236634,
                        "gpu_energy_total": 0.010457728088404394,
                        "ram_energy_total": 8.244150368786646e-05,
                        "total_energy_kwh": 0.015349077144328881,
                        "total_energy_joules": 55256.677719583975
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1809736019734646,
                        "joules_per_token": 5.525667771958398,
                        "flops_per_joule": 187225177.244657,
                        "joules_per_flop": 5.341161988556951e-09
                    },
                    "per-process_emissions": [
                        0.005847230938132088
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3228537,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 276.6724691841634,
                            "ram_power": 2.2433953285217285,
                            "cpu_energy": 0.004808907552236634,
                            "gpu_energy": 0.010457728088404394,
                            "ram_energy": 8.244150368786646e-05,
                            "total_energy_kwh": 0.015349077144328881,
                            "total_energy_joules": 55256.677719583975,
                            "final_emissions": 0.005847230938132088
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0147": {
            "setup": {
                "experiment_id": "0147",
                "date_time": "April 05, 2025 at 07:43:43 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r4_high_load_api",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 8
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 51.92303602397442,
                        "average_latency_ms_per_batch": 3994.0796941518784,
                        "throughput_queries_per_sec": 1.925927442952816,
                        "throughput_tokens_per_sec": 192.5927442952816
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 5714192384,
                        "gpu_max_memory_allocated_bytes": 5714192384,
                        "gpu_current_memory_reserved_bytes": 5892997120,
                        "gpu_max_memory_reserved_bytes": 5892997120
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.4,
                        "cpu_memory_usage_bytes": 6605332480
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0147",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 0.0
                        },
                        "ram_power": {
                            "0": 2.3068857192993164
                        },
                        "cpu_energy": {
                            "0": 0.0016957925945025635
                        },
                        "gpu_energy": {
                            "0": 0.003288175686094519
                        },
                        "ram_energy": {
                            "0": 2.858740714752347e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.0050125556877446045
                        },
                        "total_energy_joules": {
                            "0": 18045.200475880574
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 0.0,
                        "ram_power_avg": 2.3068857192993164,
                        "cpu_energy_total": 0.0016957925945025635,
                        "gpu_energy_total": 0.003288175686094519,
                        "ram_energy_total": 2.858740714752347e-05,
                        "total_energy_kwh": 0.0050125556877446045,
                        "total_energy_joules": 18045.200475880574
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5541639735932065,
                        "joules_per_token": 1.8045200475880576,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0019095330892463071
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3228537,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 0.0,
                            "ram_power": 2.3068857192993164,
                            "cpu_energy": 0.0016957925945025635,
                            "gpu_energy": 0.003288175686094519,
                            "ram_energy": 2.858740714752347e-05,
                            "total_energy_kwh": 0.0050125556877446045,
                            "total_energy_joules": 18045.200475880574,
                            "final_emissions": 0.0019095330892463071
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0149": {
            "setup": {
                "experiment_id": "0149",
                "date_time": "April 06, 2025 at 02:48:03 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "default",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 20.020246438216418,
                        "average_latency_ms_per_batch": 2860.035205459488,
                        "throughput_queries_per_sec": 4.994943509242281,
                        "throughput_tokens_per_sec": 499.4943509242281
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409393664,
                        "gpu_max_memory_allocated_bytes": 4409393664,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 0.7,
                        "cpu_memory_usage_bytes": 2213134336
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0149",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 225.7496406565326
                        },
                        "ram_power": {
                            "0": 0.7698698043823242
                        },
                        "cpu_energy": {
                            "0": 0.0006435905322359758
                        },
                        "gpu_energy": {
                            "0": 0.0013180902211331613
                        },
                        "ram_energy": {
                            "0": 3.2672158400078436e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.001964947969209145
                        },
                        "total_energy_joules": {
                            "0": 7073.812689152923
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 225.7496406565326,
                        "ram_power_avg": 0.7698698043823242,
                        "cpu_energy_total": 0.0006435905322359758,
                        "gpu_energy_total": 0.0013180902211331613,
                        "ram_energy_total": 3.2672158400078436e-06,
                        "total_energy_kwh": 0.001964947969209145,
                        "total_energy_joules": 7073.812689152923
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.4136647999365506,
                        "joules_per_token": 0.7073812689152923,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0007485469288702239
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3456122,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 225.7496406565326,
                            "ram_power": 0.7698698043823242,
                            "cpu_energy": 0.0006435905322359758,
                            "gpu_energy": 0.0013180902211331613,
                            "ram_energy": 3.2672158400078436e-06,
                            "total_energy_kwh": 0.001964947969209145,
                            "total_energy_joules": 7073.812689152923,
                            "final_emissions": 0.0007485469288702239
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0150": {
            "setup": {
                "experiment_id": "0150",
                "date_time": "April 06, 2025 at 02:54:26 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "default",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 20.21898282156326,
                        "average_latency_ms_per_batch": 2888.42611736618,
                        "throughput_queries_per_sec": 4.9458472210259465,
                        "throughput_tokens_per_sec": 494.5847221025947
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409393664,
                        "gpu_max_memory_allocated_bytes": 4409393664,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 0.7,
                        "cpu_memory_usage_bytes": 2213433344
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0150",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 176.94750379441854
                        },
                        "ram_power": {
                            "0": 0.7702431678771973
                        },
                        "cpu_energy": {
                            "0": 0.0006511992077066679
                        },
                        "gpu_energy": {
                            "0": 0.0013208668900261955
                        },
                        "ram_energy": {
                            "0": 3.5486908311536318e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0019756147885640167
                        },
                        "total_energy_joules": {
                            "0": 7112.21323883046
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 176.94750379441854,
                        "ram_power_avg": 0.7702431678771973,
                        "cpu_energy_total": 0.0006511992077066679,
                        "gpu_energy_total": 0.0013208668900261955,
                        "ram_energy_total": 3.5486908311536318e-06,
                        "total_energy_kwh": 0.0019756147885640167,
                        "total_energy_joules": 7112.21323883046
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.406032083712441,
                        "joules_per_token": 0.711221323883046,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0007526104537034622
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3457931,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 176.94750379441854,
                            "ram_power": 0.7702431678771973,
                            "cpu_energy": 0.0006511992077066679,
                            "gpu_energy": 0.0013208668900261955,
                            "ram_energy": 3.5486908311536318e-06,
                            "total_energy_kwh": 0.0019756147885640167,
                            "total_energy_joules": 7112.21323883046,
                            "final_emissions": 0.0007526104537034622
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0150": {
            "setup": {
                "experiment_id": "0150",
                "date_time": "April 06, 2025 at 02:54:26 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "default",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 20.21898282156326,
                        "average_latency_ms_per_batch": 2888.42611736618,
                        "throughput_queries_per_sec": 4.9458472210259465,
                        "throughput_tokens_per_sec": 494.5847221025947
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409393664,
                        "gpu_max_memory_allocated_bytes": 4409393664,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 0.7,
                        "cpu_memory_usage_bytes": 2213433344
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0150",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 176.94750379441854
                        },
                        "ram_power": {
                            "0": 0.7702431678771973
                        },
                        "cpu_energy": {
                            "0": 0.0006511992077066679
                        },
                        "gpu_energy": {
                            "0": 0.0013208668900261955
                        },
                        "ram_energy": {
                            "0": 3.5486908311536318e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0019756147885640167
                        },
                        "total_energy_joules": {
                            "0": 7112.21323883046
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 176.94750379441854,
                        "ram_power_avg": 0.7702431678771973,
                        "cpu_energy_total": 0.0006511992077066679,
                        "gpu_energy_total": 0.0013208668900261955,
                        "ram_energy_total": 3.5486908311536318e-06,
                        "total_energy_kwh": 0.0019756147885640167,
                        "total_energy_joules": 7112.21323883046
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.406032083712441,
                        "joules_per_token": 0.711221323883046,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0007526104537034622
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3457931,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 176.94750379441854,
                            "ram_power": 0.7702431678771973,
                            "cpu_energy": 0.0006511992077066679,
                            "gpu_energy": 0.0013208668900261955,
                            "ram_energy": 3.5486908311536318e-06,
                            "total_energy_kwh": 0.0019756147885640167,
                            "total_energy_joules": 7112.21323883046,
                            "final_emissions": 0.0007526104537034622
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0151": {
            "setup": {
                "experiment_id": "0151",
                "date_time": "April 06, 2025 at 03:01:28 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a1_max_throughput_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 9.163760991068557,
                        "average_latency_ms_per_batch": 9163.760991068557,
                        "throughput_queries_per_sec": 10.912549999663328,
                        "throughput_tokens_per_sec": 1091.254999966333
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1314871296,
                        "gpu_max_memory_allocated_bytes": 1314871296,
                        "gpu_current_memory_reserved_bytes": 2480930816,
                        "gpu_max_memory_reserved_bytes": 2480930816
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 10.8,
                        "cpu_memory_usage_bytes": 3176497152
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0151",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 174.68016219131425
                        },
                        "ram_power": {
                            "0": 1.1090168952941895
                        },
                        "cpu_energy": {
                            "0": 0.00030794063448411177
                        },
                        "gpu_energy": {
                            "0": 0.00047086148779840187
                        },
                        "ram_energy": {
                            "0": 2.326309968120211e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0007811284322506339
                        },
                        "total_energy_joules": {
                            "0": 2812.062356102282
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 174.68016219131425,
                        "ram_power_avg": 1.1090168952941895,
                        "cpu_energy_total": 0.00030794063448411177,
                        "gpu_energy_total": 0.00047086148779840187,
                        "ram_energy_total": 2.326309968120211e-06,
                        "total_energy_kwh": 0.0007811284322506339,
                        "total_energy_joules": 2812.062356102282
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 3.556108909996117,
                        "joules_per_token": 0.28120623561022823,
                        "flops_per_joule": 3678951591.3649635,
                        "joules_per_flop": 2.7181656924955085e-10
                    },
                    "per-process_emissions": [
                        0.000297570876265879
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3460071,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 174.68016219131425,
                            "ram_power": 1.1090168952941895,
                            "cpu_energy": 0.00030794063448411177,
                            "gpu_energy": 0.00047086148779840187,
                            "ram_energy": 2.326309968120211e-06,
                            "total_energy_kwh": 0.0007811284322506339,
                            "total_energy_joules": 2812.062356102282,
                            "final_emissions": 0.000297570876265879
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0151": {
            "setup": {
                "experiment_id": "0151",
                "date_time": "April 06, 2025 at 03:01:28 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a1_max_throughput_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 9.163760991068557,
                        "average_latency_ms_per_batch": 9163.760991068557,
                        "throughput_queries_per_sec": 10.912549999663328,
                        "throughput_tokens_per_sec": 1091.254999966333
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1314871296,
                        "gpu_max_memory_allocated_bytes": 1314871296,
                        "gpu_current_memory_reserved_bytes": 2480930816,
                        "gpu_max_memory_reserved_bytes": 2480930816
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 10.8,
                        "cpu_memory_usage_bytes": 3176497152
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0151",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 174.68016219131425
                        },
                        "ram_power": {
                            "0": 1.1090168952941895
                        },
                        "cpu_energy": {
                            "0": 0.00030794063448411177
                        },
                        "gpu_energy": {
                            "0": 0.00047086148779840187
                        },
                        "ram_energy": {
                            "0": 2.326309968120211e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0007811284322506339
                        },
                        "total_energy_joules": {
                            "0": 2812.062356102282
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 174.68016219131425,
                        "ram_power_avg": 1.1090168952941895,
                        "cpu_energy_total": 0.00030794063448411177,
                        "gpu_energy_total": 0.00047086148779840187,
                        "ram_energy_total": 2.326309968120211e-06,
                        "total_energy_kwh": 0.0007811284322506339,
                        "total_energy_joules": 2812.062356102282
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 3.556108909996117,
                        "joules_per_token": 0.28120623561022823,
                        "flops_per_joule": 3678951591.3649635,
                        "joules_per_flop": 2.7181656924955085e-10
                    },
                    "per-process_emissions": [
                        0.000297570876265879
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3460071,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 174.68016219131425,
                            "ram_power": 1.1090168952941895,
                            "cpu_energy": 0.00030794063448411177,
                            "gpu_energy": 0.00047086148779840187,
                            "ram_energy": 2.326309968120211e-06,
                            "total_energy_kwh": 0.0007811284322506339,
                            "total_energy_joules": 2812.062356102282,
                            "final_emissions": 0.000297570876265879
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0152": {
            "setup": {
                "experiment_id": "0152",
                "date_time": "April 06, 2025 at 03:01:43 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a2_precision_minimalist",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 5.501187722198665,
                        "average_latency_ms_per_batch": 5501.187722198665,
                        "throughput_queries_per_sec": 18.177892675153593,
                        "throughput_tokens_per_sec": 1817.7892675153594
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825651200,
                        "gpu_max_memory_allocated_bytes": 825651200,
                        "gpu_current_memory_reserved_bytes": 2571108352,
                        "gpu_max_memory_reserved_bytes": 2571108352
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 50.9,
                        "cpu_memory_usage_bytes": 5085921280
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0152",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 165.27078218958758
                        },
                        "ram_power": {
                            "0": 1.7762374877929688
                        },
                        "cpu_energy": {
                            "0": 0.00020422326852713012
                        },
                        "gpu_energy": {
                            "0": 0.0002810866137537005
                        },
                        "ram_energy": {
                            "0": 2.173355281642895e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0004874832375624735
                        },
                        "total_energy_joules": {
                            "0": 1754.9396552249048
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 165.27078218958758,
                        "ram_power_avg": 1.7762374877929688,
                        "cpu_energy_total": 0.00020422326852713012,
                        "gpu_energy_total": 0.0002810866137537005,
                        "ram_energy_total": 2.173355281642895e-06,
                        "total_energy_kwh": 0.0004874832375624735,
                        "total_energy_joules": 1754.9396552249048
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 5.698201627746823,
                        "joules_per_token": 0.17549396552249047,
                        "flops_per_joule": 5895041034.145517,
                        "joules_per_flop": 1.696341033434298e-10
                    },
                    "per-process_emissions": [
                        0.0001857067393494243
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3460071,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 165.27078218958758,
                            "ram_power": 1.7762374877929688,
                            "cpu_energy": 0.00020422326852713012,
                            "gpu_energy": 0.0002810866137537005,
                            "ram_energy": 2.173355281642895e-06,
                            "total_energy_kwh": 0.0004874832375624735,
                            "total_energy_joules": 1754.9396552249048,
                            "final_emissions": 0.0001857067393494243
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0152": {
            "setup": {
                "experiment_id": "0152",
                "date_time": "April 06, 2025 at 03:01:43 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a2_precision_minimalist",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 5.501187722198665,
                        "average_latency_ms_per_batch": 5501.187722198665,
                        "throughput_queries_per_sec": 18.177892675153593,
                        "throughput_tokens_per_sec": 1817.7892675153594
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825651200,
                        "gpu_max_memory_allocated_bytes": 825651200,
                        "gpu_current_memory_reserved_bytes": 2571108352,
                        "gpu_max_memory_reserved_bytes": 2571108352
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 50.9,
                        "cpu_memory_usage_bytes": 5085921280
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0152",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 165.27078218958758
                        },
                        "ram_power": {
                            "0": 1.7762374877929688
                        },
                        "cpu_energy": {
                            "0": 0.00020422326852713012
                        },
                        "gpu_energy": {
                            "0": 0.0002810866137537005
                        },
                        "ram_energy": {
                            "0": 2.173355281642895e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0004874832375624735
                        },
                        "total_energy_joules": {
                            "0": 1754.9396552249048
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 165.27078218958758,
                        "ram_power_avg": 1.7762374877929688,
                        "cpu_energy_total": 0.00020422326852713012,
                        "gpu_energy_total": 0.0002810866137537005,
                        "ram_energy_total": 2.173355281642895e-06,
                        "total_energy_kwh": 0.0004874832375624735,
                        "total_energy_joules": 1754.9396552249048
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 5.698201627746823,
                        "joules_per_token": 0.17549396552249047,
                        "flops_per_joule": 5895041034.145517,
                        "joules_per_flop": 1.696341033434298e-10
                    },
                    "per-process_emissions": [
                        0.0001857067393494243
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3460071,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 165.27078218958758,
                            "ram_power": 1.7762374877929688,
                            "cpu_energy": 0.00020422326852713012,
                            "gpu_energy": 0.0002810866137537005,
                            "ram_energy": 2.173355281642895e-06,
                            "total_energy_kwh": 0.0004874832375624735,
                            "total_energy_joules": 1754.9396552249048,
                            "final_emissions": 0.0001857067393494243
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0153": {
            "setup": {
                "experiment_id": "0153",
                "date_time": "April 06, 2025 at 03:02:02 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a3_quantisation_gaming",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.7,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 7.73186424607411,
                        "average_latency_ms_per_batch": 3865.932123037055,
                        "throughput_queries_per_sec": 12.933491434588426,
                        "throughput_tokens_per_sec": 1293.3491434588425
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825782272,
                        "gpu_max_memory_allocated_bytes": 825782272,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 51.0,
                        "cpu_memory_usage_bytes": 5926387712
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0153",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 126.80130160061232
                        },
                        "ram_power": {
                            "0": 2.0697669982910156
                        },
                        "cpu_energy": {
                            "0": 0.0002724476883668104
                        },
                        "gpu_energy": {
                            "0": 0.0002804285576729626
                        },
                        "ram_energy": {
                            "0": 3.785463287628517e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0005566617093274015
                        },
                        "total_energy_joules": {
                            "0": 2003.9821535786452
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 126.80130160061232,
                        "ram_power_avg": 2.0697669982910156,
                        "cpu_energy_total": 0.0002724476883668104,
                        "gpu_energy_total": 0.0002804285576729626,
                        "ram_energy_total": 3.785463287628517e-06,
                        "total_energy_kwh": 0.0005566617093274015,
                        "total_energy_joules": 2003.9821535786452
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 4.990064398598725,
                        "joules_per_token": 0.20039821535786453,
                        "flops_per_joule": 5162441821.912163,
                        "joules_per_flop": 1.9370678343637027e-10
                    },
                    "per-process_emissions": [
                        0.0002120602781682736
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3460071,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 126.80130160061232,
                            "ram_power": 2.0697669982910156,
                            "cpu_energy": 0.0002724476883668104,
                            "gpu_energy": 0.0002804285576729626,
                            "ram_energy": 3.785463287628517e-06,
                            "total_energy_kwh": 0.0005566617093274015,
                            "total_energy_joules": 2003.9821535786452,
                            "final_emissions": 0.0002120602781682736
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0153": {
            "setup": {
                "experiment_id": "0153",
                "date_time": "April 06, 2025 at 03:02:02 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a3_quantisation_gaming",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.7,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 7.73186424607411,
                        "average_latency_ms_per_batch": 3865.932123037055,
                        "throughput_queries_per_sec": 12.933491434588426,
                        "throughput_tokens_per_sec": 1293.3491434588425
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825782272,
                        "gpu_max_memory_allocated_bytes": 825782272,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 51.0,
                        "cpu_memory_usage_bytes": 5926387712
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0153",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 126.80130160061232
                        },
                        "ram_power": {
                            "0": 2.0697669982910156
                        },
                        "cpu_energy": {
                            "0": 0.0002724476883668104
                        },
                        "gpu_energy": {
                            "0": 0.0002804285576729626
                        },
                        "ram_energy": {
                            "0": 3.785463287628517e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0005566617093274015
                        },
                        "total_energy_joules": {
                            "0": 2003.9821535786452
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 126.80130160061232,
                        "ram_power_avg": 2.0697669982910156,
                        "cpu_energy_total": 0.0002724476883668104,
                        "gpu_energy_total": 0.0002804285576729626,
                        "ram_energy_total": 3.785463287628517e-06,
                        "total_energy_kwh": 0.0005566617093274015,
                        "total_energy_joules": 2003.9821535786452
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 4.990064398598725,
                        "joules_per_token": 0.20039821535786453,
                        "flops_per_joule": 5162441821.912163,
                        "joules_per_flop": 1.9370678343637027e-10
                    },
                    "per-process_emissions": [
                        0.0002120602781682736
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3460071,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 126.80130160061232,
                            "ram_power": 2.0697669982910156,
                            "cpu_energy": 0.0002724476883668104,
                            "gpu_energy": 0.0002804285576729626,
                            "ram_energy": 3.785463287628517e-06,
                            "total_energy_kwh": 0.0005566617093274015,
                            "total_energy_joules": 2003.9821535786452,
                            "final_emissions": 0.0002120602781682736
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0154": {
            "setup": {
                "experiment_id": "0154",
                "date_time": "April 06, 2025 at 03:02:49 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a4_latency_ignorance_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.261459628120065,
                        "average_latency_ms_per_batch": 9315.364907030016,
                        "throughput_queries_per_sec": 2.6837381304443886,
                        "throughput_tokens_per_sec": 268.37381304443886
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1314959360,
                        "gpu_max_memory_allocated_bytes": 1314959360,
                        "gpu_current_memory_reserved_bytes": 2199912448,
                        "gpu_max_memory_reserved_bytes": 2199912448
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.4,
                        "cpu_memory_usage_bytes": 6191919104
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0154",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 39.028762068212004
                        },
                        "ram_power": {
                            "0": 2.1625027656555176
                        },
                        "cpu_energy": {
                            "0": 0.0011865333016467044
                        },
                        "gpu_energy": {
                            "0": 0.0004527831399983029
                        },
                        "ram_energy": {
                            "0": 1.6182066923337964e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.0016554985085683457
                        },
                        "total_energy_joules": {
                            "0": 5959.794630846044
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 39.028762068212004,
                        "ram_power_avg": 2.1625027656555176,
                        "cpu_energy_total": 0.0011865333016467044,
                        "gpu_energy_total": 0.0004527831399983029,
                        "ram_energy_total": 1.6182066923337964e-05,
                        "total_energy_kwh": 0.0016554985085683457,
                        "total_energy_joules": 5959.794630846044
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.6779101662737017,
                        "joules_per_token": 0.5959794630846044,
                        "flops_per_joule": 1735872109.8299615,
                        "joules_per_flop": 5.760793058066677e-10
                    },
                    "per-process_emissions": [
                        0.0006306621568391113
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3460071,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 39.028762068212004,
                            "ram_power": 2.1625027656555176,
                            "cpu_energy": 0.0011865333016467044,
                            "gpu_energy": 0.0004527831399983029,
                            "ram_energy": 1.6182066923337964e-05,
                            "total_energy_kwh": 0.0016554985085683457,
                            "total_energy_joules": 5959.794630846044,
                            "final_emissions": 0.0006306621568391113
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0154": {
            "setup": {
                "experiment_id": "0154",
                "date_time": "April 06, 2025 at 03:02:49 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a4_latency_ignorance_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.261459628120065,
                        "average_latency_ms_per_batch": 9315.364907030016,
                        "throughput_queries_per_sec": 2.6837381304443886,
                        "throughput_tokens_per_sec": 268.37381304443886
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1314959360,
                        "gpu_max_memory_allocated_bytes": 1314959360,
                        "gpu_current_memory_reserved_bytes": 2199912448,
                        "gpu_max_memory_reserved_bytes": 2199912448
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.4,
                        "cpu_memory_usage_bytes": 6191919104
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0154",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 39.028762068212004
                        },
                        "ram_power": {
                            "0": 2.1625027656555176
                        },
                        "cpu_energy": {
                            "0": 0.0011865333016467044
                        },
                        "gpu_energy": {
                            "0": 0.0004527831399983029
                        },
                        "ram_energy": {
                            "0": 1.6182066923337964e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.0016554985085683457
                        },
                        "total_energy_joules": {
                            "0": 5959.794630846044
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 39.028762068212004,
                        "ram_power_avg": 2.1625027656555176,
                        "cpu_energy_total": 0.0011865333016467044,
                        "gpu_energy_total": 0.0004527831399983029,
                        "ram_energy_total": 1.6182066923337964e-05,
                        "total_energy_kwh": 0.0016554985085683457,
                        "total_energy_joules": 5959.794630846044
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.6779101662737017,
                        "joules_per_token": 0.5959794630846044,
                        "flops_per_joule": 1735872109.8299615,
                        "joules_per_flop": 5.760793058066677e-10
                    },
                    "per-process_emissions": [
                        0.0006306621568391113
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3460071,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 39.028762068212004,
                            "ram_power": 2.1625027656555176,
                            "cpu_energy": 0.0011865333016467044,
                            "gpu_energy": 0.0004527831399983029,
                            "ram_energy": 1.6182066923337964e-05,
                            "total_energy_kwh": 0.0016554985085683457,
                            "total_energy_joules": 5959.794630846044,
                            "final_emissions": 0.0006306621568391113
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0155": {
            "setup": {
                "experiment_id": "0155",
                "date_time": "April 06, 2025 at 03:03:05 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a5_parallel_overdrive",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 7.3762076671700925,
                        "average_latency_ms_per_batch": 3688.1038335850462,
                        "throughput_queries_per_sec": 13.55710203836565,
                        "throughput_tokens_per_sec": 1355.7102038365651
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 3517153792,
                        "gpu_max_memory_allocated_bytes": 3517153792,
                        "gpu_current_memory_reserved_bytes": 4183818240,
                        "gpu_max_memory_reserved_bytes": 4183818240
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 2.1,
                        "cpu_memory_usage_bytes": 6414581760
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0155",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 243.13595393505153
                        },
                        "ram_power": {
                            "0": 2.2387661933898926
                        },
                        "cpu_energy": {
                            "0": 0.0002587454023523605
                        },
                        "gpu_energy": {
                            "0": 0.0004448386892121903
                        },
                        "ram_energy": {
                            "0": 3.851693119496508e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0007074357846840474
                        },
                        "total_energy_joules": {
                            "0": 2546.768824862571
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 243.13595393505153,
                        "ram_power_avg": 2.2387661933898926,
                        "cpu_energy_total": 0.0002587454023523605,
                        "gpu_energy_total": 0.0004448386892121903,
                        "ram_energy_total": 3.851693119496508e-06,
                        "total_energy_kwh": 0.0007074357846840474,
                        "total_energy_joules": 2546.768824862571
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 3.9265440594277816,
                        "joules_per_token": 0.2546768824862571,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.00026949766217538787
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3460071,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 243.13595393505153,
                            "ram_power": 2.2387661933898926,
                            "cpu_energy": 0.0002587454023523605,
                            "gpu_energy": 0.0004448386892121903,
                            "ram_energy": 3.851693119496508e-06,
                            "total_energy_kwh": 0.0007074357846840474,
                            "total_energy_joules": 2546.768824862571,
                            "final_emissions": 0.00026949766217538787
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0155": {
            "setup": {
                "experiment_id": "0155",
                "date_time": "April 06, 2025 at 03:03:05 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a5_parallel_overdrive",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 7.3762076671700925,
                        "average_latency_ms_per_batch": 3688.1038335850462,
                        "throughput_queries_per_sec": 13.55710203836565,
                        "throughput_tokens_per_sec": 1355.7102038365651
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 3517153792,
                        "gpu_max_memory_allocated_bytes": 3517153792,
                        "gpu_current_memory_reserved_bytes": 4183818240,
                        "gpu_max_memory_reserved_bytes": 4183818240
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 2.1,
                        "cpu_memory_usage_bytes": 6414581760
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0155",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 243.13595393505153
                        },
                        "ram_power": {
                            "0": 2.2387661933898926
                        },
                        "cpu_energy": {
                            "0": 0.0002587454023523605
                        },
                        "gpu_energy": {
                            "0": 0.0004448386892121903
                        },
                        "ram_energy": {
                            "0": 3.851693119496508e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0007074357846840474
                        },
                        "total_energy_joules": {
                            "0": 2546.768824862571
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 243.13595393505153,
                        "ram_power_avg": 2.2387661933898926,
                        "cpu_energy_total": 0.0002587454023523605,
                        "gpu_energy_total": 0.0004448386892121903,
                        "ram_energy_total": 3.851693119496508e-06,
                        "total_energy_kwh": 0.0007074357846840474,
                        "total_energy_joules": 2546.768824862571
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 3.9265440594277816,
                        "joules_per_token": 0.2546768824862571,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.00026949766217538787
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3460071,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 243.13595393505153,
                            "ram_power": 2.2387661933898926,
                            "cpu_energy": 0.0002587454023523605,
                            "gpu_energy": 0.0004448386892121903,
                            "ram_energy": 3.851693119496508e-06,
                            "total_energy_kwh": 0.0007074357846840474,
                            "total_energy_joules": 2546.768824862571,
                            "final_emissions": 0.00026949766217538787
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0156": {
            "setup": {
                "experiment_id": "0156",
                "date_time": "April 06, 2025 at 03:03:41 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r1_standard_production",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 27.51866364525631,
                        "average_latency_ms_per_batch": 3931.2376636080444,
                        "throughput_queries_per_sec": 3.633897390116837,
                        "throughput_tokens_per_sec": 363.38973901168373
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 6611588096,
                        "gpu_max_memory_allocated_bytes": 6611588096,
                        "gpu_current_memory_reserved_bytes": 6958350336,
                        "gpu_max_memory_reserved_bytes": 6958350336
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 0.7,
                        "cpu_memory_usage_bytes": 6399938560
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0156",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 174.731113717904
                        },
                        "ram_power": {
                            "0": 2.2351527214050293
                        },
                        "cpu_energy": {
                            "0": 0.0008708181176334619
                        },
                        "gpu_energy": {
                            "0": 0.0011123320009716053
                        },
                        "ram_energy": {
                            "0": 1.4449402474423655e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.0019975995210794913
                        },
                        "total_energy_joules": {
                            "0": 7191.3582758861685
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 174.731113717904,
                        "ram_power_avg": 2.2351527214050293,
                        "cpu_energy_total": 0.0008708181176334619,
                        "gpu_energy_total": 0.0011123320009716053,
                        "ram_energy_total": 1.4449402474423655e-05,
                        "total_energy_kwh": 0.0019975995210794913,
                        "total_energy_joules": 7191.3582758861685
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.3905578913418455,
                        "joules_per_token": 0.7191358275886168,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0007609855375552323
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3460071,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 174.731113717904,
                            "ram_power": 2.2351527214050293,
                            "cpu_energy": 0.0008708181176334619,
                            "gpu_energy": 0.0011123320009716053,
                            "ram_energy": 1.4449402474423655e-05,
                            "total_energy_kwh": 0.0019975995210794913,
                            "total_energy_joules": 7191.3582758861685,
                            "final_emissions": 0.0007609855375552323
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0156": {
            "setup": {
                "experiment_id": "0156",
                "date_time": "April 06, 2025 at 03:03:41 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r1_standard_production",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 27.51866364525631,
                        "average_latency_ms_per_batch": 3931.2376636080444,
                        "throughput_queries_per_sec": 3.633897390116837,
                        "throughput_tokens_per_sec": 363.38973901168373
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 6611588096,
                        "gpu_max_memory_allocated_bytes": 6611588096,
                        "gpu_current_memory_reserved_bytes": 6958350336,
                        "gpu_max_memory_reserved_bytes": 6958350336
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 0.7,
                        "cpu_memory_usage_bytes": 6399938560
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0156",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 174.731113717904
                        },
                        "ram_power": {
                            "0": 2.2351527214050293
                        },
                        "cpu_energy": {
                            "0": 0.0008708181176334619
                        },
                        "gpu_energy": {
                            "0": 0.0011123320009716053
                        },
                        "ram_energy": {
                            "0": 1.4449402474423655e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.0019975995210794913
                        },
                        "total_energy_joules": {
                            "0": 7191.3582758861685
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 174.731113717904,
                        "ram_power_avg": 2.2351527214050293,
                        "cpu_energy_total": 0.0008708181176334619,
                        "gpu_energy_total": 0.0011123320009716053,
                        "ram_energy_total": 1.4449402474423655e-05,
                        "total_energy_kwh": 0.0019975995210794913,
                        "total_energy_joules": 7191.3582758861685
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.3905578913418455,
                        "joules_per_token": 0.7191358275886168,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0007609855375552323
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3460071,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 174.731113717904,
                            "ram_power": 2.2351527214050293,
                            "cpu_energy": 0.0008708181176334619,
                            "gpu_energy": 0.0011123320009716053,
                            "ram_energy": 1.4449402474423655e-05,
                            "total_energy_kwh": 0.0019975995210794913,
                            "total_energy_joules": 7191.3582758861685,
                            "final_emissions": 0.0007609855375552323
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0157": {
            "setup": {
                "experiment_id": "0157",
                "date_time": "April 06, 2025 at 03:04:45 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r2_low_latency_chatbot",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.05,
                    "simulate_burst": false
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 54.85869368677959,
                        "average_latency_ms_per_batch": 2194.3477474711835,
                        "throughput_queries_per_sec": 1.8228651336642934,
                        "throughput_tokens_per_sec": 182.28651336642935
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8809593856,
                        "gpu_max_memory_allocated_bytes": 8809593856,
                        "gpu_current_memory_reserved_bytes": 8933867520,
                        "gpu_max_memory_reserved_bytes": 8933867520
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.5,
                        "cpu_memory_usage_bytes": 6448386048
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0157",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 105.94200222549424
                        },
                        "ram_power": {
                            "0": 2.252072811126709
                        },
                        "cpu_energy": {
                            "0": 0.0017180998895128142
                        },
                        "gpu_energy": {
                            "0": 0.0018725939980726025
                        },
                        "ram_energy": {
                            "0": 2.7900124313280353e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.003618594011898699
                        },
                        "total_energy_joules": {
                            "0": 13026.938442835317
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 105.94200222549424,
                        "ram_power_avg": 2.252072811126709,
                        "cpu_energy_total": 0.0017180998895128142,
                        "gpu_energy_total": 0.0018725939980726025,
                        "ram_energy_total": 2.7900124313280353e-05,
                        "total_energy_kwh": 0.003618594011898699,
                        "total_energy_joules": 13026.938442835317
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.7676400747483303,
                        "joules_per_token": 1.3026938442835316,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0013785033888328096
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3460071,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 105.94200222549424,
                            "ram_power": 2.252072811126709,
                            "cpu_energy": 0.0017180998895128142,
                            "gpu_energy": 0.0018725939980726025,
                            "ram_energy": 2.7900124313280353e-05,
                            "total_energy_kwh": 0.003618594011898699,
                            "total_energy_joules": 13026.938442835317,
                            "final_emissions": 0.0013785033888328096
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0157": {
            "setup": {
                "experiment_id": "0157",
                "date_time": "April 06, 2025 at 03:04:45 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r2_low_latency_chatbot",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.05,
                    "simulate_burst": false
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 54.85869368677959,
                        "average_latency_ms_per_batch": 2194.3477474711835,
                        "throughput_queries_per_sec": 1.8228651336642934,
                        "throughput_tokens_per_sec": 182.28651336642935
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8809593856,
                        "gpu_max_memory_allocated_bytes": 8809593856,
                        "gpu_current_memory_reserved_bytes": 8933867520,
                        "gpu_max_memory_reserved_bytes": 8933867520
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.5,
                        "cpu_memory_usage_bytes": 6448386048
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0157",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 105.94200222549424
                        },
                        "ram_power": {
                            "0": 2.252072811126709
                        },
                        "cpu_energy": {
                            "0": 0.0017180998895128142
                        },
                        "gpu_energy": {
                            "0": 0.0018725939980726025
                        },
                        "ram_energy": {
                            "0": 2.7900124313280353e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.003618594011898699
                        },
                        "total_energy_joules": {
                            "0": 13026.938442835317
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 105.94200222549424,
                        "ram_power_avg": 2.252072811126709,
                        "cpu_energy_total": 0.0017180998895128142,
                        "gpu_energy_total": 0.0018725939980726025,
                        "ram_energy_total": 2.7900124313280353e-05,
                        "total_energy_kwh": 0.003618594011898699,
                        "total_energy_joules": 13026.938442835317
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.7676400747483303,
                        "joules_per_token": 1.3026938442835316,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0013785033888328096
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3460071,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 105.94200222549424,
                            "ram_power": 2.252072811126709,
                            "cpu_energy": 0.0017180998895128142,
                            "gpu_energy": 0.0018725939980726025,
                            "ram_energy": 2.7900124313280353e-05,
                            "total_energy_kwh": 0.003618594011898699,
                            "total_energy_joules": 13026.938442835317,
                            "final_emissions": 0.0013785033888328096
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0158": {
            "setup": {
                "experiment_id": "0158",
                "date_time": "April 06, 2025 at 03:05:33 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r3_balanced_enterprise_service",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.07421384588815,
                        "average_latency_ms_per_batch": 9518.553461472038,
                        "throughput_queries_per_sec": 2.6264495021424996,
                        "throughput_tokens_per_sec": 262.64495021424995
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1313688576,
                        "gpu_max_memory_allocated_bytes": 1313688576,
                        "gpu_current_memory_reserved_bytes": 5773459456,
                        "gpu_max_memory_reserved_bytes": 5773459456
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 0.9,
                        "cpu_memory_usage_bytes": 6404808704
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0158",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 0.0
                        },
                        "ram_power": {
                            "0": 2.23685359954834
                        },
                        "cpu_energy": {
                            "0": 0.001225655713031301
                        },
                        "gpu_energy": {
                            "0": 0.0008152731522272916
                        },
                        "ram_energy": {
                            "0": 1.8125359995632357e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.002059054225254225
                        },
                        "total_energy_joules": {
                            "0": 7412.5952109152095
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 0.0,
                        "ram_power_avg": 2.23685359954834,
                        "cpu_energy_total": 0.001225655713031301,
                        "gpu_energy_total": 0.0008152731522272916,
                        "ram_energy_total": 1.8125359995632357e-05,
                        "total_energy_kwh": 0.002059054225254225,
                        "total_energy_joules": 7412.5952109152095
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.3490551845155094,
                        "joules_per_token": 0.7412595210915209,
                        "flops_per_joule": 1395657119.488477,
                        "joules_per_flop": 7.165083644373272e-10
                    },
                    "per-process_emissions": [
                        0.000784396707110597
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3460071,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 0.0,
                            "ram_power": 2.23685359954834,
                            "cpu_energy": 0.001225655713031301,
                            "gpu_energy": 0.0008152731522272916,
                            "ram_energy": 1.8125359995632357e-05,
                            "total_energy_kwh": 0.002059054225254225,
                            "total_energy_joules": 7412.5952109152095,
                            "final_emissions": 0.000784396707110597
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0158": {
            "setup": {
                "experiment_id": "0158",
                "date_time": "April 06, 2025 at 03:05:33 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r3_balanced_enterprise_service",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.07421384588815,
                        "average_latency_ms_per_batch": 9518.553461472038,
                        "throughput_queries_per_sec": 2.6264495021424996,
                        "throughput_tokens_per_sec": 262.64495021424995
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1313688576,
                        "gpu_max_memory_allocated_bytes": 1313688576,
                        "gpu_current_memory_reserved_bytes": 5773459456,
                        "gpu_max_memory_reserved_bytes": 5773459456
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 0.9,
                        "cpu_memory_usage_bytes": 6404808704
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0158",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 0.0
                        },
                        "ram_power": {
                            "0": 2.23685359954834
                        },
                        "cpu_energy": {
                            "0": 0.001225655713031301
                        },
                        "gpu_energy": {
                            "0": 0.0008152731522272916
                        },
                        "ram_energy": {
                            "0": 1.8125359995632357e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.002059054225254225
                        },
                        "total_energy_joules": {
                            "0": 7412.5952109152095
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 0.0,
                        "ram_power_avg": 2.23685359954834,
                        "cpu_energy_total": 0.001225655713031301,
                        "gpu_energy_total": 0.0008152731522272916,
                        "ram_energy_total": 1.8125359995632357e-05,
                        "total_energy_kwh": 0.002059054225254225,
                        "total_energy_joules": 7412.5952109152095
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.3490551845155094,
                        "joules_per_token": 0.7412595210915209,
                        "flops_per_joule": 1395657119.488477,
                        "joules_per_flop": 7.165083644373272e-10
                    },
                    "per-process_emissions": [
                        0.000784396707110597
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3460071,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 0.0,
                            "ram_power": 2.23685359954834,
                            "cpu_energy": 0.001225655713031301,
                            "gpu_energy": 0.0008152731522272916,
                            "ram_energy": 1.8125359995632357e-05,
                            "total_energy_kwh": 0.002059054225254225,
                            "total_energy_joules": 7412.5952109152095,
                            "final_emissions": 0.000784396707110597
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0159": {
            "setup": {
                "experiment_id": "0159",
                "date_time": "April 06, 2025 at 03:06:16 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r4_high_load_api",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 8
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 34.43083297624253,
                        "average_latency_ms_per_batch": 2648.5256135571176,
                        "throughput_queries_per_sec": 2.904373532554399,
                        "throughput_tokens_per_sec": 290.4373532554399
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 5714142208,
                        "gpu_max_memory_allocated_bytes": 5714142208,
                        "gpu_current_memory_reserved_bytes": 5890899968,
                        "gpu_max_memory_reserved_bytes": 5890899968
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.1,
                        "cpu_memory_usage_bytes": 6585720832
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0159",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 126.28232280796759
                        },
                        "ram_power": {
                            "0": 2.3000364303588867
                        },
                        "cpu_energy": {
                            "0": 0.001086711515956267
                        },
                        "gpu_energy": {
                            "0": 0.0011858051153055271
                        },
                        "ram_energy": {
                            "0": 1.8159723807931722e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.002290676355069727
                        },
                        "total_energy_joules": {
                            "0": 8246.434878251017
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 126.28232280796759,
                        "ram_power_avg": 2.3000364303588867,
                        "cpu_energy_total": 0.001086711515956267,
                        "gpu_energy_total": 0.0011858051153055271,
                        "ram_energy_total": 1.8159723807931722e-05,
                        "total_energy_kwh": 0.002290676355069727,
                        "total_energy_joules": 8246.434878251017
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.2126452397476395,
                        "joules_per_token": 0.8246434878251017,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0008726331574638124
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3460071,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 126.28232280796759,
                            "ram_power": 2.3000364303588867,
                            "cpu_energy": 0.001086711515956267,
                            "gpu_energy": 0.0011858051153055271,
                            "ram_energy": 1.8159723807931722e-05,
                            "total_energy_kwh": 0.002290676355069727,
                            "total_energy_joules": 8246.434878251017,
                            "final_emissions": 0.0008726331574638124
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0159": {
            "setup": {
                "experiment_id": "0159",
                "date_time": "April 06, 2025 at 03:06:16 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r4_high_load_api",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 8
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 34.43083297624253,
                        "average_latency_ms_per_batch": 2648.5256135571176,
                        "throughput_queries_per_sec": 2.904373532554399,
                        "throughput_tokens_per_sec": 290.4373532554399
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 5714142208,
                        "gpu_max_memory_allocated_bytes": 5714142208,
                        "gpu_current_memory_reserved_bytes": 5890899968,
                        "gpu_max_memory_reserved_bytes": 5890899968
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.1,
                        "cpu_memory_usage_bytes": 6585720832
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0159",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 126.28232280796759
                        },
                        "ram_power": {
                            "0": 2.3000364303588867
                        },
                        "cpu_energy": {
                            "0": 0.001086711515956267
                        },
                        "gpu_energy": {
                            "0": 0.0011858051153055271
                        },
                        "ram_energy": {
                            "0": 1.8159723807931722e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.002290676355069727
                        },
                        "total_energy_joules": {
                            "0": 8246.434878251017
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 126.28232280796759,
                        "ram_power_avg": 2.3000364303588867,
                        "cpu_energy_total": 0.001086711515956267,
                        "gpu_energy_total": 0.0011858051153055271,
                        "ram_energy_total": 1.8159723807931722e-05,
                        "total_energy_kwh": 0.002290676355069727,
                        "total_energy_joules": 8246.434878251017
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.2126452397476395,
                        "joules_per_token": 0.8246434878251017,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0008726331574638124
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3460071,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 126.28232280796759,
                            "ram_power": 2.3000364303588867,
                            "cpu_energy": 0.001086711515956267,
                            "gpu_energy": 0.0011858051153055271,
                            "ram_energy": 1.8159723807931722e-05,
                            "total_energy_kwh": 0.002290676355069727,
                            "total_energy_joules": 8246.434878251017,
                            "final_emissions": 0.0008726331574638124
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0161": {
            "setup": {
                "experiment_id": "0161",
                "date_time": "April 06, 2025 at 03:24:01 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "default",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 20.29075324907899,
                        "average_latency_ms_per_batch": 2898.679035582713,
                        "throughput_queries_per_sec": 4.9283532637970975,
                        "throughput_tokens_per_sec": 492.8353263797098
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409393664,
                        "gpu_max_memory_allocated_bytes": 4409393664,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 2237145088
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0161",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 165.6360144276572
                        },
                        "ram_power": {
                            "0": 0.7783470153808595
                        },
                        "cpu_energy": {
                            "0": 0.0006504609346011422
                        },
                        "gpu_energy": {
                            "0": 0.0013251924490447209
                        },
                        "ram_energy": {
                            "0": 3.2542647906607694e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.001978907648436523
                        },
                        "total_energy_joules": {
                            "0": 7124.067534371483
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 165.6360144276572,
                        "ram_power_avg": 0.7783470153808595,
                        "cpu_energy_total": 0.0006504609346011422,
                        "gpu_energy_total": 0.0013251924490447209,
                        "ram_energy_total": 3.2542647906607694e-06,
                        "total_energy_kwh": 0.001978907648436523,
                        "total_energy_joules": 7124.067534371483
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.4036924764894505,
                        "joules_per_token": 0.7124067534371482,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0007538648686718935
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3474892,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 165.6360144276572,
                            "ram_power": 0.7783470153808595,
                            "cpu_energy": 0.0006504609346011422,
                            "gpu_energy": 0.0013251924490447209,
                            "ram_energy": 3.2542647906607694e-06,
                            "total_energy_kwh": 0.001978907648436523,
                            "total_energy_joules": 7124.067534371483,
                            "final_emissions": 0.0007538648686718935
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0161": {
            "setup": {
                "experiment_id": "0161",
                "date_time": "April 06, 2025 at 03:24:01 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "default",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 20.29075324907899,
                        "average_latency_ms_per_batch": 2898.679035582713,
                        "throughput_queries_per_sec": 4.9283532637970975,
                        "throughput_tokens_per_sec": 492.8353263797098
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409393664,
                        "gpu_max_memory_allocated_bytes": 4409393664,
                        "gpu_current_memory_reserved_bytes": 4756340736,
                        "gpu_max_memory_reserved_bytes": 4756340736
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 2237145088
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0161",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 165.6360144276572
                        },
                        "ram_power": {
                            "0": 0.7783470153808595
                        },
                        "cpu_energy": {
                            "0": 0.0006504609346011422
                        },
                        "gpu_energy": {
                            "0": 0.0013251924490447209
                        },
                        "ram_energy": {
                            "0": 3.2542647906607694e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.001978907648436523
                        },
                        "total_energy_joules": {
                            "0": 7124.067534371483
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 165.6360144276572,
                        "ram_power_avg": 0.7783470153808595,
                        "cpu_energy_total": 0.0006504609346011422,
                        "gpu_energy_total": 0.0013251924490447209,
                        "ram_energy_total": 3.2542647906607694e-06,
                        "total_energy_kwh": 0.001978907648436523,
                        "total_energy_joules": 7124.067534371483
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.4036924764894505,
                        "joules_per_token": 0.7124067534371482,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0007538648686718935
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3474892,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 165.6360144276572,
                            "ram_power": 0.7783470153808595,
                            "cpu_energy": 0.0006504609346011422,
                            "gpu_energy": 0.0013251924490447209,
                            "ram_energy": 3.2542647906607694e-06,
                            "total_energy_kwh": 0.001978907648436523,
                            "total_energy_joules": 7124.067534371483,
                            "final_emissions": 0.0007538648686718935
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0162": {
            "setup": {
                "experiment_id": "0162",
                "date_time": "April 06, 2025 at 03:38:48 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a1_max_throughput_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 9.85544592491351,
                        "average_latency_ms_per_batch": 9855.44592491351,
                        "throughput_queries_per_sec": 10.14667431203805,
                        "throughput_tokens_per_sec": 1014.667431203805
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1314895872,
                        "gpu_max_memory_allocated_bytes": 1314895872,
                        "gpu_current_memory_reserved_bytes": 2483027968,
                        "gpu_max_memory_reserved_bytes": 2483027968
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.6,
                        "cpu_memory_usage_bytes": 3178573824
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0162",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 215.9267955967229
                        },
                        "ram_power": {
                            "0": 1.1094346046447756
                        },
                        "cpu_energy": {
                            "0": 0.0003825099386085639
                        },
                        "gpu_energy": {
                            "0": 0.0005453198807003901
                        },
                        "ram_energy": {
                            "0": 2.951008655340991e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.000930780827964295
                        },
                        "total_energy_joules": {
                            "0": 3350.810980671462
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 215.9267955967229,
                        "ram_power_avg": 1.1094346046447756,
                        "cpu_energy_total": 0.0003825099386085639,
                        "gpu_energy_total": 0.0005453198807003901,
                        "ram_energy_total": 2.951008655340991e-06,
                        "total_energy_kwh": 0.000930780827964295,
                        "total_energy_joules": 3350.810980671462
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 2.9843521636055166,
                        "joules_per_token": 0.3350810980671462,
                        "flops_per_joule": 3087444006.7421823,
                        "joules_per_flop": 3.238925136184681e-10
                    },
                    "per-process_emissions": [
                        0.0003545809564129982
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3484480,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 215.9267955967229,
                            "ram_power": 1.1094346046447756,
                            "cpu_energy": 0.0003825099386085639,
                            "gpu_energy": 0.0005453198807003901,
                            "ram_energy": 2.951008655340991e-06,
                            "total_energy_kwh": 0.000930780827964295,
                            "total_energy_joules": 3350.810980671462,
                            "final_emissions": 0.0003545809564129982
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0162": {
            "setup": {
                "experiment_id": "0162",
                "date_time": "April 06, 2025 at 03:38:48 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a1_max_throughput_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 9.85544592491351,
                        "average_latency_ms_per_batch": 9855.44592491351,
                        "throughput_queries_per_sec": 10.14667431203805,
                        "throughput_tokens_per_sec": 1014.667431203805
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1314895872,
                        "gpu_max_memory_allocated_bytes": 1314895872,
                        "gpu_current_memory_reserved_bytes": 2483027968,
                        "gpu_max_memory_reserved_bytes": 2483027968
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.6,
                        "cpu_memory_usage_bytes": 3178573824
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0162",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 215.9267955967229
                        },
                        "ram_power": {
                            "0": 1.1094346046447756
                        },
                        "cpu_energy": {
                            "0": 0.0003825099386085639
                        },
                        "gpu_energy": {
                            "0": 0.0005453198807003901
                        },
                        "ram_energy": {
                            "0": 2.951008655340991e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.000930780827964295
                        },
                        "total_energy_joules": {
                            "0": 3350.810980671462
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 215.9267955967229,
                        "ram_power_avg": 1.1094346046447756,
                        "cpu_energy_total": 0.0003825099386085639,
                        "gpu_energy_total": 0.0005453198807003901,
                        "ram_energy_total": 2.951008655340991e-06,
                        "total_energy_kwh": 0.000930780827964295,
                        "total_energy_joules": 3350.810980671462
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 2.9843521636055166,
                        "joules_per_token": 0.3350810980671462,
                        "flops_per_joule": 3087444006.7421823,
                        "joules_per_flop": 3.238925136184681e-10
                    },
                    "per-process_emissions": [
                        0.0003545809564129982
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3484480,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 215.9267955967229,
                            "ram_power": 1.1094346046447756,
                            "cpu_energy": 0.0003825099386085639,
                            "gpu_energy": 0.0005453198807003901,
                            "ram_energy": 2.951008655340991e-06,
                            "total_energy_kwh": 0.000930780827964295,
                            "total_energy_joules": 3350.810980671462,
                            "final_emissions": 0.0003545809564129982
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0163": {
            "setup": {
                "experiment_id": "0163",
                "date_time": "April 06, 2025 at 03:39:03 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a2_precision_minimalist",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 4.722740788944066,
                        "average_latency_ms_per_batch": 4722.740788944066,
                        "throughput_queries_per_sec": 21.174145367897378,
                        "throughput_tokens_per_sec": 2117.414536789738
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825651200,
                        "gpu_max_memory_allocated_bytes": 825651200,
                        "gpu_current_memory_reserved_bytes": 2571108352,
                        "gpu_max_memory_reserved_bytes": 2571108352
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.2,
                        "cpu_memory_usage_bytes": 5046509568
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0163",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 232.7603723666269
                        },
                        "ram_power": {
                            "0": 1.7624731063842773
                        },
                        "cpu_energy": {
                            "0": 0.00020499956443381962
                        },
                        "gpu_energy": {
                            "0": 0.000273895774668631
                        },
                        "ram_energy": {
                            "0": 2.3533402517794147e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.00048124867935423003
                        },
                        "total_energy_joules": {
                            "0": 1732.495245675228
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 232.7603723666269,
                        "ram_power_avg": 1.7624731063842773,
                        "cpu_energy_total": 0.00020499956443381962,
                        "gpu_energy_total": 0.000273895774668631,
                        "ram_energy_total": 2.3533402517794147e-06,
                        "total_energy_kwh": 0.00048124867935423003,
                        "total_energy_joules": 1732.495245675228
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 5.772021611581722,
                        "joules_per_token": 0.1732495245675228,
                        "flops_per_joule": 5971411064.950967,
                        "joules_per_flop": 1.674646057896554e-10
                    },
                    "per-process_emissions": [
                        0.00018333168439999394
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3484480,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 232.7603723666269,
                            "ram_power": 1.7624731063842773,
                            "cpu_energy": 0.00020499956443381962,
                            "gpu_energy": 0.000273895774668631,
                            "ram_energy": 2.3533402517794147e-06,
                            "total_energy_kwh": 0.00048124867935423003,
                            "total_energy_joules": 1732.495245675228,
                            "final_emissions": 0.00018333168439999394
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0163": {
            "setup": {
                "experiment_id": "0163",
                "date_time": "April 06, 2025 at 03:39:03 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a2_precision_minimalist",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 4.722740788944066,
                        "average_latency_ms_per_batch": 4722.740788944066,
                        "throughput_queries_per_sec": 21.174145367897378,
                        "throughput_tokens_per_sec": 2117.414536789738
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825651200,
                        "gpu_max_memory_allocated_bytes": 825651200,
                        "gpu_current_memory_reserved_bytes": 2571108352,
                        "gpu_max_memory_reserved_bytes": 2571108352
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.2,
                        "cpu_memory_usage_bytes": 5046509568
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0163",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 232.7603723666269
                        },
                        "ram_power": {
                            "0": 1.7624731063842773
                        },
                        "cpu_energy": {
                            "0": 0.00020499956443381962
                        },
                        "gpu_energy": {
                            "0": 0.000273895774668631
                        },
                        "ram_energy": {
                            "0": 2.3533402517794147e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.00048124867935423003
                        },
                        "total_energy_joules": {
                            "0": 1732.495245675228
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 232.7603723666269,
                        "ram_power_avg": 1.7624731063842773,
                        "cpu_energy_total": 0.00020499956443381962,
                        "gpu_energy_total": 0.000273895774668631,
                        "ram_energy_total": 2.3533402517794147e-06,
                        "total_energy_kwh": 0.00048124867935423003,
                        "total_energy_joules": 1732.495245675228
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 5.772021611581722,
                        "joules_per_token": 0.1732495245675228,
                        "flops_per_joule": 5971411064.950967,
                        "joules_per_flop": 1.674646057896554e-10
                    },
                    "per-process_emissions": [
                        0.00018333168439999394
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3484480,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 232.7603723666269,
                            "ram_power": 1.7624731063842773,
                            "cpu_energy": 0.00020499956443381962,
                            "gpu_energy": 0.000273895774668631,
                            "ram_energy": 2.3533402517794147e-06,
                            "total_energy_kwh": 0.00048124867935423003,
                            "total_energy_joules": 1732.495245675228,
                            "final_emissions": 0.00018333168439999394
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0164": {
            "setup": {
                "experiment_id": "0164",
                "date_time": "April 06, 2025 at 03:39:21 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a3_quantisation_gaming",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.7,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 8.55003648600541,
                        "average_latency_ms_per_batch": 4275.018243002705,
                        "throughput_queries_per_sec": 11.695856522212356,
                        "throughput_tokens_per_sec": 1169.5856522212357
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825782272,
                        "gpu_max_memory_allocated_bytes": 825782272,
                        "gpu_current_memory_reserved_bytes": 1931476992,
                        "gpu_max_memory_reserved_bytes": 1931476992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.1,
                        "cpu_memory_usage_bytes": 5636882432
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0164",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 109.41756732657191
                        },
                        "ram_power": {
                            "0": 1.968658447265625
                        },
                        "cpu_energy": {
                            "0": 0.0003109457419704995
                        },
                        "gpu_energy": {
                            "0": 0.00029629412593124016
                        },
                        "ram_energy": {
                            "0": 3.84981470477518e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0006110896826065148
                        },
                        "total_energy_joules": {
                            "0": 2199.9228573834534
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 109.41756732657191,
                        "ram_power_avg": 1.968658447265625,
                        "cpu_energy_total": 0.0003109457419704995,
                        "gpu_energy_total": 0.00029629412593124016,
                        "ram_energy_total": 3.84981470477518e-06,
                        "total_energy_kwh": 0.0006110896826065148,
                        "total_energy_joules": 2199.9228573834534
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 4.545613936614946,
                        "joules_per_token": 0.21999228573834534,
                        "flops_per_joule": 4702638206.279957,
                        "joules_per_flop": 2.126465945571974e-10
                    },
                    "per-process_emissions": [
                        0.00023279461458895181
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3484480,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 109.41756732657191,
                            "ram_power": 1.968658447265625,
                            "cpu_energy": 0.0003109457419704995,
                            "gpu_energy": 0.00029629412593124016,
                            "ram_energy": 3.84981470477518e-06,
                            "total_energy_kwh": 0.0006110896826065148,
                            "total_energy_joules": 2199.9228573834534,
                            "final_emissions": 0.00023279461458895181
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0164": {
            "setup": {
                "experiment_id": "0164",
                "date_time": "April 06, 2025 at 03:39:21 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a3_quantisation_gaming",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.7,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 8.55003648600541,
                        "average_latency_ms_per_batch": 4275.018243002705,
                        "throughput_queries_per_sec": 11.695856522212356,
                        "throughput_tokens_per_sec": 1169.5856522212357
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825782272,
                        "gpu_max_memory_allocated_bytes": 825782272,
                        "gpu_current_memory_reserved_bytes": 1931476992,
                        "gpu_max_memory_reserved_bytes": 1931476992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.1,
                        "cpu_memory_usage_bytes": 5636882432
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0164",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 109.41756732657191
                        },
                        "ram_power": {
                            "0": 1.968658447265625
                        },
                        "cpu_energy": {
                            "0": 0.0003109457419704995
                        },
                        "gpu_energy": {
                            "0": 0.00029629412593124016
                        },
                        "ram_energy": {
                            "0": 3.84981470477518e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0006110896826065148
                        },
                        "total_energy_joules": {
                            "0": 2199.9228573834534
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 109.41756732657191,
                        "ram_power_avg": 1.968658447265625,
                        "cpu_energy_total": 0.0003109457419704995,
                        "gpu_energy_total": 0.00029629412593124016,
                        "ram_energy_total": 3.84981470477518e-06,
                        "total_energy_kwh": 0.0006110896826065148,
                        "total_energy_joules": 2199.9228573834534
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 4.545613936614946,
                        "joules_per_token": 0.21999228573834534,
                        "flops_per_joule": 4702638206.279957,
                        "joules_per_flop": 2.126465945571974e-10
                    },
                    "per-process_emissions": [
                        0.00023279461458895181
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3484480,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 109.41756732657191,
                            "ram_power": 1.968658447265625,
                            "cpu_energy": 0.0003109457419704995,
                            "gpu_energy": 0.00029629412593124016,
                            "ram_energy": 3.84981470477518e-06,
                            "total_energy_kwh": 0.0006110896826065148,
                            "total_energy_joules": 2199.9228573834534,
                            "final_emissions": 0.00023279461458895181
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0165": {
            "setup": {
                "experiment_id": "0165",
                "date_time": "April 06, 2025 at 03:40:06 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a4_latency_ignorance_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 35.14696413022466,
                        "average_latency_ms_per_batch": 8786.741032556165,
                        "throughput_queries_per_sec": 2.8451959500537605,
                        "throughput_tokens_per_sec": 284.51959500537606
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1314747392,
                        "gpu_max_memory_allocated_bytes": 1314747392,
                        "gpu_current_memory_reserved_bytes": 2197815296,
                        "gpu_max_memory_reserved_bytes": 2197815296
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.4,
                        "cpu_memory_usage_bytes": 6076940288
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0165",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 42.25848585815508
                        },
                        "ram_power": {
                            "0": 2.122346878051758
                        },
                        "cpu_energy": {
                            "0": 0.0011256662268569924
                        },
                        "gpu_energy": {
                            "0": 0.000443293687972357
                        },
                        "ram_energy": {
                            "0": 1.590409468929574e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.0015848640095186448
                        },
                        "total_energy_joules": {
                            "0": 5705.510434267121
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 42.25848585815508,
                        "ram_power_avg": 2.122346878051758,
                        "cpu_energy_total": 0.0011256662268569924,
                        "gpu_energy_total": 0.000443293687972357,
                        "ram_energy_total": 1.590409468929574e-05,
                        "total_energy_kwh": 0.0015848640095186448,
                        "total_energy_joules": 5705.510434267121
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.75269156286882,
                        "joules_per_token": 0.570551043426712,
                        "flops_per_joule": 1813236764.5610805,
                        "joules_per_flop": 5.514999582760302e-10
                    },
                    "per-process_emissions": [
                        0.0006037539444261277
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3484480,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 42.25848585815508,
                            "ram_power": 2.122346878051758,
                            "cpu_energy": 0.0011256662268569924,
                            "gpu_energy": 0.000443293687972357,
                            "ram_energy": 1.590409468929574e-05,
                            "total_energy_kwh": 0.0015848640095186448,
                            "total_energy_joules": 5705.510434267121,
                            "final_emissions": 0.0006037539444261277
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0165": {
            "setup": {
                "experiment_id": "0165",
                "date_time": "April 06, 2025 at 03:40:06 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a4_latency_ignorance_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 35.14696413022466,
                        "average_latency_ms_per_batch": 8786.741032556165,
                        "throughput_queries_per_sec": 2.8451959500537605,
                        "throughput_tokens_per_sec": 284.51959500537606
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1314747392,
                        "gpu_max_memory_allocated_bytes": 1314747392,
                        "gpu_current_memory_reserved_bytes": 2197815296,
                        "gpu_max_memory_reserved_bytes": 2197815296
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.4,
                        "cpu_memory_usage_bytes": 6076940288
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0165",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 42.25848585815508
                        },
                        "ram_power": {
                            "0": 2.122346878051758
                        },
                        "cpu_energy": {
                            "0": 0.0011256662268569924
                        },
                        "gpu_energy": {
                            "0": 0.000443293687972357
                        },
                        "ram_energy": {
                            "0": 1.590409468929574e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.0015848640095186448
                        },
                        "total_energy_joules": {
                            "0": 5705.510434267121
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 42.25848585815508,
                        "ram_power_avg": 2.122346878051758,
                        "cpu_energy_total": 0.0011256662268569924,
                        "gpu_energy_total": 0.000443293687972357,
                        "ram_energy_total": 1.590409468929574e-05,
                        "total_energy_kwh": 0.0015848640095186448,
                        "total_energy_joules": 5705.510434267121
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.75269156286882,
                        "joules_per_token": 0.570551043426712,
                        "flops_per_joule": 1813236764.5610805,
                        "joules_per_flop": 5.514999582760302e-10
                    },
                    "per-process_emissions": [
                        0.0006037539444261277
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3484480,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 42.25848585815508,
                            "ram_power": 2.122346878051758,
                            "cpu_energy": 0.0011256662268569924,
                            "gpu_energy": 0.000443293687972357,
                            "ram_energy": 1.590409468929574e-05,
                            "total_energy_kwh": 0.0015848640095186448,
                            "total_energy_joules": 5705.510434267121,
                            "final_emissions": 0.0006037539444261277
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0166": {
            "setup": {
                "experiment_id": "0166",
                "date_time": "April 06, 2025 at 03:40:20 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a5_parallel_overdrive",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 5.400178112089634,
                        "average_latency_ms_per_batch": 2700.089056044817,
                        "throughput_queries_per_sec": 18.517907729029396,
                        "throughput_tokens_per_sec": 1851.7907729029396
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 3516941824,
                        "gpu_max_memory_allocated_bytes": 3516941824,
                        "gpu_current_memory_reserved_bytes": 4179623936,
                        "gpu_max_memory_reserved_bytes": 4179623936
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.6,
                        "cpu_memory_usage_bytes": 6329229312
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0166",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 228.34084434828478
                        },
                        "ram_power": {
                            "0": 2.2090044021606445
                        },
                        "cpu_energy": {
                            "0": 0.00020840421914908802
                        },
                        "gpu_energy": {
                            "0": 0.00039143059093049715
                        },
                        "ram_energy": {
                            "0": 3.2574653903708697e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.000603092275469956
                        },
                        "total_energy_joules": {
                            "0": 2171.132191691842
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 228.34084434828478,
                        "ram_power_avg": 2.2090044021606445,
                        "cpu_energy_total": 0.00020840421914908802,
                        "gpu_energy_total": 0.00039143059093049715,
                        "ram_energy_total": 3.2574653903708697e-06,
                        "total_energy_kwh": 0.000603092275469956,
                        "total_energy_joules": 2171.132191691842
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 4.6058918191469305,
                        "joules_per_token": 0.2171132191691842,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.00022974800234027977
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3484480,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 228.34084434828478,
                            "ram_power": 2.2090044021606445,
                            "cpu_energy": 0.00020840421914908802,
                            "gpu_energy": 0.00039143059093049715,
                            "ram_energy": 3.2574653903708697e-06,
                            "total_energy_kwh": 0.000603092275469956,
                            "total_energy_joules": 2171.132191691842,
                            "final_emissions": 0.00022974800234027977
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0166": {
            "setup": {
                "experiment_id": "0166",
                "date_time": "April 06, 2025 at 03:40:20 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a5_parallel_overdrive",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 5.400178112089634,
                        "average_latency_ms_per_batch": 2700.089056044817,
                        "throughput_queries_per_sec": 18.517907729029396,
                        "throughput_tokens_per_sec": 1851.7907729029396
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 3516941824,
                        "gpu_max_memory_allocated_bytes": 3516941824,
                        "gpu_current_memory_reserved_bytes": 4179623936,
                        "gpu_max_memory_reserved_bytes": 4179623936
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.6,
                        "cpu_memory_usage_bytes": 6329229312
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0166",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 228.34084434828478
                        },
                        "ram_power": {
                            "0": 2.2090044021606445
                        },
                        "cpu_energy": {
                            "0": 0.00020840421914908802
                        },
                        "gpu_energy": {
                            "0": 0.00039143059093049715
                        },
                        "ram_energy": {
                            "0": 3.2574653903708697e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.000603092275469956
                        },
                        "total_energy_joules": {
                            "0": 2171.132191691842
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 228.34084434828478,
                        "ram_power_avg": 2.2090044021606445,
                        "cpu_energy_total": 0.00020840421914908802,
                        "gpu_energy_total": 0.00039143059093049715,
                        "ram_energy_total": 3.2574653903708697e-06,
                        "total_energy_kwh": 0.000603092275469956,
                        "total_energy_joules": 2171.132191691842
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 4.6058918191469305,
                        "joules_per_token": 0.2171132191691842,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.00022974800234027977
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3484480,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 228.34084434828478,
                            "ram_power": 2.2090044021606445,
                            "cpu_energy": 0.00020840421914908802,
                            "gpu_energy": 0.00039143059093049715,
                            "ram_energy": 3.2574653903708697e-06,
                            "total_energy_kwh": 0.000603092275469956,
                            "total_energy_joules": 2171.132191691842,
                            "final_emissions": 0.00022974800234027977
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0167": {
            "setup": {
                "experiment_id": "0167",
                "date_time": "April 06, 2025 at 03:40:59 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r1_standard_production",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 28.722950523952022,
                        "average_latency_ms_per_batch": 4103.27864627886,
                        "throughput_queries_per_sec": 3.4815364778284237,
                        "throughput_tokens_per_sec": 348.15364778284237
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 6611588096,
                        "gpu_max_memory_allocated_bytes": 6611588096,
                        "gpu_current_memory_reserved_bytes": 6956253184,
                        "gpu_max_memory_reserved_bytes": 6956253184
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.8,
                        "cpu_memory_usage_bytes": 6377308160
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0167",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 174.60689643567596
                        },
                        "ram_power": {
                            "0": 2.2272491455078125
                        },
                        "cpu_energy": {
                            "0": 0.0009322430004685884
                        },
                        "gpu_energy": {
                            "0": 0.0011649475986246216
                        },
                        "ram_energy": {
                            "0": 1.491641089256189e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.0021121070099857724
                        },
                        "total_energy_joules": {
                            "0": 7603.585235948781
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 174.60689643567596,
                        "ram_power_avg": 2.2272491455078125,
                        "cpu_energy_total": 0.0009322430004685884,
                        "gpu_energy_total": 0.0011649475986246216,
                        "ram_energy_total": 1.491641089256189e-05,
                        "total_energy_kwh": 0.0021121070099857724,
                        "total_energy_joules": 7603.585235948781
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.315169053767067,
                        "joules_per_token": 0.7603585235948781,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0008046071654540801
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3484480,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 174.60689643567596,
                            "ram_power": 2.2272491455078125,
                            "cpu_energy": 0.0009322430004685884,
                            "gpu_energy": 0.0011649475986246216,
                            "ram_energy": 1.491641089256189e-05,
                            "total_energy_kwh": 0.0021121070099857724,
                            "total_energy_joules": 7603.585235948781,
                            "final_emissions": 0.0008046071654540801
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0167": {
            "setup": {
                "experiment_id": "0167",
                "date_time": "April 06, 2025 at 03:40:59 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r1_standard_production",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 28.722950523952022,
                        "average_latency_ms_per_batch": 4103.27864627886,
                        "throughput_queries_per_sec": 3.4815364778284237,
                        "throughput_tokens_per_sec": 348.15364778284237
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 6611588096,
                        "gpu_max_memory_allocated_bytes": 6611588096,
                        "gpu_current_memory_reserved_bytes": 6956253184,
                        "gpu_max_memory_reserved_bytes": 6956253184
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.8,
                        "cpu_memory_usage_bytes": 6377308160
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0167",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 174.60689643567596
                        },
                        "ram_power": {
                            "0": 2.2272491455078125
                        },
                        "cpu_energy": {
                            "0": 0.0009322430004685884
                        },
                        "gpu_energy": {
                            "0": 0.0011649475986246216
                        },
                        "ram_energy": {
                            "0": 1.491641089256189e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.0021121070099857724
                        },
                        "total_energy_joules": {
                            "0": 7603.585235948781
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 174.60689643567596,
                        "ram_power_avg": 2.2272491455078125,
                        "cpu_energy_total": 0.0009322430004685884,
                        "gpu_energy_total": 0.0011649475986246216,
                        "ram_energy_total": 1.491641089256189e-05,
                        "total_energy_kwh": 0.0021121070099857724,
                        "total_energy_joules": 7603.585235948781
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.315169053767067,
                        "joules_per_token": 0.7603585235948781,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0008046071654540801
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3484480,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 174.60689643567596,
                            "ram_power": 2.2272491455078125,
                            "cpu_energy": 0.0009322430004685884,
                            "gpu_energy": 0.0011649475986246216,
                            "ram_energy": 1.491641089256189e-05,
                            "total_energy_kwh": 0.0021121070099857724,
                            "total_energy_joules": 7603.585235948781,
                            "final_emissions": 0.0008046071654540801
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0168": {
            "setup": {
                "experiment_id": "0168",
                "date_time": "April 06, 2025 at 03:42:26 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r2_low_latency_chatbot",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.05,
                    "simulate_burst": false
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 74.98526638094336,
                        "average_latency_ms_per_batch": 2999.4106552377343,
                        "throughput_queries_per_sec": 1.333595315804784,
                        "throughput_tokens_per_sec": 133.35953158047838
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8809593856,
                        "gpu_max_memory_allocated_bytes": 8809593856,
                        "gpu_current_memory_reserved_bytes": 8933867520,
                        "gpu_max_memory_reserved_bytes": 8933867520
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 40.0,
                        "cpu_memory_usage_bytes": 6392369152
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0168",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 62.033050088056484
                        },
                        "ram_power": {
                            "0": 2.2324419021606445
                        },
                        "cpu_energy": {
                            "0": 0.002389394987563719
                        },
                        "gpu_energy": {
                            "0": 0.002141091712871912
                        },
                        "ram_energy": {
                            "0": 3.4555441571766874e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.004565042142007399
                        },
                        "total_energy_joules": {
                            "0": 16434.151711226637
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 62.033050088056484,
                        "ram_power_avg": 2.2324419021606445,
                        "cpu_energy_total": 0.002389394987563719,
                        "gpu_energy_total": 0.002141091712871912,
                        "ram_energy_total": 3.4555441571766874e-05,
                        "total_energy_kwh": 0.004565042142007399,
                        "total_energy_joules": 16434.151711226637
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.6084889671043207,
                        "joules_per_token": 1.6434151711226637,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0017390528039977187
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3484480,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 62.033050088056484,
                            "ram_power": 2.2324419021606445,
                            "cpu_energy": 0.002389394987563719,
                            "gpu_energy": 0.002141091712871912,
                            "ram_energy": 3.4555441571766874e-05,
                            "total_energy_kwh": 0.004565042142007399,
                            "total_energy_joules": 16434.151711226637,
                            "final_emissions": 0.0017390528039977187
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0168": {
            "setup": {
                "experiment_id": "0168",
                "date_time": "April 06, 2025 at 03:42:26 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r2_low_latency_chatbot",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.05,
                    "simulate_burst": false
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 74.98526638094336,
                        "average_latency_ms_per_batch": 2999.4106552377343,
                        "throughput_queries_per_sec": 1.333595315804784,
                        "throughput_tokens_per_sec": 133.35953158047838
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8809593856,
                        "gpu_max_memory_allocated_bytes": 8809593856,
                        "gpu_current_memory_reserved_bytes": 8933867520,
                        "gpu_max_memory_reserved_bytes": 8933867520
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 40.0,
                        "cpu_memory_usage_bytes": 6392369152
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0168",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 62.033050088056484
                        },
                        "ram_power": {
                            "0": 2.2324419021606445
                        },
                        "cpu_energy": {
                            "0": 0.002389394987563719
                        },
                        "gpu_energy": {
                            "0": 0.002141091712871912
                        },
                        "ram_energy": {
                            "0": 3.4555441571766874e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.004565042142007399
                        },
                        "total_energy_joules": {
                            "0": 16434.151711226637
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 62.033050088056484,
                        "ram_power_avg": 2.2324419021606445,
                        "cpu_energy_total": 0.002389394987563719,
                        "gpu_energy_total": 0.002141091712871912,
                        "ram_energy_total": 3.4555441571766874e-05,
                        "total_energy_kwh": 0.004565042142007399,
                        "total_energy_joules": 16434.151711226637
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.6084889671043207,
                        "joules_per_token": 1.6434151711226637,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0017390528039977187
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3484480,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 62.033050088056484,
                            "ram_power": 2.2324419021606445,
                            "cpu_energy": 0.002389394987563719,
                            "gpu_energy": 0.002141091712871912,
                            "ram_energy": 3.4555441571766874e-05,
                            "total_energy_kwh": 0.004565042142007399,
                            "total_energy_joules": 16434.151711226637,
                            "final_emissions": 0.0017390528039977187
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0169": {
            "setup": {
                "experiment_id": "0169",
                "date_time": "April 06, 2025 at 03:43:21 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r3_balanced_enterprise_service",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 44.42555675422773,
                        "average_latency_ms_per_batch": 11106.389188556932,
                        "throughput_queries_per_sec": 2.250956595844656,
                        "throughput_tokens_per_sec": 225.09565958446558
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1313808384,
                        "gpu_max_memory_allocated_bytes": 1313808384,
                        "gpu_current_memory_reserved_bytes": 5773459456,
                        "gpu_max_memory_reserved_bytes": 5773459456
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 24.3,
                        "cpu_memory_usage_bytes": 6384369664
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0169",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 74.67847339976232
                        },
                        "ram_power": {
                            "0": 2.229715347290039
                        },
                        "cpu_energy": {
                            "0": 0.0014042452753637912
                        },
                        "gpu_energy": {
                            "0": 0.0009401399187751736
                        },
                        "ram_energy": {
                            "0": 1.980178073508791e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.0023641869748740538
                        },
                        "total_energy_joules": {
                            "0": 8511.073109546594
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 74.67847339976232,
                        "ram_power_avg": 2.229715347290039,
                        "cpu_energy_total": 0.0014042452753637912,
                        "gpu_energy_total": 0.0009401399187751736,
                        "ram_energy_total": 1.980178073508791e-05,
                        "total_energy_kwh": 0.0023641869748740538,
                        "total_energy_joules": 8511.073109546594
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.1749399718800824,
                        "joules_per_token": 0.8511073109546594,
                        "flops_per_joule": 1215527248.6610243,
                        "joules_per_flop": 8.226882623170806e-10
                    },
                    "per-process_emissions": [
                        0.0009006370280782709
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3484480,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 74.67847339976232,
                            "ram_power": 2.229715347290039,
                            "cpu_energy": 0.0014042452753637912,
                            "gpu_energy": 0.0009401399187751736,
                            "ram_energy": 1.980178073508791e-05,
                            "total_energy_kwh": 0.0023641869748740538,
                            "total_energy_joules": 8511.073109546594,
                            "final_emissions": 0.0009006370280782709
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0169": {
            "setup": {
                "experiment_id": "0169",
                "date_time": "April 06, 2025 at 03:43:21 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r3_balanced_enterprise_service",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 44.42555675422773,
                        "average_latency_ms_per_batch": 11106.389188556932,
                        "throughput_queries_per_sec": 2.250956595844656,
                        "throughput_tokens_per_sec": 225.09565958446558
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1313808384,
                        "gpu_max_memory_allocated_bytes": 1313808384,
                        "gpu_current_memory_reserved_bytes": 5773459456,
                        "gpu_max_memory_reserved_bytes": 5773459456
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 24.3,
                        "cpu_memory_usage_bytes": 6384369664
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0169",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 74.67847339976232
                        },
                        "ram_power": {
                            "0": 2.229715347290039
                        },
                        "cpu_energy": {
                            "0": 0.0014042452753637912
                        },
                        "gpu_energy": {
                            "0": 0.0009401399187751736
                        },
                        "ram_energy": {
                            "0": 1.980178073508791e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.0023641869748740538
                        },
                        "total_energy_joules": {
                            "0": 8511.073109546594
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 74.67847339976232,
                        "ram_power_avg": 2.229715347290039,
                        "cpu_energy_total": 0.0014042452753637912,
                        "gpu_energy_total": 0.0009401399187751736,
                        "ram_energy_total": 1.980178073508791e-05,
                        "total_energy_kwh": 0.0023641869748740538,
                        "total_energy_joules": 8511.073109546594
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.1749399718800824,
                        "joules_per_token": 0.8511073109546594,
                        "flops_per_joule": 1215527248.6610243,
                        "joules_per_flop": 8.226882623170806e-10
                    },
                    "per-process_emissions": [
                        0.0009006370280782709
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3484480,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 74.67847339976232,
                            "ram_power": 2.229715347290039,
                            "cpu_energy": 0.0014042452753637912,
                            "gpu_energy": 0.0009401399187751736,
                            "ram_energy": 1.980178073508791e-05,
                            "total_energy_kwh": 0.0023641869748740538,
                            "total_energy_joules": 8511.073109546594,
                            "final_emissions": 0.0009006370280782709
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0170": {
            "setup": {
                "experiment_id": "0170",
                "date_time": "April 06, 2025 at 03:44:08 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r4_high_load_api",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 8
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.08260853984393,
                        "average_latency_ms_per_batch": 2852.508349218764,
                        "throughput_queries_per_sec": 2.696681920112324,
                        "throughput_tokens_per_sec": 269.6681920112324
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 5714262016,
                        "gpu_max_memory_allocated_bytes": 5714262016,
                        "gpu_current_memory_reserved_bytes": 5892997120,
                        "gpu_max_memory_reserved_bytes": 5892997120
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 11.5,
                        "cpu_memory_usage_bytes": 6523154432
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0170",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 136.82543100404374
                        },
                        "ram_power": {
                            "0": 2.2781853675842285
                        },
                        "cpu_energy": {
                            "0": 0.0011755972163082335
                        },
                        "gpu_energy": {
                            "0": 0.0012427951609055299
                        },
                        "ram_energy": {
                            "0": 1.8846343157084165e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.002437238720370847
                        },
                        "total_energy_joules": {
                            "0": 8774.05939333505
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 136.82543100404374,
                        "ram_power_avg": 2.2781853675842285,
                        "cpu_energy_total": 0.0011755972163082335,
                        "gpu_energy_total": 0.0012427951609055299,
                        "ram_energy_total": 1.8846343157084165e-05,
                        "total_energy_kwh": 0.002437238720370847,
                        "total_energy_joules": 8774.05939333505
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.1397233084148255,
                        "joules_per_token": 0.877405939333505,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0009284660905252742
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3484480,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 136.82543100404374,
                            "ram_power": 2.2781853675842285,
                            "cpu_energy": 0.0011755972163082335,
                            "gpu_energy": 0.0012427951609055299,
                            "ram_energy": 1.8846343157084165e-05,
                            "total_energy_kwh": 0.002437238720370847,
                            "total_energy_joules": 8774.05939333505,
                            "final_emissions": 0.0009284660905252742
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0170": {
            "setup": {
                "experiment_id": "0170",
                "date_time": "April 06, 2025 at 03:44:08 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r4_high_load_api",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 8
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.08260853984393,
                        "average_latency_ms_per_batch": 2852.508349218764,
                        "throughput_queries_per_sec": 2.696681920112324,
                        "throughput_tokens_per_sec": 269.6681920112324
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 5714262016,
                        "gpu_max_memory_allocated_bytes": 5714262016,
                        "gpu_current_memory_reserved_bytes": 5892997120,
                        "gpu_max_memory_reserved_bytes": 5892997120
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 11.5,
                        "cpu_memory_usage_bytes": 6523154432
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0170",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 136.82543100404374
                        },
                        "ram_power": {
                            "0": 2.2781853675842285
                        },
                        "cpu_energy": {
                            "0": 0.0011755972163082335
                        },
                        "gpu_energy": {
                            "0": 0.0012427951609055299
                        },
                        "ram_energy": {
                            "0": 1.8846343157084165e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.002437238720370847
                        },
                        "total_energy_joules": {
                            "0": 8774.05939333505
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 136.82543100404374,
                        "ram_power_avg": 2.2781853675842285,
                        "cpu_energy_total": 0.0011755972163082335,
                        "gpu_energy_total": 0.0012427951609055299,
                        "ram_energy_total": 1.8846343157084165e-05,
                        "total_energy_kwh": 0.002437238720370847,
                        "total_energy_joules": 8774.05939333505
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.1397233084148255,
                        "joules_per_token": 0.877405939333505,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0009284660905252742
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3484480,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 136.82543100404374,
                            "ram_power": 2.2781853675842285,
                            "cpu_energy": 0.0011755972163082335,
                            "gpu_energy": 0.0012427951609055299,
                            "ram_energy": 1.8846343157084165e-05,
                            "total_energy_kwh": 0.002437238720370847,
                            "total_energy_joules": 8774.05939333505,
                            "final_emissions": 0.0009284660905252742
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0172": {
            "setup": {
                "experiment_id": "0172",
                "date_time": "April 06, 2025 at 03:58:57 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a1_max_throughput_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 8.521022778004408,
                        "average_latency_ms_per_batch": 8521.022778004408,
                        "throughput_queries_per_sec": 11.735680399556406,
                        "throughput_tokens_per_sec": 1173.5680399556404
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1314917376,
                        "gpu_max_memory_allocated_bytes": 1314917376,
                        "gpu_current_memory_reserved_bytes": 2483027968,
                        "gpu_max_memory_reserved_bytes": 2483027968
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 2.7,
                        "cpu_memory_usage_bytes": 3124830208
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0172",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 190.9155013412325
                        },
                        "ram_power": {
                            "0": 1.0899481773376467
                        },
                        "cpu_energy": {
                            "0": 0.0002871061012192513
                        },
                        "gpu_energy": {
                            "0": 0.0004491098037391339
                        },
                        "ram_energy": {
                            "0": 2.0244595283150954e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0007382403644867005
                        },
                        "total_energy_joules": {
                            "0": 2657.6653121521217
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 190.9155013412325,
                        "ram_power_avg": 1.0899481773376467,
                        "cpu_energy_total": 0.0002871061012192513,
                        "gpu_energy_total": 0.0004491098037391339,
                        "ram_energy_total": 2.0244595283150954e-06,
                        "total_energy_kwh": 0.0007382403644867005,
                        "total_energy_joules": 2657.6653121521217
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 3.7627010272042907,
                        "joules_per_token": 0.26576653121521215,
                        "flops_per_joule": 3892680253.113767,
                        "joules_per_flop": 2.568924070247221e-10
                    },
                    "per-process_emissions": [
                        0.00028123266685120855
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3496608,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 190.9155013412325,
                            "ram_power": 1.0899481773376467,
                            "cpu_energy": 0.0002871061012192513,
                            "gpu_energy": 0.0004491098037391339,
                            "ram_energy": 2.0244595283150954e-06,
                            "total_energy_kwh": 0.0007382403644867005,
                            "total_energy_joules": 2657.6653121521217,
                            "final_emissions": 0.00028123266685120855
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0172": {
            "setup": {
                "experiment_id": "0172",
                "date_time": "April 06, 2025 at 03:58:57 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a1_max_throughput_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 8.521022778004408,
                        "average_latency_ms_per_batch": 8521.022778004408,
                        "throughput_queries_per_sec": 11.735680399556406,
                        "throughput_tokens_per_sec": 1173.5680399556404
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1314917376,
                        "gpu_max_memory_allocated_bytes": 1314917376,
                        "gpu_current_memory_reserved_bytes": 2483027968,
                        "gpu_max_memory_reserved_bytes": 2483027968
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 2.7,
                        "cpu_memory_usage_bytes": 3124830208
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0172",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 190.9155013412325
                        },
                        "ram_power": {
                            "0": 1.0899481773376467
                        },
                        "cpu_energy": {
                            "0": 0.0002871061012192513
                        },
                        "gpu_energy": {
                            "0": 0.0004491098037391339
                        },
                        "ram_energy": {
                            "0": 2.0244595283150954e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0007382403644867005
                        },
                        "total_energy_joules": {
                            "0": 2657.6653121521217
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 190.9155013412325,
                        "ram_power_avg": 1.0899481773376467,
                        "cpu_energy_total": 0.0002871061012192513,
                        "gpu_energy_total": 0.0004491098037391339,
                        "ram_energy_total": 2.0244595283150954e-06,
                        "total_energy_kwh": 0.0007382403644867005,
                        "total_energy_joules": 2657.6653121521217
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 3.7627010272042907,
                        "joules_per_token": 0.26576653121521215,
                        "flops_per_joule": 3892680253.113767,
                        "joules_per_flop": 2.568924070247221e-10
                    },
                    "per-process_emissions": [
                        0.00028123266685120855
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3496608,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 190.9155013412325,
                            "ram_power": 1.0899481773376467,
                            "cpu_energy": 0.0002871061012192513,
                            "gpu_energy": 0.0004491098037391339,
                            "ram_energy": 2.0244595283150954e-06,
                            "total_energy_kwh": 0.0007382403644867005,
                            "total_energy_joules": 2657.6653121521217,
                            "final_emissions": 0.00028123266685120855
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0173": {
            "setup": {
                "experiment_id": "0173",
                "date_time": "April 06, 2025 at 03:59:09 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a2_precision_minimalist",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 4.081896479008719,
                        "average_latency_ms_per_batch": 4081.8964790087193,
                        "throughput_queries_per_sec": 24.49841648710425,
                        "throughput_tokens_per_sec": 2449.841648710425
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825651200,
                        "gpu_max_memory_allocated_bytes": 825651200,
                        "gpu_current_memory_reserved_bytes": 2571108352,
                        "gpu_max_memory_reserved_bytes": 2571108352
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 2.7,
                        "cpu_memory_usage_bytes": 5009010688
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0173",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 235.20738312097805
                        },
                        "ram_power": {
                            "0": 1.74918794631958
                        },
                        "cpu_energy": {
                            "0": 0.0001500144904712215
                        },
                        "gpu_energy": {
                            "0": 0.00023924074695003128
                        },
                        "ram_energy": {
                            "0": 1.668542536637647e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0003909237799578905
                        },
                        "total_energy_joules": {
                            "0": 1407.3256078484058
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 235.20738312097805,
                        "ram_power_avg": 1.74918794631958,
                        "cpu_energy_total": 0.0001500144904712215,
                        "gpu_energy_total": 0.00023924074695003128,
                        "ram_energy_total": 1.668542536637647e-06,
                        "total_energy_kwh": 0.0003909237799578905,
                        "total_energy_joules": 1407.3256078484058
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 7.105676145045447,
                        "joules_per_token": 0.1407325607848406,
                        "flops_per_joule": 7351135531.326444,
                        "joules_per_flop": 1.3603340541587856e-10
                    },
                    "per-process_emissions": [
                        0.00014892241397495838
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3496608,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 235.20738312097805,
                            "ram_power": 1.74918794631958,
                            "cpu_energy": 0.0001500144904712215,
                            "gpu_energy": 0.00023924074695003128,
                            "ram_energy": 1.668542536637647e-06,
                            "total_energy_kwh": 0.0003909237799578905,
                            "total_energy_joules": 1407.3256078484058,
                            "final_emissions": 0.00014892241397495838
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0173": {
            "setup": {
                "experiment_id": "0173",
                "date_time": "April 06, 2025 at 03:59:09 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a2_precision_minimalist",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 4.081896479008719,
                        "average_latency_ms_per_batch": 4081.8964790087193,
                        "throughput_queries_per_sec": 24.49841648710425,
                        "throughput_tokens_per_sec": 2449.841648710425
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825651200,
                        "gpu_max_memory_allocated_bytes": 825651200,
                        "gpu_current_memory_reserved_bytes": 2571108352,
                        "gpu_max_memory_reserved_bytes": 2571108352
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 2.7,
                        "cpu_memory_usage_bytes": 5009010688
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0173",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 235.20738312097805
                        },
                        "ram_power": {
                            "0": 1.74918794631958
                        },
                        "cpu_energy": {
                            "0": 0.0001500144904712215
                        },
                        "gpu_energy": {
                            "0": 0.00023924074695003128
                        },
                        "ram_energy": {
                            "0": 1.668542536637647e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0003909237799578905
                        },
                        "total_energy_joules": {
                            "0": 1407.3256078484058
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 235.20738312097805,
                        "ram_power_avg": 1.74918794631958,
                        "cpu_energy_total": 0.0001500144904712215,
                        "gpu_energy_total": 0.00023924074695003128,
                        "ram_energy_total": 1.668542536637647e-06,
                        "total_energy_kwh": 0.0003909237799578905,
                        "total_energy_joules": 1407.3256078484058
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 7.105676145045447,
                        "joules_per_token": 0.1407325607848406,
                        "flops_per_joule": 7351135531.326444,
                        "joules_per_flop": 1.3603340541587856e-10
                    },
                    "per-process_emissions": [
                        0.00014892241397495838
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3496608,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 235.20738312097805,
                            "ram_power": 1.74918794631958,
                            "cpu_energy": 0.0001500144904712215,
                            "gpu_energy": 0.00023924074695003128,
                            "ram_energy": 1.668542536637647e-06,
                            "total_energy_kwh": 0.0003909237799578905,
                            "total_energy_joules": 1407.3256078484058,
                            "final_emissions": 0.00014892241397495838
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0174": {
            "setup": {
                "experiment_id": "0174",
                "date_time": "April 06, 2025 at 03:59:25 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a3_quantisation_gaming",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.7,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 7.7824889060575515,
                        "average_latency_ms_per_batch": 3891.2444530287758,
                        "throughput_queries_per_sec": 12.849359787993317,
                        "throughput_tokens_per_sec": 1284.9359787993317
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825782272,
                        "gpu_max_memory_allocated_bytes": 825782272,
                        "gpu_current_memory_reserved_bytes": 1931476992,
                        "gpu_max_memory_reserved_bytes": 1931476992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 2.5,
                        "cpu_memory_usage_bytes": 5902323712
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0174",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 120.4364203156182
                        },
                        "ram_power": {
                            "0": 2.0613627433776855
                        },
                        "cpu_energy": {
                            "0": 0.00026140234728518405
                        },
                        "gpu_energy": {
                            "0": 0.0002752741091072153
                        },
                        "ram_energy": {
                            "0": 3.3585581973865784e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0005400350145897859
                        },
                        "total_energy_joules": {
                            "0": 1944.1260525232292
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 120.4364203156182,
                        "ram_power_avg": 2.0613627433776855,
                        "cpu_energy_total": 0.00026140234728518405,
                        "gpu_energy_total": 0.0002752741091072153,
                        "ram_energy_total": 3.3585581973865784e-06,
                        "total_energy_kwh": 0.0005400350145897859,
                        "total_energy_joules": 1944.1260525232292
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 5.143699394913857,
                        "joules_per_token": 0.19441260525232293,
                        "flops_per_joule": 5321384005.205284,
                        "joules_per_flop": 1.8792103689976472e-10
                    },
                    "per-process_emissions": [
                        0.00020572633880797895
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3496608,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 120.4364203156182,
                            "ram_power": 2.0613627433776855,
                            "cpu_energy": 0.00026140234728518405,
                            "gpu_energy": 0.0002752741091072153,
                            "ram_energy": 3.3585581973865784e-06,
                            "total_energy_kwh": 0.0005400350145897859,
                            "total_energy_joules": 1944.1260525232292,
                            "final_emissions": 0.00020572633880797895
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0174": {
            "setup": {
                "experiment_id": "0174",
                "date_time": "April 06, 2025 at 03:59:25 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a3_quantisation_gaming",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.7,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 7.7824889060575515,
                        "average_latency_ms_per_batch": 3891.2444530287758,
                        "throughput_queries_per_sec": 12.849359787993317,
                        "throughput_tokens_per_sec": 1284.9359787993317
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825782272,
                        "gpu_max_memory_allocated_bytes": 825782272,
                        "gpu_current_memory_reserved_bytes": 1931476992,
                        "gpu_max_memory_reserved_bytes": 1931476992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 2.5,
                        "cpu_memory_usage_bytes": 5902323712
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0174",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 120.4364203156182
                        },
                        "ram_power": {
                            "0": 2.0613627433776855
                        },
                        "cpu_energy": {
                            "0": 0.00026140234728518405
                        },
                        "gpu_energy": {
                            "0": 0.0002752741091072153
                        },
                        "ram_energy": {
                            "0": 3.3585581973865784e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0005400350145897859
                        },
                        "total_energy_joules": {
                            "0": 1944.1260525232292
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 120.4364203156182,
                        "ram_power_avg": 2.0613627433776855,
                        "cpu_energy_total": 0.00026140234728518405,
                        "gpu_energy_total": 0.0002752741091072153,
                        "ram_energy_total": 3.3585581973865784e-06,
                        "total_energy_kwh": 0.0005400350145897859,
                        "total_energy_joules": 1944.1260525232292
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 5.143699394913857,
                        "joules_per_token": 0.19441260525232293,
                        "flops_per_joule": 5321384005.205284,
                        "joules_per_flop": 1.8792103689976472e-10
                    },
                    "per-process_emissions": [
                        0.00020572633880797895
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3496608,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 120.4364203156182,
                            "ram_power": 2.0613627433776855,
                            "cpu_energy": 0.00026140234728518405,
                            "gpu_energy": 0.0002752741091072153,
                            "ram_energy": 3.3585581973865784e-06,
                            "total_energy_kwh": 0.0005400350145897859,
                            "total_energy_joules": 1944.1260525232292,
                            "final_emissions": 0.00020572633880797895
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0175": {
            "setup": {
                "experiment_id": "0175",
                "date_time": "April 06, 2025 at 04:00:09 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a4_latency_ignorance_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 33.89416416292079,
                        "average_latency_ms_per_batch": 8473.541040730197,
                        "throughput_queries_per_sec": 2.9503604077482177,
                        "throughput_tokens_per_sec": 295.03604077482174
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1314728960,
                        "gpu_max_memory_allocated_bytes": 1314728960,
                        "gpu_current_memory_reserved_bytes": 2197815296,
                        "gpu_max_memory_reserved_bytes": 2197815296
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.2,
                        "cpu_memory_usage_bytes": 6238261248
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0175",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 38.425386845361956
                        },
                        "ram_power": {
                            "0": 2.178687572479248
                        },
                        "cpu_energy": {
                            "0": 0.0010859587997183555
                        },
                        "gpu_energy": {
                            "0": 0.0004217275596047898
                        },
                        "ram_energy": {
                            "0": 1.4988730050942724e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.0015226750893740876
                        },
                        "total_energy_joules": {
                            "0": 5481.630321746716
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 38.425386845361956,
                        "ram_power_avg": 2.178687572479248,
                        "cpu_energy_total": 0.0010859587997183555,
                        "gpu_energy_total": 0.0004217275596047898,
                        "ram_energy_total": 1.4988730050942724e-05,
                        "total_energy_kwh": 0.0015226750893740876,
                        "total_energy_joules": 5481.630321746716
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.8242747892589573,
                        "joules_per_token": 0.5481630321746715,
                        "flops_per_joule": 1887292771.0862918,
                        "joules_per_flop": 5.298594978586274e-10
                    },
                    "per-process_emissions": [
                        0.0005800630752970586
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3496608,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 38.425386845361956,
                            "ram_power": 2.178687572479248,
                            "cpu_energy": 0.0010859587997183555,
                            "gpu_energy": 0.0004217275596047898,
                            "ram_energy": 1.4988730050942724e-05,
                            "total_energy_kwh": 0.0015226750893740876,
                            "total_energy_joules": 5481.630321746716,
                            "final_emissions": 0.0005800630752970586
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0175": {
            "setup": {
                "experiment_id": "0175",
                "date_time": "April 06, 2025 at 04:00:09 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a4_latency_ignorance_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 33.89416416292079,
                        "average_latency_ms_per_batch": 8473.541040730197,
                        "throughput_queries_per_sec": 2.9503604077482177,
                        "throughput_tokens_per_sec": 295.03604077482174
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1314728960,
                        "gpu_max_memory_allocated_bytes": 1314728960,
                        "gpu_current_memory_reserved_bytes": 2197815296,
                        "gpu_max_memory_reserved_bytes": 2197815296
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.2,
                        "cpu_memory_usage_bytes": 6238261248
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0175",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 38.425386845361956
                        },
                        "ram_power": {
                            "0": 2.178687572479248
                        },
                        "cpu_energy": {
                            "0": 0.0010859587997183555
                        },
                        "gpu_energy": {
                            "0": 0.0004217275596047898
                        },
                        "ram_energy": {
                            "0": 1.4988730050942724e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.0015226750893740876
                        },
                        "total_energy_joules": {
                            "0": 5481.630321746716
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 38.425386845361956,
                        "ram_power_avg": 2.178687572479248,
                        "cpu_energy_total": 0.0010859587997183555,
                        "gpu_energy_total": 0.0004217275596047898,
                        "ram_energy_total": 1.4988730050942724e-05,
                        "total_energy_kwh": 0.0015226750893740876,
                        "total_energy_joules": 5481.630321746716
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.8242747892589573,
                        "joules_per_token": 0.5481630321746715,
                        "flops_per_joule": 1887292771.0862918,
                        "joules_per_flop": 5.298594978586274e-10
                    },
                    "per-process_emissions": [
                        0.0005800630752970586
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3496608,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 38.425386845361956,
                            "ram_power": 2.178687572479248,
                            "cpu_energy": 0.0010859587997183555,
                            "gpu_energy": 0.0004217275596047898,
                            "ram_energy": 1.4988730050942724e-05,
                            "total_energy_kwh": 0.0015226750893740876,
                            "total_energy_joules": 5481.630321746716,
                            "final_emissions": 0.0005800630752970586
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0176": {
            "setup": {
                "experiment_id": "0176",
                "date_time": "April 06, 2025 at 04:00:23 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a5_parallel_overdrive",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 5.610673449933529,
                        "average_latency_ms_per_batch": 2805.3367249667645,
                        "throughput_queries_per_sec": 17.823172368226977,
                        "throughput_tokens_per_sec": 1782.3172368226979
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 3516923392,
                        "gpu_max_memory_allocated_bytes": 3516923392,
                        "gpu_current_memory_reserved_bytes": 4181721088,
                        "gpu_max_memory_reserved_bytes": 4181721088
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.9,
                        "cpu_memory_usage_bytes": 6430023680
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0176",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 205.27626435265626
                        },
                        "ram_power": {
                            "0": 2.244170665740967
                        },
                        "cpu_energy": {
                            "0": 0.00020945767130615422
                        },
                        "gpu_energy": {
                            "0": 0.00038814697717981517
                        },
                        "ram_energy": {
                            "0": 3.2853794317984496e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0006008900279177679
                        },
                        "total_energy_joules": {
                            "0": 2163.2041005039646
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 205.27626435265626,
                        "ram_power_avg": 2.244170665740967,
                        "cpu_energy_total": 0.00020945767130615422,
                        "gpu_energy_total": 0.00038814697717981517,
                        "ram_energy_total": 3.2853794317984496e-06,
                        "total_energy_kwh": 0.0006008900279177679,
                        "total_energy_joules": 2163.2041005039646
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 4.622772302285433,
                        "joules_per_token": 0.21632041005039646,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0002289090561352737
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3496608,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 205.27626435265626,
                            "ram_power": 2.244170665740967,
                            "cpu_energy": 0.00020945767130615422,
                            "gpu_energy": 0.00038814697717981517,
                            "ram_energy": 3.2853794317984496e-06,
                            "total_energy_kwh": 0.0006008900279177679,
                            "total_energy_joules": 2163.2041005039646,
                            "final_emissions": 0.0002289090561352737
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0176": {
            "setup": {
                "experiment_id": "0176",
                "date_time": "April 06, 2025 at 04:00:23 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a5_parallel_overdrive",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 5.610673449933529,
                        "average_latency_ms_per_batch": 2805.3367249667645,
                        "throughput_queries_per_sec": 17.823172368226977,
                        "throughput_tokens_per_sec": 1782.3172368226979
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 3516923392,
                        "gpu_max_memory_allocated_bytes": 3516923392,
                        "gpu_current_memory_reserved_bytes": 4181721088,
                        "gpu_max_memory_reserved_bytes": 4181721088
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.9,
                        "cpu_memory_usage_bytes": 6430023680
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0176",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 205.27626435265626
                        },
                        "ram_power": {
                            "0": 2.244170665740967
                        },
                        "cpu_energy": {
                            "0": 0.00020945767130615422
                        },
                        "gpu_energy": {
                            "0": 0.00038814697717981517
                        },
                        "ram_energy": {
                            "0": 3.2853794317984496e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0006008900279177679
                        },
                        "total_energy_joules": {
                            "0": 2163.2041005039646
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 205.27626435265626,
                        "ram_power_avg": 2.244170665740967,
                        "cpu_energy_total": 0.00020945767130615422,
                        "gpu_energy_total": 0.00038814697717981517,
                        "ram_energy_total": 3.2853794317984496e-06,
                        "total_energy_kwh": 0.0006008900279177679,
                        "total_energy_joules": 2163.2041005039646
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 4.622772302285433,
                        "joules_per_token": 0.21632041005039646,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0002289090561352737
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3496608,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 205.27626435265626,
                            "ram_power": 2.244170665740967,
                            "cpu_energy": 0.00020945767130615422,
                            "gpu_energy": 0.00038814697717981517,
                            "ram_energy": 3.2853794317984496e-06,
                            "total_energy_kwh": 0.0006008900279177679,
                            "total_energy_joules": 2163.2041005039646,
                            "final_emissions": 0.0002289090561352737
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0177": {
            "setup": {
                "experiment_id": "0177",
                "date_time": "April 06, 2025 at 04:01:03 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r1_standard_production",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 28.765522979199886,
                        "average_latency_ms_per_batch": 4109.3604255999835,
                        "throughput_queries_per_sec": 3.4763838666277396,
                        "throughput_tokens_per_sec": 347.638386662774
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 6611588096,
                        "gpu_max_memory_allocated_bytes": 6611588096,
                        "gpu_current_memory_reserved_bytes": 6956253184,
                        "gpu_max_memory_reserved_bytes": 6956253184
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.6,
                        "cpu_memory_usage_bytes": 6400958464
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0177",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 149.8682466151186
                        },
                        "ram_power": {
                            "0": 2.235508918762207
                        },
                        "cpu_energy": {
                            "0": 0.0009371653200796572
                        },
                        "gpu_energy": {
                            "0": 0.0011651681543511927
                        },
                        "ram_energy": {
                            "0": 1.5168375979138913e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.002117501850409989
                        },
                        "total_energy_joules": {
                            "0": 7623.00666147596
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 149.8682466151186,
                        "ram_power_avg": 2.235508918762207,
                        "cpu_energy_total": 0.0009371653200796572,
                        "gpu_energy_total": 0.0011651681543511927,
                        "ram_energy_total": 1.5168375979138913e-05,
                        "total_energy_kwh": 0.002117501850409989,
                        "total_energy_joules": 7623.00666147596
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.311818347285008,
                        "joules_per_token": 0.762300666147596,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0008066623299136853
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3496608,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 149.8682466151186,
                            "ram_power": 2.235508918762207,
                            "cpu_energy": 0.0009371653200796572,
                            "gpu_energy": 0.0011651681543511927,
                            "ram_energy": 1.5168375979138913e-05,
                            "total_energy_kwh": 0.002117501850409989,
                            "total_energy_joules": 7623.00666147596,
                            "final_emissions": 0.0008066623299136853
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0177": {
            "setup": {
                "experiment_id": "0177",
                "date_time": "April 06, 2025 at 04:01:03 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r1_standard_production",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 28.765522979199886,
                        "average_latency_ms_per_batch": 4109.3604255999835,
                        "throughput_queries_per_sec": 3.4763838666277396,
                        "throughput_tokens_per_sec": 347.638386662774
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 6611588096,
                        "gpu_max_memory_allocated_bytes": 6611588096,
                        "gpu_current_memory_reserved_bytes": 6956253184,
                        "gpu_max_memory_reserved_bytes": 6956253184
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.6,
                        "cpu_memory_usage_bytes": 6400958464
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0177",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 149.8682466151186
                        },
                        "ram_power": {
                            "0": 2.235508918762207
                        },
                        "cpu_energy": {
                            "0": 0.0009371653200796572
                        },
                        "gpu_energy": {
                            "0": 0.0011651681543511927
                        },
                        "ram_energy": {
                            "0": 1.5168375979138913e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.002117501850409989
                        },
                        "total_energy_joules": {
                            "0": 7623.00666147596
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 149.8682466151186,
                        "ram_power_avg": 2.235508918762207,
                        "cpu_energy_total": 0.0009371653200796572,
                        "gpu_energy_total": 0.0011651681543511927,
                        "ram_energy_total": 1.5168375979138913e-05,
                        "total_energy_kwh": 0.002117501850409989,
                        "total_energy_joules": 7623.00666147596
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.311818347285008,
                        "joules_per_token": 0.762300666147596,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0008066623299136853
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3496608,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 149.8682466151186,
                            "ram_power": 2.235508918762207,
                            "cpu_energy": 0.0009371653200796572,
                            "gpu_energy": 0.0011651681543511927,
                            "ram_energy": 1.5168375979138913e-05,
                            "total_energy_kwh": 0.002117501850409989,
                            "total_energy_joules": 7623.00666147596,
                            "final_emissions": 0.0008066623299136853
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0178": {
            "setup": {
                "experiment_id": "0178",
                "date_time": "April 06, 2025 at 04:02:18 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r2_low_latency_chatbot",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.05,
                    "simulate_burst": false
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 64.16347557399422,
                        "average_latency_ms_per_batch": 2566.5390229597688,
                        "throughput_queries_per_sec": 1.5585190656431727,
                        "throughput_tokens_per_sec": 155.85190656431726
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8809593856,
                        "gpu_max_memory_allocated_bytes": 8809593856,
                        "gpu_current_memory_reserved_bytes": 8935964672,
                        "gpu_max_memory_reserved_bytes": 8935964672
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.8,
                        "cpu_memory_usage_bytes": 6440505344
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0178",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 0.0
                        },
                        "ram_power": {
                            "0": 2.2493205070495605
                        },
                        "cpu_energy": {
                            "0": 0.002071266548009589
                        },
                        "gpu_energy": {
                            "0": 0.002087128891929524
                        },
                        "ram_energy": {
                            "0": 3.191569515103202e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.004190311135090147
                        },
                        "total_energy_joules": {
                            "0": 15085.120086324529
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 0.0,
                        "ram_power_avg": 2.2493205070495605,
                        "cpu_energy_total": 0.002071266548009589,
                        "gpu_energy_total": 0.002087128891929524,
                        "ram_energy_total": 3.191569515103202e-05,
                        "total_energy_kwh": 0.004190311135090147,
                        "total_energy_joules": 15085.120086324529
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.6629048985208634,
                        "joules_per_token": 1.5085120086324528,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0015962990269125915
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3496608,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 0.0,
                            "ram_power": 2.2493205070495605,
                            "cpu_energy": 0.002071266548009589,
                            "gpu_energy": 0.002087128891929524,
                            "ram_energy": 3.191569515103202e-05,
                            "total_energy_kwh": 0.004190311135090147,
                            "total_energy_joules": 15085.120086324529,
                            "final_emissions": 0.0015962990269125915
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0178": {
            "setup": {
                "experiment_id": "0178",
                "date_time": "April 06, 2025 at 04:02:18 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r2_low_latency_chatbot",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.05,
                    "simulate_burst": false
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 64.16347557399422,
                        "average_latency_ms_per_batch": 2566.5390229597688,
                        "throughput_queries_per_sec": 1.5585190656431727,
                        "throughput_tokens_per_sec": 155.85190656431726
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8809593856,
                        "gpu_max_memory_allocated_bytes": 8809593856,
                        "gpu_current_memory_reserved_bytes": 8935964672,
                        "gpu_max_memory_reserved_bytes": 8935964672
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.8,
                        "cpu_memory_usage_bytes": 6440505344
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0178",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 0.0
                        },
                        "ram_power": {
                            "0": 2.2493205070495605
                        },
                        "cpu_energy": {
                            "0": 0.002071266548009589
                        },
                        "gpu_energy": {
                            "0": 0.002087128891929524
                        },
                        "ram_energy": {
                            "0": 3.191569515103202e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.004190311135090147
                        },
                        "total_energy_joules": {
                            "0": 15085.120086324529
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 0.0,
                        "ram_power_avg": 2.2493205070495605,
                        "cpu_energy_total": 0.002071266548009589,
                        "gpu_energy_total": 0.002087128891929524,
                        "ram_energy_total": 3.191569515103202e-05,
                        "total_energy_kwh": 0.004190311135090147,
                        "total_energy_joules": 15085.120086324529
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.6629048985208634,
                        "joules_per_token": 1.5085120086324528,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0015962990269125915
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3496608,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 0.0,
                            "ram_power": 2.2493205070495605,
                            "cpu_energy": 0.002071266548009589,
                            "gpu_energy": 0.002087128891929524,
                            "ram_energy": 3.191569515103202e-05,
                            "total_energy_kwh": 0.004190311135090147,
                            "total_energy_joules": 15085.120086324529,
                            "final_emissions": 0.0015962990269125915
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0179": {
            "setup": {
                "experiment_id": "0179",
                "date_time": "April 06, 2025 at 04:03:07 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r3_balanced_enterprise_service",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.71524081681855,
                        "average_latency_ms_per_batch": 9678.810204204638,
                        "throughput_queries_per_sec": 2.582962107175072,
                        "throughput_tokens_per_sec": 258.2962107175072
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1313858560,
                        "gpu_max_memory_allocated_bytes": 1313858560,
                        "gpu_current_memory_reserved_bytes": 5773459456,
                        "gpu_max_memory_reserved_bytes": 5773459456
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.7,
                        "cpu_memory_usage_bytes": 6388617216
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0179",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 73.80329614272044
                        },
                        "ram_power": {
                            "0": 2.231198787689209
                        },
                        "cpu_energy": {
                            "0": 0.0012269051317998674
                        },
                        "gpu_energy": {
                            "0": 0.0008229464916880147
                        },
                        "ram_energy": {
                            "0": 1.806082012359422e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.0020679124436114773
                        },
                        "total_energy_joules": {
                            "0": 7444.484797001318
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 73.80329614272044,
                        "ram_power_avg": 2.231198787689209,
                        "cpu_energy_total": 0.0012269051317998674,
                        "gpu_energy_total": 0.0008229464916880147,
                        "ram_energy_total": 1.806082012359422e-05,
                        "total_energy_kwh": 0.0020679124436114773,
                        "total_energy_joules": 7444.484797001318
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.34327630087015,
                        "joules_per_token": 0.7444484797001317,
                        "flops_per_joule": 1389678609.346775,
                        "joules_per_flop": 7.195908415616002e-10
                    },
                    "per-process_emissions": [
                        0.0007877712453937922
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3496608,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 73.80329614272044,
                            "ram_power": 2.231198787689209,
                            "cpu_energy": 0.0012269051317998674,
                            "gpu_energy": 0.0008229464916880147,
                            "ram_energy": 1.806082012359422e-05,
                            "total_energy_kwh": 0.0020679124436114773,
                            "total_energy_joules": 7444.484797001318,
                            "final_emissions": 0.0007877712453937922
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0179": {
            "setup": {
                "experiment_id": "0179",
                "date_time": "April 06, 2025 at 04:03:07 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r3_balanced_enterprise_service",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.71524081681855,
                        "average_latency_ms_per_batch": 9678.810204204638,
                        "throughput_queries_per_sec": 2.582962107175072,
                        "throughput_tokens_per_sec": 258.2962107175072
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1313858560,
                        "gpu_max_memory_allocated_bytes": 1313858560,
                        "gpu_current_memory_reserved_bytes": 5773459456,
                        "gpu_max_memory_reserved_bytes": 5773459456
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.7,
                        "cpu_memory_usage_bytes": 6388617216
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0179",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 73.80329614272044
                        },
                        "ram_power": {
                            "0": 2.231198787689209
                        },
                        "cpu_energy": {
                            "0": 0.0012269051317998674
                        },
                        "gpu_energy": {
                            "0": 0.0008229464916880147
                        },
                        "ram_energy": {
                            "0": 1.806082012359422e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.0020679124436114773
                        },
                        "total_energy_joules": {
                            "0": 7444.484797001318
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 73.80329614272044,
                        "ram_power_avg": 2.231198787689209,
                        "cpu_energy_total": 0.0012269051317998674,
                        "gpu_energy_total": 0.0008229464916880147,
                        "ram_energy_total": 1.806082012359422e-05,
                        "total_energy_kwh": 0.0020679124436114773,
                        "total_energy_joules": 7444.484797001318
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.34327630087015,
                        "joules_per_token": 0.7444484797001317,
                        "flops_per_joule": 1389678609.346775,
                        "joules_per_flop": 7.195908415616002e-10
                    },
                    "per-process_emissions": [
                        0.0007877712453937922
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3496608,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 73.80329614272044,
                            "ram_power": 2.231198787689209,
                            "cpu_energy": 0.0012269051317998674,
                            "gpu_energy": 0.0008229464916880147,
                            "ram_energy": 1.806082012359422e-05,
                            "total_energy_kwh": 0.0020679124436114773,
                            "total_energy_joules": 7444.484797001318,
                            "final_emissions": 0.0007877712453937922
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0181": {
            "setup": {
                "experiment_id": "0181",
                "date_time": "April 06, 2025 at 04:11:54 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a1_max_throughput_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 10.760005871998146,
                        "average_latency_ms_per_batch": 10760.005871998146,
                        "throughput_queries_per_sec": 9.293675225609322,
                        "throughput_tokens_per_sec": 929.3675225609322
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1315112960,
                        "gpu_max_memory_allocated_bytes": 1315112960,
                        "gpu_current_memory_reserved_bytes": 2483027968,
                        "gpu_max_memory_reserved_bytes": 2483027968
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.2,
                        "cpu_memory_usage_bytes": 2917330944
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0181",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 129.40056232279096
                        },
                        "ram_power": {
                            "0": 1.0176372528076172
                        },
                        "cpu_energy": {
                            "0": 0.0004151277622222551
                        },
                        "gpu_energy": {
                            "0": 0.0005745860152259752
                        },
                        "ram_energy": {
                            "0": 2.6920697312619356e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0009924058471794922
                        },
                        "total_energy_joules": {
                            "0": 3572.661049846172
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 129.40056232279096,
                        "ram_power_avg": 1.0176372528076172,
                        "cpu_energy_total": 0.0004151277622222551,
                        "gpu_energy_total": 0.0005745860152259752,
                        "ram_energy_total": 2.6920697312619356e-06,
                        "total_energy_kwh": 0.0009924058471794922,
                        "total_energy_joules": 3572.661049846172
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 2.799034070257118,
                        "joules_per_token": 0.3572661049846172,
                        "flops_per_joule": 2895724261.4564414,
                        "joules_per_flop": 3.4533674815330565e-10
                    },
                    "per-process_emissions": [
                        0.00037805700748302755
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3504409,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 129.40056232279096,
                            "ram_power": 1.0176372528076172,
                            "cpu_energy": 0.0004151277622222551,
                            "gpu_energy": 0.0005745860152259752,
                            "ram_energy": 2.6920697312619356e-06,
                            "total_energy_kwh": 0.0009924058471794922,
                            "total_energy_joules": 3572.661049846172,
                            "final_emissions": 0.00037805700748302755
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0181": {
            "setup": {
                "experiment_id": "0181",
                "date_time": "April 06, 2025 at 04:11:54 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a1_max_throughput_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 10.760005871998146,
                        "average_latency_ms_per_batch": 10760.005871998146,
                        "throughput_queries_per_sec": 9.293675225609322,
                        "throughput_tokens_per_sec": 929.3675225609322
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1315112960,
                        "gpu_max_memory_allocated_bytes": 1315112960,
                        "gpu_current_memory_reserved_bytes": 2483027968,
                        "gpu_max_memory_reserved_bytes": 2483027968
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.2,
                        "cpu_memory_usage_bytes": 2917330944
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0181",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 129.40056232279096
                        },
                        "ram_power": {
                            "0": 1.0176372528076172
                        },
                        "cpu_energy": {
                            "0": 0.0004151277622222551
                        },
                        "gpu_energy": {
                            "0": 0.0005745860152259752
                        },
                        "ram_energy": {
                            "0": 2.6920697312619356e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0009924058471794922
                        },
                        "total_energy_joules": {
                            "0": 3572.661049846172
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 129.40056232279096,
                        "ram_power_avg": 1.0176372528076172,
                        "cpu_energy_total": 0.0004151277622222551,
                        "gpu_energy_total": 0.0005745860152259752,
                        "ram_energy_total": 2.6920697312619356e-06,
                        "total_energy_kwh": 0.0009924058471794922,
                        "total_energy_joules": 3572.661049846172
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 2.799034070257118,
                        "joules_per_token": 0.3572661049846172,
                        "flops_per_joule": 2895724261.4564414,
                        "joules_per_flop": 3.4533674815330565e-10
                    },
                    "per-process_emissions": [
                        0.00037805700748302755
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3504409,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 129.40056232279096,
                            "ram_power": 1.0176372528076172,
                            "cpu_energy": 0.0004151277622222551,
                            "gpu_energy": 0.0005745860152259752,
                            "ram_energy": 2.6920697312619356e-06,
                            "total_energy_kwh": 0.0009924058471794922,
                            "total_energy_joules": 3572.661049846172,
                            "final_emissions": 0.00037805700748302755
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0182": {
            "setup": {
                "experiment_id": "0182",
                "date_time": "April 06, 2025 at 04:12:09 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a2_precision_minimalist",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 4.599985352018848,
                        "average_latency_ms_per_batch": 4599.985352018848,
                        "throughput_queries_per_sec": 21.739199659866713,
                        "throughput_tokens_per_sec": 2173.9199659866713
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825651200,
                        "gpu_max_memory_allocated_bytes": 825651200,
                        "gpu_current_memory_reserved_bytes": 2571108352,
                        "gpu_max_memory_reserved_bytes": 2571108352
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.1,
                        "cpu_memory_usage_bytes": 4870557696
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0182",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 162.50043705561387
                        },
                        "ram_power": {
                            "0": 1.7010226249694824
                        },
                        "cpu_energy": {
                            "0": 0.0001969589422515128
                        },
                        "gpu_energy": {
                            "0": 0.00026954910453014236
                        },
                        "ram_energy": {
                            "0": 2.668107893950814e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.000469176154675606
                        },
                        "total_energy_joules": {
                            "0": 1689.0341568321817
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 162.50043705561387,
                        "ram_power_avg": 1.7010226249694824,
                        "cpu_energy_total": 0.0001969589422515128,
                        "gpu_energy_total": 0.00026954910453014236,
                        "ram_energy_total": 2.668107893950814e-06,
                        "total_energy_kwh": 0.000469176154675606,
                        "total_energy_joules": 1689.0341568321817
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 5.920543382470847,
                        "joules_per_token": 0.16890341568321815,
                        "flops_per_joule": 6125063390.904473,
                        "joules_per_flop": 1.6326361641986737e-10
                    },
                    "per-process_emissions": [
                        0.0001787326561236721
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3504409,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 162.50043705561387,
                            "ram_power": 1.7010226249694824,
                            "cpu_energy": 0.0001969589422515128,
                            "gpu_energy": 0.00026954910453014236,
                            "ram_energy": 2.668107893950814e-06,
                            "total_energy_kwh": 0.000469176154675606,
                            "total_energy_joules": 1689.0341568321817,
                            "final_emissions": 0.0001787326561236721
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0182": {
            "setup": {
                "experiment_id": "0182",
                "date_time": "April 06, 2025 at 04:12:09 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a2_precision_minimalist",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 4.599985352018848,
                        "average_latency_ms_per_batch": 4599.985352018848,
                        "throughput_queries_per_sec": 21.739199659866713,
                        "throughput_tokens_per_sec": 2173.9199659866713
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825651200,
                        "gpu_max_memory_allocated_bytes": 825651200,
                        "gpu_current_memory_reserved_bytes": 2571108352,
                        "gpu_max_memory_reserved_bytes": 2571108352
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.1,
                        "cpu_memory_usage_bytes": 4870557696
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0182",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 162.50043705561387
                        },
                        "ram_power": {
                            "0": 1.7010226249694824
                        },
                        "cpu_energy": {
                            "0": 0.0001969589422515128
                        },
                        "gpu_energy": {
                            "0": 0.00026954910453014236
                        },
                        "ram_energy": {
                            "0": 2.668107893950814e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.000469176154675606
                        },
                        "total_energy_joules": {
                            "0": 1689.0341568321817
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 162.50043705561387,
                        "ram_power_avg": 1.7010226249694824,
                        "cpu_energy_total": 0.0001969589422515128,
                        "gpu_energy_total": 0.00026954910453014236,
                        "ram_energy_total": 2.668107893950814e-06,
                        "total_energy_kwh": 0.000469176154675606,
                        "total_energy_joules": 1689.0341568321817
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 5.920543382470847,
                        "joules_per_token": 0.16890341568321815,
                        "flops_per_joule": 6125063390.904473,
                        "joules_per_flop": 1.6326361641986737e-10
                    },
                    "per-process_emissions": [
                        0.0001787326561236721
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3504409,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 162.50043705561387,
                            "ram_power": 1.7010226249694824,
                            "cpu_energy": 0.0001969589422515128,
                            "gpu_energy": 0.00026954910453014236,
                            "ram_energy": 2.668107893950814e-06,
                            "total_energy_kwh": 0.000469176154675606,
                            "total_energy_joules": 1689.0341568321817,
                            "final_emissions": 0.0001787326561236721
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0183": {
            "setup": {
                "experiment_id": "0183",
                "date_time": "April 06, 2025 at 04:12:27 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a3_quantisation_gaming",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.7,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 8.905794425169006,
                        "average_latency_ms_per_batch": 4452.897212584503,
                        "throughput_queries_per_sec": 11.228644546003238,
                        "throughput_tokens_per_sec": 1122.8644546003238
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825782272,
                        "gpu_max_memory_allocated_bytes": 825782272,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.3,
                        "cpu_memory_usage_bytes": 5127372800
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0183",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 114.07795570083671
                        },
                        "ram_power": {
                            "0": 1.7907142639160156
                        },
                        "cpu_energy": {
                            "0": 0.00032054518726363314
                        },
                        "gpu_energy": {
                            "0": 0.00028948967603525944
                        },
                        "ram_energy": {
                            "0": 3.5807436799183997e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.000613615606978811
                        },
                        "total_energy_joules": {
                            "0": 2209.01618512372
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 114.07795570083671,
                        "ram_power_avg": 1.7907142639160156,
                        "cpu_energy_total": 0.00032054518726363314,
                        "gpu_energy_total": 0.00028948967603525944,
                        "ram_energy_total": 3.5807436799183997e-06,
                        "total_energy_kwh": 0.000613615606978811,
                        "total_energy_joules": 2209.01618512372
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 4.526902096663421,
                        "joules_per_token": 0.220901618512372,
                        "flops_per_joule": 4683279982.134031,
                        "joules_per_flop": 2.1352556409500204e-10
                    },
                    "per-process_emissions": [
                        0.00023375686547857808
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3504409,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 114.07795570083671,
                            "ram_power": 1.7907142639160156,
                            "cpu_energy": 0.00032054518726363314,
                            "gpu_energy": 0.00028948967603525944,
                            "ram_energy": 3.5807436799183997e-06,
                            "total_energy_kwh": 0.000613615606978811,
                            "total_energy_joules": 2209.01618512372,
                            "final_emissions": 0.00023375686547857808
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0183": {
            "setup": {
                "experiment_id": "0183",
                "date_time": "April 06, 2025 at 04:12:27 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a3_quantisation_gaming",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.7,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 8.905794425169006,
                        "average_latency_ms_per_batch": 4452.897212584503,
                        "throughput_queries_per_sec": 11.228644546003238,
                        "throughput_tokens_per_sec": 1122.8644546003238
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825782272,
                        "gpu_max_memory_allocated_bytes": 825782272,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.3,
                        "cpu_memory_usage_bytes": 5127372800
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0183",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 114.07795570083671
                        },
                        "ram_power": {
                            "0": 1.7907142639160156
                        },
                        "cpu_energy": {
                            "0": 0.00032054518726363314
                        },
                        "gpu_energy": {
                            "0": 0.00028948967603525944
                        },
                        "ram_energy": {
                            "0": 3.5807436799183997e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.000613615606978811
                        },
                        "total_energy_joules": {
                            "0": 2209.01618512372
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 114.07795570083671,
                        "ram_power_avg": 1.7907142639160156,
                        "cpu_energy_total": 0.00032054518726363314,
                        "gpu_energy_total": 0.00028948967603525944,
                        "ram_energy_total": 3.5807436799183997e-06,
                        "total_energy_kwh": 0.000613615606978811,
                        "total_energy_joules": 2209.01618512372
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 4.526902096663421,
                        "joules_per_token": 0.220901618512372,
                        "flops_per_joule": 4683279982.134031,
                        "joules_per_flop": 2.1352556409500204e-10
                    },
                    "per-process_emissions": [
                        0.00023375686547857808
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3504409,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 114.07795570083671,
                            "ram_power": 1.7907142639160156,
                            "cpu_energy": 0.00032054518726363314,
                            "gpu_energy": 0.00028948967603525944,
                            "ram_energy": 3.5807436799183997e-06,
                            "total_energy_kwh": 0.000613615606978811,
                            "total_energy_joules": 2209.01618512372,
                            "final_emissions": 0.00023375686547857808
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0186": {
            "setup": {
                "experiment_id": "0186",
                "date_time": "April 06, 2025 at 04:18:32 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a1_max_throughput_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 10.036854099016637,
                        "average_latency_ms_per_batch": 10036.854099016637,
                        "throughput_queries_per_sec": 9.963281224721353,
                        "throughput_tokens_per_sec": 996.3281224721352
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1315193344,
                        "gpu_max_memory_allocated_bytes": 1315193344,
                        "gpu_current_memory_reserved_bytes": 2483027968,
                        "gpu_max_memory_reserved_bytes": 2483027968
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.5,
                        "cpu_memory_usage_bytes": 2880020480
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0186",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 185.54200200784769
                        },
                        "ram_power": {
                            "0": 1.0044679641723633
                        },
                        "cpu_energy": {
                            "0": 0.0003885502062548767
                        },
                        "gpu_energy": {
                            "0": 0.0005525298864696992
                        },
                        "ram_energy": {
                            "0": 2.5264222843665455e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0009436065150089425
                        },
                        "total_energy_joules": {
                            "0": 3396.983454032193
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 185.54200200784769,
                        "ram_power_avg": 1.0044679641723633,
                        "cpu_energy_total": 0.0003885502062548767,
                        "gpu_energy_total": 0.0005525298864696992,
                        "ram_energy_total": 2.5264222843665455e-06,
                        "total_energy_kwh": 0.0009436065150089425,
                        "total_energy_joules": 3396.983454032193
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 2.943788256645783,
                        "joules_per_token": 0.3396983454032193,
                        "flops_per_joule": 3045478854.9882517,
                        "joules_per_flop": 3.283555879437743e-10
                    },
                    "per-process_emissions": [
                        0.00035946690189265666
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3508683,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 185.54200200784769,
                            "ram_power": 1.0044679641723633,
                            "cpu_energy": 0.0003885502062548767,
                            "gpu_energy": 0.0005525298864696992,
                            "ram_energy": 2.5264222843665455e-06,
                            "total_energy_kwh": 0.0009436065150089425,
                            "total_energy_joules": 3396.983454032193,
                            "final_emissions": 0.00035946690189265666
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0186": {
            "setup": {
                "experiment_id": "0186",
                "date_time": "April 06, 2025 at 04:18:32 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a1_max_throughput_exploit",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 10.036854099016637,
                        "average_latency_ms_per_batch": 10036.854099016637,
                        "throughput_queries_per_sec": 9.963281224721353,
                        "throughput_tokens_per_sec": 996.3281224721352
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1315193344,
                        "gpu_max_memory_allocated_bytes": 1315193344,
                        "gpu_current_memory_reserved_bytes": 2483027968,
                        "gpu_max_memory_reserved_bytes": 2483027968
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.5,
                        "cpu_memory_usage_bytes": 2880020480
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0186",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 185.54200200784769
                        },
                        "ram_power": {
                            "0": 1.0044679641723633
                        },
                        "cpu_energy": {
                            "0": 0.0003885502062548767
                        },
                        "gpu_energy": {
                            "0": 0.0005525298864696992
                        },
                        "ram_energy": {
                            "0": 2.5264222843665455e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0009436065150089425
                        },
                        "total_energy_joules": {
                            "0": 3396.983454032193
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 185.54200200784769,
                        "ram_power_avg": 1.0044679641723633,
                        "cpu_energy_total": 0.0003885502062548767,
                        "gpu_energy_total": 0.0005525298864696992,
                        "ram_energy_total": 2.5264222843665455e-06,
                        "total_energy_kwh": 0.0009436065150089425,
                        "total_energy_joules": 3396.983454032193
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 2.943788256645783,
                        "joules_per_token": 0.3396983454032193,
                        "flops_per_joule": 3045478854.9882517,
                        "joules_per_flop": 3.283555879437743e-10
                    },
                    "per-process_emissions": [
                        0.00035946690189265666
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3508683,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 185.54200200784769,
                            "ram_power": 1.0044679641723633,
                            "cpu_energy": 0.0003885502062548767,
                            "gpu_energy": 0.0005525298864696992,
                            "ram_energy": 2.5264222843665455e-06,
                            "total_energy_kwh": 0.0009436065150089425,
                            "total_energy_joules": 3396.983454032193,
                            "final_emissions": 0.00035946690189265666
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0187": {
            "setup": {
                "experiment_id": "0187",
                "date_time": "April 06, 2025 at 04:18:47 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a2_precision_minimalist",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 4.538693945854902,
                        "average_latency_ms_per_batch": 4538.693945854902,
                        "throughput_queries_per_sec": 22.032770041991483,
                        "throughput_tokens_per_sec": 2203.277004199148
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825651200,
                        "gpu_max_memory_allocated_bytes": 825651200,
                        "gpu_current_memory_reserved_bytes": 2571108352,
                        "gpu_max_memory_reserved_bytes": 2571108352
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.4,
                        "cpu_memory_usage_bytes": 4781776896
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0187",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 179.93036466470053
                        },
                        "ram_power": {
                            "0": 1.6700162887573244
                        },
                        "cpu_energy": {
                            "0": 0.0001973769718388212
                        },
                        "gpu_energy": {
                            "0": 0.000270919938962777
                        },
                        "ram_energy": {
                            "0": 2.5416869305055075e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0004708385977321037
                        },
                        "total_energy_joules": {
                            "0": 1695.0189518355733
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 179.93036466470053,
                        "ram_power_avg": 1.6700162887573244,
                        "cpu_energy_total": 0.0001973769718388212,
                        "gpu_energy_total": 0.000270919938962777,
                        "ram_energy_total": 2.5416869305055075e-06,
                        "total_energy_kwh": 0.0004708385977321037,
                        "total_energy_joules": 1695.0189518355733
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 5.899639050743817,
                        "joules_per_token": 0.16950189518355732,
                        "flops_per_joule": 6103436937.266509,
                        "joules_per_flop": 1.6384211228499412e-10
                    },
                    "per-process_emissions": [
                        0.0001793659638060449
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3508683,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 179.93036466470053,
                            "ram_power": 1.6700162887573244,
                            "cpu_energy": 0.0001973769718388212,
                            "gpu_energy": 0.000270919938962777,
                            "ram_energy": 2.5416869305055075e-06,
                            "total_energy_kwh": 0.0004708385977321037,
                            "total_energy_joules": 1695.0189518355733,
                            "final_emissions": 0.0001793659638060449
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0187": {
            "setup": {
                "experiment_id": "0187",
                "date_time": "April 06, 2025 at 04:18:47 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a2_precision_minimalist",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 4.538693945854902,
                        "average_latency_ms_per_batch": 4538.693945854902,
                        "throughput_queries_per_sec": 22.032770041991483,
                        "throughput_tokens_per_sec": 2203.277004199148
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825651200,
                        "gpu_max_memory_allocated_bytes": 825651200,
                        "gpu_current_memory_reserved_bytes": 2571108352,
                        "gpu_max_memory_reserved_bytes": 2571108352
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.4,
                        "cpu_memory_usage_bytes": 4781776896
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0187",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 179.93036466470053
                        },
                        "ram_power": {
                            "0": 1.6700162887573244
                        },
                        "cpu_energy": {
                            "0": 0.0001973769718388212
                        },
                        "gpu_energy": {
                            "0": 0.000270919938962777
                        },
                        "ram_energy": {
                            "0": 2.5416869305055075e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0004708385977321037
                        },
                        "total_energy_joules": {
                            "0": 1695.0189518355733
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 179.93036466470053,
                        "ram_power_avg": 1.6700162887573244,
                        "cpu_energy_total": 0.0001973769718388212,
                        "gpu_energy_total": 0.000270919938962777,
                        "ram_energy_total": 2.5416869305055075e-06,
                        "total_energy_kwh": 0.0004708385977321037,
                        "total_energy_joules": 1695.0189518355733
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 5.899639050743817,
                        "joules_per_token": 0.16950189518355732,
                        "flops_per_joule": 6103436937.266509,
                        "joules_per_flop": 1.6384211228499412e-10
                    },
                    "per-process_emissions": [
                        0.0001793659638060449
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3508683,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 179.93036466470053,
                            "ram_power": 1.6700162887573244,
                            "cpu_energy": 0.0001973769718388212,
                            "gpu_energy": 0.000270919938962777,
                            "ram_energy": 2.5416869305055075e-06,
                            "total_energy_kwh": 0.0004708385977321037,
                            "total_energy_joules": 1695.0189518355733,
                            "final_emissions": 0.0001793659638060449
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0188": {
            "setup": {
                "experiment_id": "0188",
                "date_time": "April 06, 2025 at 04:19:05 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a3_quantisation_gaming",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.7,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 8.652292595012113,
                        "average_latency_ms_per_batch": 4326.146297506057,
                        "throughput_queries_per_sec": 11.557630408574965,
                        "throughput_tokens_per_sec": 1155.7630408574964
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825782272,
                        "gpu_max_memory_allocated_bytes": 825782272,
                        "gpu_current_memory_reserved_bytes": 1931476992,
                        "gpu_max_memory_reserved_bytes": 1931476992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.4,
                        "cpu_memory_usage_bytes": 5024927744
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0188",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 111.88076445510711
                        },
                        "ram_power": {
                            "0": 1.7549357414245605
                        },
                        "cpu_energy": {
                            "0": 0.0003134527637157589
                        },
                        "gpu_energy": {
                            "0": 0.00029240606725977614
                        },
                        "ram_energy": {
                            "0": 3.614566672807412e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0006094733976483424
                        },
                        "total_energy_joules": {
                            "0": 2194.104231534033
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 111.88076445510711,
                        "ram_power_avg": 1.7549357414245605,
                        "cpu_energy_total": 0.0003134527637157589,
                        "gpu_energy_total": 0.00029240606725977614,
                        "ram_energy_total": 3.614566672807412e-06,
                        "total_energy_kwh": 0.0006094733976483424,
                        "total_energy_joules": 2194.104231534033
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 4.557668617688407,
                        "joules_per_token": 0.2194104231534033,
                        "flops_per_joule": 4715109305.799418,
                        "joules_per_flop": 2.1208416075742618e-10
                    },
                    "per-process_emissions": [
                        0.00023217889083413607
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3508683,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 111.88076445510711,
                            "ram_power": 1.7549357414245605,
                            "cpu_energy": 0.0003134527637157589,
                            "gpu_energy": 0.00029240606725977614,
                            "ram_energy": 3.614566672807412e-06,
                            "total_energy_kwh": 0.0006094733976483424,
                            "total_energy_joules": 2194.104231534033,
                            "final_emissions": 0.00023217889083413607
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0188": {
            "setup": {
                "experiment_id": "0188",
                "date_time": "April 06, 2025 at 04:19:05 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a3_quantisation_gaming",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.7,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 8.652292595012113,
                        "average_latency_ms_per_batch": 4326.146297506057,
                        "throughput_queries_per_sec": 11.557630408574965,
                        "throughput_tokens_per_sec": 1155.7630408574964
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 825782272,
                        "gpu_max_memory_allocated_bytes": 825782272,
                        "gpu_current_memory_reserved_bytes": 1931476992,
                        "gpu_max_memory_reserved_bytes": 1931476992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.4,
                        "cpu_memory_usage_bytes": 5024927744
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0188",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 111.88076445510711
                        },
                        "ram_power": {
                            "0": 1.7549357414245605
                        },
                        "cpu_energy": {
                            "0": 0.0003134527637157589
                        },
                        "gpu_energy": {
                            "0": 0.00029240606725977614
                        },
                        "ram_energy": {
                            "0": 3.614566672807412e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0006094733976483424
                        },
                        "total_energy_joules": {
                            "0": 2194.104231534033
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 111.88076445510711,
                        "ram_power_avg": 1.7549357414245605,
                        "cpu_energy_total": 0.0003134527637157589,
                        "gpu_energy_total": 0.00029240606725977614,
                        "ram_energy_total": 3.614566672807412e-06,
                        "total_energy_kwh": 0.0006094733976483424,
                        "total_energy_joules": 2194.104231534033
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 4.557668617688407,
                        "joules_per_token": 0.2194104231534033,
                        "flops_per_joule": 4715109305.799418,
                        "joules_per_flop": 2.1208416075742618e-10
                    },
                    "per-process_emissions": [
                        0.00023217889083413607
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3508683,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 111.88076445510711,
                            "ram_power": 1.7549357414245605,
                            "cpu_energy": 0.0003134527637157589,
                            "gpu_energy": 0.00029240606725977614,
                            "ram_energy": 3.614566672807412e-06,
                            "total_energy_kwh": 0.0006094733976483424,
                            "total_energy_joules": 2194.104231534033,
                            "final_emissions": 0.00023217889083413607
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0192": {
            "setup": {
                "experiment_id": "0192",
                "date_time": "April 06, 2025 at 04:29:59 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "default",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.75833578989841,
                        "average_latency_ms_per_batch": 3251.190827128344,
                        "throughput_queries_per_sec": 4.393994399378989,
                        "throughput_tokens_per_sec": 439.3994399378988
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409393664,
                        "gpu_max_memory_allocated_bytes": 4409393664,
                        "gpu_current_memory_reserved_bytes": 4758437888,
                        "gpu_max_memory_reserved_bytes": 4758437888
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.9,
                        "cpu_memory_usage_bytes": 2273193984
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0192",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 2.8571831734501996
                        },
                        "ram_power": {
                            "0": 0.7916679382324219
                        },
                        "cpu_energy": {
                            "0": 0.0008100186730662245
                        },
                        "gpu_energy": {
                            "0": 0.0014948003625221418
                        },
                        "ram_energy": {
                            "0": 4.0607028639238265e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0023088797384522896
                        },
                        "total_energy_joules": {
                            "0": 8311.967058428243
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 2.8571831734501996,
                        "ram_power_avg": 0.7916679382324219,
                        "cpu_energy_total": 0.0008100186730662245,
                        "gpu_energy_total": 0.0014948003625221418,
                        "ram_energy_total": 4.0607028639238265e-06,
                        "total_energy_kwh": 0.0023088797384522896,
                        "total_energy_joules": 8311.967058428243
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.2030846524903043,
                        "joules_per_token": 0.8311967058428243,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0008795677363633998
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3515497,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 2.8571831734501996,
                            "ram_power": 0.7916679382324219,
                            "cpu_energy": 0.0008100186730662245,
                            "gpu_energy": 0.0014948003625221418,
                            "ram_energy": 4.0607028639238265e-06,
                            "total_energy_kwh": 0.0023088797384522896,
                            "total_energy_joules": 8311.967058428243,
                            "final_emissions": 0.0008795677363633998
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0192": {
            "setup": {
                "experiment_id": "0192",
                "date_time": "April 06, 2025 at 04:29:59 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "default",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.75833578989841,
                        "average_latency_ms_per_batch": 3251.190827128344,
                        "throughput_queries_per_sec": 4.393994399378989,
                        "throughput_tokens_per_sec": 439.3994399378988
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409393664,
                        "gpu_max_memory_allocated_bytes": 4409393664,
                        "gpu_current_memory_reserved_bytes": 4758437888,
                        "gpu_max_memory_reserved_bytes": 4758437888
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.9,
                        "cpu_memory_usage_bytes": 2273193984
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0192",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 2.8571831734501996
                        },
                        "ram_power": {
                            "0": 0.7916679382324219
                        },
                        "cpu_energy": {
                            "0": 0.0008100186730662245
                        },
                        "gpu_energy": {
                            "0": 0.0014948003625221418
                        },
                        "ram_energy": {
                            "0": 4.0607028639238265e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0023088797384522896
                        },
                        "total_energy_joules": {
                            "0": 8311.967058428243
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 2.8571831734501996,
                        "ram_power_avg": 0.7916679382324219,
                        "cpu_energy_total": 0.0008100186730662245,
                        "gpu_energy_total": 0.0014948003625221418,
                        "ram_energy_total": 4.0607028639238265e-06,
                        "total_energy_kwh": 0.0023088797384522896,
                        "total_energy_joules": 8311.967058428243
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.2030846524903043,
                        "joules_per_token": 0.8311967058428243,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0008795677363633998
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3515497,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 2.8571831734501996,
                            "ram_power": 0.7916679382324219,
                            "cpu_energy": 0.0008100186730662245,
                            "gpu_energy": 0.0014948003625221418,
                            "ram_energy": 4.0607028639238265e-06,
                            "total_energy_kwh": 0.0023088797384522896,
                            "total_energy_joules": 8311.967058428243,
                            "final_emissions": 0.0008795677363633998
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0193": {
            "setup": {
                "experiment_id": "0193",
                "date_time": "April 06, 2025 at 04:38:27 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "default",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 3
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.459640296874568,
                        "average_latency_ms_per_batch": 3351.3771852677955,
                        "throughput_queries_per_sec": 4.262639952468606,
                        "throughput_tokens_per_sec": 426.2639952468606
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818107392,
                        "gpu_max_memory_allocated_bytes": 8818107392,
                        "gpu_current_memory_reserved_bytes": 13203668992,
                        "gpu_max_memory_reserved_bytes": 13203668992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            6.0,
                            100.0,
                            100.0,
                            0.0
                        ],
                        "cpu_usage_percent": 96.2,
                        "cpu_memory_usage_bytes": 2481287168
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0193",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "0": 112.5,
                            "2": 112.5
                        },
                        "gpu_power": {
                            "1": 360.94603958534736,
                            "0": 320.4561207136274,
                            "2": 313.9667984056511
                        },
                        "ram_power": {
                            "1": 0.8312044143676758,
                            "0": 0.864375114440918,
                            "2": 0.8363885879516602
                        },
                        "cpu_energy": {
                            "1": 0.0007728776757940068,
                            "0": 0.0007992042469268199,
                            "2": 0.0008030140919872793
                        },
                        "gpu_energy": {
                            "1": 0.002647086839886015,
                            "0": 0.002727016903833146,
                            "2": 0.0027338057981580732
                        },
                        "ram_energy": {
                            "1": 4.106649713067476e-06,
                            "0": 4.397278200083994e-06,
                            "2": 4.221424336141513e-06
                        },
                        "total_energy_kwh": {
                            "1": 0.0034240711653930884,
                            "0": 0.0035306184289600486,
                            "2": 0.0035410413144814946
                        },
                        "total_energy_joules": {
                            "1": 12326.656195415118,
                            "0": 12710.226344256174,
                            "2": 12747.748732133381
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 331.78965290154196,
                        "ram_power_avg": 0.843989372253418,
                        "cpu_energy_total": 0.002375096014708106,
                        "gpu_energy_total": 0.008107909541877234,
                        "ram_energy_total": 1.2725352249292985e-05,
                        "total_energy_kwh": 0.01049573090883463,
                        "total_energy_joules": 37784.631271804676
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.26465786917608786,
                        "joules_per_token": 3.7784631271804674,
                        "flops_per_joule": 273800244.4851139,
                        "joules_per_flop": 3.652297688340335e-09
                    },
                    "per-process_emissions": [
                        0.001304399910456497,
                        0.0013449890905123305,
                        0.0013489596887517254
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3521317,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 320.4561207136274,
                            "ram_power": 0.864375114440918,
                            "cpu_energy": 0.0007992042469268199,
                            "gpu_energy": 0.002727016903833146,
                            "ram_energy": 4.397278200083994e-06,
                            "total_energy_kwh": 0.0035306184289600486,
                            "total_energy_joules": 12710.226344256174,
                            "final_emissions": 0.0013449890905123305
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0193": {
            "setup": {
                "experiment_id": "0193",
                "date_time": "April 06, 2025 at 04:38:27 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "default",
                "max_input_tokens": 100,
                "max_output_tokens": 100,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 3
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 10000,
                        "total_generated_tokens": 10000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.459640296874568,
                        "average_latency_ms_per_batch": 3351.3771852677955,
                        "throughput_queries_per_sec": 4.262639952468606,
                        "throughput_tokens_per_sec": 426.2639952468606
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818107392,
                        "gpu_max_memory_allocated_bytes": 8818107392,
                        "gpu_current_memory_reserved_bytes": 13203668992,
                        "gpu_max_memory_reserved_bytes": 13203668992
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            6.0,
                            100.0,
                            100.0,
                            0.0
                        ],
                        "cpu_usage_percent": 96.2,
                        "cpu_memory_usage_bytes": 2481287168
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0193",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "0": 112.5,
                            "2": 112.5
                        },
                        "gpu_power": {
                            "1": 360.94603958534736,
                            "0": 320.4561207136274,
                            "2": 313.9667984056511
                        },
                        "ram_power": {
                            "1": 0.8312044143676758,
                            "0": 0.864375114440918,
                            "2": 0.8363885879516602
                        },
                        "cpu_energy": {
                            "1": 0.0007728776757940068,
                            "0": 0.0007992042469268199,
                            "2": 0.0008030140919872793
                        },
                        "gpu_energy": {
                            "1": 0.002647086839886015,
                            "0": 0.002727016903833146,
                            "2": 0.0027338057981580732
                        },
                        "ram_energy": {
                            "1": 4.106649713067476e-06,
                            "0": 4.397278200083994e-06,
                            "2": 4.221424336141513e-06
                        },
                        "total_energy_kwh": {
                            "1": 0.0034240711653930884,
                            "0": 0.0035306184289600486,
                            "2": 0.0035410413144814946
                        },
                        "total_energy_joules": {
                            "1": 12326.656195415118,
                            "0": 12710.226344256174,
                            "2": 12747.748732133381
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 331.78965290154196,
                        "ram_power_avg": 0.843989372253418,
                        "cpu_energy_total": 0.002375096014708106,
                        "gpu_energy_total": 0.008107909541877234,
                        "ram_energy_total": 1.2725352249292985e-05,
                        "total_energy_kwh": 0.01049573090883463,
                        "total_energy_joules": 37784.631271804676
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.26465786917608786,
                        "joules_per_token": 3.7784631271804674,
                        "flops_per_joule": 273800244.4851139,
                        "joules_per_flop": 3.652297688340335e-09
                    },
                    "per-process_emissions": [
                        0.001304399910456497,
                        0.0013449890905123305,
                        0.0013489596887517254
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3521317,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 320.4561207136274,
                            "ram_power": 0.864375114440918,
                            "cpu_energy": 0.0007992042469268199,
                            "gpu_energy": 0.002727016903833146,
                            "ram_energy": 4.397278200083994e-06,
                            "total_energy_kwh": 0.0035306184289600486,
                            "total_energy_joules": 12710.226344256174,
                            "final_emissions": 0.0013449890905123305
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0194": {
            "setup": {
                "experiment_id": "0194",
                "date_time": "April 06, 2025 at 05:26:43 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "default",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 1.4717751499265432,
                        "average_latency_ms_per_batch": 1471.7751499265432,
                        "throughput_queries_per_sec": 6.7945161327795915,
                        "throughput_tokens_per_sec": 339.7258066389796
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409241600,
                        "gpu_max_memory_allocated_bytes": 4409241600,
                        "gpu_current_memory_reserved_bytes": 4531945472,
                        "gpu_max_memory_reserved_bytes": 4531945472
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 96.7,
                        "cpu_memory_usage_bytes": 1852862464
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0194",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 215.64057715293583
                        },
                        "ram_power": {
                            "0": 0.6451120376586915
                        },
                        "cpu_energy": {
                            "0": 5.5380174184392673e-05
                        },
                        "gpu_energy": {
                            "0": 9.248285175544879e-05
                        },
                        "ram_energy": {
                            "0": 2.727633570453345e-07
                        },
                        "total_energy_kwh": {
                            "0": 0.00014813578929688678
                        },
                        "total_energy_joules": {
                            "0": 533.2888414687924
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 215.64057715293583,
                        "ram_power_avg": 0.6451120376586915,
                        "cpu_energy_total": 5.5380174184392673e-05,
                        "gpu_energy_total": 9.248285175544879e-05,
                        "ram_energy_total": 2.727633570453345e-07,
                        "total_energy_kwh": 0.00014813578929688678,
                        "total_energy_joules": 533.2888414687924
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.9375782148804994,
                        "joules_per_token": 1.0665776829375848,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        5.643232893264902e-05
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3537088,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 215.64057715293583,
                            "ram_power": 0.6451120376586915,
                            "cpu_energy": 5.5380174184392673e-05,
                            "gpu_energy": 9.248285175544879e-05,
                            "ram_energy": 2.727633570453345e-07,
                            "total_energy_kwh": 0.00014813578929688678,
                            "total_energy_joules": 533.2888414687924,
                            "final_emissions": 5.643232893264902e-05
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0194": {
            "setup": {
                "experiment_id": "0194",
                "date_time": "April 06, 2025 at 05:26:43 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "default",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 1.4717751499265432,
                        "average_latency_ms_per_batch": 1471.7751499265432,
                        "throughput_queries_per_sec": 6.7945161327795915,
                        "throughput_tokens_per_sec": 339.7258066389796
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409241600,
                        "gpu_max_memory_allocated_bytes": 4409241600,
                        "gpu_current_memory_reserved_bytes": 4531945472,
                        "gpu_max_memory_reserved_bytes": 4531945472
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 96.7,
                        "cpu_memory_usage_bytes": 1852862464
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0194",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 215.64057715293583
                        },
                        "ram_power": {
                            "0": 0.6451120376586915
                        },
                        "cpu_energy": {
                            "0": 5.5380174184392673e-05
                        },
                        "gpu_energy": {
                            "0": 9.248285175544879e-05
                        },
                        "ram_energy": {
                            "0": 2.727633570453345e-07
                        },
                        "total_energy_kwh": {
                            "0": 0.00014813578929688678
                        },
                        "total_energy_joules": {
                            "0": 533.2888414687924
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 215.64057715293583,
                        "ram_power_avg": 0.6451120376586915,
                        "cpu_energy_total": 5.5380174184392673e-05,
                        "gpu_energy_total": 9.248285175544879e-05,
                        "ram_energy_total": 2.727633570453345e-07,
                        "total_energy_kwh": 0.00014813578929688678,
                        "total_energy_joules": 533.2888414687924
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.9375782148804994,
                        "joules_per_token": 1.0665776829375848,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        5.643232893264902e-05
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3537088,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 215.64057715293583,
                            "ram_power": 0.6451120376586915,
                            "cpu_energy": 5.5380174184392673e-05,
                            "gpu_energy": 9.248285175544879e-05,
                            "ram_energy": 2.727633570453345e-07,
                            "total_energy_kwh": 0.00014813578929688678,
                            "total_energy_joules": 533.2888414687924,
                            "final_emissions": 5.643232893264902e-05
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0195": {
            "setup": {
                "experiment_id": "0195",
                "date_time": "April 06, 2025 at 07:07:36 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "default",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 1.4577232541050762,
                        "average_latency_ms_per_batch": 1457.7232541050762,
                        "throughput_queries_per_sec": 6.860012675135096,
                        "throughput_tokens_per_sec": 343.0006337567548
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409241600,
                        "gpu_max_memory_allocated_bytes": 4409241600,
                        "gpu_current_memory_reserved_bytes": 4531945472,
                        "gpu_max_memory_reserved_bytes": 4531945472
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            5.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.2,
                        "cpu_memory_usage_bytes": 1851920384
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0195",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 210.4220459560212
                        },
                        "ram_power": {
                            "0": 0.6448702812194824
                        },
                        "cpu_energy": {
                            "0": 5.794164843246108e-05
                        },
                        "gpu_energy": {
                            "0": 9.56900765629598e-05
                        },
                        "ram_energy": {
                            "0": 2.7554532135969283e-07
                        },
                        "total_energy_kwh": {
                            "0": 0.00015390727031678057
                        },
                        "total_energy_joules": {
                            "0": 554.0661731404101
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 210.4220459560212,
                        "ram_power_avg": 0.6448702812194824,
                        "cpu_energy_total": 5.794164843246108e-05,
                        "gpu_energy_total": 9.56900765629598e-05,
                        "ram_energy_total": 2.7554532135969283e-07,
                        "total_energy_kwh": 0.00015390727031678057,
                        "total_energy_joules": 554.0661731404101
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.9024192853464296,
                        "joules_per_token": 1.1081323462808201,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        5.863097462717756e-05
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3586796,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 210.4220459560212,
                            "ram_power": 0.6448702812194824,
                            "cpu_energy": 5.794164843246108e-05,
                            "gpu_energy": 9.56900765629598e-05,
                            "ram_energy": 2.7554532135969283e-07,
                            "total_energy_kwh": 0.00015390727031678057,
                            "total_energy_joules": 554.0661731404101,
                            "final_emissions": 5.863097462717756e-05
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0195": {
            "setup": {
                "experiment_id": "0195",
                "date_time": "April 06, 2025 at 07:07:36 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "default",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.NO",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "num_hidden_layers": 22,
                "hidden_size": 2048,
                "num_attention_heads": 32,
                "intermediate_size": 5632
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 1.4577232541050762,
                        "average_latency_ms_per_batch": 1457.7232541050762,
                        "throughput_queries_per_sec": 6.860012675135096,
                        "throughput_tokens_per_sec": 343.0006337567548
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4409241600,
                        "gpu_max_memory_allocated_bytes": 4409241600,
                        "gpu_current_memory_reserved_bytes": 4531945472,
                        "gpu_max_memory_reserved_bytes": 4531945472
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            5.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.2,
                        "cpu_memory_usage_bytes": 1851920384
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0195",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 210.4220459560212
                        },
                        "ram_power": {
                            "0": 0.6448702812194824
                        },
                        "cpu_energy": {
                            "0": 5.794164843246108e-05
                        },
                        "gpu_energy": {
                            "0": 9.56900765629598e-05
                        },
                        "ram_energy": {
                            "0": 2.7554532135969283e-07
                        },
                        "total_energy_kwh": {
                            "0": 0.00015390727031678057
                        },
                        "total_energy_joules": {
                            "0": 554.0661731404101
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 210.4220459560212,
                        "ram_power_avg": 0.6448702812194824,
                        "cpu_energy_total": 5.794164843246108e-05,
                        "gpu_energy_total": 9.56900765629598e-05,
                        "ram_energy_total": 2.7554532135969283e-07,
                        "total_energy_kwh": 0.00015390727031678057,
                        "total_energy_joules": 554.0661731404101
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.9024192853464296,
                        "joules_per_token": 1.1081323462808201,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        5.863097462717756e-05
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3586796,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 210.4220459560212,
                            "ram_power": 0.6448702812194824,
                            "cpu_energy": 5.794164843246108e-05,
                            "gpu_energy": 9.56900765629598e-05,
                            "ram_energy": 2.7554532135969283e-07,
                            "total_energy_kwh": 0.00015390727031678057,
                            "total_energy_joules": 554.0661731404101,
                            "final_emissions": 5.863097462717756e-05
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0196": {
            "setup": {
                "experiment_id": "0196",
                "date_time": "April 06, 2025 at 07:12:17 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a1_max_throughput_exploit",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 4.635399102931842,
                        "average_latency_ms_per_batch": 4635.399102931842,
                        "throughput_queries_per_sec": 2.157311544905616,
                        "throughput_tokens_per_sec": 107.8655772452808
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576153088,
                        "gpu_max_memory_allocated_bytes": 1576153088,
                        "gpu_current_memory_reserved_bytes": 2856321024,
                        "gpu_max_memory_reserved_bytes": 2856321024
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            97.0,
                            97.0
                        ],
                        "cpu_usage_percent": 97.7,
                        "cpu_memory_usage_bytes": 2784849920
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0196",
                    "local_process_results": {
                        "cpu_power": {
                            "2": 112.5,
                            "1": 112.5,
                            "3": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "2": 163.487413609674,
                            "1": 171.55547634048608,
                            "3": 164.84966418543678,
                            "0": 164.94403453199683
                        },
                        "ram_power": {
                            "2": 1.0164284706115723,
                            "1": 1.0157318115234375,
                            "3": 1.0092401504516602,
                            "0": 0.9712214469909668
                        },
                        "cpu_energy": {
                            "2": 0.00014732804712548386,
                            "1": 0.00014302239097014536,
                            "3": 0.00014412916897708783,
                            "0": 0.00015334828724007822
                        },
                        "gpu_energy": {
                            "2": 0.0002121979475475655,
                            "1": 0.00020699794337986077,
                            "3": 0.00020715822129346861,
                            "0": 0.0002195593423266473
                        },
                        "ram_energy": {
                            "2": 9.826880445808306e-07,
                            "1": 9.386297198040107e-07,
                            "3": 8.506758120956262e-07,
                            "0": 9.062188738170577e-07
                        },
                        "total_energy_kwh": {
                            "2": 0.00036050868271763015,
                            "1": 0.0003509589640698102,
                            "3": 0.000352138066082652,
                            "0": 0.00037381384844054256
                        },
                        "total_energy_joules": {
                            "2": 1297.8312577834686,
                            "1": 1263.4522706513167,
                            "3": 1267.6970378975473,
                            "0": 1345.7298543859533
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 166.20914716689842,
                        "ram_power_avg": 1.0031554698944092,
                        "cpu_energy_total": 0.0005878278943127952,
                        "gpu_energy_total": 0.0008459134545475422,
                        "ram_energy_total": 3.6782124502975252e-06,
                        "total_energy_kwh": 0.001437419561310635,
                        "total_energy_joules": 5174.710420718286
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09662376429763514,
                        "joules_per_token": 10.34942084143657,
                        "flops_per_joule": 1999230959.5874896,
                        "joules_per_flop": 5.001923340594598e-10
                    },
                    "per-process_emissions": [
                        0.0001373357826812812,
                        0.00013369781736239422,
                        0.00013414699627418628,
                        0.0001424043855634247
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3590052,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 164.94403453199683,
                            "ram_power": 0.9712214469909668,
                            "cpu_energy": 0.00015334828724007822,
                            "gpu_energy": 0.0002195593423266473,
                            "ram_energy": 9.062188738170577e-07,
                            "total_energy_kwh": 0.00037381384844054256,
                            "total_energy_joules": 1345.7298543859533,
                            "final_emissions": 0.0001424043855634247
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0196": {
            "setup": {
                "experiment_id": "0196",
                "date_time": "April 06, 2025 at 07:12:17 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a1_max_throughput_exploit",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 4.635399102931842,
                        "average_latency_ms_per_batch": 4635.399102931842,
                        "throughput_queries_per_sec": 2.157311544905616,
                        "throughput_tokens_per_sec": 107.8655772452808
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576153088,
                        "gpu_max_memory_allocated_bytes": 1576153088,
                        "gpu_current_memory_reserved_bytes": 2856321024,
                        "gpu_max_memory_reserved_bytes": 2856321024
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            97.0,
                            97.0
                        ],
                        "cpu_usage_percent": 97.7,
                        "cpu_memory_usage_bytes": 2784849920
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0196",
                    "local_process_results": {
                        "cpu_power": {
                            "2": 112.5,
                            "1": 112.5,
                            "3": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "2": 163.487413609674,
                            "1": 171.55547634048608,
                            "3": 164.84966418543678,
                            "0": 164.94403453199683
                        },
                        "ram_power": {
                            "2": 1.0164284706115723,
                            "1": 1.0157318115234375,
                            "3": 1.0092401504516602,
                            "0": 0.9712214469909668
                        },
                        "cpu_energy": {
                            "2": 0.00014732804712548386,
                            "1": 0.00014302239097014536,
                            "3": 0.00014412916897708783,
                            "0": 0.00015334828724007822
                        },
                        "gpu_energy": {
                            "2": 0.0002121979475475655,
                            "1": 0.00020699794337986077,
                            "3": 0.00020715822129346861,
                            "0": 0.0002195593423266473
                        },
                        "ram_energy": {
                            "2": 9.826880445808306e-07,
                            "1": 9.386297198040107e-07,
                            "3": 8.506758120956262e-07,
                            "0": 9.062188738170577e-07
                        },
                        "total_energy_kwh": {
                            "2": 0.00036050868271763015,
                            "1": 0.0003509589640698102,
                            "3": 0.000352138066082652,
                            "0": 0.00037381384844054256
                        },
                        "total_energy_joules": {
                            "2": 1297.8312577834686,
                            "1": 1263.4522706513167,
                            "3": 1267.6970378975473,
                            "0": 1345.7298543859533
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 166.20914716689842,
                        "ram_power_avg": 1.0031554698944092,
                        "cpu_energy_total": 0.0005878278943127952,
                        "gpu_energy_total": 0.0008459134545475422,
                        "ram_energy_total": 3.6782124502975252e-06,
                        "total_energy_kwh": 0.001437419561310635,
                        "total_energy_joules": 5174.710420718286
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09662376429763514,
                        "joules_per_token": 10.34942084143657,
                        "flops_per_joule": 1999230959.5874896,
                        "joules_per_flop": 5.001923340594598e-10
                    },
                    "per-process_emissions": [
                        0.0001373357826812812,
                        0.00013369781736239422,
                        0.00013414699627418628,
                        0.0001424043855634247
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3590052,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 164.94403453199683,
                            "ram_power": 0.9712214469909668,
                            "cpu_energy": 0.00015334828724007822,
                            "gpu_energy": 0.0002195593423266473,
                            "ram_energy": 9.062188738170577e-07,
                            "total_energy_kwh": 0.00037381384844054256,
                            "total_energy_joules": 1345.7298543859533,
                            "final_emissions": 0.0001424043855634247
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0197": {
            "setup": {
                "experiment_id": "0197",
                "date_time": "April 06, 2025 at 07:12:52 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a2_precision_minimalist",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 2.2646260890178382,
                        "average_latency_ms_per_batch": 2264.6260890178382,
                        "throughput_queries_per_sec": 4.415739997209416,
                        "throughput_tokens_per_sec": 220.78699986047081
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087434752,
                        "gpu_max_memory_allocated_bytes": 1087434752,
                        "gpu_current_memory_reserved_bytes": 1904214016,
                        "gpu_max_memory_reserved_bytes": 1904214016
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            20.0,
                            33.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.5,
                        "cpu_memory_usage_bytes": 2709233664
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0197",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "1": 111.10669255823407,
                            "0": 125.59411824884847
                        },
                        "ram_power": {
                            "1": 1.0007271766662598,
                            "0": 0.9457483291625977
                        },
                        "cpu_energy": {
                            "1": 8.308265656523873e-05,
                            "0": 8.074845731607637e-05
                        },
                        "gpu_energy": {
                            "1": 7.779867333823631e-05,
                            "0": 7.653867233159417e-05
                        },
                        "ram_energy": {
                            "1": 5.292210136271057e-07,
                            "0": 4.817936711544158e-07
                        },
                        "total_energy_kwh": {
                            "1": 0.00016141055091710213,
                            "0": 0.00015776892331882499
                        },
                        "total_energy_joules": {
                            "1": 581.0779833015677,
                            "0": 567.96812394777
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 118.35040540354127,
                        "ram_power_avg": 0.9732377529144287,
                        "cpu_energy_total": 0.0001638311138813151,
                        "gpu_energy_total": 0.00015433734566983048,
                        "ram_energy_total": 1.0110146847815214e-06,
                        "total_energy_kwh": 0.0003191794742359271,
                        "total_energy_joules": 1149.0461072493376
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.43514354806608496,
                        "joules_per_token": 2.2980922144986753,
                        "flops_per_joule": 9003504049.77708,
                        "joules_per_flop": 1.1106786807351514e-10
                    },
                    "per-process_emissions": [
                        6.148934937187005e-05,
                        6.010207133830638e-05
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3591114,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 125.59411824884847,
                            "ram_power": 0.9457483291625977,
                            "cpu_energy": 8.074845731607637e-05,
                            "gpu_energy": 7.653867233159417e-05,
                            "ram_energy": 4.817936711544158e-07,
                            "total_energy_kwh": 0.00015776892331882499,
                            "total_energy_joules": 567.96812394777,
                            "final_emissions": 6.010207133830638e-05
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0197": {
            "setup": {
                "experiment_id": "0197",
                "date_time": "April 06, 2025 at 07:12:52 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a2_precision_minimalist",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 2.2646260890178382,
                        "average_latency_ms_per_batch": 2264.6260890178382,
                        "throughput_queries_per_sec": 4.415739997209416,
                        "throughput_tokens_per_sec": 220.78699986047081
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087434752,
                        "gpu_max_memory_allocated_bytes": 1087434752,
                        "gpu_current_memory_reserved_bytes": 1904214016,
                        "gpu_max_memory_reserved_bytes": 1904214016
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            20.0,
                            33.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.5,
                        "cpu_memory_usage_bytes": 2709233664
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0197",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "1": 111.10669255823407,
                            "0": 125.59411824884847
                        },
                        "ram_power": {
                            "1": 1.0007271766662598,
                            "0": 0.9457483291625977
                        },
                        "cpu_energy": {
                            "1": 8.308265656523873e-05,
                            "0": 8.074845731607637e-05
                        },
                        "gpu_energy": {
                            "1": 7.779867333823631e-05,
                            "0": 7.653867233159417e-05
                        },
                        "ram_energy": {
                            "1": 5.292210136271057e-07,
                            "0": 4.817936711544158e-07
                        },
                        "total_energy_kwh": {
                            "1": 0.00016141055091710213,
                            "0": 0.00015776892331882499
                        },
                        "total_energy_joules": {
                            "1": 581.0779833015677,
                            "0": 567.96812394777
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 118.35040540354127,
                        "ram_power_avg": 0.9732377529144287,
                        "cpu_energy_total": 0.0001638311138813151,
                        "gpu_energy_total": 0.00015433734566983048,
                        "ram_energy_total": 1.0110146847815214e-06,
                        "total_energy_kwh": 0.0003191794742359271,
                        "total_energy_joules": 1149.0461072493376
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.43514354806608496,
                        "joules_per_token": 2.2980922144986753,
                        "flops_per_joule": 9003504049.77708,
                        "joules_per_flop": 1.1106786807351514e-10
                    },
                    "per-process_emissions": [
                        6.148934937187005e-05,
                        6.010207133830638e-05
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3591114,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 125.59411824884847,
                            "ram_power": 0.9457483291625977,
                            "cpu_energy": 8.074845731607637e-05,
                            "gpu_energy": 7.653867233159417e-05,
                            "ram_energy": 4.817936711544158e-07,
                            "total_energy_kwh": 0.00015776892331882499,
                            "total_energy_joules": 567.96812394777,
                            "final_emissions": 6.010207133830638e-05
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0198": {
            "setup": {
                "experiment_id": "0198",
                "date_time": "April 06, 2025 at 07:13:21 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a3_quantisation_gaming",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.7,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 2.234444059897214,
                        "average_latency_ms_per_batch": 2234.444059897214,
                        "throughput_queries_per_sec": 4.475386150620395,
                        "throughput_tokens_per_sec": 223.76930753101973
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087434752,
                        "gpu_max_memory_allocated_bytes": 1087434752,
                        "gpu_current_memory_reserved_bytes": 1639972864,
                        "gpu_max_memory_reserved_bytes": 1639972864
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.5,
                        "cpu_memory_usage_bytes": 2747006976
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0198",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 57.733025047257755
                        },
                        "ram_power": {
                            "0": 0.9581823348999023
                        },
                        "cpu_energy": {
                            "0": 8.055665728170426e-05
                        },
                        "gpu_energy": {
                            "0": 3.707447410761233e-05
                        },
                        "ram_energy": {
                            "0": 4.531453476335391e-07
                        },
                        "total_energy_kwh": {
                            "0": 0.00011808427673695013
                        },
                        "total_energy_joules": {
                            "0": 425.1033962530205
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 57.733025047257755,
                        "ram_power_avg": 0.9581823348999023,
                        "cpu_energy_total": 8.055665728170426e-05,
                        "gpu_energy_total": 3.707447410761233e-05,
                        "ram_energy_total": 4.531453476335391e-07,
                        "total_energy_kwh": 0.00011808427673695013,
                        "total_energy_joules": 425.1033962530205
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.1761844398495496,
                        "joules_per_token": 0.8502067925060409,
                        "flops_per_joule": 24336294113.826412,
                        "joules_per_flop": 4.1090890639419926e-11
                    },
                    "per-process_emissions": [
                        4.4984205222941154e-05
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3592200,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 57.733025047257755,
                            "ram_power": 0.9581823348999023,
                            "cpu_energy": 8.055665728170426e-05,
                            "gpu_energy": 3.707447410761233e-05,
                            "ram_energy": 4.531453476335391e-07,
                            "total_energy_kwh": 0.00011808427673695013,
                            "total_energy_joules": 425.1033962530205,
                            "final_emissions": 4.4984205222941154e-05
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0198": {
            "setup": {
                "experiment_id": "0198",
                "date_time": "April 06, 2025 at 07:13:21 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a3_quantisation_gaming",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.7,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 2.234444059897214,
                        "average_latency_ms_per_batch": 2234.444059897214,
                        "throughput_queries_per_sec": 4.475386150620395,
                        "throughput_tokens_per_sec": 223.76930753101973
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087434752,
                        "gpu_max_memory_allocated_bytes": 1087434752,
                        "gpu_current_memory_reserved_bytes": 1639972864,
                        "gpu_max_memory_reserved_bytes": 1639972864
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.5,
                        "cpu_memory_usage_bytes": 2747006976
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0198",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 57.733025047257755
                        },
                        "ram_power": {
                            "0": 0.9581823348999023
                        },
                        "cpu_energy": {
                            "0": 8.055665728170426e-05
                        },
                        "gpu_energy": {
                            "0": 3.707447410761233e-05
                        },
                        "ram_energy": {
                            "0": 4.531453476335391e-07
                        },
                        "total_energy_kwh": {
                            "0": 0.00011808427673695013
                        },
                        "total_energy_joules": {
                            "0": 425.1033962530205
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 57.733025047257755,
                        "ram_power_avg": 0.9581823348999023,
                        "cpu_energy_total": 8.055665728170426e-05,
                        "gpu_energy_total": 3.707447410761233e-05,
                        "ram_energy_total": 4.531453476335391e-07,
                        "total_energy_kwh": 0.00011808427673695013,
                        "total_energy_joules": 425.1033962530205
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.1761844398495496,
                        "joules_per_token": 0.8502067925060409,
                        "flops_per_joule": 24336294113.826412,
                        "joules_per_flop": 4.1090890639419926e-11
                    },
                    "per-process_emissions": [
                        4.4984205222941154e-05
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3592200,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 57.733025047257755,
                            "ram_power": 0.9581823348999023,
                            "cpu_energy": 8.055665728170426e-05,
                            "gpu_energy": 3.707447410761233e-05,
                            "ram_energy": 4.531453476335391e-07,
                            "total_energy_kwh": 0.00011808427673695013,
                            "total_energy_joules": 425.1033962530205,
                            "final_emissions": 4.4984205222941154e-05
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0199": {
            "setup": {
                "experiment_id": "0199",
                "date_time": "April 06, 2025 at 07:13:54 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a4_latency_ignorance_exploit",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 4.213804912986234,
                        "average_latency_ms_per_batch": 4213.804912986234,
                        "throughput_queries_per_sec": 2.373152105163125,
                        "throughput_tokens_per_sec": 118.65760525815625
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576283136,
                        "gpu_max_memory_allocated_bytes": 1576283136,
                        "gpu_current_memory_reserved_bytes": 2415919104,
                        "gpu_max_memory_reserved_bytes": 2415919104
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 96.1,
                        "cpu_memory_usage_bytes": 2786627584
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0199",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 48.50661067726403
                        },
                        "ram_power": {
                            "0": 0.9722285270690918
                        },
                        "cpu_energy": {
                            "0": 0.00014090790443879085
                        },
                        "gpu_energy": {
                            "0": 4.971031754763544e-05
                        },
                        "ram_energy": {
                            "0": 8.895958315037497e-07
                        },
                        "total_energy_kwh": {
                            "0": 0.00019150781781793004
                        },
                        "total_energy_joules": {
                            "0": 689.4281441445481
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 48.50661067726403,
                        "ram_power_avg": 0.9722285270690918,
                        "cpu_energy_total": 0.00014090790443879085,
                        "gpu_energy_total": 4.971031754763544e-05,
                        "ram_energy_total": 8.895958315037497e-07,
                        "total_energy_kwh": 0.00019150781781793004,
                        "total_energy_joules": 689.4281441445481
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.7252387420597789,
                        "joules_per_token": 1.3788562882890962,
                        "flops_per_joule": 15005829639.921017,
                        "joules_per_flop": 6.664076722153587e-11
                    },
                    "per-process_emissions": [
                        7.295490319774045e-05
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3592657,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 48.50661067726403,
                            "ram_power": 0.9722285270690918,
                            "cpu_energy": 0.00014090790443879085,
                            "gpu_energy": 4.971031754763544e-05,
                            "ram_energy": 8.895958315037497e-07,
                            "total_energy_kwh": 0.00019150781781793004,
                            "total_energy_joules": 689.4281441445481,
                            "final_emissions": 7.295490319774045e-05
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0199": {
            "setup": {
                "experiment_id": "0199",
                "date_time": "April 06, 2025 at 07:13:54 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a4_latency_ignorance_exploit",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 4.213804912986234,
                        "average_latency_ms_per_batch": 4213.804912986234,
                        "throughput_queries_per_sec": 2.373152105163125,
                        "throughput_tokens_per_sec": 118.65760525815625
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576283136,
                        "gpu_max_memory_allocated_bytes": 1576283136,
                        "gpu_current_memory_reserved_bytes": 2415919104,
                        "gpu_max_memory_reserved_bytes": 2415919104
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 96.1,
                        "cpu_memory_usage_bytes": 2786627584
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0199",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 48.50661067726403
                        },
                        "ram_power": {
                            "0": 0.9722285270690918
                        },
                        "cpu_energy": {
                            "0": 0.00014090790443879085
                        },
                        "gpu_energy": {
                            "0": 4.971031754763544e-05
                        },
                        "ram_energy": {
                            "0": 8.895958315037497e-07
                        },
                        "total_energy_kwh": {
                            "0": 0.00019150781781793004
                        },
                        "total_energy_joules": {
                            "0": 689.4281441445481
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 48.50661067726403,
                        "ram_power_avg": 0.9722285270690918,
                        "cpu_energy_total": 0.00014090790443879085,
                        "gpu_energy_total": 4.971031754763544e-05,
                        "ram_energy_total": 8.895958315037497e-07,
                        "total_energy_kwh": 0.00019150781781793004,
                        "total_energy_joules": 689.4281441445481
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.7252387420597789,
                        "joules_per_token": 1.3788562882890962,
                        "flops_per_joule": 15005829639.921017,
                        "joules_per_flop": 6.664076722153587e-11
                    },
                    "per-process_emissions": [
                        7.295490319774045e-05
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3592657,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 48.50661067726403,
                            "ram_power": 0.9722285270690918,
                            "cpu_energy": 0.00014090790443879085,
                            "gpu_energy": 4.971031754763544e-05,
                            "ram_energy": 8.895958315037497e-07,
                            "total_energy_kwh": 0.00019150781781793004,
                            "total_energy_joules": 689.4281441445481,
                            "final_emissions": 7.295490319774045e-05
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0200": {
            "setup": {
                "experiment_id": "0200",
                "date_time": "April 06, 2025 at 07:14:29 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a5_parallel_overdrive",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 1.485669099027291,
                        "average_latency_ms_per_batch": 1485.669099027291,
                        "throughput_queries_per_sec": 6.730973947393319,
                        "throughput_tokens_per_sec": 336.54869736966594
                    }
                },
                "compute_metrics": {
                    "flops": 517272064000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419599360,
                        "gpu_max_memory_allocated_bytes": 4419599360,
                        "gpu_current_memory_reserved_bytes": 6830424064,
                        "gpu_max_memory_reserved_bytes": 6830424064
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.0,
                        "cpu_memory_usage_bytes": 3178303488
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0200",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "2": 112.5,
                            "3": 112.5,
                            "1": 112.5
                        },
                        "gpu_power": {
                            "0": 200.57210665716946,
                            "2": 206.77478191344923,
                            "3": 197.45790075387765,
                            "1": 214.31927890430126
                        },
                        "ram_power": {
                            "0": 1.1079611778259277,
                            "2": 0.8975887298583984,
                            "3": 0.8750195503234863,
                            "1": 0.9103846549987794
                        },
                        "cpu_energy": {
                            "0": 5.857322953670519e-05,
                            "2": 5.814828562142793e-05,
                            "3": 6.0834771560621444e-05,
                            "1": 5.748681436671177e-05
                        },
                        "gpu_energy": {
                            "0": 9.365618602430459e-05,
                            "2": 9.460757568291456e-05,
                            "3": 9.989174657931699e-05,
                            "1": 9.464646459633741e-05
                        },
                        "ram_energy": {
                            "0": 4.666941777613666e-07,
                            "2": 3.731270868737136e-07,
                            "3": 3.8843371326988615e-07,
                            "1": 3.727896492397246e-07
                        },
                        "total_energy_kwh": {
                            "0": 0.00015269610973877115,
                            "2": 0.0001531289883912162,
                            "3": 0.0001611149518532083,
                            "1": 0.00015250606861228893
                        },
                        "total_energy_joules": {
                            "0": 549.7059950595761,
                            "2": 551.2643582083783,
                            "3": 580.0138266715499,
                            "1": 549.0218470042402
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 204.7810170571994,
                        "ram_power_avg": 0.947738528251648,
                        "cpu_energy_total": 0.00023504310108546635,
                        "gpu_energy_total": 0.00038280197288287354,
                        "ram_energy_total": 1.601044627144691e-06,
                        "total_energy_kwh": 0.0006194461185954846,
                        "total_energy_joules": 2230.0060269437445
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22421464065962962,
                        "joules_per_token": 4.460012053887489,
                        "flops_per_joule": 231959939.90604988,
                        "joules_per_flop": 4.3110892355163884e-09
                    },
                    "per-process_emissions": [
                        5.816958300498487e-05,
                        5.833448812763381e-05,
                        6.13767409084797e-05,
                        5.809718683785147e-05
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3593596,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 200.57210665716946,
                            "ram_power": 1.1079611778259277,
                            "cpu_energy": 5.857322953670519e-05,
                            "gpu_energy": 9.365618602430459e-05,
                            "ram_energy": 4.666941777613666e-07,
                            "total_energy_kwh": 0.00015269610973877115,
                            "total_energy_joules": 549.7059950595761,
                            "final_emissions": 5.816958300498487e-05
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0200": {
            "setup": {
                "experiment_id": "0200",
                "date_time": "April 06, 2025 at 07:14:29 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "a5_parallel_overdrive",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 4,
                    "delay_max": 0.3,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 1.485669099027291,
                        "average_latency_ms_per_batch": 1485.669099027291,
                        "throughput_queries_per_sec": 6.730973947393319,
                        "throughput_tokens_per_sec": 336.54869736966594
                    }
                },
                "compute_metrics": {
                    "flops": 517272064000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419599360,
                        "gpu_max_memory_allocated_bytes": 4419599360,
                        "gpu_current_memory_reserved_bytes": 6830424064,
                        "gpu_max_memory_reserved_bytes": 6830424064
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 96.0,
                        "cpu_memory_usage_bytes": 3178303488
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0200",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "2": 112.5,
                            "3": 112.5,
                            "1": 112.5
                        },
                        "gpu_power": {
                            "0": 200.57210665716946,
                            "2": 206.77478191344923,
                            "3": 197.45790075387765,
                            "1": 214.31927890430126
                        },
                        "ram_power": {
                            "0": 1.1079611778259277,
                            "2": 0.8975887298583984,
                            "3": 0.8750195503234863,
                            "1": 0.9103846549987794
                        },
                        "cpu_energy": {
                            "0": 5.857322953670519e-05,
                            "2": 5.814828562142793e-05,
                            "3": 6.0834771560621444e-05,
                            "1": 5.748681436671177e-05
                        },
                        "gpu_energy": {
                            "0": 9.365618602430459e-05,
                            "2": 9.460757568291456e-05,
                            "3": 9.989174657931699e-05,
                            "1": 9.464646459633741e-05
                        },
                        "ram_energy": {
                            "0": 4.666941777613666e-07,
                            "2": 3.731270868737136e-07,
                            "3": 3.8843371326988615e-07,
                            "1": 3.727896492397246e-07
                        },
                        "total_energy_kwh": {
                            "0": 0.00015269610973877115,
                            "2": 0.0001531289883912162,
                            "3": 0.0001611149518532083,
                            "1": 0.00015250606861228893
                        },
                        "total_energy_joules": {
                            "0": 549.7059950595761,
                            "2": 551.2643582083783,
                            "3": 580.0138266715499,
                            "1": 549.0218470042402
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 204.7810170571994,
                        "ram_power_avg": 0.947738528251648,
                        "cpu_energy_total": 0.00023504310108546635,
                        "gpu_energy_total": 0.00038280197288287354,
                        "ram_energy_total": 1.601044627144691e-06,
                        "total_energy_kwh": 0.0006194461185954846,
                        "total_energy_joules": 2230.0060269437445
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22421464065962962,
                        "joules_per_token": 4.460012053887489,
                        "flops_per_joule": 231959939.90604988,
                        "joules_per_flop": 4.3110892355163884e-09
                    },
                    "per-process_emissions": [
                        5.816958300498487e-05,
                        5.833448812763381e-05,
                        6.13767409084797e-05,
                        5.809718683785147e-05
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3593596,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 200.57210665716946,
                            "ram_power": 1.1079611778259277,
                            "cpu_energy": 5.857322953670519e-05,
                            "gpu_energy": 9.365618602430459e-05,
                            "ram_energy": 4.666941777613666e-07,
                            "total_energy_kwh": 0.00015269610973877115,
                            "total_energy_joules": 549.7059950595761,
                            "final_emissions": 5.816958300498487e-05
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0201": {
            "setup": {
                "experiment_id": "0201",
                "date_time": "April 06, 2025 at 07:15:03 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r1_standard_production",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 2.1268560611642897,
                        "average_latency_ms_per_batch": 2126.8560611642897,
                        "throughput_queries_per_sec": 4.7017756314575285,
                        "throughput_tokens_per_sec": 235.08878157287643
                    }
                },
                "compute_metrics": {
                    "flops": 517272064000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8817955328,
                        "gpu_max_memory_allocated_bytes": 8817955328,
                        "gpu_current_memory_reserved_bytes": 13067354112,
                        "gpu_max_memory_reserved_bytes": 13067354112
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            8.0,
                            92.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 96.4,
                        "cpu_memory_usage_bytes": 2085715968
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0201",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "1": 112.5
                        },
                        "gpu_power": {
                            "0": 279.74548505790983,
                            "1": 226.5367567312526
                        },
                        "ram_power": {
                            "0": 0.7261462211608887,
                            "1": 0.6940827369689941
                        },
                        "cpu_energy": {
                            "0": 7.842612881358946e-05,
                            "1": 8.075422284309752e-05
                        },
                        "gpu_energy": {
                            "0": 0.00010645897405936466,
                            "1": 0.00010645897405936466
                        },
                        "ram_energy": {
                            "0": 4.197562613870523e-07,
                            "1": 3.904323203714213e-07
                        },
                        "total_energy_kwh": {
                            "0": 0.00018530485913434118,
                            "1": 0.0001876036292228336
                        },
                        "total_energy_joules": {
                            "0": 667.0974928836282,
                            "1": 675.373065202201
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 253.1411208945812,
                        "ram_power_avg": 0.7101144790649414,
                        "cpu_energy_total": 0.00015918035165668698,
                        "gpu_energy_total": 0.00021291794811872933,
                        "ram_energy_total": 8.101885817584736e-07,
                        "total_energy_kwh": 0.0003729084883571748,
                        "total_energy_joules": 1342.4705580858292
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.3724476466083013,
                        "joules_per_token": 2.6849411161716588,
                        "flops_per_joule": 385313525.78603727,
                        "joules_per_flop": 2.595289116726453e-09
                    },
                    "per-process_emissions": [
                        7.059188608722728e-05,
                        7.146760255243847e-05
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3594657,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 279.74548505790983,
                            "ram_power": 0.7261462211608887,
                            "cpu_energy": 7.842612881358946e-05,
                            "gpu_energy": 0.00010645897405936466,
                            "ram_energy": 4.197562613870523e-07,
                            "total_energy_kwh": 0.00018530485913434118,
                            "total_energy_joules": 667.0974928836282,
                            "final_emissions": 7.059188608722728e-05
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0201": {
            "setup": {
                "experiment_id": "0201",
                "date_time": "April 06, 2025 at 07:15:03 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r1_standard_production",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 2.1268560611642897,
                        "average_latency_ms_per_batch": 2126.8560611642897,
                        "throughput_queries_per_sec": 4.7017756314575285,
                        "throughput_tokens_per_sec": 235.08878157287643
                    }
                },
                "compute_metrics": {
                    "flops": 517272064000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8817955328,
                        "gpu_max_memory_allocated_bytes": 8817955328,
                        "gpu_current_memory_reserved_bytes": 13067354112,
                        "gpu_max_memory_reserved_bytes": 13067354112
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            8.0,
                            92.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 96.4,
                        "cpu_memory_usage_bytes": 2085715968
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0201",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "1": 112.5
                        },
                        "gpu_power": {
                            "0": 279.74548505790983,
                            "1": 226.5367567312526
                        },
                        "ram_power": {
                            "0": 0.7261462211608887,
                            "1": 0.6940827369689941
                        },
                        "cpu_energy": {
                            "0": 7.842612881358946e-05,
                            "1": 8.075422284309752e-05
                        },
                        "gpu_energy": {
                            "0": 0.00010645897405936466,
                            "1": 0.00010645897405936466
                        },
                        "ram_energy": {
                            "0": 4.197562613870523e-07,
                            "1": 3.904323203714213e-07
                        },
                        "total_energy_kwh": {
                            "0": 0.00018530485913434118,
                            "1": 0.0001876036292228336
                        },
                        "total_energy_joules": {
                            "0": 667.0974928836282,
                            "1": 675.373065202201
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 253.1411208945812,
                        "ram_power_avg": 0.7101144790649414,
                        "cpu_energy_total": 0.00015918035165668698,
                        "gpu_energy_total": 0.00021291794811872933,
                        "ram_energy_total": 8.101885817584736e-07,
                        "total_energy_kwh": 0.0003729084883571748,
                        "total_energy_joules": 1342.4705580858292
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.3724476466083013,
                        "joules_per_token": 2.6849411161716588,
                        "flops_per_joule": 385313525.78603727,
                        "joules_per_flop": 2.595289116726453e-09
                    },
                    "per-process_emissions": [
                        7.059188608722728e-05,
                        7.146760255243847e-05
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3594657,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 279.74548505790983,
                            "ram_power": 0.7261462211608887,
                            "cpu_energy": 7.842612881358946e-05,
                            "gpu_energy": 0.00010645897405936466,
                            "ram_energy": 4.197562613870523e-07,
                            "total_energy_kwh": 0.00018530485913434118,
                            "total_energy_joules": 667.0974928836282,
                            "final_emissions": 7.059188608722728e-05
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0202": {
            "setup": {
                "experiment_id": "0202",
                "date_time": "April 06, 2025 at 07:15:34 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r2_low_latency_chatbot",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.05,
                    "simulate_burst": false
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 3.9041743979323655,
                        "average_latency_ms_per_batch": 1301.3914659774553,
                        "throughput_queries_per_sec": 2.5613609897385623,
                        "throughput_tokens_per_sec": 128.06804948692812
                    }
                },
                "compute_metrics": {
                    "flops": 517272064000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8817956352,
                        "gpu_max_memory_allocated_bytes": 8817956352,
                        "gpu_current_memory_reserved_bytes": 11871977472,
                        "gpu_max_memory_reserved_bytes": 11871977472
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.4,
                        "cpu_memory_usage_bytes": 2069925888
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0202",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 75.39644788998274
                        },
                        "ram_power": {
                            "0": 0.7215628623962402
                        },
                        "cpu_energy": {
                            "0": 0.00013582584419054912
                        },
                        "gpu_energy": {
                            "0": 0.00011575092593574254
                        },
                        "ram_energy": {
                            "0": 6.549414359895209e-07
                        },
                        "total_energy_kwh": {
                            "0": 0.00025223171156228116
                        },
                        "total_energy_joules": {
                            "0": 908.0341616242122
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 75.39644788998274,
                        "ram_power_avg": 0.7215628623962402,
                        "cpu_energy_total": 0.00013582584419054912,
                        "gpu_energy_total": 0.00011575092593574254,
                        "ram_energy_total": 6.549414359895209e-07,
                        "total_energy_kwh": 0.00025223171156228116,
                        "total_energy_joules": 908.0341616242122
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5506400762562101,
                        "joules_per_token": 1.8160683232484243,
                        "flops_per_joule": 569661457.5323343,
                        "joules_per_flop": 1.7554285738968733e-09
                    },
                    "per-process_emissions": [
                        9.608767051965101e-05
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3595350,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 75.39644788998274,
                            "ram_power": 0.7215628623962402,
                            "cpu_energy": 0.00013582584419054912,
                            "gpu_energy": 0.00011575092593574254,
                            "ram_energy": 6.549414359895209e-07,
                            "total_energy_kwh": 0.00025223171156228116,
                            "total_energy_joules": 908.0341616242122,
                            "final_emissions": 9.608767051965101e-05
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0202": {
            "setup": {
                "experiment_id": "0202",
                "date_time": "April 06, 2025 at 07:15:34 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r2_low_latency_chatbot",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.05,
                    "simulate_burst": false
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 3.9041743979323655,
                        "average_latency_ms_per_batch": 1301.3914659774553,
                        "throughput_queries_per_sec": 2.5613609897385623,
                        "throughput_tokens_per_sec": 128.06804948692812
                    }
                },
                "compute_metrics": {
                    "flops": 517272064000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8817956352,
                        "gpu_max_memory_allocated_bytes": 8817956352,
                        "gpu_current_memory_reserved_bytes": 11871977472,
                        "gpu_max_memory_reserved_bytes": 11871977472
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 97.4,
                        "cpu_memory_usage_bytes": 2069925888
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0202",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 75.39644788998274
                        },
                        "ram_power": {
                            "0": 0.7215628623962402
                        },
                        "cpu_energy": {
                            "0": 0.00013582584419054912
                        },
                        "gpu_energy": {
                            "0": 0.00011575092593574254
                        },
                        "ram_energy": {
                            "0": 6.549414359895209e-07
                        },
                        "total_energy_kwh": {
                            "0": 0.00025223171156228116
                        },
                        "total_energy_joules": {
                            "0": 908.0341616242122
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 75.39644788998274,
                        "ram_power_avg": 0.7215628623962402,
                        "cpu_energy_total": 0.00013582584419054912,
                        "gpu_energy_total": 0.00011575092593574254,
                        "ram_energy_total": 6.549414359895209e-07,
                        "total_energy_kwh": 0.00025223171156228116,
                        "total_energy_joules": 908.0341616242122
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5506400762562101,
                        "joules_per_token": 1.8160683232484243,
                        "flops_per_joule": 569661457.5323343,
                        "joules_per_flop": 1.7554285738968733e-09
                    },
                    "per-process_emissions": [
                        9.608767051965101e-05
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3595350,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 75.39644788998274,
                            "ram_power": 0.7215628623962402,
                            "cpu_energy": 0.00013582584419054912,
                            "gpu_energy": 0.00011575092593574254,
                            "ram_energy": 6.549414359895209e-07,
                            "total_energy_kwh": 0.00025223171156228116,
                            "total_energy_joules": 908.0341616242122,
                            "final_emissions": 9.608767051965101e-05
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0203": {
            "setup": {
                "experiment_id": "0203",
                "date_time": "April 06, 2025 at 07:16:11 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r3_balanced_enterprise_service",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 5.224967560963705,
                        "average_latency_ms_per_batch": 5224.967560963705,
                        "throughput_queries_per_sec": 1.9138874803187442,
                        "throughput_tokens_per_sec": 95.69437401593721
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576167424,
                        "gpu_max_memory_allocated_bytes": 1576167424,
                        "gpu_current_memory_reserved_bytes": 2856321024,
                        "gpu_max_memory_reserved_bytes": 2856321024
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 96.8,
                        "cpu_memory_usage_bytes": 2796732416
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0203",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "1": 72.83377798795287,
                            "0": 79.14543347514372
                        },
                        "ram_power": {
                            "1": 1.0076708793640137,
                            "0": 0.9757575988769532
                        },
                        "cpu_energy": {
                            "1": 0.0001700486804402317,
                            "0": 0.00017141949327196927
                        },
                        "gpu_energy": {
                            "1": 0.00011962481791982782,
                            "0": 0.00012061065203994303
                        },
                        "ram_energy": {
                            "1": 1.1056562493298531e-06,
                            "0": 1.0663051369082345e-06
                        },
                        "total_energy_kwh": {
                            "1": 0.0002907791546093894,
                            "0": 0.0002930964504488205
                        },
                        "total_energy_joules": {
                            "1": 1046.8049565938018,
                            "0": 1055.1472216157538
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 75.9896057315483,
                        "ram_power_avg": 0.9917142391204834,
                        "cpu_energy_total": 0.000341468173712201,
                        "gpu_energy_total": 0.00024023546995977085,
                        "ram_energy_total": 2.1719613862380874e-06,
                        "total_energy_kwh": 0.0005838756050582099,
                        "total_energy_joules": 2101.9521782095553
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2378741082615402,
                        "joules_per_token": 4.203904356419111,
                        "flops_per_joule": 4921825238.104254,
                        "joules_per_flop": 2.031766573624161e-10
                    },
                    "per-process_emissions": [
                        0.0001107723189484469,
                        0.00011165509279847817
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3596245,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 79.14543347514372,
                            "ram_power": 0.9757575988769532,
                            "cpu_energy": 0.00017141949327196927,
                            "gpu_energy": 0.00012061065203994303,
                            "ram_energy": 1.0663051369082345e-06,
                            "total_energy_kwh": 0.0002930964504488205,
                            "total_energy_joules": 1055.1472216157538,
                            "final_emissions": 0.00011165509279847817
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0203": {
            "setup": {
                "experiment_id": "0203",
                "date_time": "April 06, 2025 at 07:16:11 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r3_balanced_enterprise_service",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 5.224967560963705,
                        "average_latency_ms_per_batch": 5224.967560963705,
                        "throughput_queries_per_sec": 1.9138874803187442,
                        "throughput_tokens_per_sec": 95.69437401593721
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576167424,
                        "gpu_max_memory_allocated_bytes": 1576167424,
                        "gpu_current_memory_reserved_bytes": 2856321024,
                        "gpu_max_memory_reserved_bytes": 2856321024
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 96.8,
                        "cpu_memory_usage_bytes": 2796732416
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0203",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "1": 72.83377798795287,
                            "0": 79.14543347514372
                        },
                        "ram_power": {
                            "1": 1.0076708793640137,
                            "0": 0.9757575988769532
                        },
                        "cpu_energy": {
                            "1": 0.0001700486804402317,
                            "0": 0.00017141949327196927
                        },
                        "gpu_energy": {
                            "1": 0.00011962481791982782,
                            "0": 0.00012061065203994303
                        },
                        "ram_energy": {
                            "1": 1.1056562493298531e-06,
                            "0": 1.0663051369082345e-06
                        },
                        "total_energy_kwh": {
                            "1": 0.0002907791546093894,
                            "0": 0.0002930964504488205
                        },
                        "total_energy_joules": {
                            "1": 1046.8049565938018,
                            "0": 1055.1472216157538
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 75.9896057315483,
                        "ram_power_avg": 0.9917142391204834,
                        "cpu_energy_total": 0.000341468173712201,
                        "gpu_energy_total": 0.00024023546995977085,
                        "ram_energy_total": 2.1719613862380874e-06,
                        "total_energy_kwh": 0.0005838756050582099,
                        "total_energy_joules": 2101.9521782095553
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2378741082615402,
                        "joules_per_token": 4.203904356419111,
                        "flops_per_joule": 4921825238.104254,
                        "joules_per_flop": 2.031766573624161e-10
                    },
                    "per-process_emissions": [
                        0.0001107723189484469,
                        0.00011165509279847817
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3596245,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 79.14543347514372,
                            "ram_power": 0.9757575988769532,
                            "cpu_energy": 0.00017141949327196927,
                            "gpu_energy": 0.00012061065203994303,
                            "ram_energy": 1.0663051369082345e-06,
                            "total_energy_kwh": 0.0002930964504488205,
                            "total_energy_joules": 1055.1472216157538,
                            "final_emissions": 0.00011165509279847817
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0204": {
            "setup": {
                "experiment_id": "0204",
                "date_time": "April 06, 2025 at 07:16:43 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r4_high_load_api",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 8
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 2.9121723279822618,
                        "average_latency_ms_per_batch": 1456.086163991131,
                        "throughput_queries_per_sec": 3.4338627229964223,
                        "throughput_tokens_per_sec": 171.69313614982113
                    }
                },
                "compute_metrics": {
                    "flops": 517272064000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8817955840,
                        "gpu_max_memory_allocated_bytes": 8817955840,
                        "gpu_current_memory_reserved_bytes": 11871977472,
                        "gpu_max_memory_reserved_bytes": 11871977472
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            7.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 96.4,
                        "cpu_memory_usage_bytes": 2081546240
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0204",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 422.54803228441125
                        },
                        "ram_power": {
                            "0": 0.7253680229187012
                        },
                        "cpu_energy": {
                            "0": 0.00010370986430643823
                        },
                        "gpu_energy": {
                            "0": 8.344840009044674e-05
                        },
                        "ram_energy": {
                            "0": 7.266697950210798e-07
                        },
                        "total_energy_kwh": {
                            "0": 0.00018788493419190608
                        },
                        "total_energy_joules": {
                            "0": 676.3857630908618
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 422.54803228441125,
                        "ram_power_avg": 0.7253680229187012,
                        "cpu_energy_total": 0.00010370986430643823,
                        "gpu_energy_total": 8.344840009044674e-05,
                        "ram_energy_total": 7.266697950210798e-07,
                        "total_energy_kwh": 0.00018788493419190608,
                        "total_energy_joules": 676.3857630908618
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.7392231286997577,
                        "joules_per_token": 1.3527715261817237,
                        "flops_per_joule": 764758947.0781226,
                        "joules_per_flop": 1.3076015701688113e-09
                    },
                    "per-process_emissions": [
                        7.157476568040663e-05
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3596972,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 422.54803228441125,
                            "ram_power": 0.7253680229187012,
                            "cpu_energy": 0.00010370986430643823,
                            "gpu_energy": 8.344840009044674e-05,
                            "ram_energy": 7.266697950210798e-07,
                            "total_energy_kwh": 0.00018788493419190608,
                            "total_energy_joules": 676.3857630908618,
                            "final_emissions": 7.157476568040663e-05
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0204": {
            "setup": {
                "experiment_id": "0204",
                "date_time": "April 06, 2025 at 07:16:43 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r4_high_load_api",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 8
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 2.9121723279822618,
                        "average_latency_ms_per_batch": 1456.086163991131,
                        "throughput_queries_per_sec": 3.4338627229964223,
                        "throughput_tokens_per_sec": 171.69313614982113
                    }
                },
                "compute_metrics": {
                    "flops": 517272064000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8817955840,
                        "gpu_max_memory_allocated_bytes": 8817955840,
                        "gpu_current_memory_reserved_bytes": 11871977472,
                        "gpu_max_memory_reserved_bytes": 11871977472
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            7.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 96.4,
                        "cpu_memory_usage_bytes": 2081546240
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0204",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 422.54803228441125
                        },
                        "ram_power": {
                            "0": 0.7253680229187012
                        },
                        "cpu_energy": {
                            "0": 0.00010370986430643823
                        },
                        "gpu_energy": {
                            "0": 8.344840009044674e-05
                        },
                        "ram_energy": {
                            "0": 7.266697950210798e-07
                        },
                        "total_energy_kwh": {
                            "0": 0.00018788493419190608
                        },
                        "total_energy_joules": {
                            "0": 676.3857630908618
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 422.54803228441125,
                        "ram_power_avg": 0.7253680229187012,
                        "cpu_energy_total": 0.00010370986430643823,
                        "gpu_energy_total": 8.344840009044674e-05,
                        "ram_energy_total": 7.266697950210798e-07,
                        "total_energy_kwh": 0.00018788493419190608,
                        "total_energy_joules": 676.3857630908618
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.7392231286997577,
                        "joules_per_token": 1.3527715261817237,
                        "flops_per_joule": 764758947.0781226,
                        "joules_per_flop": 1.3076015701688113e-09
                    },
                    "per-process_emissions": [
                        7.157476568040663e-05
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3596972,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 422.54803228441125,
                            "ram_power": 0.7253680229187012,
                            "cpu_energy": 0.00010370986430643823,
                            "gpu_energy": 8.344840009044674e-05,
                            "ram_energy": 7.266697950210798e-07,
                            "total_energy_kwh": 0.00018788493419190608,
                            "total_energy_joules": 676.3857630908618,
                            "final_emissions": 7.157476568040663e-05
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0205": {
            "setup": {
                "experiment_id": "0205",
                "date_time": "April 06, 2025 at 07:17:55 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r5_real_time_mobile",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.6,
                    "simulate_burst": true,
                    "burst_interval": 5.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.92757510370575,
                        "average_latency_ms_per_batch": 4392.757510370575,
                        "throughput_queries_per_sec": 0.22764743959555364,
                        "throughput_tokens_per_sec": 11.382371979777682
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1575855104,
                        "gpu_max_memory_allocated_bytes": 1575855104,
                        "gpu_current_memory_reserved_bytes": 2392850432,
                        "gpu_max_memory_reserved_bytes": 2392850432
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 96.2,
                        "cpu_memory_usage_bytes": 2722807808
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0205",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 30.972670507366672
                        },
                        "ram_power": {
                            "0": 0.9501628875732422
                        },
                        "cpu_energy": {
                            "0": 0.0013606451140149144
                        },
                        "gpu_energy": {
                            "0": 0.0004407342414793902
                        },
                        "ram_energy": {
                            "0": 8.714992322740371e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.001810094347817046
                        },
                        "total_energy_joules": {
                            "0": 6516.339652141365
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 30.972670507366672,
                        "ram_power_avg": 0.9501628875732422,
                        "cpu_energy_total": 0.0013606451140149144,
                        "gpu_energy_total": 0.0004407342414793902,
                        "ram_energy_total": 8.714992322740371e-06,
                        "total_energy_kwh": 0.001810094347817046,
                        "total_energy_joules": 6516.339652141365
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0767301931285446,
                        "joules_per_token": 13.032679304282729,
                        "flops_per_joule": 1587615414.828835,
                        "joules_per_flop": 6.298754664761255e-10
                    },
                    "per-process_emissions": [
                        0.0006895554418009037
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3597882,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 30.972670507366672,
                            "ram_power": 0.9501628875732422,
                            "cpu_energy": 0.0013606451140149144,
                            "gpu_energy": 0.0004407342414793902,
                            "ram_energy": 8.714992322740371e-06,
                            "total_energy_kwh": 0.001810094347817046,
                            "total_energy_joules": 6516.339652141365,
                            "final_emissions": 0.0006895554418009037
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0205": {
            "setup": {
                "experiment_id": "0205",
                "date_time": "April 06, 2025 at 07:17:55 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r5_real_time_mobile",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.6,
                    "simulate_burst": true,
                    "burst_interval": 5.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 10345441280000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.92757510370575,
                        "average_latency_ms_per_batch": 4392.757510370575,
                        "throughput_queries_per_sec": 0.22764743959555364,
                        "throughput_tokens_per_sec": 11.382371979777682
                    }
                },
                "compute_metrics": {
                    "flops": 10345441280000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1575855104,
                        "gpu_max_memory_allocated_bytes": 1575855104,
                        "gpu_current_memory_reserved_bytes": 2392850432,
                        "gpu_max_memory_reserved_bytes": 2392850432
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 96.2,
                        "cpu_memory_usage_bytes": 2722807808
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0205",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 30.972670507366672
                        },
                        "ram_power": {
                            "0": 0.9501628875732422
                        },
                        "cpu_energy": {
                            "0": 0.0013606451140149144
                        },
                        "gpu_energy": {
                            "0": 0.0004407342414793902
                        },
                        "ram_energy": {
                            "0": 8.714992322740371e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.001810094347817046
                        },
                        "total_energy_joules": {
                            "0": 6516.339652141365
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 30.972670507366672,
                        "ram_power_avg": 0.9501628875732422,
                        "cpu_energy_total": 0.0013606451140149144,
                        "gpu_energy_total": 0.0004407342414793902,
                        "ram_energy_total": 8.714992322740371e-06,
                        "total_energy_kwh": 0.001810094347817046,
                        "total_energy_joules": 6516.339652141365
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0767301931285446,
                        "joules_per_token": 13.032679304282729,
                        "flops_per_joule": 1587615414.828835,
                        "joules_per_flop": 6.298754664761255e-10
                    },
                    "per-process_emissions": [
                        0.0006895554418009037
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3597882,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 30.972670507366672,
                            "ram_power": 0.9501628875732422,
                            "cpu_energy": 0.0013606451140149144,
                            "gpu_energy": 0.0004407342414793902,
                            "ram_energy": 8.714992322740371e-06,
                            "total_energy_kwh": 0.001810094347817046,
                            "total_energy_joules": 6516.339652141365,
                            "final_emissions": 0.0006895554418009037
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0206": {
            "setup": {
                "experiment_id": "0206",
                "date_time": "April 06, 2025 at 07:18:28 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r6_medium_scale_serving",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.1,
                    "simulate_burst": false
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 1.499408639036119,
                        "average_latency_ms_per_batch": 1499.408639036119,
                        "throughput_queries_per_sec": 6.669295974197139,
                        "throughput_tokens_per_sec": 333.46479870985695
                    }
                },
                "compute_metrics": {
                    "flops": 517272064000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8817955328,
                        "gpu_max_memory_allocated_bytes": 8817955328,
                        "gpu_current_memory_reserved_bytes": 13067354112,
                        "gpu_max_memory_reserved_bytes": 13067354112
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            7.0,
                            100.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 95.9,
                        "cpu_memory_usage_bytes": 2087342080
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0206",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "1": 112.5
                        },
                        "gpu_power": {
                            "0": 305.3311998042563,
                            "1": 297.0754508231247
                        },
                        "ram_power": {
                            "0": 0.7277998924255371,
                            "1": 0.6994128227233887
                        },
                        "cpu_energy": {
                            "0": 6.0197840997716406e-05,
                            "1": 6.0608483407122545e-05
                        },
                        "gpu_energy": {
                            "0": 0.0001257751006207286,
                            "1": 0.00012850010280374136
                        },
                        "ram_energy": {
                            "0": 3.1873754824642905e-07,
                            "1": 3.1704737202128953e-07
                        },
                        "total_energy_kwh": {
                            "0": 0.00018629167916669147,
                            "1": 0.0001894256335828852
                        },
                        "total_energy_joules": {
                            "0": 670.6500450000893,
                            "1": 681.9322808983867
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 301.20332531369047,
                        "ram_power_avg": 0.7136063575744629,
                        "cpu_energy_total": 0.00012080632440483895,
                        "gpu_energy_total": 0.00025427520342446996,
                        "ram_energy_total": 6.357849202677186e-07,
                        "total_energy_kwh": 0.0003757173127495767,
                        "total_energy_joules": 1352.582325898476
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.3696632659072093,
                        "joules_per_token": 2.705164651796952,
                        "flops_per_joule": 382432961.081606,
                        "joules_per_flop": 2.6148373748219196e-09
                    },
                    "per-process_emissions": [
                        7.096781517855111e-05,
                        7.216169511340011e-05
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3599319,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 305.3311998042563,
                            "ram_power": 0.7277998924255371,
                            "cpu_energy": 6.0197840997716406e-05,
                            "gpu_energy": 0.0001257751006207286,
                            "ram_energy": 3.1873754824642905e-07,
                            "total_energy_kwh": 0.00018629167916669147,
                            "total_energy_joules": 670.6500450000893,
                            "final_emissions": 7.096781517855111e-05
                        }
                    }
                }
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0206": {
            "setup": {
                "experiment_id": "0206",
                "date_time": "April 06, 2025 at 07:18:28 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "r6_medium_scale_serving",
                "max_input_tokens": 50,
                "max_output_tokens": 50,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.95
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.1,
                    "simulate_burst": false
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": null
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 500,
                        "total_generated_tokens": 500
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 1.499408639036119,
                        "average_latency_ms_per_batch": 1499.408639036119,
                        "throughput_queries_per_sec": 6.669295974197139,
                        "throughput_tokens_per_sec": 333.46479870985695
                    }
                },
                "compute_metrics": {
                    "flops": 517272064000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8817955328,
                        "gpu_max_memory_allocated_bytes": 8817955328,
                        "gpu_current_memory_reserved_bytes": 13067354112,
                        "gpu_max_memory_reserved_bytes": 13067354112
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            7.0,
                            100.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 95.9,
                        "cpu_memory_usage_bytes": 2087342080
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0206",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "1": 112.5
                        },
                        "gpu_power": {
                            "0": 305.3311998042563,
                            "1": 297.0754508231247
                        },
                        "ram_power": {
                            "0": 0.7277998924255371,
                            "1": 0.6994128227233887
                        },
                        "cpu_energy": {
                            "0": 6.0197840997716406e-05,
                            "1": 6.0608483407122545e-05
                        },
                        "gpu_energy": {
                            "0": 0.0001257751006207286,
                            "1": 0.00012850010280374136
                        },
                        "ram_energy": {
                            "0": 3.1873754824642905e-07,
                            "1": 3.1704737202128953e-07
                        },
                        "total_energy_kwh": {
                            "0": 0.00018629167916669147,
                            "1": 0.0001894256335828852
                        },
                        "total_energy_joules": {
                            "0": 670.6500450000893,
                            "1": 681.9322808983867
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 301.20332531369047,
                        "ram_power_avg": 0.7136063575744629,
                        "cpu_energy_total": 0.00012080632440483895,
                        "gpu_energy_total": 0.00025427520342446996,
                        "ram_energy_total": 6.357849202677186e-07,
                        "total_energy_kwh": 0.0003757173127495767,
                        "total_energy_joules": 1352.582325898476
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.3696632659072093,
                        "joules_per_token": 2.705164651796952,
                        "flops_per_joule": 382432961.081606,
                        "joules_per_flop": 2.6148373748219196e-09
                    },
                    "per-process_emissions": [
                        7.096781517855111e-05,
                        7.216169511340011e-05
                    ]
                },
                "local_energy_results": {
                    "process_0": {
                        "process_id": 3599319,
                        "local_process_index": 0,
                        "energy_results": {
                            "cpu_power": 112.5,
                            "gpu_power": 305.3311998042563,
                            "ram_power": 0.7277998924255371,
                            "cpu_energy": 6.0197840997716406e-05,
                            "gpu_energy": 0.0001257751006207286,
                            "ram_energy": 3.1873754824642905e-07,
                            "total_energy_kwh": 0.00018629167916669147,
                            "total_energy_joules": 670.6500450000893,
                            "final_emissions": 7.096781517855111e-05
                        }
                    }
                }
            }
        }
    }
]