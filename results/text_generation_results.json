[
    {
        "EXPERIMENT_#0001": {
            "setup": {
                "experiment_id": "0001",
                "date_time": "March 25, 2025 at 05:42:27 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 256,
                "max_output_tokens": 256,
                "number_input_prompts": 50,
                "decode_token_to_text": false,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 512
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 50,
                        "total_input_tokens": 12800,
                        "total_generated_tokens": 12800
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 49.126075163832866,
                        "average_latency_ms_per_batch": 7018.01073769041,
                        "throughput_queries_per_sec": 1.0177894292033842,
                        "throughput_tokens_per_sec": 260.55409387606636
                    }
                },
                "compute_metrics": {
                    "flops": 26484329676800,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419591168,
                        "gpu_max_memory_allocated_bytes": 4419591168,
                        "gpu_current_memory_reserved_bytes": 6933184512,
                        "gpu_max_memory_reserved_bytes": 6933184512
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            97.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.5,
                        "cpu_memory_usage_bytes": 3067555840
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0001",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 561.2057741283866
                        },
                        "ram_power": {
                            "0": 3.770768165588379
                        },
                        "cpu_energy": {
                            "0": 0.0015193637531156125
                        },
                        "gpu_energy": {
                            "0": 0.010537814263578582
                        },
                        "ram_energy": {
                            "0": 4.198130508231487e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.012099159321776519
                        },
                        "total_energy_joules": {
                            "0": 43556.97355839547
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 561.2057741283866,
                        "ram_power_avg": 3.770768165588379,
                        "cpu_energy_total": 0.0015193637531156125,
                        "gpu_energy_total": 0.010537814263578582,
                        "ram_energy_total": 4.198130508231487e-05,
                        "total_energy_kwh": 0.012099159321776519,
                        "total_energy_joules": 43556.97355839547
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.29386798379918294,
                        "joules_per_token": 3.402888559249646,
                        "flops_per_joule": 608038794.0932877,
                        "joules_per_flop": 1.6446319045994556e-09
                    },
                    "per-process_emissions": [
                        0.004609174743630765
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0002": {
            "setup": {
                "experiment_id": "0002",
                "date_time": "March 25, 2025 at 05:45:36 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 256,
                "max_output_tokens": 256,
                "number_input_prompts": 50,
                "decode_token_to_text": false,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 512
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 50,
                        "total_input_tokens": 12800,
                        "total_generated_tokens": 12800
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 49.277192656183615,
                        "average_latency_ms_per_batch": 7039.598950883374,
                        "throughput_queries_per_sec": 1.0146681924203667,
                        "throughput_tokens_per_sec": 259.75505725961386
                    }
                },
                "compute_metrics": {
                    "flops": 26484329676800,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419591168,
                        "gpu_max_memory_allocated_bytes": 4419591168,
                        "gpu_current_memory_reserved_bytes": 6933184512,
                        "gpu_max_memory_reserved_bytes": 6933184512
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            6.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.5,
                        "cpu_memory_usage_bytes": 3064434688
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0002",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 683.6146042745928
                        },
                        "ram_power": {
                            "0": 3.8891515731811523
                        },
                        "cpu_energy": {
                            "0": 0.0015293307303691106
                        },
                        "gpu_energy": {
                            "0": 0.010558724835861177
                        },
                        "ram_energy": {
                            "0": 4.362659772638547e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.012131682163956667
                        },
                        "total_energy_joules": {
                            "0": 43674.055790244005
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 683.6146042745928,
                        "ram_power_avg": 3.8891515731811523,
                        "cpu_energy_total": 0.0015293307303691106,
                        "gpu_energy_total": 0.010558724835861177,
                        "ram_energy_total": 4.362659772638547e-05,
                        "total_energy_kwh": 0.012131682163956667,
                        "total_energy_joules": 43674.055790244005
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2930801769699458,
                        "joules_per_token": 3.412035608612813,
                        "flops_per_joule": 606408752.2349166,
                        "joules_per_flop": 1.649052716199271e-09
                    },
                    "per-process_emissions": [
                        0.004621564320359293
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0185": {
            "setup": {
                "experiment_id": "0185",
                "date_time": "March 25, 2025 at 05:24:42 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 256,
                "max_output_tokens": 128,
                "number_input_prompts": 50,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 512
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 50,
                        "total_input_tokens": 12800,
                        "total_generated_tokens": 6400
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 24.828756752889603,
                        "average_latency_ms_per_batch": 3546.9652504128003,
                        "throughput_queries_per_sec": 2.0137939445631297,
                        "throughput_tokens_per_sec": 257.7656249040806
                    }
                },
                "compute_metrics": {
                    "flops": 13242164838400,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419744768,
                        "gpu_max_memory_allocated_bytes": 4419744768,
                        "gpu_current_memory_reserved_bytes": 6933184512,
                        "gpu_max_memory_reserved_bytes": 6933184512
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            99.0,
                            50.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.7,
                        "cpu_memory_usage_bytes": 3054272512
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0185",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5
                        },
                        "gpu_power": {
                            "1": 527.1462216119259
                        },
                        "ram_power": {
                            "1": 2.137950897216797
                        },
                        "cpu_energy": {
                            "1": 0.0007474686795612797
                        },
                        "gpu_energy": {
                            "1": 0.003118414439173378
                        },
                        "ram_energy": {
                            "1": 1.1799675844031128e-05
                        },
                        "total_energy_kwh": {
                            "1": 0.003877682794578688
                        },
                        "total_energy_joules": {
                            "1": 13959.658060483276
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 527.1462216119259,
                        "ram_power_avg": 2.137950897216797,
                        "cpu_energy_total": 0.0007474686795612797,
                        "gpu_energy_total": 0.003118414439173378,
                        "ram_energy_total": 1.1799675844031128e-05,
                        "total_energy_kwh": 0.003877682794578688,
                        "total_energy_joules": 13959.658060483276
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.45846395178668403,
                        "joules_per_token": 2.1811965719505118,
                        "flops_per_joule": 948602378.4411782,
                        "joules_per_flop": 1.0541824717362428e-09
                    },
                    "per-process_emissions": [
                        0.0014772032605947511
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0186": {
            "setup": {
                "experiment_id": "0186",
                "date_time": "March 25, 2025 at 05:30:57 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 256,
                "max_output_tokens": 128,
                "number_input_prompts": 50,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 512
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 50,
                        "total_input_tokens": 12800,
                        "total_generated_tokens": 6400
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 24.916690498008393,
                        "average_latency_ms_per_batch": 3559.527214001199,
                        "throughput_queries_per_sec": 2.006687043931317,
                        "throughput_tokens_per_sec": 256.8559416232086
                    }
                },
                "compute_metrics": {
                    "flops": 13242164838400,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419744768,
                        "gpu_max_memory_allocated_bytes": 4419744768,
                        "gpu_current_memory_reserved_bytes": 6933184512,
                        "gpu_max_memory_reserved_bytes": 6933184512
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            68.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.8,
                        "cpu_memory_usage_bytes": 3068715008
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0186",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "2": 112.5,
                            "3": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "1": 745.4446839850712,
                            "2": 645.6149966768064,
                            "3": 826.9045438254708,
                            "0": 737.0377680446375
                        },
                        "ram_power": {
                            "1": 3.8182196617126465,
                            "2": 3.8208532333374023,
                            "3": 3.8182196617126465,
                            "0": 3.819994926452637
                        },
                        "cpu_energy": {
                            "1": 0.0007505887925908612,
                            "2": 0.0007940554781780522,
                            "3": 0.0007505877392177356,
                            "0": 0.0007791859869066685
                        },
                        "gpu_energy": {
                            "1": 0.005160261350428463,
                            "2": 0.0054386332397937664,
                            "3": 0.005176906641524148,
                            "0": 0.005375635689394365
                        },
                        "ram_energy": {
                            "1": 2.098028978400795e-05,
                            "2": 2.2335033018967845e-05,
                            "3": 2.0780040898623844e-05,
                            "0": 2.166656364000457e-05
                        },
                        "total_energy_kwh": {
                            "1": 0.005931830432803333,
                            "2": 0.006255023750990785,
                            "3": 0.005948274421640506,
                            "0": 0.006176488239941039
                        },
                        "total_energy_joules": {
                            "1": 21354.589558092,
                            "2": 22518.08550356683,
                            "3": 21413.78791790582,
                            "0": 22235.35766378774
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 738.7504981329964,
                        "ram_power_avg": 3.819321870803833,
                        "cpu_energy_total": 0.0030744179968933173,
                        "gpu_energy_total": 0.021151436921140743,
                        "ram_energy_total": 8.57619273416042e-05,
                        "total_energy_kwh": 0.024311616845375666,
                        "total_energy_joules": 87521.82064335239
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.07312462141389542,
                        "joules_per_token": 13.67528447552381,
                        "flops_per_joule": 151301295.39193714,
                        "joules_per_flop": 6.609328739780837e-09
                    },
                    "per-process_emissions": [
                        0.0022597308033764298,
                        0.0023828512979399397,
                        0.002265995140923951,
                        0.002352933195005539
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0187": {
            "setup": {
                "experiment_id": "0187",
                "date_time": "March 25, 2025 at 05:36:31 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 512,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 512
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 51200,
                        "total_generated_tokens": 51200
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 192.94055001600645,
                        "average_latency_ms_per_batch": 14841.580770462035,
                        "throughput_queries_per_sec": 0.5182943657603544,
                        "throughput_tokens_per_sec": 265.36671526930144
                    }
                },
                "compute_metrics": {
                    "flops": 52968659353600,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4420410368,
                        "gpu_max_memory_allocated_bytes": 4420410368,
                        "gpu_current_memory_reserved_bytes": 6765412352,
                        "gpu_max_memory_reserved_bytes": 6765412352
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.7,
                        "cpu_memory_usage_bytes": 3218513920
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0187",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "3": 112.5,
                            "1": 112.5,
                            "2": 112.5
                        },
                        "gpu_power": {
                            "0": 624.2787829353038,
                            "3": 650.8773050563557,
                            "1": 1031.2294474663054,
                            "2": 690.6068061554549
                        },
                        "ram_power": {
                            "0": 4.018325328826904,
                            "3": 4.0155029296875,
                            "1": 4.016873359680176,
                            "2": 4.018567085266113
                        },
                        "cpu_energy": {
                            "0": 0.0059670262015206425,
                            "3": 0.005724373856832247,
                            "1": 0.005808944580701791,
                            "2": 0.006114131294460096
                        },
                        "gpu_energy": {
                            "0": 0.04384397340848345,
                            "3": 0.042240829903750665,
                            "1": 0.042886790420516796,
                            "2": 0.044741038848360226
                        },
                        "ram_energy": {
                            "0": 0.00016856435068822784,
                            "3": 0.00015990385382373714,
                            "1": 0.00016290215636673974,
                            "2": 0.00017308471852652362
                        },
                        "total_energy_kwh": {
                            "0": 0.04997956396069232,
                            "3": 0.048125107614406665,
                            "1": 0.04885863715758537,
                            "2": 0.05102825486134679
                        },
                        "total_energy_joules": {
                            "0": 179926.43025849236,
                            "3": 173250.38741186398,
                            "1": 175891.09376730735,
                            "2": 183701.71750084843
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 749.2480854033549,
                        "ram_power_avg": 4.017317175865173,
                        "cpu_energy_total": 0.02361447593351478,
                        "gpu_energy_total": 0.17371263258111114,
                        "ram_energy_total": 0.0006644550794052284,
                        "total_energy_kwh": 0.19799156359403117,
                        "total_energy_joules": 712769.6289385122
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.07183246580841174,
                        "joules_per_token": 13.921281815205317,
                        "flops_per_joule": 74313855.70185314,
                        "joules_per_flop": 1.3456440801726068e-08
                    },
                    "per-process_emissions": [
                        0.019039714890825742,
                        0.01833325974570822,
                        0.01861269782518215,
                        0.01943921368943006
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0188": {
            "setup": {
                "experiment_id": "0188",
                "date_time": "March 25, 2025 at 06:15:47 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 512,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 512
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 51200,
                        "total_generated_tokens": 51200
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 190.90247411001474,
                        "average_latency_ms_per_batch": 14684.805700770365,
                        "throughput_queries_per_sec": 0.523827679375026,
                        "throughput_tokens_per_sec": 268.1997718400133
                    }
                },
                "compute_metrics": {
                    "flops": 52968659353600,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4420410368,
                        "gpu_max_memory_allocated_bytes": 4420410368,
                        "gpu_current_memory_reserved_bytes": 6765412352,
                        "gpu_max_memory_reserved_bytes": 6765412352
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.3,
                        "cpu_memory_usage_bytes": 3231035392
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0188",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "3": 112.5,
                            "2": 112.5,
                            "1": 112.5
                        },
                        "gpu_power": {
                            "0": 11.80424837441682,
                            "3": 996.4716450190048,
                            "2": 607.1974605505388,
                            "1": 852.4330707843009
                        },
                        "ram_power": {
                            "0": 4.002030372619629,
                            "3": 4.0009989738464355,
                            "2": 4.003671169281006,
                            "1": 4.001235008239746
                        },
                        "cpu_energy": {
                            "0": 0.005951900788066267,
                            "3": 0.005680048145939511,
                            "2": 0.00609863012325877,
                            "1": 0.005763871142407882
                        },
                        "gpu_energy": {
                            "0": 0.043275827676190204,
                            "3": 0.041819120121934006,
                            "2": 0.04437549494480919,
                            "1": 0.04237416334374533
                        },
                        "ram_energy": {
                            "0": 0.00016715407873149496,
                            "3": 0.00015821863042030982,
                            "2": 0.00017221138232915718,
                            "1": 0.00016129529243666917
                        },
                        "total_energy_kwh": {
                            "0": 0.049394882542987996,
                            "3": 0.04765738689829382,
                            "2": 0.0506463364503971,
                            "1": 0.04829932977858986
                        },
                        "total_energy_joules": {
                            "0": 177821.5771547568,
                            "3": 171566.59283385775,
                            "2": 182326.81122142955,
                            "1": 173877.5872029235
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 616.9766061820653,
                        "ram_power_avg": 4.001983880996704,
                        "cpu_energy_total": 0.02349445019967243,
                        "gpu_energy_total": 0.17184460608667873,
                        "ram_energy_total": 0.0006588793839176311,
                        "total_energy_kwh": 0.19599793567026877,
                        "total_energy_joules": 705592.5684129676
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.07256312253282377,
                        "joules_per_token": 13.781104851815773,
                        "flops_per_joule": 75069752.3256773,
                        "joules_per_flop": 1.332094444193276e-08
                    },
                    "per-process_emissions": [
                        0.018816980504751276,
                        0.01815508153890503,
                        0.019293721870778775,
                        0.018399629679153807
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0191": {
            "setup": {
                "experiment_id": "0191",
                "date_time": "March 25, 2025 at 06:26:12 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 512,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 512
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 51200,
                        "total_generated_tokens": 51200
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 191.41053537512198,
                        "average_latency_ms_per_batch": 14723.887336547845,
                        "throughput_queries_per_sec": 0.5224372827964892,
                        "throughput_tokens_per_sec": 267.48788879180245
                    }
                },
                "compute_metrics": {
                    "flops": 105937318707200,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4420410368,
                        "gpu_max_memory_allocated_bytes": 4420410368,
                        "gpu_current_memory_reserved_bytes": 6765412352,
                        "gpu_max_memory_reserved_bytes": 6765412352
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.9,
                        "cpu_memory_usage_bytes": 3186335744
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0191",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 0.0
                        },
                        "ram_power": {
                            "0": 4.013806343078613
                        },
                        "cpu_energy": {
                            "0": 0.005965438991828708
                        },
                        "gpu_energy": {
                            "0": 0.04319745094682226
                        },
                        "ram_energy": {
                            "0": 0.00016783373558103065
                        },
                        "total_energy_kwh": {
                            "0": 0.04933072367423201
                        },
                        "total_energy_joules": {
                            "0": 177590.60522723524
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 0.0,
                        "ram_power_avg": 4.013806343078613,
                        "cpu_energy_total": 0.005965438991828708,
                        "gpu_energy_total": 0.04319745094682226,
                        "ram_energy_total": 0.00016783373558103065,
                        "total_energy_kwh": 0.04933072367423201,
                        "total_energy_joules": 177590.60522723524
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2883035391116961,
                        "joules_per_token": 3.4685665083444386,
                        "flops_per_joule": 596525466.9392471,
                        "joules_per_flop": 1.6763743635807665e-09
                    },
                    "per-process_emissions": [
                        0.018792539183698685
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0193": {
            "setup": {
                "experiment_id": "0193",
                "date_time": "March 25, 2025 at 06:29:03 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 50,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 512
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 50,
                        "total_input_tokens": 6400,
                        "total_generated_tokens": 6400
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 24.91698152513709,
                        "average_latency_ms_per_batch": 3559.5687893052987,
                        "throughput_queries_per_sec": 2.0066636060856053,
                        "throughput_tokens_per_sec": 256.8529415789575
                    }
                },
                "compute_metrics": {
                    "flops": 13242164838400,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419591168,
                        "gpu_max_memory_allocated_bytes": 4419591168,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            1.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.6,
                        "cpu_memory_usage_bytes": 3065069568
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0193",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 420.82484592777325
                        },
                        "ram_power": {
                            "0": 3.890533447265625
                        },
                        "cpu_energy": {
                            "0": 0.0007788822062611872
                        },
                        "gpu_energy": {
                            "0": 0.005296661737321351
                        },
                        "ram_energy": {
                            "0": 2.225659425647733e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.006097800537839015
                        },
                        "total_energy_joules": {
                            "0": 21952.081936220453
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 420.82484592777325,
                        "ram_power_avg": 3.890533447265625,
                        "cpu_energy_total": 0.0007788822062611872,
                        "gpu_energy_total": 0.005296661737321351,
                        "ram_energy_total": 2.225659425647733e-05,
                        "total_energy_kwh": 0.006097800537839015,
                        "total_energy_joules": 21952.081936220453
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.29154410131096226,
                        "joules_per_token": 3.4300128025344456,
                        "flops_per_joule": 603230476.1285862,
                        "joules_per_flop": 1.657741177829413e-09
                    },
                    "per-process_emissions": [
                        0.0023229571148897727
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0194": {
            "setup": {
                "experiment_id": "0194",
                "date_time": "March 25, 2025 at 06:31:18 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 50,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 512
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 50,
                        "total_input_tokens": 6400,
                        "total_generated_tokens": 6400
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 24.617630601976998,
                        "average_latency_ms_per_batch": 3516.804371711,
                        "throughput_queries_per_sec": 2.0310646791484714,
                        "throughput_tokens_per_sec": 259.97627893100434
                    }
                },
                "compute_metrics": {
                    "flops": 13242164838400,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419591168,
                        "gpu_max_memory_allocated_bytes": 4419591168,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.3,
                        "cpu_memory_usage_bytes": 3057463296
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0194",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 1776.0150345971706
                        },
                        "ram_power": {
                            "0": 3.909639358520508
                        },
                        "cpu_energy": {
                            "0": 0.0007715240958314098
                        },
                        "gpu_energy": {
                            "0": 0.005173164694082466
                        },
                        "ram_energy": {
                            "0": 2.306614024489227e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.00596775493015877
                        },
                        "total_energy_joules": {
                            "0": 21483.91774857157
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1776.0150345971706,
                        "ram_power_avg": 3.909639358520508,
                        "cpu_energy_total": 0.0007715240958314098,
                        "gpu_energy_total": 0.005173164694082466,
                        "ram_energy_total": 2.306614024489227e-05,
                        "total_energy_kwh": 0.00596775493015877,
                        "total_energy_joules": 21483.91774857157
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2978972492307892,
                        "joules_per_token": 3.3568621482143075,
                        "flops_per_joule": 616375699.8781309,
                        "joules_per_flop": 1.6223871255757143e-09
                    },
                    "per-process_emissions": [
                        0.0022734162406439834
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0197": {
            "setup": {
                "experiment_id": "0197",
                "date_time": "March 25, 2025 at 06:34:33 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 50,
                "decode_token_to_text": false,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 512
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 50,
                        "total_input_tokens": 6400,
                        "total_generated_tokens": 6400
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 24.81859972316306,
                        "average_latency_ms_per_batch": 3545.5142461661517,
                        "throughput_queries_per_sec": 2.014618091178419,
                        "throughput_tokens_per_sec": 257.87111567083764
                    }
                },
                "compute_metrics": {
                    "flops": 6621082419200,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419693568,
                        "gpu_max_memory_allocated_bytes": 4419693568,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.3,
                        "cpu_memory_usage_bytes": 3068641280
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0197",
                    "local_process_results": {
                        "cpu_power": {
                            "3": 112.5,
                            "1": 112.5,
                            "2": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "3": 929.0134613749925,
                            "1": 834.4351479758318,
                            "2": 735.3701757290179,
                            "0": 648.9412963043585
                        },
                        "ram_power": {
                            "3": 3.909740924835205,
                            "1": 3.9100828170776367,
                            "2": 3.9109311103820805,
                            "0": 3.9105076789855957
                        },
                        "cpu_energy": {
                            "3": 0.0007531952084755178,
                            "1": 0.0007620169229703605,
                            "2": 0.0007981992667919258,
                            "0": 0.0007776107669596966
                        },
                        "gpu_energy": {
                            "3": 0.005070224889510477,
                            "1": 0.0051330710509001065,
                            "2": 0.005358218453240582,
                            "0": 0.005228403627165257
                        },
                        "ram_energy": {
                            "3": 2.152336925627928e-05,
                            "1": 2.198921196428662e-05,
                            "2": 2.303230795173916e-05,
                            "0": 2.2230324711194696e-05
                        },
                        "total_energy_kwh": {
                            "3": 0.005844943467242273,
                            "1": 0.0059170771858347545,
                            "2": 0.006179450027984247,
                            "0": 0.006028244718836148
                        },
                        "total_energy_joules": {
                            "3": 21041.796482072183,
                            "1": 21301.477869005117,
                            "2": 22246.02010074329,
                            "0": 21701.680987810134
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 786.9400203460502,
                        "ram_power_avg": 3.9103156328201294,
                        "cpu_energy_total": 0.0030910221651975003,
                        "gpu_energy_total": 0.020789918020816422,
                        "ram_energy_total": 8.877521388349975e-05,
                        "total_energy_kwh": 0.023969715399897423,
                        "total_energy_joules": 86290.97543963073
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.07416766315821112,
                        "joules_per_token": 13.4829649124423,
                        "flops_per_joule": 76729720.40780924,
                        "joules_per_flop": 1.303275959673931e-08
                    },
                    "per-process_emissions": [
                        0.0022266312138459437,
                        0.00225411055394375,
                        0.002354061488160599,
                        0.0022964598256406308
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0198": {
            "setup": {
                "experiment_id": "0198",
                "date_time": "March 25, 2025 at 06:37:52 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 50,
                "decode_token_to_text": false,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 12,
                    "adaptive_batching": true,
                    "adaptive_max_tokens": 256
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 50,
                        "total_input_tokens": 6400,
                        "total_generated_tokens": 6400
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 85.6345681278035,
                        "average_latency_ms_per_batch": 3425.38272511214,
                        "throughput_queries_per_sec": 0.5838763608333792,
                        "throughput_tokens_per_sec": 74.73617418667254
                    }
                },
                "compute_metrics": {
                    "flops": 6621082419200,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419693568,
                        "gpu_max_memory_allocated_bytes": 4419693568,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.7,
                        "cpu_memory_usage_bytes": 3001053184
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0198",
                    "local_process_results": {
                        "cpu_power": {
                            "2": 112.5,
                            "0": 112.5,
                            "3": 112.5,
                            "1": 112.5
                        },
                        "gpu_power": {
                            "2": 703.2758097520293,
                            "0": 818.6203494798565,
                            "3": 824.3204573368414,
                            "1": 8.107898941060036
                        },
                        "ram_power": {
                            "2": 3.8210792541503906,
                            "0": 3.820744514465332,
                            "3": 3.8037014007568364,
                            "1": 3.8198018074035645
                        },
                        "cpu_energy": {
                            "2": 0.0027216013865399864,
                            "0": 0.0026932725367696544,
                            "3": 0.0026022411789053883,
                            "1": 0.0026651445542047445
                        },
                        "gpu_energy": {
                            "2": 0.01803356748239704,
                            "0": 0.017812137027476638,
                            "3": 0.017378840847505295,
                            "1": 0.017503989836514577
                        },
                        "ram_energy": {
                            "2": 7.719233884932531e-05,
                            "0": 7.60305043412613e-05,
                            "3": 7.344718923410894e-05,
                            "1": 7.52603764256521e-05
                        },
                        "total_energy_kwh": {
                            "2": 0.02083236120778634,
                            "0": 0.020581440068587552,
                            "3": 0.020054529215644795,
                            "1": 0.020244394767144975
                        },
                        "total_energy_joules": {
                            "2": 74996.50034803082,
                            "0": 74093.18424691519,
                            "3": 72196.30517632126,
                            "1": 72879.8211617219
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 588.5811288774468,
                        "ram_power_avg": 3.8163317441940308,
                        "cpu_energy_total": 0.010682259656419774,
                        "gpu_energy_total": 0.07072853519389355,
                        "ram_energy_total": 0.0003019304088503476,
                        "total_energy_kwh": 0.08171272525916366,
                        "total_energy_joules": 294165.8109329892
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.021756437227363302,
                        "joules_per_token": 45.963407958279554,
                        "flops_per_joule": 22507994.379769303,
                        "joules_per_flop": 4.442865868577001e-08
                    },
                    "per-process_emissions": [
                        0.007936088002106207,
                        0.007840499594128428,
                        0.007639772904699885,
                        0.007712102186543879
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0200": {
            "setup": {
                "experiment_id": "0200",
                "date_time": "March 25, 2025 at 06:41:43 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 50,
                "decode_token_to_text": false,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 12,
                    "adaptive_batching": true,
                    "adaptive_max_tokens": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 50,
                        "total_input_tokens": 6400,
                        "total_generated_tokens": 6193
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 161.02878463629168,
                        "average_latency_ms_per_batch": 3220.5756927258335,
                        "throughput_queries_per_sec": 0.3105034923596592,
                        "throughput_tokens_per_sec": 38.45896256366739
                    }
                },
                "compute_metrics": {
                    "flops": 6621082419200,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419693056,
                        "gpu_max_memory_allocated_bytes": 4419693056,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.6,
                        "cpu_memory_usage_bytes": 2913443840
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0200",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "2": 112.5,
                            "3": 112.5,
                            "1": 112.5
                        },
                        "gpu_power": {
                            "0": 683.7490040899218,
                            "2": 685.5838140535868,
                            "3": 641.9991236441701,
                            "1": 0.0
                        },
                        "ram_power": {
                            "0": 3.6454339027404785,
                            "2": 3.64522647857666,
                            "3": 3.6433067321777344,
                            "1": 3.643099308013916
                        },
                        "cpu_energy": {
                            "0": 0.005010544945671426,
                            "2": 0.005012312781283982,
                            "3": 0.004786353573123052,
                            "1": 0.0046894236191765215
                        },
                        "gpu_energy": {
                            "0": 0.033527511266432164,
                            "2": 0.03351411736682586,
                            "3": 0.032175067128923374,
                            "1": 0.03139202789138018
                        },
                        "ram_energy": {
                            "0": 0.00013766478774517954,
                            "2": 0.00013859157465282516,
                            "3": 0.00013091939141658465,
                            "1": 0.0001287119925363183
                        },
                        "total_energy_kwh": {
                            "0": 0.03867572099984874,
                            "2": 0.03866502172276267,
                            "3": 0.037092340093463017,
                            "1": 0.03621016350309304
                        },
                        "total_energy_joules": {
                            "0": 139232.59559945547,
                            "2": 139194.0782019456,
                            "3": 133532.42433646685,
                            "1": 130356.58861113495
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 502.8329854469197,
                        "ram_power_avg": 3.6442666053771973,
                        "cpu_energy_total": 0.019498634919254982,
                        "gpu_energy_total": 0.13060872365356158,
                        "ram_energy_total": 0.0005358877463509077,
                        "total_energy_kwh": 0.15064324631916748,
                        "total_energy_joules": 542315.6867490028
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.011419547970527865,
                        "joules_per_token": 87.56914044065925,
                        "flops_per_joule": 12208908.170979023,
                        "joules_per_flop": 8.190740613292784e-08
                    },
                    "per-process_emissions": [
                        0.014733515914892378,
                        0.01472944002528644,
                        0.014130326958604736,
                        0.013794261786503295
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0201": {
            "setup": {
                "experiment_id": "0201",
                "date_time": "March 25, 2025 at 07:13:58 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 50,
                "decode_token_to_text": false,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 12,
                    "adaptive_batching": true,
                    "adaptive_max_tokens": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 50,
                        "total_input_tokens": 6400,
                        "total_generated_tokens": 5905
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 153.27998909843154,
                        "average_latency_ms_per_batch": 3065.599781968631,
                        "throughput_queries_per_sec": 0.3262004407365373,
                        "throughput_tokens_per_sec": 38.524272050985054
                    }
                },
                "compute_metrics": {
                    "flops": 6621082419200,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419692032,
                        "gpu_max_memory_allocated_bytes": 4419692032,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.4,
                        "cpu_memory_usage_bytes": 2909859840
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0201",
                    "local_process_results": {
                        "cpu_power": {
                            "2": 112.5,
                            "3": 112.5,
                            "0": 112.5,
                            "1": 112.5
                        },
                        "gpu_power": {
                            "2": 641.4653358891833,
                            "3": 0.0,
                            "0": 759.300655126364,
                            "1": 799.7148515095746
                        },
                        "ram_power": {
                            "2": 3.721123695373535,
                            "3": 3.7196459770202637,
                            "0": 3.7198677062988286,
                            "1": 3.7193984985351562
                        },
                        "cpu_energy": {
                            "2": 0.004977676058620997,
                            "3": 0.004779779581662296,
                            "0": 0.004782748806661404,
                            "1": 0.004731661282083226
                        },
                        "gpu_energy": {
                            "2": 0.03308340424447831,
                            "3": 0.03179044182122226,
                            "0": 0.03195273806216914,
                            "1": 0.031674949784382456
                        },
                        "ram_energy": {
                            "2": 0.00014063717076864758,
                            "3": 0.0001342262411538581,
                            "0": 0.00013479203492391248,
                            "1": 0.00013304665488991104
                        },
                        "total_energy_kwh": {
                            "2": 0.03820171747386797,
                            "3": 0.03670444764403843,
                            "0": 0.036870278903754446,
                            "1": 0.03653965772135558
                        },
                        "total_energy_joules": {
                            "2": 137526.18290592468,
                            "3": 132136.01151853835,
                            "0": 132733.004053516,
                            "1": 131542.7677968801
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 550.1202106312804,
                        "ram_power_avg": 3.720008969306946,
                        "cpu_energy_total": 0.019271865729027924,
                        "gpu_energy_total": 0.12850153391225216,
                        "ram_energy_total": 0.0005427021017363292,
                        "total_energy_kwh": 0.14831610174301643,
                        "total_energy_joules": 533937.9662748592
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.01105933717581012,
                        "joules_per_token": 90.42133213799477,
                        "flops_per_joule": 12400471.28581903,
                        "joules_per_flop": 8.064209633254691e-08
                    },
                    "per-process_emissions": [
                        0.014552944271670002,
                        0.01398255932999644,
                        0.014045732748385257,
                        0.01391978260895041
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0202": {
            "setup": {
                "experiment_id": "0202",
                "date_time": "March 25, 2025 at 07:18:45 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 50,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 12,
                    "adaptive_batching": true,
                    "adaptive_max_tokens": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 50,
                        "total_input_tokens": 6400,
                        "total_generated_tokens": 5986
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 154.68733856163453,
                        "average_latency_ms_per_batch": 3093.7467712326907,
                        "throughput_queries_per_sec": 0.3232326605714902,
                        "throughput_tokens_per_sec": 38.697414123618806
                    }
                },
                "compute_metrics": {
                    "flops": 6621082419200,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419693056,
                        "gpu_max_memory_allocated_bytes": 4419693056,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 7.5,
                        "cpu_memory_usage_bytes": 2918801408
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0202",
                    "local_process_results": {
                        "cpu_power": {
                            "3": 112.5,
                            "1": 112.5,
                            "2": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "3": 802.4193734457006,
                            "1": 901.2004660783236,
                            "2": 656.1669939189526,
                            "0": 766.2376679178631
                        },
                        "ram_power": {
                            "3": 3.749842643737793,
                            "1": 3.7512903213500977,
                            "2": 3.7525777816772465,
                            "0": 3.752354621887207
                        },
                        "cpu_energy": {
                            "3": 0.00455518827400738,
                            "1": 0.004656935344133672,
                            "2": 0.004954578442899218,
                            "0": 0.004813179265722281
                        },
                        "gpu_energy": {
                            "3": 0.030465872428232288,
                            "1": 0.031242880827617725,
                            "2": 0.032987049167394034,
                            "0": 0.032140271545531895
                        },
                        "ram_energy": {
                            "3": 0.00012867502542418419,
                            "1": 0.0001321121897403158,
                            "2": 0.00014116490546162174,
                            "0": 0.00013674718003225478
                        },
                        "total_energy_kwh": {
                            "3": 0.03514973572766385,
                            "1": 0.03603192836149171,
                            "2": 0.03808279251575488,
                            "0": 0.03709019799128644
                        },
                        "total_energy_joules": {
                            "3": 126539.04861958986,
                            "1": 129714.94210137015,
                            "2": 137098.05305671756,
                            "0": 133524.71276863117
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 781.50612534021,
                        "ram_power_avg": 3.751516342163086,
                        "cpu_energy_total": 0.01897988132676255,
                        "gpu_energy_total": 0.12683607396877594,
                        "ram_energy_total": 0.0005386993006583766,
                        "total_energy_kwh": 0.14635465459619687,
                        "total_energy_joules": 526876.7565463088
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.01136129071101635,
                        "joules_per_token": 88.01816848418122,
                        "flops_per_joule": 12566662.577034852,
                        "joules_per_flop": 7.957562271366035e-08
                    },
                    "per-process_emissions": [
                        0.013390291825453543,
                        0.013726363109310268,
                        0.014507639808876822,
                        0.01412951092478057
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0203": {
            "setup": {
                "experiment_id": "0203",
                "date_time": "March 25, 2025 at 07:19:52 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 12,
                    "adaptive_batching": true,
                    "adaptive_max_tokens": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 1280,
                        "total_generated_tokens": 1280
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 33.1768284002319,
                        "average_latency_ms_per_batch": 3317.68284002319,
                        "throughput_queries_per_sec": 0.3014151889193273,
                        "throughput_tokens_per_sec": 38.58114418167389
                    }
                },
                "compute_metrics": {
                    "flops": 1324216483840,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419611648,
                        "gpu_max_memory_allocated_bytes": 4419611648,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.5,
                        "cpu_memory_usage_bytes": 2915684352
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0203",
                    "local_process_results": {
                        "cpu_power": {
                            "2": 112.5,
                            "3": 112.5,
                            "1": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "2": 775.5903789102614,
                            "3": 864.6013796215154,
                            "1": 715.2688432725956,
                            "0": 770.3995262680294
                        },
                        "ram_power": {
                            "2": 3.6990394592285156,
                            "3": 3.698910713195801,
                            "1": 3.6991653442382812,
                            "0": 3.6998391151428223
                        },
                        "cpu_energy": {
                            "2": 0.0009764949278360293,
                            "3": 0.000971954481818102,
                            "1": 0.0009971322160563434,
                            "0": 0.0010374952919373758
                        },
                        "gpu_energy": {
                            "2": 0.006475306013576265,
                            "3": 0.006447510713556426,
                            "1": 0.006594287497643236,
                            "0": 0.006846519088319347
                        },
                        "ram_energy": {
                            "2": 2.74550702945284e-05,
                            "3": 2.7169541783661232e-05,
                            "1": 2.7871629541695966e-05,
                            "0": 2.9024917257933408e-05
                        },
                        "total_energy_kwh": {
                            "2": 0.007479256011706824,
                            "3": 0.00744663473715819,
                            "1": 0.007619291343241275,
                            "0": 0.007913039297514655
                        },
                        "total_energy_joules": {
                            "2": 26925.321642144565,
                            "3": 26807.885053769485,
                            "1": 27429.44883566859,
                            "0": 28486.94147105276
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 781.4650320181004,
                        "ram_power_avg": 3.699238657951355,
                        "cpu_energy_total": 0.00398307691764785,
                        "gpu_energy_total": 0.026363623313095275,
                        "ram_energy_total": 0.00011152115887781901,
                        "total_energy_kwh": 0.030458221389620944,
                        "total_energy_joules": 109649.5970026354
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.01167354951582025,
                        "joules_per_token": 85.6637476583089,
                        "flops_per_joule": 12076802.104509082,
                        "joules_per_flop": 8.280337719756398e-08
                    },
                    "per-process_emissions": [
                        0.0028492225776597145,
                        0.0028367955031204125,
                        0.002902569037207764,
                        0.003014472320388208
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0205": {
            "setup": {
                "experiment_id": "0205",
                "date_time": "March 25, 2025 at 07:26:17 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 12,
                    "adaptive_batching": true,
                    "adaptive_max_tokens": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 1280,
                        "total_generated_tokens": 1280
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 33.37413607328199,
                        "average_latency_ms_per_batch": 3337.413607328199,
                        "throughput_queries_per_sec": 0.2996332243040623,
                        "throughput_tokens_per_sec": 38.353052710919975
                    }
                },
                "compute_metrics": {
                    "flops": 1324216483840,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419611648,
                        "gpu_max_memory_allocated_bytes": 4419611648,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            32.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.5,
                        "cpu_memory_usage_bytes": 2911440896
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0205",
                    "local_process_results": {
                        "cpu_power": {
                            "2": 112.5,
                            "1": 112.5,
                            "0": 112.5,
                            "3": 112.5
                        },
                        "gpu_power": {
                            "2": 633.0983414607966,
                            "1": 624.4466681194945,
                            "0": 526.4124564739959,
                            "3": 884.8928585424659
                        },
                        "ram_power": {
                            "2": 3.6901817321777344,
                            "1": 3.6898112297058105,
                            "0": 3.6900572776794434,
                            "3": 3.6895737648010254
                        },
                        "cpu_energy": {
                            "2": 0.001068296605069918,
                            "1": 0.0010160600336530476,
                            "0": 0.0010443709331084392,
                            "3": 0.0009387460695288608
                        },
                        "gpu_energy": {
                            "2": 0.006994922262596681,
                            "1": 0.0067207703766127835,
                            "0": 0.0068675549384833445,
                            "3": 0.006239519158275364
                        },
                        "ram_energy": {
                            "2": 2.9971428220099616e-05,
                            "1": 2.850077129459362e-05,
                            "0": 2.941344042670714e-05,
                            "3": 2.6002191118097327e-05
                        },
                        "total_energy_kwh": {
                            "2": 0.008093190295886697,
                            "1": 0.007765331181560426,
                            "0": 0.00794133931201849,
                            "3": 0.007204267418922323
                        },
                        "total_energy_joules": {
                            "2": 29135.48506519211,
                            "1": 27955.192253617533,
                            "0": 28588.82152326656,
                            "3": 25935.362708120363
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 667.2125811491883,
                        "ram_power_avg": 3.6899060010910034,
                        "cpu_energy_total": 0.0040674736413602655,
                        "gpu_energy_total": 0.026822766735968173,
                        "ram_energy_total": 0.00011388783105949771,
                        "total_energy_kwh": 0.031004128208387938,
                        "total_energy_joules": 111614.86155019655
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.011468006878495706,
                        "joules_per_token": 87.19911058609105,
                        "flops_per_joule": 11864159.176011343,
                        "joules_per_flop": 8.428747331896418e-08
                    },
                    "per-process_emissions": [
                        0.0030831008432180376,
                        0.002958202913615444,
                        0.0030252532109134437,
                        0.002744465673238459
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0206": {
            "setup": {
                "experiment_id": "0206",
                "date_time": "March 25, 2025 at 07:30:57 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 12,
                    "adaptive_batching": true,
                    "adaptive_max_tokens": 400
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 1280,
                        "total_generated_tokens": 1280
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 14.172894368995912,
                        "average_latency_ms_per_batch": 3543.223592248978,
                        "throughput_queries_per_sec": 0.7055721816339521,
                        "throughput_tokens_per_sec": 90.31323924914587
                    }
                },
                "compute_metrics": {
                    "flops": 1324216483840,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419611648,
                        "gpu_max_memory_allocated_bytes": 4419611648,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.4,
                        "cpu_memory_usage_bytes": 2951081984
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0206",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "3": 112.5,
                            "1": 112.5,
                            "2": 112.5
                        },
                        "gpu_power": {
                            "0": 597.2027015140771,
                            "3": 843.8501650365638,
                            "1": 830.0658172859701,
                            "2": 596.8787339755304
                        },
                        "ram_power": {
                            "0": 3.8097825050354004,
                            "3": 3.807297706604004,
                            "1": 3.8081459999084473,
                            "2": 3.8089284896850586
                        },
                        "cpu_energy": {
                            "0": 0.00044952754868791094,
                            "3": 0.0004296749082932365,
                            "1": 0.0004329604969025241,
                            "2": 0.0004458462709699234
                        },
                        "gpu_energy": {
                            "0": 0.0030381290971668307,
                            "3": 0.002917729834178928,
                            "1": 0.002939499851597027,
                            "2": 0.003013291855075373
                        },
                        "ram_energy": {
                            "0": 1.2964181236221648e-05,
                            "3": 1.2354418390389963e-05,
                            "1": 1.2541964781143953e-05,
                            "2": 1.2873250400244114e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.0035006208270909633,
                            "3": 0.003359759160862555,
                            "1": 0.003385002313280695,
                            "2": 0.0034720113764455406
                        },
                        "total_energy_joules": {
                            "0": 12602.234977527467,
                            "3": 12095.132979105198,
                            "1": 12186.008327810501,
                            "2": 12499.240955203946
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 716.9993544530354,
                        "ram_power_avg": 3.8085386753082275,
                        "cpu_energy_total": 0.0017580092248535948,
                        "gpu_energy_total": 0.011908650638018159,
                        "ram_energy_total": 5.073381480799968e-05,
                        "total_energy_kwh": 0.013717393677679754,
                        "total_energy_joules": 49382.61723964711
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.025920051863357798,
                        "joules_per_token": 38.5801697184743,
                        "flops_per_joule": 26815437.45269227,
                        "joules_per_flop": 3.7291951763389934e-08
                    },
                    "per-process_emissions": [
                        0.0013335615040803025,
                        0.0012799002523305903,
                        0.001289516631244281,
                        0.0013226627338569288
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0208": {
            "setup": {
                "experiment_id": "0208",
                "date_time": "March 25, 2025 at 07:41:52 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 12,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 1280,
                        "total_generated_tokens": 1280
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 3.7016645069234073,
                        "average_latency_ms_per_batch": 3701.6645069234073,
                        "throughput_queries_per_sec": 2.7014873933865435,
                        "throughput_tokens_per_sec": 345.7903863534776
                    }
                },
                "compute_metrics": {
                    "flops": 1324216483840,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419611648,
                        "gpu_max_memory_allocated_bytes": 4419611648,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.4,
                        "cpu_memory_usage_bytes": 2966941696
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0208",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "2": 112.5,
                            "0": 112.5,
                            "3": 112.5
                        },
                        "gpu_power": {
                            "1": 824.6916296103049,
                            "2": 762.2270006771164,
                            "0": 770.1931857049356,
                            "3": 783.3709654126754
                        },
                        "ram_power": {
                            "1": 3.7597804069519043,
                            "2": 3.7618045806884766,
                            "0": 3.760883331298828,
                            "3": 3.7597804069519043
                        },
                        "cpu_energy": {
                            "1": 0.00011343942343955859,
                            "2": 0.0001243045483424794,
                            "0": 0.0001204911037166312,
                            "3": 0.00011545601415491547
                        },
                        "gpu_energy": {
                            "1": 0.0008011273075716474,
                            "2": 0.0008662473596636744,
                            "0": 0.0008455478986624243,
                            "3": 0.0008048059216250181
                        },
                        "ram_energy": {
                            "1": 3.2745504605817323e-06,
                            "2": 3.6382751010137217e-06,
                            "0": 3.521419084227362e-06,
                            "3": 3.308579125196339e-06
                        },
                        "total_energy_kwh": {
                            "1": 0.0009178412814717878,
                            "2": 0.0009941901831071675,
                            "0": 0.0009695604214632828,
                            "3": 0.00092357051490513
                        },
                        "total_energy_joules": {
                            "1": 3304.228613298436,
                            "2": 3579.084659185803,
                            "0": 3490.4175172678183,
                            "3": 3324.853853658468
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 785.1206953512582,
                        "ram_power_avg": 3.7605621814727783,
                        "cpu_energy_total": 0.0004736910896535847,
                        "gpu_energy_total": 0.003317728487522764,
                        "ram_energy_total": 1.3742823771019155e-05,
                        "total_energy_kwh": 0.003805162400947368,
                        "total_energy_joules": 13698.584643410524
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09344031031817018,
                        "joules_per_token": 10.702019252664472,
                        "flops_per_joule": 96668124.35816078,
                        "joules_per_flop": 1.034467159303665e-08
                    },
                    "per-process_emissions": [
                        0.00034965163617667756,
                        0.00037873675025467546,
                        0.0003693540425564376,
                        0.0003518341876531093
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0211": {
            "setup": {
                "experiment_id": "0211",
                "date_time": "March 25, 2025 at 07:59:47 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 100,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": true,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 100,
                        "total_input_tokens": 12800,
                        "total_generated_tokens": 12800
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 17.84354467713274,
                        "average_latency_ms_per_batch": 3568.708935426548,
                        "throughput_queries_per_sec": 5.604267639050118,
                        "throughput_tokens_per_sec": 717.3462577984151
                    }
                },
                "compute_metrics": {
                    "flops": 13242164838400,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419795968,
                        "gpu_max_memory_allocated_bytes": 4419795968,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 7.7,
                        "cpu_memory_usage_bytes": 3386163200
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0211",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "2": 112.5,
                            "3": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "1": 999.0041363083446,
                            "2": 19.17273114881676,
                            "3": 0.0,
                            "0": 874.8504585455955
                        },
                        "ram_power": {
                            "1": 4.340941429138184,
                            "2": 4.342165946960449,
                            "3": 4.340143203735352,
                            "0": 4.341397762298584
                        },
                        "cpu_energy": {
                            "1": 0.000654735225300101,
                            "2": 0.00071183341996948,
                            "3": 0.0006815467250671645,
                            "0": 0.0006674222091023695
                        },
                        "gpu_energy": {
                            "1": 0.004692115142576725,
                            "2": 0.004887548076699666,
                            "3": 0.0046569637255675644,
                            "0": 0.004786993551810781
                        },
                        "ram_energy": {
                            "1": 1.9777679364332538e-05,
                            "2": 2.184078325871265e-05,
                            "3": 2.0631935447700813e-05,
                            "0": 2.0365369556330533e-05
                        },
                        "total_energy_kwh": {
                            "1": 0.005366628047241157,
                            "2": 0.00562122227992786,
                            "3": 0.0053591423860824315,
                            "0": 0.0054747811304694805
                        },
                        "total_energy_joules": {
                            "1": 19319.860970068166,
                            "2": 20236.400207740295,
                            "3": 19292.912589896754,
                            "0": 19709.21206969013
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 473.25683150068926,
                        "ram_power_avg": 4.341162085533142,
                        "cpu_energy_total": 0.0027155375794391153,
                        "gpu_energy_total": 0.019023620496654736,
                        "ram_energy_total": 8.261576762707654e-05,
                        "total_energy_kwh": 0.02182177384372093,
                        "total_energy_joules": 78558.38583739534
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1629361380527112,
                        "joules_per_token": 6.137373893546511,
                        "flops_per_joule": 168564624.86142972,
                        "joules_per_flop": 5.932442829105218e-09
                    },
                    "per-process_emissions": [
                        0.002044416954596519,
                        0.0021414046275385185,
                        0.0020415652919781022,
                        0.0020856178716523485
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0212": {
            "setup": {
                "experiment_id": "0212",
                "date_time": "March 25, 2025 at 08:45:11 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 1280,
                        "total_generated_tokens": 1280
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 3.67052646598313,
                        "average_latency_ms_per_batch": 3670.52646598313,
                        "throughput_queries_per_sec": 2.724404821127357,
                        "throughput_tokens_per_sec": 348.7238171043017
                    }
                },
                "compute_metrics": {
                    "flops": 1324216483840,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419611648,
                        "gpu_max_memory_allocated_bytes": 4419611648,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.5,
                        "cpu_memory_usage_bytes": 2974248960
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0212",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "2": 112.5,
                            "3": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "1": 819.869755075208,
                            "2": 732.1610879683109,
                            "3": 834.7486967497529,
                            "0": 866.5488017203767
                        },
                        "ram_power": {
                            "1": 3.767344951629639,
                            "2": 3.7686767578125,
                            "3": 3.766940116882324,
                            "0": 3.767344951629639
                        },
                        "cpu_energy": {
                            "1": 0.00011649399821544647,
                            "2": 0.0001226111600881268,
                            "3": 0.00011211502321384614,
                            "0": 0.00011871713652726612
                        },
                        "gpu_energy": {
                            "1": 0.000845265676213458,
                            "2": 0.0008775015353332805,
                            "3": 0.0008102081481657919,
                            "0": 0.0008613104112722425
                        },
                        "ram_energy": {
                            "1": 3.375816339337159e-06,
                            "2": 3.5993398411554894e-06,
                            "3": 3.2428293471737353e-06,
                            "0": 3.4915870893341103e-06
                        },
                        "total_energy_kwh": {
                            "1": 0.0009651354907682418,
                            "2": 0.0010037120352625628,
                            "3": 0.0009255660007268118,
                            "0": 0.0009835191348888427
                        },
                        "total_energy_joules": {
                            "1": 3474.4877667656706,
                            "2": 3613.3633269452257,
                            "3": 3332.0376026165222,
                            "0": 3540.6688855998336
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 813.3320853784121,
                        "ram_power_avg": 3.7675766944885254,
                        "cpu_energy_total": 0.0004699373180446855,
                        "gpu_energy_total": 0.003394285770984773,
                        "ram_energy_total": 1.3709572617000493e-05,
                        "total_energy_kwh": 0.003877932661646459,
                        "total_energy_joules": 13960.557581927253
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09168688231027633,
                        "joules_per_token": 10.906685610880666,
                        "flops_per_joule": 94854125.70872346,
                        "joules_per_flop": 1.0542504003155163e-08
                    },
                    "per-process_emissions": [
                        0.0003676683652081617,
                        0.00038236409983327327,
                        0.00035259436797687897,
                        0.00037467161443590464
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0219": {
            "setup": {
                "experiment_id": "0219",
                "date_time": "March 25, 2025 at 09:23:00 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 1280,
                        "total_generated_tokens": 1280
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 27.09226178796962,
                        "average_latency_ms_per_batch": 27092.26178796962,
                        "throughput_queries_per_sec": 0.36910908650825613,
                        "throughput_tokens_per_sec": 47.245963073056785
                    }
                },
                "compute_metrics": {
                    "flops": 84100382720,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1584502784,
                        "gpu_max_memory_allocated_bytes": 1584502784,
                        "gpu_current_memory_reserved_bytes": 2986344448,
                        "gpu_max_memory_reserved_bytes": 2986344448
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            0.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.3,
                        "cpu_memory_usage_bytes": 2509512704
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0219",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "3": 112.5,
                            "2": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "1": 610.7322443855179,
                            "3": 588.3168523967275,
                            "2": 398.44540267992323,
                            "0": 140.72217218016723
                        },
                        "ram_power": {
                            "1": 3.821298122406006,
                            "3": 3.847404956817627,
                            "2": 3.8470258712768555,
                            "0": 3.8476896286010747
                        },
                        "cpu_energy": {
                            "1": 0.0010593112039969127,
                            "3": 0.0006094574886192278,
                            "2": 0.0002927721987543919,
                            "0": 0.0008387277042238565
                        },
                        "gpu_energy": {
                            "1": 0.005728373749361637,
                            "3": 0.003408671338045899,
                            "2": 0.0016099710101995512,
                            "0": 0.004624166199328705
                        },
                        "ram_energy": {
                            "1": 3.2511506219613687e-05,
                            "3": 1.829396015135974e-05,
                            "2": 7.899427460922904e-06,
                            "0": 2.6539090259426494e-05
                        },
                        "total_energy_kwh": {
                            "1": 0.006820196459578163,
                            "3": 0.0040364227868164865,
                            "2": 0.0019106426364148658,
                            "0": 0.005489432993811989
                        },
                        "total_energy_joules": {
                            "1": 24552.707254481385,
                            "3": 14531.122032539351,
                            "2": 6878.313491093517,
                            "0": 19761.95877772316
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 434.554167910584,
                        "ram_power_avg": 3.8408546447753906,
                        "cpu_energy_total": 0.0028002685955943888,
                        "gpu_energy_total": 0.015371182296935793,
                        "ram_energy_total": 8.524398409132282e-05,
                        "total_energy_kwh": 0.018256694876621505,
                        "total_energy_joules": 65724.10155583742
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.019475351807016282,
                        "joules_per_token": 51.34695434049799,
                        "flops_per_joule": 1279597.297325557,
                        "joules_per_flop": 7.814958675593221e-07
                    },
                    "per-process_emissions": [
                        0.002598153841276301,
                        0.0015376752606377406,
                        0.0007278593123422432,
                        0.0020911994989926772
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0220": {
            "setup": {
                "experiment_id": "0220",
                "date_time": "March 25, 2025 at 09:27:12 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "llm_int8_enable_fp32_cpu_offload": false,
                    "llm_int8_has_fp16_weight": false,
                    "load_in_4bit": false
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 1280,
                        "total_generated_tokens": 1280
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.638007175992243,
                        "average_latency_ms_per_batch": 23638.007175992243,
                        "throughput_queries_per_sec": 0.42304750673552644,
                        "throughput_tokens_per_sec": 54.150080862147384
                    }
                },
                "compute_metrics": {
                    "flops": 84100382720,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1584586752,
                        "gpu_max_memory_allocated_bytes": 1584586752,
                        "gpu_current_memory_reserved_bytes": 2988441600,
                        "gpu_max_memory_reserved_bytes": 2988441600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            98.0,
                            0.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.7,
                        "cpu_memory_usage_bytes": 2551894016
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0220",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "0": 112.5,
                            "2": 112.5,
                            "3": 112.5
                        },
                        "gpu_power": {
                            "1": 583.9404992496939,
                            "0": 548.217608957011,
                            "2": 629.4583236095705,
                            "3": 580.9857900548155
                        },
                        "ram_power": {
                            "1": 3.9179477691650395,
                            "0": 3.9436626434326176,
                            "2": 3.9426097869873047,
                            "3": 3.943365097045898
                        },
                        "cpu_energy": {
                            "1": 0.001121335820906097,
                            "0": 0.000731107055216853,
                            "2": 0.00028761404706892797,
                            "3": 0.0006228170024987776
                        },
                        "gpu_energy": {
                            "1": 0.0059095363942924806,
                            "0": 0.004001870145934561,
                            "2": 0.001534597616561939,
                            "3": 0.0033800993707409077
                        },
                        "ram_energy": {
                            "1": 3.51617548217363e-05,
                            "0": 2.2565331353363454e-05,
                            "2": 7.881783671103303e-06,
                            "3": 2.012446022427285e-05
                        },
                        "total_energy_kwh": {
                            "1": 0.00706603397002031,
                            "0": 0.004755542532504775,
                            "2": 0.0018300934473019696,
                            "3": 0.00402304083346396
                        },
                        "total_energy_joules": {
                            "1": 25437.722292073115,
                            "0": 17119.95311701719,
                            "2": 6588.336410287091,
                            "3": 14482.947000470254
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 585.6505554677727,
                        "ram_power_avg": 3.936896324157715,
                        "cpu_energy_total": 0.0027628739256906555,
                        "gpu_energy_total": 0.014826103527529888,
                        "ram_energy_total": 8.57333300704759e-05,
                        "total_energy_kwh": 0.01767471078329101,
                        "total_energy_joules": 63628.95881984765
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.020116626513158223,
                        "joules_per_token": 49.710124078005975,
                        "flops_per_joule": 1321731.2412436763,
                        "joules_per_flop": 7.565834632606967e-07
                    },
                    "per-process_emissions": [
                        0.002691805640879237,
                        0.0018116239277576943,
                        0.0006971740987496854,
                        0.0015325774055080954
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0221": {
            "setup": {
                "experiment_id": "0221",
                "date_time": "March 25, 2025 at 09:29:38 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "llm_int8_enable_fp32_cpu_offload": false,
                    "llm_int8_has_fp16_weight": false,
                    "load_in_4bit": false
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 1280,
                        "total_generated_tokens": 1280
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 26.779342535999604,
                        "average_latency_ms_per_batch": 26779.342535999604,
                        "throughput_queries_per_sec": 0.3734221624954739,
                        "throughput_tokens_per_sec": 47.79803679942066
                    }
                },
                "compute_metrics": {
                    "flops": 84100382720,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1584487424,
                        "gpu_max_memory_allocated_bytes": 1584487424,
                        "gpu_current_memory_reserved_bytes": 2988441600,
                        "gpu_max_memory_reserved_bytes": 2988441600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            0.0,
                            26.0
                        ],
                        "cpu_usage_percent": 5.5,
                        "cpu_memory_usage_bytes": 2518134784
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0221",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "0": 112.5,
                            "2": 112.5,
                            "3": 112.5
                        },
                        "gpu_power": {
                            "1": 595.9927876149675,
                            "0": 601.3377875615265,
                            "2": 839.1708369978986,
                            "3": 410.81709415618064
                        },
                        "ram_power": {
                            "1": 3.800853252410889,
                            "0": 3.8273634910583496,
                            "2": 3.825702667236328,
                            "3": 3.8263635635375977
                        },
                        "cpu_energy": {
                            "1": 0.0011261718504974854,
                            "0": 0.0008283058222550609,
                            "2": 0.00028885321671623393,
                            "3": 0.0006003300500633487
                        },
                        "gpu_energy": {
                            "1": 0.006026331487729131,
                            "0": 0.004504726103782275,
                            "2": 0.0015476695714671251,
                            "3": 0.0032809531803188463
                        },
                        "ram_energy": {
                            "1": 3.4533330572543165e-05,
                            "0": 2.4943074787302058e-05,
                            "2": 7.559456372095075e-06,
                            "3": 1.7680730768046898e-05
                        },
                        "total_energy_kwh": {
                            "1": 0.00718703666879916,
                            "0": 0.005357975000824636,
                            "2": 0.0018440822445554542,
                            "3": 0.003898963961150242
                        },
                        "total_energy_joules": {
                            "1": 25873.332007676978,
                            "0": 19288.71000296869,
                            "2": 6638.696080399635,
                            "3": 14036.270260140871
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 611.8296265826433,
                        "ram_power_avg": 3.820070743560791,
                        "cpu_energy_total": 0.0028436609395321288,
                        "gpu_energy_total": 0.015359680343297377,
                        "ram_energy_total": 8.471659249998719e-05,
                        "total_energy_kwh": 0.018288057875329493,
                        "total_energy_joules": 65837.00835118617
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.019441952665471295,
                        "joules_per_token": 51.4351627743642,
                        "flops_per_joule": 1277402.8593673906,
                        "joules_per_flop": 7.828383917155398e-07
                    },
                    "per-process_emissions": [
                        0.00273790161897904,
                        0.0020411205765641454,
                        0.0007025031310634003,
                        0.0014853103210001847
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0222": {
            "setup": {
                "experiment_id": "0222",
                "date_time": "March 25, 2025 at 09:31:06 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_temperature": 2.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "llm_int8_enable_fp32_cpu_offload": false,
                    "llm_int8_has_fp16_weight": false,
                    "load_in_4bit": false
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 1280,
                        "total_generated_tokens": 1280
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 25.27771397307515,
                        "average_latency_ms_per_batch": 25277.71397307515,
                        "throughput_queries_per_sec": 0.39560539416861884,
                        "throughput_tokens_per_sec": 50.63749045358321
                    }
                },
                "compute_metrics": {
                    "flops": 84100382720,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1584640000,
                        "gpu_max_memory_allocated_bytes": 1584640000,
                        "gpu_current_memory_reserved_bytes": 2988441600,
                        "gpu_max_memory_reserved_bytes": 2988441600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            0.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.6,
                        "cpu_memory_usage_bytes": 2552737792
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0222",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "2": 112.5,
                            "3": 112.5,
                            "1": 112.5
                        },
                        "gpu_power": {
                            "0": 391.4191691108303,
                            "2": 616.0764581951612,
                            "3": 344.5293721373695,
                            "1": 561.7600606481627
                        },
                        "ram_power": {
                            "0": 3.825234889984131,
                            "2": 3.8235955238342285,
                            "3": 3.8246984481811523,
                            "1": 3.7985000610351562
                        },
                        "cpu_energy": {
                            "0": 0.0007817937605104819,
                            "2": 0.00028687149333927667,
                            "3": 0.0006025643181819759,
                            "1": 0.001088156206871645
                        },
                        "gpu_energy": {
                            "0": 0.0042482025652264,
                            "2": 0.001555029299577626,
                            "3": 0.0032385550908431426,
                            "1": 0.005796494637193561
                        },
                        "ram_energy": {
                            "0": 2.3320535871649036e-05,
                            "2": 7.448552812923254e-06,
                            "3": 1.789776316245032e-05,
                            "1": 3.3068925262805687e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.00505331686160853,
                            "2": 0.0018493493457298262,
                            "3": 0.0038590171721875678,
                            "1": 0.006917719769328012
                        },
                        "total_energy_joules": {
                            "0": 18191.940701790707,
                            "2": 6657.657644627374,
                            "3": 13892.461819875243,
                            "1": 24903.791169580843
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 478.446265022881,
                        "ram_power_avg": 3.818007230758667,
                        "cpu_energy_total": 0.0027593857789033798,
                        "gpu_energy_total": 0.01483828159284073,
                        "ram_energy_total": 8.17357771098283e-05,
                        "total_energy_kwh": 0.017679403148853936,
                        "total_energy_joules": 63645.85133587417
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.020111287273779057,
                        "joules_per_token": 49.72332135615169,
                        "flops_per_joule": 1321380.4349349095,
                        "joules_per_flop": 7.567843246061529e-07
                    },
                    "per-process_emissions": [
                        0.0019250610584297695,
                        0.0007045096332557773,
                        0.001470092591744854,
                        0.0026353053461255064
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0225": {
            "setup": {
                "experiment_id": "0225",
                "date_time": "March 28, 2025 at 04:10:28 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 500,
                "max_output_tokens": 200,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_temperature": 2.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "llm_int8_enable_fp32_cpu_offload": false,
                    "llm_int8_has_fp16_weight": false,
                    "load_in_4bit": false
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 5000,
                        "total_generated_tokens": 2000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 4.689698926056735,
                        "average_latency_ms_per_batch": 4689.698926056735,
                        "throughput_queries_per_sec": 2.1323330468909982,
                        "throughput_tokens_per_sec": 426.4666093781996
                    }
                },
                "compute_metrics": {
                    "flops": 5172720640000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419647488,
                        "gpu_max_memory_allocated_bytes": 4419647488,
                        "gpu_current_memory_reserved_bytes": 6725566464,
                        "gpu_max_memory_reserved_bytes": 6725566464
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            3.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.9,
                        "cpu_memory_usage_bytes": 2961887232
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0225",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "0": 112.5,
                            "2": 112.5,
                            "3": 112.5
                        },
                        "gpu_power": {
                            "1": 636.7104287626048,
                            "0": 716.282055497344,
                            "2": 585.4680095282523,
                            "3": 555.4852779790824
                        },
                        "ram_power": {
                            "1": 3.7400879859924316,
                            "0": 3.7398319244384766,
                            "2": 3.740936279296875,
                            "3": 3.7419047355651855
                        },
                        "cpu_energy": {
                            "1": 0.00017698877228394848,
                            "0": 0.00015196183091029526,
                            "2": 0.00018807060018662014,
                            "3": 0.00019642775681131752
                        },
                        "gpu_energy": {
                            "1": 0.0011599700946351987,
                            "0": 0.0010214966505248668,
                            "2": 0.00121332708177313,
                            "3": 0.0012588432292925944
                        },
                        "ram_energy": {
                            "1": 4.945702717878255e-06,
                            "0": 4.167531534618425e-06,
                            "2": 5.390939665332612e-06,
                            "3": 5.493267013794092e-06
                        },
                        "total_energy_kwh": {
                            "1": 0.0013419045696370256,
                            "0": 0.0011776260129697804,
                            "2": 0.001406788621625083,
                            "3": 0.001460764253117706
                        },
                        "total_energy_joules": {
                            "1": 4830.856450693292,
                            "0": 4239.4536466912095,
                            "2": 5064.4390378502985,
                            "3": 5258.751311223742
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 623.486442941821,
                        "ram_power_avg": 3.740690231323242,
                        "cpu_energy_total": 0.0007134489601921814,
                        "gpu_energy_total": 0.00465363705622579,
                        "ram_energy_total": 1.9997440931623382e-05,
                        "total_energy_kwh": 0.005387083457349596,
                        "total_energy_joules": 19393.500446458544
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.10312733410461859,
                        "joules_per_token": 9.696750223229271,
                        "flops_per_joule": 266724444.83556825,
                        "joules_per_flop": 3.749187670505737e-09
                    },
                    "per-process_emissions": [
                        0.0005111985458032249,
                        0.00044861662964083783,
                        0.0005359161254080754,
                        0.0005564781422251902
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0229": {
            "setup": {
                "experiment_id": "0229",
                "date_time": "March 28, 2025 at 04:32:28 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 500,
                "max_output_tokens": 200,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "llm_int8_enable_fp32_cpu_offload": false,
                    "llm_int8_has_fp16_weight": false,
                    "load_in_4bit": false
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 5000,
                        "total_generated_tokens": 2000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 4.605450682924129,
                        "average_latency_ms_per_batch": 4605.450682924129,
                        "throughput_queries_per_sec": 2.171340155063982,
                        "throughput_tokens_per_sec": 434.26803101279637
                    }
                },
                "compute_metrics": {
                    "flops": 5172720640000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419647488,
                        "gpu_max_memory_allocated_bytes": 4419647488,
                        "gpu_current_memory_reserved_bytes": 6725566464,
                        "gpu_max_memory_reserved_bytes": 6725566464
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            34.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.7,
                        "cpu_memory_usage_bytes": 2977472512
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0229",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "3": 112.5,
                            "1": 112.5,
                            "2": 112.5
                        },
                        "gpu_power": {
                            "0": 835.4984389896919,
                            "3": 984.2252989593372,
                            "1": 890.4179582205574,
                            "2": 804.8718883432722
                        },
                        "ram_power": {
                            "0": 3.85598087310791,
                            "3": 3.8563013076782227,
                            "1": 3.8563013076782227,
                            "2": 3.8570494651794434
                        },
                        "cpu_energy": {
                            "0": 0.00014815280243419694,
                            "3": 0.00019716343600521211,
                            "1": 0.0001991730665358773,
                            "2": 0.0002014604920623242
                        },
                        "gpu_energy": {
                            "0": 0.0010518700081583177,
                            "3": 0.0013901488898966363,
                            "1": 0.0014063272361752865,
                            "2": 0.0014063272361752865
                        },
                        "ram_energy": {
                            "0": 4.2094971862216125e-06,
                            "3": 5.703136772064338e-06,
                            "1": 5.787295609878197e-06,
                            "2": 5.869182955828741e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0012042323077787363,
                            "3": 0.0015930154626739126,
                            "1": 0.0016112875983210421,
                            "2": 0.0016136569111934397
                        },
                        "total_energy_joules": {
                            "0": 4335.23630800345,
                            "3": 5734.855665626085,
                            "1": 5800.635353955751,
                            "2": 5809.164880296383
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 878.7533961282146,
                        "ram_power_avg": 3.8564082384109497,
                        "cpu_energy_total": 0.0007459497970376105,
                        "gpu_energy_total": 0.005254673370405527,
                        "ram_energy_total": 2.156911252399289e-05,
                        "total_energy_kwh": 0.006022192279967131,
                        "total_energy_joules": 21679.89220788167
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0922513811795109,
                        "joules_per_token": 10.839946103940834,
                        "flops_per_joule": 238595311.7478818,
                        "joules_per_flop": 4.1911971893927116e-09
                    },
                    "per-process_emissions": [
                        0.0004587522976483096,
                        0.000606859240505627,
                        0.000613820010580401,
                        0.0006147226003191409
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0230": {
            "setup": {
                "experiment_id": "0230",
                "date_time": "March 28, 2025 at 04:34:46 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 500,
                "max_output_tokens": 200,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "llm_int8_enable_fp32_cpu_offload": false,
                    "llm_int8_has_fp16_weight": false,
                    "load_in_4bit": false
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 5000,
                        "total_generated_tokens": 2000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 9.418737957952544,
                        "average_latency_ms_per_batch": 9418.737957952544,
                        "throughput_queries_per_sec": 1.0617133680374533,
                        "throughput_tokens_per_sec": 212.34267360749064
                    }
                },
                "compute_metrics": {
                    "flops": 328299520000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1096002560,
                        "gpu_max_memory_allocated_bytes": 1096002560,
                        "gpu_current_memory_reserved_bytes": 2120220672,
                        "gpu_max_memory_reserved_bytes": 2120220672
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            27.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.8,
                        "cpu_memory_usage_bytes": 2574630912
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0230",
                    "local_process_results": {
                        "cpu_power": {
                            "3": 112.5,
                            "2": 112.5,
                            "1": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "3": 536.8230018953936,
                            "2": 696.0436344088813,
                            "1": 0.0,
                            "0": 865.9351875849735
                        },
                        "ram_power": {
                            "3": 3.966186046600342,
                            "2": 3.9634308815002437,
                            "1": 3.9648041725158696,
                            "0": 3.962850093841553
                        },
                        "cpu_energy": {
                            "3": 0.00043328812824620396,
                            "2": 0.0004023410993140714,
                            "1": 0.0004087254564765317,
                            "0": 0.0002969624850593391
                        },
                        "gpu_energy": {
                            "3": 0.0030612902268085662,
                            "2": 0.0029053503798337488,
                            "1": 0.0029397409629048354,
                            "0": 0.0022468012418812577
                        },
                        "ram_energy": {
                            "3": 1.2529087631226746e-05,
                            "2": 1.1452749087070781e-05,
                            "1": 1.2674257946755096e-05,
                            "0": 7.859978007821229e-06
                        },
                        "total_energy_kwh": {
                            "3": 0.0035071074426859966,
                            "2": 0.0033191442282348915,
                            "1": 0.0033611406773281223,
                            "0": 0.002551623704948418
                        },
                        "total_energy_joules": {
                            "3": 12625.586793669587,
                            "2": 11948.919221645609,
                            "1": 12100.10643838124,
                            "0": 9185.845337814306
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 524.700455972312,
                        "ram_power_avg": 3.964317798614502,
                        "cpu_energy_total": 0.001541317169096146,
                        "gpu_energy_total": 0.011153182811428408,
                        "ram_energy_total": 4.451607267287385e-05,
                        "total_energy_kwh": 0.01273901605319743,
                        "total_energy_joules": 45860.45779151074
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.043610554632758626,
                        "joules_per_token": 22.93022889575537,
                        "flops_per_joule": 7158662.076434216,
                        "joules_per_flop": 1.3969090722859033e-07
                    },
                    "per-process_emissions": [
                        0.0013360325802912305,
                        0.001264427993746082,
                        0.0012804265410281482,
                        0.0009720410504000999
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0233": {
            "setup": {
                "experiment_id": "0233",
                "date_time": "March 28, 2025 at 04:45:15 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 500,
                "max_output_tokens": 200,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "llm_int8_enable_fp32_cpu_offload": false,
                    "llm_int8_has_fp16_weight": false,
                    "load_in_4bit": false
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 5000,
                        "total_generated_tokens": 2000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 14.389978655963205,
                        "average_latency_ms_per_batch": 14389.978655963205,
                        "throughput_queries_per_sec": 0.694928063416967,
                        "throughput_tokens_per_sec": 138.9856126833934
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1584580608,
                        "gpu_max_memory_allocated_bytes": 1584580608,
                        "gpu_current_memory_reserved_bytes": 2845835264,
                        "gpu_max_memory_reserved_bytes": 2845835264
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            43.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.9,
                        "cpu_memory_usage_bytes": 2428854272
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0233",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "2": 112.5,
                            "1": 112.5,
                            "3": 112.5
                        },
                        "gpu_power": {
                            "0": 449.44298892473967,
                            "2": 585.7335612811288,
                            "1": 681.0110536440384,
                            "3": 765.8546960244189
                        },
                        "ram_power": {
                            "0": 3.9068012237548833,
                            "2": 3.9085006713867183,
                            "1": 3.907196044921875,
                            "3": 3.88128662109375
                        },
                        "cpu_energy": {
                            "0": 0.0004484647540593869,
                            "2": 0.0015088147729438784,
                            "1": 0.001497143481421517,
                            "3": 0.0024667778698785696
                        },
                        "gpu_energy": {
                            "0": 0.002691128819565236,
                            "2": 0.009245218785061127,
                            "1": 0.009215573761347073,
                            "3": 0.01463415642954402
                        },
                        "ram_energy": {
                            "0": 1.1640883819043895e-05,
                            "2": 4.620101855842636e-05,
                            "1": 4.590050855774139e-05,
                            "3": 7.724916515164119e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.0031512344574436673,
                            "2": 0.010800234576563431,
                            "1": 0.010758617751326333,
                            "3": 0.017178183464574236
                        },
                        "total_energy_joules": {
                            "0": 11344.444046797202,
                            "2": 38880.84447562835,
                            "1": 38731.0239047748,
                            "3": 61841.46047246725
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 620.5105749685814,
                        "ram_power_avg": 3.9009461402893066,
                        "cpu_energy_total": 0.005921200878303352,
                        "gpu_energy_total": 0.03578607779551746,
                        "ram_energy_total": 0.00018099157608685284,
                        "total_energy_kwh": 0.04188827024990767,
                        "total_energy_joules": 150797.77289966762
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.01326279534201535,
                        "joules_per_token": 75.39888644983381,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.0012004627665631652,
                        0.004114349361941839,
                        0.004098495432367766,
                        0.006544028990829556
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0238": {
            "setup": {
                "experiment_id": "0238",
                "date_time": "March 28, 2025 at 05:16:23 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 500,
                "max_output_tokens": 200,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "llm_int8_enable_fp32_cpu_offload": false,
                    "llm_int8_has_fp16_weight": false,
                    "load_in_4bit": false
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 5000,
                        "total_generated_tokens": 2000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 4.646431545028463,
                        "average_latency_ms_per_batch": 4646.431545028463,
                        "throughput_queries_per_sec": 2.152189245249871,
                        "throughput_tokens_per_sec": 430.4378490499742
                    }
                },
                "compute_metrics": {
                    "flops": 5172720640000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419647488,
                        "gpu_max_memory_allocated_bytes": 4419647488,
                        "gpu_current_memory_reserved_bytes": 6725566464,
                        "gpu_max_memory_reserved_bytes": 6725566464
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            6.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 7.1,
                        "cpu_memory_usage_bytes": 2979549184
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0238",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "2": 112.5,
                            "3": 112.5,
                            "1": 112.5
                        },
                        "gpu_power": {
                            "0": 798.7621955459322,
                            "2": 661.1896370263843,
                            "3": 1031.634435421414,
                            "1": 663.7779320952218
                        },
                        "ram_power": {
                            "0": 3.7550740242004395,
                            "2": 3.757474422454834,
                            "3": 3.7554101943969727,
                            "1": 3.756277084350586
                        },
                        "cpu_energy": {
                            "0": 0.00015053988271756678,
                            "2": 0.00020742608396540164,
                            "3": 0.00019679083121809524,
                            "1": 0.00020098381012212486
                        },
                        "gpu_energy": {
                            "0": 0.0010692644665208917,
                            "2": 0.001443798099478144,
                            "3": 0.0014056986245574166,
                            "1": 0.0014124702966427094
                        },
                        "ram_energy": {
                            "0": 4.1024176730875145e-06,
                            "2": 5.872212089621812e-06,
                            "3": 5.497673814737991e-06,
                            "1": 5.701458787751016e-06
                        },
                        "total_energy_kwh": {
                            "0": 0.0012239067669115459,
                            "2": 0.0016570963955331675,
                            "3": 0.00160798712959025,
                            "1": 0.0016191555655525854
                        },
                        "total_energy_joules": {
                            "0": 4406.064360881565,
                            "2": 5965.547023919403,
                            "3": 5788.7536665249,
                            "1": 5828.9600359893075
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 788.8410500222382,
                        "ram_power_avg": 3.756058931350708,
                        "cpu_energy_total": 0.0007557406080231884,
                        "gpu_energy_total": 0.005331231487199162,
                        "ram_energy_total": 2.1173762365198333e-05,
                        "total_energy_kwh": 0.0061081458575875484,
                        "total_energy_joules": 21989.325087315177
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09095322353270976,
                        "joules_per_token": 10.994662543657588,
                        "flops_per_joule": 235237808.32109076,
                        "joules_per_flop": 4.251017330662415e-09
                    },
                    "per-process_emissions": [
                        0.0004662472828549534,
                        0.0006312708718783602,
                        0.0006125626970174057,
                        0.0006168173126972574
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0239": {
            "setup": {
                "experiment_id": "0239",
                "date_time": "March 28, 2025 at 05:22:54 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 500,
                "max_output_tokens": 200,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "llm_int8_enable_fp32_cpu_offload": false,
                    "llm_int8_has_fp16_weight": false,
                    "load_in_4bit": false
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 5000,
                        "total_generated_tokens": 2000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 4.650933608994819,
                        "average_latency_ms_per_batch": 4650.933608994819,
                        "throughput_queries_per_sec": 2.1501059444624593,
                        "throughput_tokens_per_sec": 430.02118889249186
                    }
                },
                "compute_metrics": {
                    "flops": 5172720640000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419647488,
                        "gpu_max_memory_allocated_bytes": 4419647488,
                        "gpu_current_memory_reserved_bytes": 6725566464,
                        "gpu_max_memory_reserved_bytes": 6725566464
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            8.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.7,
                        "cpu_memory_usage_bytes": 2967252992
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0239",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "0": 112.5,
                            "3": 112.5,
                            "2": 112.5
                        },
                        "gpu_power": {
                            "1": 705.7247637341509,
                            "0": 687.7579993032783,
                            "3": 678.9676888040982,
                            "2": 426.83894275043093
                        },
                        "ram_power": {
                            "1": 3.857295513153076,
                            "0": 3.854674816131592,
                            "3": 3.854674816131592,
                            "2": 3.8576316833496094
                        },
                        "cpu_energy": {
                            "1": 0.0001626167912836536,
                            "0": 0.00015038743994227844,
                            "3": 0.00014989557643275476,
                            "2": 0.00019176153124863052
                        },
                        "gpu_energy": {
                            "1": 0.0008507723472845896,
                            "0": 0.0007813350695098364,
                            "3": 0.0007804353465683533,
                            "2": 0.000978801338595403
                        },
                        "ram_energy": {
                            "1": 4.585109665421915e-06,
                            "0": 4.3146876139290505e-06,
                            "3": 4.316713825916256e-06,
                            "2": 6.5325732090478434e-06
                        },
                        "total_energy_kwh": {
                            "1": 0.0010179742482336654,
                            "0": 0.0009360371970660438,
                            "3": 0.0009346476368270244,
                            "2": 0.0011770954430530814
                        },
                        "total_energy_joules": {
                            "1": 3664.7072936411955,
                            "0": 3369.733909437758,
                            "3": 3364.731492577288,
                            "2": 4237.543594991093
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 624.8223486479895,
                        "ram_power_avg": 3.8560692071914673,
                        "cpu_energy_total": 0.0006546613389073173,
                        "gpu_energy_total": 0.0033913441019581825,
                        "ram_energy_total": 1.9749084314315065e-05,
                        "total_energy_kwh": 0.004065754525179815,
                        "total_energy_joules": 14636.716290647335
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.13664267041084707,
                        "joules_per_token": 7.318358145323668,
                        "flops_per_joule": 353407180.769453,
                        "joules_per_flop": 2.829597287250242e-09
                    },
                    "per-process_emissions": [
                        0.00038779728986461484,
                        0.0003565833702223094,
                        0.00035605401724925494,
                        0.00044841450903107137
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0241": {
            "setup": {
                "experiment_id": "0241",
                "date_time": "March 28, 2025 at 05:24:11 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 500,
                "max_output_tokens": 200,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "llm_int8_enable_fp32_cpu_offload": false,
                    "llm_int8_has_fp16_weight": false,
                    "load_in_4bit": false
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 5000,
                        "total_generated_tokens": 2000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 5.389122053980827,
                        "average_latency_ms_per_batch": 5389.122053980827,
                        "throughput_queries_per_sec": 1.8555898158983462,
                        "throughput_tokens_per_sec": 371.11796317966923
                    }
                },
                "compute_metrics": {
                    "flops": 5172720640000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419647488,
                        "gpu_max_memory_allocated_bytes": 4419647488,
                        "gpu_current_memory_reserved_bytes": 6725566464,
                        "gpu_max_memory_reserved_bytes": 6725566464
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.6,
                        "cpu_memory_usage_bytes": 2962198528
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0241",
                    "local_process_results": {
                        "cpu_power": {
                            "3": 112.5,
                            "0": 112.5,
                            "1": 112.5,
                            "2": 112.5
                        },
                        "gpu_power": {
                            "3": 546.0692215330771,
                            "0": 582.0812482167195,
                            "1": 323.44198747486183,
                            "2": 552.6380491720985
                        },
                        "ram_power": {
                            "3": 3.803521156311035,
                            "0": 3.80432939529419,
                            "1": 3.8052592277526855,
                            "2": 3.8033924102783203
                        },
                        "cpu_energy": {
                            "3": 0.00017400014978193214,
                            "0": 0.00017397345971767212,
                            "1": 0.0002104974724970816,
                            "2": 0.0001727436355649843
                        },
                        "gpu_energy": {
                            "3": 0.0008396801161900669,
                            "0": 0.0008387078931910708,
                            "1": 0.0009529177067779671,
                            "2": 0.00083094177586851
                        },
                        "ram_energy": {
                            "3": 4.687027360150639e-06,
                            "0": 4.665186828625907e-06,
                            "1": 5.851361039782996e-06,
                            "2": 4.701853260226521e-06
                        },
                        "total_energy_kwh": {
                            "3": 0.0010183672933321494,
                            "0": 0.0010173465397373686,
                            "1": 0.001169266540314832,
                            "2": 0.0010083872646937208
                        },
                        "total_energy_joules": {
                            "3": 3666.1222559957378,
                            "0": 3662.447543054527,
                            "1": 4209.359545133395,
                            "2": 3630.1941528973953
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 501.05762659918923,
                        "ram_power_avg": 3.8041255474090576,
                        "cpu_energy_total": 0.0007312147175616701,
                        "gpu_energy_total": 0.0034622474920276147,
                        "ram_energy_total": 1.9905428488786062e-05,
                        "total_energy_kwh": 0.00421336763807807,
                        "total_energy_joules": 15168.123497081055
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.13185546652391636,
                        "joules_per_token": 7.584061748540527,
                        "flops_per_joule": 341025746.59254557,
                        "joules_per_flop": 2.9323299193441566e-09
                    },
                    "per-process_emissions": [
                        0.0003879470203948823,
                        0.0003875581643129506,
                        0.0004454320885329352,
                        0.00038414512848507297
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0245": {
            "setup": {
                "experiment_id": "0245",
                "date_time": "March 28, 2025 at 05:31:57 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 500,
                "max_output_tokens": 200,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "llm_int8_enable_fp32_cpu_offload": false,
                    "llm_int8_has_fp16_weight": false,
                    "load_in_4bit": false
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 5000,
                        "total_generated_tokens": 2000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 16.026552106020972,
                        "average_latency_ms_per_batch": 16026.552106020972,
                        "throughput_queries_per_sec": 0.6239645267333033,
                        "throughput_tokens_per_sec": 124.79290534666065
                    }
                },
                "compute_metrics": {
                    "flops": 0.0,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576243200,
                        "gpu_max_memory_allocated_bytes": 1576243200,
                        "gpu_current_memory_reserved_bytes": 2835349504,
                        "gpu_max_memory_reserved_bytes": 2835349504
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": "Error: Process 3662202 got signal: 2",
                        "cpu_usage_percent": 7.1,
                        "cpu_memory_usage_bytes": 5054316544
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0245",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5,
                            "1": 112.5,
                            "2": 112.5,
                            "3": 112.5
                        },
                        "gpu_power": {
                            "0": 821.4356804588446,
                            "1": 421.0377024945521,
                            "2": 500.0398416884414,
                            "3": 339.858321155007
                        },
                        "ram_power": {
                            "0": 3.927065849304199,
                            "1": 4.611936092376709,
                            "2": 4.616138935089111,
                            "3": 3.92746353149414
                        },
                        "cpu_energy": {
                            "0": 0.0005018954668375954,
                            "1": 0.0015258318395608508,
                            "2": 0.001646151115430257,
                            "3": 0.0005378228005356505
                        },
                        "gpu_energy": {
                            "0": 0.0022751262645481773,
                            "1": 0.006879519670278,
                            "2": 0.007347812267134657,
                            "3": 0.0024003169202497787
                        },
                        "ram_energy": {
                            "0": 1.4533432730384343e-05,
                            "1": 5.028069028700605e-05,
                            "2": 5.494535016104695e-05,
                            "3": 1.4150626695977681e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.0027915551641161567,
                            "1": 0.00845563220012586,
                            "2": 0.009048908732725961,
                            "3": 0.0029522903474814076
                        },
                        "total_energy_joules": {
                            "0": 10049.598590818165,
                            "1": 30440.275920453096,
                            "2": 32576.07143781346,
                            "3": 10628.245250933067
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 520.5928864492113,
                        "ram_power_avg": 4.27065110206604,
                        "cpu_energy_total": 0.0042117012223643535,
                        "gpu_energy_total": 0.018902775122210613,
                        "ram_energy_total": 0.00013391009987441502,
                        "total_energy_kwh": 0.023248386444449386,
                        "total_energy_joules": 83694.19120001778
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.023896521028804386,
                        "joules_per_token": 41.84709560000889,
                        "flops_per_joule": 0.0,
                        "joules_per_flop": 0
                    },
                    "per-process_emissions": [
                        0.00106344293977005,
                        0.0032211730866379463,
                        0.003447181781731955,
                        0.0011246750078730422
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0246": {
            "setup": {
                "experiment_id": "0246",
                "date_time": "March 28, 2025 at 06:00:03 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 500,
                "max_output_tokens": 200,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "cached_flops_for_quantised_models": 5172720640000,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "llm_int8_enable_fp32_cpu_offload": false,
                    "llm_int8_has_fp16_weight": false
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 5000,
                        "total_generated_tokens": 2000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 14.770646497025155,
                        "average_latency_ms_per_batch": 14770.646497025155,
                        "throughput_queries_per_sec": 0.677018436668566,
                        "throughput_tokens_per_sec": 135.40368733371318
                    }
                },
                "compute_metrics": {
                    "flops": 5172720640000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576345088,
                        "gpu_max_memory_allocated_bytes": 1576345088,
                        "gpu_current_memory_reserved_bytes": 2835349504,
                        "gpu_max_memory_reserved_bytes": 2835349504
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.9,
                        "cpu_memory_usage_bytes": 2557669376
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0246",
                    "local_process_results": {
                        "cpu_power": {
                            "0": 112.5
                        },
                        "gpu_power": {
                            "0": 569.7303046313178
                        },
                        "ram_power": {
                            "0": 3.9489898681640625
                        },
                        "cpu_energy": {
                            "0": 0.00046153291869632085
                        },
                        "gpu_energy": {
                            "0": 0.002692348264988098
                        },
                        "ram_energy": {
                            "0": 1.2215598638091158e-05
                        },
                        "total_energy_kwh": {
                            "0": 0.00316609678232251
                        },
                        "total_energy_joules": {
                            "0": 11397.948416361036
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 569.7303046313178,
                        "ram_power_avg": 3.9489898681640625,
                        "cpu_energy_total": 0.00046153291869632085,
                        "gpu_energy_total": 0.002692348264988098,
                        "ram_energy_total": 1.2215598638091158e-05,
                        "total_energy_kwh": 0.00316609678232251,
                        "total_energy_joules": 11397.948416361036
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1754701747139973,
                        "joules_per_token": 5.698974208180518,
                        "flops_per_joule": 453829097.22375,
                        "joules_per_flop": 2.2034726422730293e-09
                    },
                    "per-process_emissions": [
                        0.00120612456922576
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0247": {
            "setup": {
                "experiment_id": "0247",
                "date_time": "March 28, 2025 at 06:02:08 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 500,
                "max_output_tokens": 200,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "cached_flops_for_quantised_models": 5172720640000,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "llm_int8_enable_fp32_cpu_offload": false,
                    "llm_int8_has_fp16_weight": false
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 5000,
                        "total_generated_tokens": 2000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 9.370280603063293,
                        "average_latency_ms_per_batch": 9370.280603063293,
                        "throughput_queries_per_sec": 1.0672038996068955,
                        "throughput_tokens_per_sec": 213.44077992137912
                    }
                },
                "compute_metrics": {
                    "flops": 5172720640000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087482880,
                        "gpu_max_memory_allocated_bytes": 1087482880,
                        "gpu_current_memory_reserved_bytes": 1881145344,
                        "gpu_max_memory_reserved_bytes": 1881145344
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.9,
                        "cpu_memory_usage_bytes": 2526007296
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0247",
                    "local_process_results": {
                        "cpu_power": {
                            "3": 112.5,
                            "2": 112.5,
                            "0": 112.5,
                            "1": 112.5
                        },
                        "gpu_power": {
                            "3": 777.0206581124299,
                            "2": 696.9357167494494,
                            "0": 833.8770773584041,
                            "1": 732.0938294424174
                        },
                        "ram_power": {
                            "3": 3.888087272644043,
                            "2": 3.888087272644043,
                            "0": 3.8877511024475098,
                            "1": 3.8880815505981445
                        },
                        "cpu_energy": {
                            "3": 0.0003915760248419246,
                            "2": 0.00039472270449186905,
                            "0": 0.00029519852128214553,
                            "1": 0.0003912589485007629
                        },
                        "gpu_energy": {
                            "3": 0.0027663944353388814,
                            "2": 0.002780228613071145,
                            "0": 0.002141353101968946,
                            "1": 0.0027589738738438285
                        },
                        "ram_energy": {
                            "3": 1.1014726498533095e-05,
                            "2": 1.0968435345064943e-05,
                            "0": 7.913303032665625e-06,
                            "1": 1.0928086793807118e-05
                        },
                        "total_energy_kwh": {
                            "3": 0.0031689851866793387,
                            "2": 0.003185919752908079,
                            "0": 0.002444464926283757,
                            "1": 0.0031611609091383988
                        },
                        "total_energy_joules": {
                            "3": 11408.34667204562,
                            "2": 11469.311110469083,
                            "0": 8800.073734621525,
                            "1": 11380.179272898236
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 759.9818204156752,
                        "ram_power_avg": 3.888001799583435,
                        "cpu_energy_total": 0.001472756199116702,
                        "gpu_energy_total": 0.0104469500242228,
                        "ram_energy_total": 4.082455167007078e-05,
                        "total_energy_kwh": 0.011960530775009574,
                        "total_energy_joules": 43057.91079003447
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.04644907203586129,
                        "joules_per_token": 21.528955395017235,
                        "flops_per_joule": 120134036.81437325,
                        "joules_per_flop": 8.324035606538084e-09
                    },
                    "per-process_emissions": [
                        0.0012072249068654942,
                        0.0012136761298703327,
                        0.0009312189136677973,
                        0.0012042442483362731
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0248": {
            "setup": {
                "experiment_id": "0248",
                "date_time": "March 28, 2025 at 06:03:32 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 500,
                "max_output_tokens": 200,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "cached_flops_for_quantised_models": 5172720640000,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "llm_int8_enable_fp32_cpu_offload": false,
                    "llm_int8_has_fp16_weight": false
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 5000,
                        "total_generated_tokens": 2000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 9.281060153036378,
                        "average_latency_ms_per_batch": 9281.060153036378,
                        "throughput_queries_per_sec": 1.0774631168324467,
                        "throughput_tokens_per_sec": 215.4926233664893
                    }
                },
                "compute_metrics": {
                    "flops": 5172720640000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087482880,
                        "gpu_max_memory_allocated_bytes": 1087482880,
                        "gpu_current_memory_reserved_bytes": 1881145344,
                        "gpu_max_memory_reserved_bytes": 1881145344
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            76.0,
                            36.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.0,
                        "cpu_memory_usage_bytes": 2527498240
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0248",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "3": 112.5,
                            "2": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "1": 653.1090733661877,
                            "3": 694.0820240078708,
                            "2": 732.9841162494794,
                            "0": 854.1525526328057
                        },
                        "ram_power": {
                            "1": 3.8857898712158208,
                            "3": 3.8845224380493164,
                            "2": 3.8835039138793945,
                            "0": 3.8832249641418457
                        },
                        "cpu_energy": {
                            "1": 0.0004206531429917959,
                            "3": 0.00041196195178054047,
                            "2": 0.0003933807654648263,
                            "0": 0.00029142337831581245
                        },
                        "gpu_energy": {
                            "1": 0.0030080390730962137,
                            "3": 0.0029643793159532805,
                            "2": 0.002861099233319564,
                            "0": 0.002192348976103986
                        },
                        "ram_energy": {
                            "1": 1.1809316231140254e-05,
                            "3": 1.164455288635889e-05,
                            "2": 1.095199614797346e-05,
                            "0": 7.711038139441983e-06
                        },
                        "total_energy_kwh": {
                            "1": 0.00344050153231915,
                            "3": 0.0033879858206201795,
                            "2": 0.0032654319949323636,
                            "0": 0.0024914833925592404
                        },
                        "total_energy_joules": {
                            "1": 12385.80551634894,
                            "3": 12196.748954232646,
                            "2": 11755.555181756508,
                            "0": 8969.340213213265
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 733.5819415640858,
                        "ram_power_avg": 3.8842602968215942,
                        "cpu_energy_total": 0.001517419238552975,
                        "gpu_energy_total": 0.011025866598473044,
                        "ram_energy_total": 4.2116903404914585e-05,
                        "total_energy_kwh": 0.012585402740430934,
                        "total_energy_joules": 45307.44986555136
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0441428508100753,
                        "joules_per_token": 22.65372493277568,
                        "flops_per_joule": 114169317.74685861,
                        "joules_per_flop": 8.758920695464304e-09
                    },
                    "per-process_emissions": [
                        0.0013106590587369802,
                        0.0012906531983652575,
                        0.0012439663184694839,
                        0.0009491305983954426
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0263": {
            "setup": {
                "experiment_id": "0263",
                "date_time": "March 28, 2025 at 06:51:22 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 500,
                "max_output_tokens": 200,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "cached_flops_for_quantised_models": 5172720640000,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "llm_int8_enable_fp32_cpu_offload": false,
                    "llm_int8_has_fp16_weight": false
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 5000,
                        "total_generated_tokens": 2000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 7.426042906008661,
                        "average_latency_ms_per_batch": 7426.042906008661,
                        "throughput_queries_per_sec": 1.3466122033726284,
                        "throughput_tokens_per_sec": 269.3224406745257
                    }
                },
                "compute_metrics": {
                    "flops": 5172720640000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087482880,
                        "gpu_max_memory_allocated_bytes": 1087482880,
                        "gpu_current_memory_reserved_bytes": 1881145344,
                        "gpu_max_memory_reserved_bytes": 1881145344
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            28.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.7,
                        "cpu_memory_usage_bytes": 2493550592
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0263",
                    "local_process_results": {
                        "cpu_power": {
                            "1": 112.5,
                            "3": 112.5,
                            "2": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "1": 372.95228389277537,
                            "3": 653.7123782512841,
                            "2": 692.2098264247489,
                            "0": 742.278126909533
                        },
                        "ram_power": {
                            "1": 3.8852763175964355,
                            "3": 3.8857884407043457,
                            "2": 3.8868112564086914,
                            "0": 3.884871482849121
                        },
                        "cpu_energy": {
                            "1": 0.0002554201822858886,
                            "3": 0.00026696333541258356,
                            "2": 0.0002742621176003013,
                            "0": 0.0002354832479359174
                        },
                        "gpu_energy": {
                            "1": 0.0016883588506928504,
                            "3": 0.001747793064899028,
                            "2": 0.0018038517208562865,
                            "0": 0.0015548937439096022
                        },
                        "ram_energy": {
                            "1": 7.749339278383101e-06,
                            "3": 7.212884428329945e-06,
                            "2": 7.490097068069351e-06,
                            "0": 6.107663226185252e-06
                        },
                        "total_energy_kwh": {
                            "1": 0.0019515283722571222,
                            "3": 0.002021969284739942,
                            "2": 0.0020856039355246574,
                            "0": 0.0017964846550717047
                        },
                        "total_energy_joules": {
                            "1": 7025.50214012564,
                            "3": 7279.089425063791,
                            "2": 7508.1741678887665,
                            "0": 6467.344758258137
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 615.2881538695854,
                        "ram_power_avg": 3.8856868743896484,
                        "cpu_energy_total": 0.001032128883234691,
                        "gpu_energy_total": 0.006794897380357767,
                        "ram_energy_total": 2.8559984000967648e-05,
                        "total_energy_kwh": 0.007855586247593426,
                        "total_energy_joules": 28280.110491336334
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.07072108153936328,
                        "joules_per_token": 14.140055245668169,
                        "flops_per_joule": 182910199.08089373,
                        "joules_per_flop": 5.467163695763847e-09
                    },
                    "per-process_emissions": [
                        0.0007434347334113507,
                        0.0007702691990216809,
                        0.0007945108192381182,
                        0.0006843708293495659
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0264": {
            "setup": {
                "experiment_id": "0264",
                "date_time": "March 28, 2025 at 06:54:52 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 500,
                "max_output_tokens": 200,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "cached_flops_for_quantised_models": 5172720640000,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "llm_int8_enable_fp32_cpu_offload": false,
                    "llm_int8_has_fp16_weight": false
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 5000,
                        "total_generated_tokens": 2000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 7.451714263996109,
                        "average_latency_ms_per_batch": 7451.714263996109,
                        "throughput_queries_per_sec": 1.3419730877653553,
                        "throughput_tokens_per_sec": 268.39461755307104
                    }
                },
                "compute_metrics": {
                    "flops": 5172720640000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087482880,
                        "gpu_max_memory_allocated_bytes": 1087482880,
                        "gpu_current_memory_reserved_bytes": 1881145344,
                        "gpu_max_memory_reserved_bytes": 1881145344
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            9.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.8,
                        "cpu_memory_usage_bytes": 2533646336
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0264",
                    "local_process_results": {
                        "cpu_power": {
                            "3": 112.5,
                            "1": 112.5,
                            "0": 112.5,
                            "2": 112.5
                        },
                        "gpu_power": {
                            "3": 800.4302382178423,
                            "1": 720.2984445234748,
                            "0": 827.0649879541822,
                            "2": 735.1273146944961
                        },
                        "ram_power": {
                            "3": 3.8926305770874023,
                            "1": 3.892115592956543,
                            "0": 3.891679286956787,
                            "2": 3.8933300971984863
                        },
                        "cpu_energy": {
                            "3": 0.00026674170031037645,
                            "1": 0.0002576297859741317,
                            "0": 0.00023542501902920775,
                            "2": 0.0002743793383342564
                        },
                        "gpu_energy": {
                            "3": 0.0017493530661472079,
                            "1": 0.001689942740839001,
                            "0": 0.001558455413432469,
                            "2": 0.0017939564351614479
                        },
                        "ram_energy": {
                            "3": 7.229818116730922e-06,
                            "1": 6.774114228900429e-06,
                            "0": 6.030071249503189e-06,
                            "2": 7.535294570446464e-06
                        },
                        "total_energy_kwh": {
                            "3": 0.002023324584574315,
                            "1": 0.0019543466410420333,
                            "0": 0.0017999105037111798,
                            "2": 0.0020758710680661503
                        },
                        "total_energy_joules": {
                            "3": 7283.968504467534,
                            "1": 7035.6479077513195,
                            "0": 6479.6778133602475,
                            "2": 7473.135845038141
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 770.7302463474989,
                        "ram_power_avg": 3.8924388885498047,
                        "cpu_energy_total": 0.0010341758436479723,
                        "gpu_energy_total": 0.006791707655580126,
                        "ram_energy_total": 2.7569298165581004e-05,
                        "total_energy_kwh": 0.00785345279739368,
                        "total_energy_joules": 28272.43007061724
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.07074029345919385,
                        "joules_per_token": 14.136215035308618,
                        "flops_per_joule": 182959888.02801448,
                        "joules_per_flop": 5.465678902508301e-09
                    },
                    "per-process_emissions": [
                        0.0007707855004935853,
                        0.0007445083529049626,
                        0.000685675906388774,
                        0.0007908030833798
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0265": {
            "setup": {
                "experiment_id": "0265",
                "date_time": "March 28, 2025 at 07:02:11 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 500,
                "max_output_tokens": 200,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "cached_flops_for_quantised_models": 5172720640000,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "llm_int8_enable_fp32_cpu_offload": false,
                    "llm_int8_has_fp16_weight": false
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 5000,
                        "total_generated_tokens": 2000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 7.448171228985302,
                        "average_latency_ms_per_batch": 7448.171228985302,
                        "throughput_queries_per_sec": 1.3426114535449993,
                        "throughput_tokens_per_sec": 268.52229070899983
                    }
                },
                "compute_metrics": {
                    "flops": 5172720640000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087482880,
                        "gpu_max_memory_allocated_bytes": 1087482880,
                        "gpu_current_memory_reserved_bytes": 1881145344,
                        "gpu_max_memory_reserved_bytes": 1881145344
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.9,
                        "cpu_memory_usage_bytes": 2518409216
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0265",
                    "local_process_results": {
                        "cpu_power": {
                            "2": 112.5,
                            "1": 112.5,
                            "3": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "2": 553.5729084725829,
                            "1": 657.6897726646869,
                            "3": 775.5617270921193,
                            "0": 746.2049752868357
                        },
                        "ram_power": {
                            "2": 3.865410804748535,
                            "1": 3.8651161193847656,
                            "3": 3.863320827484131,
                            "0": 3.8617215156555176
                        },
                        "cpu_energy": {
                            "2": 0.0002731201038150175,
                            "1": 0.0002605394644670014,
                            "3": 0.00024220159988180967,
                            "0": 0.0002354608405694307
                        },
                        "gpu_energy": {
                            "2": 0.0016262090787435568,
                            "1": 0.0015744101484109763,
                            "3": 0.001470448398578128,
                            "0": 0.001427418641933187
                        },
                        "ram_energy": {
                            "2": 7.5545265587883635e-06,
                            "1": 7.076004339100441e-06,
                            "3": 6.519125252641842e-06,
                            "0": 6.203068810164325e-06
                        },
                        "total_energy_kwh": {
                            "2": 0.0019068837091173625,
                            "1": 0.0018420256172170782,
                            "3": 0.0017191691237125797,
                            "0": 0.001669082551312782
                        },
                        "total_energy_joules": {
                            "2": 6864.781352822505,
                            "1": 6631.292221981482,
                            "3": 6189.008845365287,
                            "0": 6008.697184726016
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 683.2573458790562,
                        "ram_power_avg": 3.8638923168182373,
                        "cpu_energy_total": 0.0010113220087332592,
                        "gpu_energy_total": 0.006098486267665848,
                        "ram_energy_total": 2.7352724960694973e-05,
                        "total_energy_kwh": 0.007137161001359802,
                        "total_energy_joules": 25693.779604895288
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.07783985193128035,
                        "joules_per_token": 12.846889802447643,
                        "flops_per_joule": 201321904.34973884,
                        "joules_per_flop": 4.967169385914351e-09
                    },
                    "per-process_emissions": [
                        0.0007264273489882593,
                        0.000701719658878846,
                        0.0006549174776783073,
                        0.0006358369979226043
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0267": {
            "setup": {
                "experiment_id": "0267",
                "date_time": "March 28, 2025 at 07:12:02 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 500,
                "max_output_tokens": 200,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "cached_flops_for_quantised_models": 5172720640000,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "llm_int8_enable_fp32_cpu_offload": false,
                    "llm_int8_has_fp16_weight": false
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 5000,
                        "total_generated_tokens": 2000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 8.540011944016442,
                        "average_latency_ms_per_batch": 8540.011944016442,
                        "throughput_queries_per_sec": 1.170958549654781,
                        "throughput_tokens_per_sec": 234.19170993095622
                    }
                },
                "compute_metrics": {
                    "flops": 5172720640000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087482880,
                        "gpu_max_memory_allocated_bytes": 1087482880,
                        "gpu_current_memory_reserved_bytes": 1881145344,
                        "gpu_max_memory_reserved_bytes": 1881145344
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            100.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.3,
                        "cpu_memory_usage_bytes": 2392014848
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0267",
                    "local_process_results": {
                        "cpu_power": {
                            "2": 112.5,
                            "3": 112.5,
                            "1": 112.5,
                            "0": 112.5
                        },
                        "gpu_power": {
                            "2": 751.5255184604358,
                            "3": 810.3154795658252,
                            "1": 865.1817335238643,
                            "0": 850.7781798298398
                        },
                        "ram_power": {
                            "2": 3.853437423706055,
                            "3": 3.8503904342651367,
                            "1": 3.8515849113464355,
                            "0": 3.8515849113464355
                        },
                        "cpu_energy": {
                            "2": 0.0002761057302232075,
                            "3": 0.0002450060886294523,
                            "1": 0.0002705718196848466,
                            "0": 0.00026908405828362446
                        },
                        "gpu_energy": {
                            "2": 0.0019542257300440014,
                            "3": 0.0017310786070812156,
                            "1": 0.0019300376551409926,
                            "0": 0.001920566814229474
                        },
                        "ram_energy": {
                            "2": 7.477774551393161e-06,
                            "3": 6.480357697934705e-06,
                            "1": 7.33297790129368e-06,
                            "0": 7.274617328866506e-06
                        },
                        "total_energy_kwh": {
                            "2": 0.0022378092348186015,
                            "3": 0.0019825650534086026,
                            "1": 0.0022079424527271336,
                            "0": 0.0021969254898419653
                        },
                        "total_energy_joules": {
                            "2": 8056.113245346965,
                            "3": 7137.234192270969,
                            "1": 7948.592829817681,
                            "0": 7908.9317634310755
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 819.4502278449912,
                        "ram_power_avg": 3.8517494201660156,
                        "cpu_energy_total": 0.0010607676968211307,
                        "gpu_energy_total": 0.007535908806495684,
                        "ram_energy_total": 2.8565727479488056e-05,
                        "total_energy_kwh": 0.008625242230796304,
                        "total_energy_joules": 31050.872030866693
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0644104293757632,
                        "joules_per_token": 15.525436015433346,
                        "flops_per_joule": 166588578.73163632,
                        "joules_per_flop": 6.002812483387213e-09
                    },
                    "per-process_emissions": [
                        0.0008524934280041463,
                        0.0007552581570960072,
                        0.0008411156773664016,
                        0.0008369187653552967
                    ]
                }
            }
        }
    },
    {
        "EXPERIMENT_#0268": {
            "setup": {
                "experiment_id": "0268",
                "date_time": "March 28, 2025 at 07:17:47 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "max_input_tokens": 500,
                "max_output_tokens": 200,
                "number_input_prompts": 10,
                "decode_token_to_text": true,
                "decoder_temperature": 1.0,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "cached_flops_for_quantised_models": 5172720640000,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "llm_int8_enable_fp32_cpu_offload": false,
                    "llm_int8_has_fp16_weight": false
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 3000,
                    "max_batch_size___adaptive_batching": 100
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 10,
                        "total_input_tokens": 5000,
                        "total_generated_tokens": 2000
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 9.440358299063519,
                        "average_latency_ms_per_batch": 9440.358299063519,
                        "throughput_queries_per_sec": 1.059281828422974,
                        "throughput_tokens_per_sec": 211.85636568459478
                    }
                },
                "compute_metrics": {
                    "flops": 5172720640000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087482880,
                        "gpu_max_memory_allocated_bytes": 1087482880,
                        "gpu_current_memory_reserved_bytes": 1881145344,
                        "gpu_max_memory_reserved_bytes": 1881145344
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            99.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 55.2,
                        "cpu_memory_usage_bytes": 2472202240
                    }
                },
                "energy_metrics": {
                    "experiment_id": "0268",
                    "local_process_results": {
                        "cpu_power": {
                            "3": 112.5,
                            "0": 112.5,
                            "1": 112.5,
                            "2": 112.5
                        },
                        "gpu_power": {
                            "3": 690.2418029943183,
                            "0": 618.1203356870573,
                            "1": 700.3627816766635,
                            "2": 545.5024644044947
                        },
                        "ram_power": {
                            "3": 3.8689956665039062,
                            "0": 3.8695993423461914,
                            "1": 3.8686251640319824,
                            "2": 3.870297431945801
                        },
                        "cpu_energy": {
                            "3": 0.000274790890274744,
                            "0": 0.0002994888446519326,
                            "1": 0.0002718770169994969,
                            "2": 0.0003107287525344874
                        },
                        "gpu_energy": {
                            "3": 0.001612976568161173,
                            "0": 0.00174970584420997,
                            "1": 0.001592989885503826,
                            "2": 0.001791169766276468
                        },
                        "ram_energy": {
                            "3": 7.324968895102891e-06,
                            "0": 7.970470326490563e-06,
                            "1": 7.006888509683143e-06,
                            "2": 8.505727047074521e-06
                        },
                        "total_energy_kwh": {
                            "3": 0.0018950924273310197,
                            "0": 0.0020571651591883926,
                            "1": 0.001871873791013006,
                            "2": 0.00211040424585803
                        },
                        "total_energy_joules": {
                            "3": 6822.33273839167,
                            "0": 7405.794573078213,
                            "1": 6738.745647646821,
                            "2": 7597.455285088908
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 638.5568461906335,
                        "ram_power_avg": 3.86937940120697,
                        "cpu_energy_total": 0.0011568855044606608,
                        "gpu_energy_total": 0.006746842064151437,
                        "ram_energy_total": 3.080805477835112e-05,
                        "total_energy_kwh": 0.007934535623390448,
                        "total_energy_joules": 28564.328244205615
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.07001740012582679,
                        "joules_per_token": 14.282164122102808,
                        "flops_per_joule": 181090225.3950014,
                        "joules_per_flop": 5.52210920174603e-09
                    },
                    "per-process_emissions": [
                        0.0007219354601917519,
                        0.0007836770673928182,
                        0.0007130903206864046,
                        0.0008039584974596165
                    ]
                }
            }
        }
    }
]