[
    {
        "CONFIGURATION_RUN_#0109": {
            "setup": {
                "experiment_id": "0109",
                "date_time": "April 11, 2025 at 01:21:09 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_1",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.22631846799959,
                        "average_latency_ms_per_batch": 2903.289808499949,
                        "throughput_queries_per_sec": 5.510989620518376,
                        "throughput_tokens_per_sec": 705.4066714263521
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 12016680960,
                        "gpu_max_memory_reserved_bytes": 12016680960
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            2.0,
                            67.0,
                            100.0,
                            89.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 2013147136
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0109",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 696.1754452422587
                        },
                        "ram_power": {
                            "process_0": 0.7018489837646484
                        },
                        "cpu_energy": {
                            "process_0": 0.0007161488071873803
                        },
                        "gpu_energy": {
                            "process_0": 0.0037788024674839382
                        },
                        "ram_energy": {
                            "process_0": 3.876286308789185e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.004498827560980107
                        },
                        "total_energy_joules": {
                            "process_0": 16195.779219528387
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 696.1754452422587,
                        "ram_power_avg": 0.7018489837646484,
                        "cpu_energy_total": 0.0007161488071873803,
                        "gpu_energy_total": 0.0037788024674839382,
                        "ram_energy_total": 3.876286308789185e-06,
                        "total_energy_kwh": 0.004498827560980107,
                        "total_energy_joules": 16195.779219528387
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.0116215946093325,
                        "joules_per_token": 0.9885119152544183,
                        "flops_per_joule": 1046567180.4610815,
                        "joules_per_flop": 9.55504833965302e-10
                    },
                    "per-process_emissions": [
                        0.001713828359355372
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0109": {
            "setup": {
                "experiment_id": "0109",
                "date_time": "April 11, 2025 at 01:21:09 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_1",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.22631846799959,
                        "average_latency_ms_per_batch": 2903.289808499949,
                        "throughput_queries_per_sec": 5.510989620518376,
                        "throughput_tokens_per_sec": 705.4066714263521
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 12016680960,
                        "gpu_max_memory_reserved_bytes": 12016680960
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            2.0,
                            67.0,
                            100.0,
                            89.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 2013147136
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0109",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 696.1754452422587
                        },
                        "ram_power": {
                            "process_0": 0.7018489837646484
                        },
                        "cpu_energy": {
                            "process_0": 0.0007161488071873803
                        },
                        "gpu_energy": {
                            "process_0": 0.0037788024674839382
                        },
                        "ram_energy": {
                            "process_0": 3.876286308789185e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.004498827560980107
                        },
                        "total_energy_joules": {
                            "process_0": 16195.779219528387
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 696.1754452422587,
                        "ram_power_avg": 0.7018489837646484,
                        "cpu_energy_total": 0.0007161488071873803,
                        "gpu_energy_total": 0.0037788024674839382,
                        "ram_energy_total": 3.876286308789185e-06,
                        "total_energy_kwh": 0.004498827560980107,
                        "total_energy_joules": 16195.779219528387
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.0116215946093325,
                        "joules_per_token": 0.9885119152544183,
                        "flops_per_joule": 1046567180.4610815,
                        "joules_per_flop": 9.55504833965302e-10
                    },
                    "per-process_emissions": [
                        0.001713828359355372
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0110": {
            "setup": {
                "experiment_id": "0110",
                "date_time": "April 11, 2025 at 01:22:13 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.26491598999928,
                        "average_latency_ms_per_batch": 2908.11449874991,
                        "throughput_queries_per_sec": 5.501846645611032,
                        "throughput_tokens_per_sec": 704.2363706382121
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.4,
                        "cpu_memory_usage_bytes": 2011279360
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0110",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 473.28516894085715,
                            "process_1": 852.2836530668404
                        },
                        "ram_power": {
                            "process_0": 0.7012195587158203,
                            "process_1": 0.6558823585510255
                        },
                        "cpu_energy": {
                            "process_0": 0.0007164897692188676,
                            "process_1": 0.00108366329471869
                        },
                        "gpu_energy": {
                            "process_0": 0.0041876055723040345,
                            "process_1": 0.005985419510553966
                        },
                        "ram_energy": {
                            "process_0": 3.8376475285984905e-06,
                            "process_1": 5.622894387410731e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.004907932989051498,
                            "process_1": 0.007074705699660064
                        },
                        "total_energy_joules": {
                            "process_0": 17668.558760585394,
                            "process_1": 25468.94051877623
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 662.7844110038488,
                        "ram_power_avg": 0.6785509586334229,
                        "cpu_energy_total": 0.0018001530639375576,
                        "gpu_energy_total": 0.010173025082858,
                        "ram_energy_total": 9.460541916009223e-06,
                        "total_energy_kwh": 0.011982638688711562,
                        "total_energy_joules": 43137.499279361626
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.3798087574315796,
                        "joules_per_token": 2.6329040087500992,
                        "flops_per_joule": 392928919.7638171,
                        "joules_per_flop": 2.5449895635095607e-09
                    },
                    "per-process_emissions": [
                        0.0018696770721791682,
                        0.0026951091362855013
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0110": {
            "setup": {
                "experiment_id": "0110",
                "date_time": "April 11, 2025 at 01:22:13 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.26491598999928,
                        "average_latency_ms_per_batch": 2908.11449874991,
                        "throughput_queries_per_sec": 5.501846645611032,
                        "throughput_tokens_per_sec": 704.2363706382121
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.4,
                        "cpu_memory_usage_bytes": 2011279360
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0110",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 473.28516894085715,
                            "process_1": 852.2836530668404
                        },
                        "ram_power": {
                            "process_0": 0.7012195587158203,
                            "process_1": 0.6558823585510255
                        },
                        "cpu_energy": {
                            "process_0": 0.0007164897692188676,
                            "process_1": 0.00108366329471869
                        },
                        "gpu_energy": {
                            "process_0": 0.0041876055723040345,
                            "process_1": 0.005985419510553966
                        },
                        "ram_energy": {
                            "process_0": 3.8376475285984905e-06,
                            "process_1": 5.622894387410731e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.004907932989051498,
                            "process_1": 0.007074705699660064
                        },
                        "total_energy_joules": {
                            "process_0": 17668.558760585394,
                            "process_1": 25468.94051877623
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 662.7844110038488,
                        "ram_power_avg": 0.6785509586334229,
                        "cpu_energy_total": 0.0018001530639375576,
                        "gpu_energy_total": 0.010173025082858,
                        "ram_energy_total": 9.460541916009223e-06,
                        "total_energy_kwh": 0.011982638688711562,
                        "total_energy_joules": 43137.499279361626
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.3798087574315796,
                        "joules_per_token": 2.6329040087500992,
                        "flops_per_joule": 392928919.7638171,
                        "joules_per_flop": 2.5449895635095607e-09
                    },
                    "per-process_emissions": [
                        0.0018696770721791682,
                        0.0026951091362855013
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0111": {
            "setup": {
                "experiment_id": "0111",
                "date_time": "April 11, 2025 at 01:23:18 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_3",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 3
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.683534305002468,
                        "average_latency_ms_per_batch": 2960.4417881253084,
                        "throughput_queries_per_sec": 5.4045987542055185,
                        "throughput_tokens_per_sec": 691.7886405383064
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.7,
                        "cpu_memory_usage_bytes": 2011983872
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0111",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 0.0,
                            "process_0": 748.4481735873105,
                            "process_2": 934.2520329595256
                        },
                        "ram_power": {
                            "process_1": 0.6505751609802246,
                            "process_0": 0.7011780738830566,
                            "process_2": 0.6519927978515625
                        },
                        "cpu_energy": {
                            "process_1": 0.0011424874566250765,
                            "process_0": 0.0007305820803435948,
                            "process_2": 0.001020133911593802
                        },
                        "gpu_energy": {
                            "process_1": 0.006776018754143978,
                            "process_0": 0.004743234627918022,
                            "process_2": 0.006342773685326042
                        },
                        "ram_energy": {
                            "process_1": 5.842497486456087e-06,
                            "process_0": 3.902804201702759e-06,
                            "process_2": 5.2022448338566775e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.007924348708255511,
                            "process_0": 0.0054777195124633225,
                            "process_2": 0.0073681098417537
                        },
                        "total_energy_joules": {
                            "process_1": 28527.65534971984,
                            "process_0": 19719.79024486796,
                            "process_2": 26525.19543031332
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 560.9000688489454,
                        "ram_power_avg": 0.6679153442382812,
                        "cpu_energy_total": 0.002893203448562473,
                        "gpu_energy_total": 0.017862027067388042,
                        "ram_energy_total": 1.4947546522015524e-05,
                        "total_energy_kwh": 0.020770178062472534,
                        "total_energy_joules": 74772.64102490112
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21911757797272036,
                        "joules_per_token": 4.56375982818,
                        "flops_per_joule": 226686803.63325998,
                        "joules_per_flop": 4.4113728014702915e-09
                    },
                    "per-process_emissions": [
                        0.003018780640409937,
                        0.0020867372482729026,
                        0.002806881444216072
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0111": {
            "setup": {
                "experiment_id": "0111",
                "date_time": "April 11, 2025 at 01:23:18 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_3",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 3
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.683534305002468,
                        "average_latency_ms_per_batch": 2960.4417881253084,
                        "throughput_queries_per_sec": 5.4045987542055185,
                        "throughput_tokens_per_sec": 691.7886405383064
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.7,
                        "cpu_memory_usage_bytes": 2011983872
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0111",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 0.0,
                            "process_0": 748.4481735873105,
                            "process_2": 934.2520329595256
                        },
                        "ram_power": {
                            "process_1": 0.6505751609802246,
                            "process_0": 0.7011780738830566,
                            "process_2": 0.6519927978515625
                        },
                        "cpu_energy": {
                            "process_1": 0.0011424874566250765,
                            "process_0": 0.0007305820803435948,
                            "process_2": 0.001020133911593802
                        },
                        "gpu_energy": {
                            "process_1": 0.006776018754143978,
                            "process_0": 0.004743234627918022,
                            "process_2": 0.006342773685326042
                        },
                        "ram_energy": {
                            "process_1": 5.842497486456087e-06,
                            "process_0": 3.902804201702759e-06,
                            "process_2": 5.2022448338566775e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.007924348708255511,
                            "process_0": 0.0054777195124633225,
                            "process_2": 0.0073681098417537
                        },
                        "total_energy_joules": {
                            "process_1": 28527.65534971984,
                            "process_0": 19719.79024486796,
                            "process_2": 26525.19543031332
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 560.9000688489454,
                        "ram_power_avg": 0.6679153442382812,
                        "cpu_energy_total": 0.002893203448562473,
                        "gpu_energy_total": 0.017862027067388042,
                        "ram_energy_total": 1.4947546522015524e-05,
                        "total_energy_kwh": 0.020770178062472534,
                        "total_energy_joules": 74772.64102490112
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21911757797272036,
                        "joules_per_token": 4.56375982818,
                        "flops_per_joule": 226686803.63325998,
                        "joules_per_flop": 4.4113728014702915e-09
                    },
                    "per-process_emissions": [
                        0.003018780640409937,
                        0.0020867372482729026,
                        0.002806881444216072
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0112": {
            "setup": {
                "experiment_id": "0112",
                "date_time": "April 11, 2025 at 01:24:25 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.531788863996553,
                        "average_latency_ms_per_batch": 2941.473607999569,
                        "throughput_queries_per_sec": 5.439450470161194,
                        "throughput_tokens_per_sec": 696.2496601806329
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.7,
                        "cpu_memory_usage_bytes": 2012934144
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0112",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 840.551155741769,
                            "process_2": 590.5232843177822,
                            "process_1": 770.7608801305829,
                            "process_3": 702.5229945021922
                        },
                        "ram_power": {
                            "process_0": 0.7011022567749023,
                            "process_2": 0.6526765823364258,
                            "process_1": 0.6519670486450195,
                            "process_3": 0.6561455726623535
                        },
                        "cpu_energy": {
                            "process_0": 0.0007281144658749667,
                            "process_2": 0.0010278602135622352,
                            "process_1": 0.0011160055792499293,
                            "process_3": 0.0009472713789687646
                        },
                        "gpu_energy": {
                            "process_0": 0.0052870256185059555,
                            "process_2": 0.00714252904735195,
                            "process_1": 0.0075448385358660025,
                            "process_3": 0.006706771198745992
                        },
                        "ram_energy": {
                            "process_0": 3.8574063639027855e-06,
                            "process_2": 5.230916206363691e-06,
                            "process_1": 5.69429140474461e-06,
                            "process_3": 4.854766540663868e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.006018997490744823,
                            "process_2": 0.00817562017712055,
                            "process_1": 0.008666538406520674,
                            "process_3": 0.007658897344255418
                        },
                        "total_energy_joules": {
                            "process_0": 21668.390966681363,
                            "process_2": 29432.23263763398,
                            "process_1": 31199.538263474427,
                            "process_3": 27572.030439319504
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 726.0895786730816,
                        "ram_power_avg": 0.6654728651046753,
                        "cpu_energy_total": 0.003819251637655896,
                        "gpu_energy_total": 0.0266811644004699,
                        "ram_energy_total": 1.9637380515674954e-05,
                        "total_energy_kwh": 0.030520053418641464,
                        "total_energy_joules": 109872.19230710928
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.14911871380707742,
                        "joules_per_token": 6.706066424994462,
                        "flops_per_joule": 154269889.74402446,
                        "joules_per_flop": 6.482146332374197e-09
                    },
                    "per-process_emissions": [
                        0.0022929370940992407,
                        0.003114502506474074,
                        0.003301517805964051,
                        0.0029176569432941015
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0112": {
            "setup": {
                "experiment_id": "0112",
                "date_time": "April 11, 2025 at 01:24:25 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.531788863996553,
                        "average_latency_ms_per_batch": 2941.473607999569,
                        "throughput_queries_per_sec": 5.439450470161194,
                        "throughput_tokens_per_sec": 696.2496601806329
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.7,
                        "cpu_memory_usage_bytes": 2012934144
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0112",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 840.551155741769,
                            "process_2": 590.5232843177822,
                            "process_1": 770.7608801305829,
                            "process_3": 702.5229945021922
                        },
                        "ram_power": {
                            "process_0": 0.7011022567749023,
                            "process_2": 0.6526765823364258,
                            "process_1": 0.6519670486450195,
                            "process_3": 0.6561455726623535
                        },
                        "cpu_energy": {
                            "process_0": 0.0007281144658749667,
                            "process_2": 0.0010278602135622352,
                            "process_1": 0.0011160055792499293,
                            "process_3": 0.0009472713789687646
                        },
                        "gpu_energy": {
                            "process_0": 0.0052870256185059555,
                            "process_2": 0.00714252904735195,
                            "process_1": 0.0075448385358660025,
                            "process_3": 0.006706771198745992
                        },
                        "ram_energy": {
                            "process_0": 3.8574063639027855e-06,
                            "process_2": 5.230916206363691e-06,
                            "process_1": 5.69429140474461e-06,
                            "process_3": 4.854766540663868e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.006018997490744823,
                            "process_2": 0.00817562017712055,
                            "process_1": 0.008666538406520674,
                            "process_3": 0.007658897344255418
                        },
                        "total_energy_joules": {
                            "process_0": 21668.390966681363,
                            "process_2": 29432.23263763398,
                            "process_1": 31199.538263474427,
                            "process_3": 27572.030439319504
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 726.0895786730816,
                        "ram_power_avg": 0.6654728651046753,
                        "cpu_energy_total": 0.003819251637655896,
                        "gpu_energy_total": 0.0266811644004699,
                        "ram_energy_total": 1.9637380515674954e-05,
                        "total_energy_kwh": 0.030520053418641464,
                        "total_energy_joules": 109872.19230710928
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.14911871380707742,
                        "joules_per_token": 6.706066424994462,
                        "flops_per_joule": 154269889.74402446,
                        "joules_per_flop": 6.482146332374197e-09
                    },
                    "per-process_emissions": [
                        0.0022929370940992407,
                        0.003114502506474074,
                        0.003301517805964051,
                        0.0029176569432941015
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0113": {
            "setup": {
                "experiment_id": "0113",
                "date_time": "April 11, 2025 at 01:30:52 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_1",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 14722
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 300.51725118999457,
                        "average_latency_ms_per_batch": 2347.7910249218326,
                        "throughput_queries_per_sec": 0.4259322867261127,
                        "throughput_tokens_per_sec": 48.988868165483055
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818202112,
                        "gpu_max_memory_allocated_bytes": 8818202112,
                        "gpu_current_memory_reserved_bytes": 13214154752,
                        "gpu_max_memory_reserved_bytes": 13214154752
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.8,
                        "cpu_memory_usage_bytes": 1982603264
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0113",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 587.8680111887087,
                            "process_0": 627.6912006847322,
                            "process_3": 464.52870404416996,
                            "process_2": 530.4466134840911
                        },
                        "ram_power": {
                            "process_1": 0.6450848579406739,
                            "process_0": 0.6908526420593262,
                            "process_3": 0.6436543464660646,
                            "process_2": 0.6444683074951172
                        },
                        "cpu_energy": {
                            "process_1": 0.010927628552624859,
                            "process_0": 0.009208174153905695,
                            "process_3": 0.01023415136928138,
                            "process_2": 0.01055103023615635
                        },
                        "gpu_energy": {
                            "process_1": 0.05506055932619007,
                            "process_0": 0.04709415795307402,
                            "process_3": 0.051982999641922004,
                            "process_2": 0.053491719182230044
                        },
                        "ram_energy": {
                            "process_1": 5.436799569817735e-05,
                            "process_0": 4.826776472680989e-05,
                            "process_3": 5.046311185903239e-05,
                            "process_2": 5.244484981317963e-05
                        },
                        "total_energy_kwh": {
                            "process_1": 0.06604255587451312,
                            "process_0": 0.056350599871706514,
                            "process_3": 0.06226761412306248,
                            "process_2": 0.06409519426819964
                        },
                        "total_energy_joules": {
                            "process_1": 237753.20114824723,
                            "process_0": 202862.15953814346,
                            "process_3": 224163.41084302493,
                            "process_2": 230742.6993655187
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 552.6336323504254,
                        "ram_power_avg": 0.6560150384902954,
                        "cpu_energy_total": 0.040920984311968285,
                        "gpu_energy_total": 0.20762943610341617,
                        "ram_energy_total": 0.00020554372209719926,
                        "total_energy_kwh": 0.24875596413748174,
                        "total_energy_joules": 895521.4708949344
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0164395835035509,
                        "joules_per_token": 60.82879166519049,
                        "flops_per_joule": 18927486.993933424,
                        "joules_per_flop": 5.283321554100218e-08
                    },
                    "per-process_emissions": [
                        0.025158911660395772,
                        0.021466761021126598,
                        0.023720847600180655,
                        0.024417064256470652
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0113": {
            "setup": {
                "experiment_id": "0113",
                "date_time": "April 11, 2025 at 01:30:52 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_1",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 14722
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 300.51725118999457,
                        "average_latency_ms_per_batch": 2347.7910249218326,
                        "throughput_queries_per_sec": 0.4259322867261127,
                        "throughput_tokens_per_sec": 48.988868165483055
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818202112,
                        "gpu_max_memory_allocated_bytes": 8818202112,
                        "gpu_current_memory_reserved_bytes": 13214154752,
                        "gpu_max_memory_reserved_bytes": 13214154752
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.8,
                        "cpu_memory_usage_bytes": 1982603264
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0113",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 587.8680111887087,
                            "process_0": 627.6912006847322,
                            "process_3": 464.52870404416996,
                            "process_2": 530.4466134840911
                        },
                        "ram_power": {
                            "process_1": 0.6450848579406739,
                            "process_0": 0.6908526420593262,
                            "process_3": 0.6436543464660646,
                            "process_2": 0.6444683074951172
                        },
                        "cpu_energy": {
                            "process_1": 0.010927628552624859,
                            "process_0": 0.009208174153905695,
                            "process_3": 0.01023415136928138,
                            "process_2": 0.01055103023615635
                        },
                        "gpu_energy": {
                            "process_1": 0.05506055932619007,
                            "process_0": 0.04709415795307402,
                            "process_3": 0.051982999641922004,
                            "process_2": 0.053491719182230044
                        },
                        "ram_energy": {
                            "process_1": 5.436799569817735e-05,
                            "process_0": 4.826776472680989e-05,
                            "process_3": 5.046311185903239e-05,
                            "process_2": 5.244484981317963e-05
                        },
                        "total_energy_kwh": {
                            "process_1": 0.06604255587451312,
                            "process_0": 0.056350599871706514,
                            "process_3": 0.06226761412306248,
                            "process_2": 0.06409519426819964
                        },
                        "total_energy_joules": {
                            "process_1": 237753.20114824723,
                            "process_0": 202862.15953814346,
                            "process_3": 224163.41084302493,
                            "process_2": 230742.6993655187
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 552.6336323504254,
                        "ram_power_avg": 0.6560150384902954,
                        "cpu_energy_total": 0.040920984311968285,
                        "gpu_energy_total": 0.20762943610341617,
                        "ram_energy_total": 0.00020554372209719926,
                        "total_energy_kwh": 0.24875596413748174,
                        "total_energy_joules": 895521.4708949344
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0164395835035509,
                        "joules_per_token": 60.82879166519049,
                        "flops_per_joule": 18927486.993933424,
                        "joules_per_flop": 5.283321554100218e-08
                    },
                    "per-process_emissions": [
                        0.025158911660395772,
                        0.021466761021126598,
                        0.023720847600180655,
                        0.024417064256470652
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0114": {
            "setup": {
                "experiment_id": "0114",
                "date_time": "April 11, 2025 at 01:35:11 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 2,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 170.67274896000342,
                        "average_latency_ms_per_batch": 2666.7617025000536,
                        "throughput_queries_per_sec": 0.7499732721243997,
                        "throughput_tokens_per_sec": 95.99657883192316
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13220446208,
                        "gpu_max_memory_reserved_bytes": 13220446208
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.4,
                        "cpu_memory_usage_bytes": 2007461888
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0114",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 700.4589852230454,
                            "process_2": 494.3051053042842,
                            "process_0": 607.8513475677056,
                            "process_1": 443.86245866289323
                        },
                        "ram_power": {
                            "process_3": 0.6484837532043457,
                            "process_2": 0.6482806205749513,
                            "process_0": 0.6996002197265626,
                            "process_1": 0.6500101089477539
                        },
                        "cpu_energy": {
                            "process_3": 0.0063199332881873176,
                            "process_2": 0.006642295250562766,
                            "process_0": 0.0052414073857807485,
                            "process_1": 0.007029716246906329
                        },
                        "gpu_energy": {
                            "process_3": 0.037197613646956035,
                            "process_2": 0.03869503928933987,
                            "process_0": 0.03125217027949406,
                            "process_1": 0.04047684460367201
                        },
                        "ram_energy": {
                            "process_3": 3.15745551523686e-05,
                            "process_2": 3.3436677521666486e-05,
                            "process_0": 2.7749520005385277e-05,
                            "process_1": 3.532686396046065e-05
                        },
                        "total_energy_kwh": {
                            "process_3": 0.04354912149029574,
                            "process_2": 0.04537077121742426,
                            "process_0": 0.036521327185280145,
                            "process_1": 0.047541887714538836
                        },
                        "total_energy_joules": {
                            "process_3": 156776.83736506465,
                            "process_2": 163334.77638272734,
                            "process_0": 131476.77786700852,
                            "process_1": 171150.7957723398
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 561.6194741894822,
                        "ram_power_avg": 0.6615936756134033,
                        "cpu_energy_total": 0.025233352171437164,
                        "gpu_energy_total": 0.14762166781946198,
                        "ram_energy_total": 0.000128087616639881,
                        "total_energy_kwh": 0.172983107607539,
                        "total_energy_joules": 622739.1873871402
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.02630956961090439,
                        "joules_per_token": 38.00898360517213,
                        "flops_per_joule": 27218410.75116838,
                        "joules_per_flop": 3.6739837940651027e-08
                    },
                    "per-process_emissions": [
                        0.01659003783172816,
                        0.017283995295277774,
                        0.013912799591232471,
                        0.01811108212485357
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0114": {
            "setup": {
                "experiment_id": "0114",
                "date_time": "April 11, 2025 at 01:35:11 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 2,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 170.67274896000342,
                        "average_latency_ms_per_batch": 2666.7617025000536,
                        "throughput_queries_per_sec": 0.7499732721243997,
                        "throughput_tokens_per_sec": 95.99657883192316
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13220446208,
                        "gpu_max_memory_reserved_bytes": 13220446208
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.4,
                        "cpu_memory_usage_bytes": 2007461888
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0114",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 700.4589852230454,
                            "process_2": 494.3051053042842,
                            "process_0": 607.8513475677056,
                            "process_1": 443.86245866289323
                        },
                        "ram_power": {
                            "process_3": 0.6484837532043457,
                            "process_2": 0.6482806205749513,
                            "process_0": 0.6996002197265626,
                            "process_1": 0.6500101089477539
                        },
                        "cpu_energy": {
                            "process_3": 0.0063199332881873176,
                            "process_2": 0.006642295250562766,
                            "process_0": 0.0052414073857807485,
                            "process_1": 0.007029716246906329
                        },
                        "gpu_energy": {
                            "process_3": 0.037197613646956035,
                            "process_2": 0.03869503928933987,
                            "process_0": 0.03125217027949406,
                            "process_1": 0.04047684460367201
                        },
                        "ram_energy": {
                            "process_3": 3.15745551523686e-05,
                            "process_2": 3.3436677521666486e-05,
                            "process_0": 2.7749520005385277e-05,
                            "process_1": 3.532686396046065e-05
                        },
                        "total_energy_kwh": {
                            "process_3": 0.04354912149029574,
                            "process_2": 0.04537077121742426,
                            "process_0": 0.036521327185280145,
                            "process_1": 0.047541887714538836
                        },
                        "total_energy_joules": {
                            "process_3": 156776.83736506465,
                            "process_2": 163334.77638272734,
                            "process_0": 131476.77786700852,
                            "process_1": 171150.7957723398
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 561.6194741894822,
                        "ram_power_avg": 0.6615936756134033,
                        "cpu_energy_total": 0.025233352171437164,
                        "gpu_energy_total": 0.14762166781946198,
                        "ram_energy_total": 0.000128087616639881,
                        "total_energy_kwh": 0.172983107607539,
                        "total_energy_joules": 622739.1873871402
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.02630956961090439,
                        "joules_per_token": 38.00898360517213,
                        "flops_per_joule": 27218410.75116838,
                        "joules_per_flop": 3.6739837940651027e-08
                    },
                    "per-process_emissions": [
                        0.01659003783172816,
                        0.017283995295277774,
                        0.013912799591232471,
                        0.01811108212485357
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0115": {
            "setup": {
                "experiment_id": "0115",
                "date_time": "April 11, 2025 at 01:37:39 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 89.14964829099335,
                        "average_latency_ms_per_batch": 2785.9265090935423,
                        "throughput_queries_per_sec": 1.4357880536129006,
                        "throughput_tokens_per_sec": 183.78087086245128
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13230931968,
                        "gpu_max_memory_reserved_bytes": 13230931968
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.2,
                        "cpu_memory_usage_bytes": 2008850432
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0115",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 652.8404590271028,
                            "process_0": 1115.4418224010076,
                            "process_1": 472.7769991652675,
                            "process_3": 495.6929873129303
                        },
                        "ram_power": {
                            "process_2": 0.6517624855041504,
                            "process_0": 0.6999406814575195,
                            "process_1": 0.6512947082519531,
                            "process_3": 0.6527552604675294
                        },
                        "cpu_energy": {
                            "process_2": 0.0027159240406563197,
                            "process_0": 0.002727139851906204,
                            "process_1": 0.0035828151012186693,
                            "process_3": 0.003210426923343733
                        },
                        "gpu_energy": {
                            "process_2": 0.015501225178747996,
                            "process_0": 0.015567889120967987,
                            "process_1": 0.019340131305426,
                            "process_3": 0.01784459899789001
                        },
                        "ram_energy": {
                            "process_2": 1.3405201116093497e-05,
                            "process_0": 1.439778458209099e-05,
                            "process_1": 1.807212589344287e-05,
                            "process_3": 1.608493731436124e-05
                        },
                        "total_energy_kwh": {
                            "process_2": 0.018230554420520413,
                            "process_0": 0.018309426757456265,
                            "process_1": 0.022941018532538122,
                            "process_3": 0.0210711108585481
                        },
                        "total_energy_joules": {
                            "process_2": 65629.99591387348,
                            "process_0": 65913.93632684255,
                            "process_1": 82587.66671713724,
                            "process_3": 75855.99909077317
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 684.188066976577,
                        "ram_power_avg": 0.6639382839202881,
                        "cpu_energy_total": 0.012236305917124927,
                        "gpu_energy_total": 0.068253844603032,
                        "ram_energy_total": 6.196004890598859e-05,
                        "total_energy_kwh": 0.08055211056906289,
                        "total_energy_joules": 289987.59804862645
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.056498967922251134,
                        "joules_per_token": 17.69943835746011,
                        "flops_per_joule": 58450675.50202527,
                        "joules_per_flop": 1.7108442142218713e-08
                    },
                    "per-process_emissions": [
                        0.006944929706497251,
                        0.006974976123252964,
                        0.008739381009970397,
                        0.008027039681563899
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0115": {
            "setup": {
                "experiment_id": "0115",
                "date_time": "April 11, 2025 at 01:37:39 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 89.14964829099335,
                        "average_latency_ms_per_batch": 2785.9265090935423,
                        "throughput_queries_per_sec": 1.4357880536129006,
                        "throughput_tokens_per_sec": 183.78087086245128
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13230931968,
                        "gpu_max_memory_reserved_bytes": 13230931968
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.2,
                        "cpu_memory_usage_bytes": 2008850432
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0115",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 652.8404590271028,
                            "process_0": 1115.4418224010076,
                            "process_1": 472.7769991652675,
                            "process_3": 495.6929873129303
                        },
                        "ram_power": {
                            "process_2": 0.6517624855041504,
                            "process_0": 0.6999406814575195,
                            "process_1": 0.6512947082519531,
                            "process_3": 0.6527552604675294
                        },
                        "cpu_energy": {
                            "process_2": 0.0027159240406563197,
                            "process_0": 0.002727139851906204,
                            "process_1": 0.0035828151012186693,
                            "process_3": 0.003210426923343733
                        },
                        "gpu_energy": {
                            "process_2": 0.015501225178747996,
                            "process_0": 0.015567889120967987,
                            "process_1": 0.019340131305426,
                            "process_3": 0.01784459899789001
                        },
                        "ram_energy": {
                            "process_2": 1.3405201116093497e-05,
                            "process_0": 1.439778458209099e-05,
                            "process_1": 1.807212589344287e-05,
                            "process_3": 1.608493731436124e-05
                        },
                        "total_energy_kwh": {
                            "process_2": 0.018230554420520413,
                            "process_0": 0.018309426757456265,
                            "process_1": 0.022941018532538122,
                            "process_3": 0.0210711108585481
                        },
                        "total_energy_joules": {
                            "process_2": 65629.99591387348,
                            "process_0": 65913.93632684255,
                            "process_1": 82587.66671713724,
                            "process_3": 75855.99909077317
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 684.188066976577,
                        "ram_power_avg": 0.6639382839202881,
                        "cpu_energy_total": 0.012236305917124927,
                        "gpu_energy_total": 0.068253844603032,
                        "ram_energy_total": 6.196004890598859e-05,
                        "total_energy_kwh": 0.08055211056906289,
                        "total_energy_joules": 289987.59804862645
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.056498967922251134,
                        "joules_per_token": 17.69943835746011,
                        "flops_per_joule": 58450675.50202527,
                        "joules_per_flop": 1.7108442142218713e-08
                    },
                    "per-process_emissions": [
                        0.006944929706497251,
                        0.006974976123252964,
                        0.008739381009970397,
                        0.008027039681563899
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0116": {
            "setup": {
                "experiment_id": "0116",
                "date_time": "April 11, 2025 at 01:39:15 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 44.280597053002566,
                        "average_latency_ms_per_batch": 2767.5373158126604,
                        "throughput_queries_per_sec": 2.8906565972176885,
                        "throughput_tokens_per_sec": 370.00404444386413
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.5,
                        "cpu_memory_usage_bytes": 2010365952
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0116",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 646.758340510093,
                            "process_1": 503.53311143181054,
                            "process_2": 630.9017161978595,
                            "process_0": 737.2303203200806
                        },
                        "ram_power": {
                            "process_3": 0.6565589904785156,
                            "process_1": 0.6512203216552734,
                            "process_2": 0.653660774230957,
                            "process_0": 0.7008790969848634
                        },
                        "cpu_energy": {
                            "process_3": 0.0017121941408749992,
                            "process_1": 0.0019374923980314523,
                            "process_2": 0.0014941112477498567,
                            "process_0": 0.0013640428659375064
                        },
                        "gpu_energy": {
                            "process_3": 0.010616217381856058,
                            "process_1": 0.011640930701625973,
                            "process_2": 0.009445673945422062,
                            "process_0": 0.008683476113442001
                        },
                        "ram_energy": {
                            "process_3": 8.708636035448292e-06,
                            "process_1": 9.799293676007533e-06,
                            "process_2": 7.451254564029873e-06,
                            "process_0": 7.293617695400067e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.012337120158766503,
                            "process_1": 0.013588222393333429,
                            "process_2": 0.010947236447735952,
                            "process_0": 0.010054812597074907
                        },
                        "total_energy_joules": {
                            "process_3": 44413.632571559414,
                            "process_1": 48917.600616000345,
                            "process_2": 39410.05121184942,
                            "process_0": 36197.32534946966
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 629.6058721149609,
                        "ram_power_avg": 0.6655797958374023,
                        "cpu_energy_total": 0.006507840652593815,
                        "gpu_energy_total": 0.040386298142346094,
                        "ram_energy_total": 3.325280197088576e-05,
                        "total_energy_kwh": 0.04692739159691079,
                        "total_energy_joules": 168938.60974887887
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09698197483899165,
                        "joules_per_token": 10.311194442680595,
                        "flops_per_joule": 100332132.59152256,
                        "joules_per_flop": 9.966896687736645e-09
                    },
                    "per-process_emissions": [
                        0.0046998259244821,
                        0.00517643332074037,
                        0.0041703497247650104,
                        0.0038303808588556858
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0116": {
            "setup": {
                "experiment_id": "0116",
                "date_time": "April 11, 2025 at 01:39:15 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 44.280597053002566,
                        "average_latency_ms_per_batch": 2767.5373158126604,
                        "throughput_queries_per_sec": 2.8906565972176885,
                        "throughput_tokens_per_sec": 370.00404444386413
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.5,
                        "cpu_memory_usage_bytes": 2010365952
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0116",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 646.758340510093,
                            "process_1": 503.53311143181054,
                            "process_2": 630.9017161978595,
                            "process_0": 737.2303203200806
                        },
                        "ram_power": {
                            "process_3": 0.6565589904785156,
                            "process_1": 0.6512203216552734,
                            "process_2": 0.653660774230957,
                            "process_0": 0.7008790969848634
                        },
                        "cpu_energy": {
                            "process_3": 0.0017121941408749992,
                            "process_1": 0.0019374923980314523,
                            "process_2": 0.0014941112477498567,
                            "process_0": 0.0013640428659375064
                        },
                        "gpu_energy": {
                            "process_3": 0.010616217381856058,
                            "process_1": 0.011640930701625973,
                            "process_2": 0.009445673945422062,
                            "process_0": 0.008683476113442001
                        },
                        "ram_energy": {
                            "process_3": 8.708636035448292e-06,
                            "process_1": 9.799293676007533e-06,
                            "process_2": 7.451254564029873e-06,
                            "process_0": 7.293617695400067e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.012337120158766503,
                            "process_1": 0.013588222393333429,
                            "process_2": 0.010947236447735952,
                            "process_0": 0.010054812597074907
                        },
                        "total_energy_joules": {
                            "process_3": 44413.632571559414,
                            "process_1": 48917.600616000345,
                            "process_2": 39410.05121184942,
                            "process_0": 36197.32534946966
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 629.6058721149609,
                        "ram_power_avg": 0.6655797958374023,
                        "cpu_energy_total": 0.006507840652593815,
                        "gpu_energy_total": 0.040386298142346094,
                        "ram_energy_total": 3.325280197088576e-05,
                        "total_energy_kwh": 0.04692739159691079,
                        "total_energy_joules": 168938.60974887887
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.09698197483899165,
                        "joules_per_token": 10.311194442680595,
                        "flops_per_joule": 100332132.59152256,
                        "joules_per_flop": 9.966896687736645e-09
                    },
                    "per-process_emissions": [
                        0.0046998259244821,
                        0.00517643332074037,
                        0.0041703497247650104,
                        0.0038303808588556858
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0117": {
            "setup": {
                "experiment_id": "0117",
                "date_time": "April 11, 2025 at 01:40:22 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_16",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.435157784000694,
                        "average_latency_ms_per_batch": 2929.3947230000867,
                        "throughput_queries_per_sec": 5.461879163765916,
                        "throughput_tokens_per_sec": 699.1205329620373
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.7,
                        "cpu_memory_usage_bytes": 2011435008
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0117",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 860.5753758328476,
                            "process_3": 682.7791358029963,
                            "process_2": 606.9006549992879,
                            "process_1": 451.59433799810654
                        },
                        "ram_power": {
                            "process_0": 0.7005772590637207,
                            "process_3": 0.6537795066833496,
                            "process_2": 0.6519126892089844,
                            "process_1": 0.6602439880371094
                        },
                        "cpu_energy": {
                            "process_0": 0.0007255553946249053,
                            "process_3": 0.0009417213138750073,
                            "process_2": 0.0010453227453125465,
                            "process_1": 0.0011067888925934994
                        },
                        "gpu_energy": {
                            "process_0": 0.005243008638848107,
                            "process_3": 0.0066234427987500455,
                            "process_2": 0.007182074078987993,
                            "process_1": 0.00744444428888405
                        },
                        "ram_energy": {
                            "process_0": 3.8693149639603445e-06,
                            "process_3": 4.804021411821816e-06,
                            "process_2": 5.343008105571681e-06,
                            "process_1": 5.735111877239817e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005972433348436974,
                            "process_3": 0.0075699681340368735,
                            "process_2": 0.008232739832406112,
                            "process_1": 0.008556968293354792
                        },
                        "total_energy_joules": {
                            "process_0": 21500.760054373106,
                            "process_3": 27251.885282532745,
                            "process_2": 29637.863396662004,
                            "process_1": 30805.08585607725
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 650.4623761583096,
                        "ram_power_avg": 0.666628360748291,
                        "cpu_energy_total": 0.0038193883464059586,
                        "gpu_energy_total": 0.026492969805470196,
                        "ram_energy_total": 1.975145635859366e-05,
                        "total_energy_kwh": 0.03033210960823475,
                        "total_energy_joules": 109195.5945896451
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.15004268314642866,
                        "joules_per_token": 6.664770177590643,
                        "flops_per_joule": 155225776.79850233,
                        "joules_per_flop": 6.442228994596008e-09
                    },
                    "per-process_emissions": [
                        0.002275198484087065,
                        0.002883779360661347,
                        0.0031362622391551084,
                        0.003259777071353508
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0117": {
            "setup": {
                "experiment_id": "0117",
                "date_time": "April 11, 2025 at 01:40:22 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_16",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.435157784000694,
                        "average_latency_ms_per_batch": 2929.3947230000867,
                        "throughput_queries_per_sec": 5.461879163765916,
                        "throughput_tokens_per_sec": 699.1205329620373
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.7,
                        "cpu_memory_usage_bytes": 2011435008
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0117",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 860.5753758328476,
                            "process_3": 682.7791358029963,
                            "process_2": 606.9006549992879,
                            "process_1": 451.59433799810654
                        },
                        "ram_power": {
                            "process_0": 0.7005772590637207,
                            "process_3": 0.6537795066833496,
                            "process_2": 0.6519126892089844,
                            "process_1": 0.6602439880371094
                        },
                        "cpu_energy": {
                            "process_0": 0.0007255553946249053,
                            "process_3": 0.0009417213138750073,
                            "process_2": 0.0010453227453125465,
                            "process_1": 0.0011067888925934994
                        },
                        "gpu_energy": {
                            "process_0": 0.005243008638848107,
                            "process_3": 0.0066234427987500455,
                            "process_2": 0.007182074078987993,
                            "process_1": 0.00744444428888405
                        },
                        "ram_energy": {
                            "process_0": 3.8693149639603445e-06,
                            "process_3": 4.804021411821816e-06,
                            "process_2": 5.343008105571681e-06,
                            "process_1": 5.735111877239817e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005972433348436974,
                            "process_3": 0.0075699681340368735,
                            "process_2": 0.008232739832406112,
                            "process_1": 0.008556968293354792
                        },
                        "total_energy_joules": {
                            "process_0": 21500.760054373106,
                            "process_3": 27251.885282532745,
                            "process_2": 29637.863396662004,
                            "process_1": 30805.08585607725
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 650.4623761583096,
                        "ram_power_avg": 0.666628360748291,
                        "cpu_energy_total": 0.0038193883464059586,
                        "gpu_energy_total": 0.026492969805470196,
                        "ram_energy_total": 1.975145635859366e-05,
                        "total_energy_kwh": 0.03033210960823475,
                        "total_energy_joules": 109195.5945896451
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.15004268314642866,
                        "joules_per_token": 6.664770177590643,
                        "flops_per_joule": 155225776.79850233,
                        "joules_per_flop": 6.442228994596008e-09
                    },
                    "per-process_emissions": [
                        0.002275198484087065,
                        0.002883779360661347,
                        0.0031362622391551084,
                        0.003259777071353508
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0118": {
            "setup": {
                "experiment_id": "0118",
                "date_time": "April 11, 2025 at 01:41:15 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_32",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 13.354169559001093,
                        "average_latency_ms_per_batch": 3338.5423897502733,
                        "throughput_queries_per_sec": 9.585021324948231,
                        "throughput_tokens_per_sec": 1226.8827295933736
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.8,
                        "cpu_memory_usage_bytes": 2011807744
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0118",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 545.2273874351157,
                            "process_0": 898.9078684533573,
                            "process_1": 528.6226220179184,
                            "process_3": 741.4726063721648
                        },
                        "ram_power": {
                            "process_2": 0.6480460166931152,
                            "process_0": 0.7007288932800293,
                            "process_1": 0.6504592895507812,
                            "process_3": 0.648500919342041
                        },
                        "cpu_energy": {
                            "process_2": 0.0006316163684999766,
                            "process_0": 0.00041443596074992677,
                            "process_1": 0.0006772934900938026,
                            "process_3": 0.000553033400562299
                        },
                        "gpu_energy": {
                            "process_2": 0.0047746527086080515,
                            "process_0": 0.0033637010242920373,
                            "process_1": 0.004993452050313957,
                            "process_3": 0.004320352622945889
                        },
                        "ram_energy": {
                            "process_2": 3.198334151074282e-06,
                            "process_0": 2.2066525583964047e-06,
                            "process_1": 3.5171621778156316e-06,
                            "process_3": 2.832616519521128e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005409467411259101,
                            "process_0": 0.0037803436376003616,
                            "process_1": 0.005674262702585576,
                            "process_3": 0.004876218640027707
                        },
                        "total_energy_joules": {
                            "process_2": 19474.082680532763,
                            "process_0": 13609.237095361303,
                            "process_1": 20427.345729308072,
                            "process_3": 17554.387104099744
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 678.557621069639,
                        "ram_power_avg": 0.6619337797164917,
                        "cpu_energy_total": 0.002276379219906005,
                        "gpu_energy_total": 0.017452158406159934,
                        "ram_energy_total": 1.1754765406807447e-05,
                        "total_energy_kwh": 0.019740292391472746,
                        "total_energy_joules": 71065.05260930189
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.230549326264137,
                        "joules_per_token": 4.337466589923211,
                        "flops_per_joule": 238513451.7009191,
                        "joules_per_flop": 4.19263564745999e-09
                    },
                    "per-process_emissions": [
                        0.0020607366103191547,
                        0.0014401219087438579,
                        0.0021616103765499752,
                        0.001857595490918555
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0118": {
            "setup": {
                "experiment_id": "0118",
                "date_time": "April 11, 2025 at 01:41:15 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_32",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 13.354169559001093,
                        "average_latency_ms_per_batch": 3338.5423897502733,
                        "throughput_queries_per_sec": 9.585021324948231,
                        "throughput_tokens_per_sec": 1226.8827295933736
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.8,
                        "cpu_memory_usage_bytes": 2011807744
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0118",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 545.2273874351157,
                            "process_0": 898.9078684533573,
                            "process_1": 528.6226220179184,
                            "process_3": 741.4726063721648
                        },
                        "ram_power": {
                            "process_2": 0.6480460166931152,
                            "process_0": 0.7007288932800293,
                            "process_1": 0.6504592895507812,
                            "process_3": 0.648500919342041
                        },
                        "cpu_energy": {
                            "process_2": 0.0006316163684999766,
                            "process_0": 0.00041443596074992677,
                            "process_1": 0.0006772934900938026,
                            "process_3": 0.000553033400562299
                        },
                        "gpu_energy": {
                            "process_2": 0.0047746527086080515,
                            "process_0": 0.0033637010242920373,
                            "process_1": 0.004993452050313957,
                            "process_3": 0.004320352622945889
                        },
                        "ram_energy": {
                            "process_2": 3.198334151074282e-06,
                            "process_0": 2.2066525583964047e-06,
                            "process_1": 3.5171621778156316e-06,
                            "process_3": 2.832616519521128e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005409467411259101,
                            "process_0": 0.0037803436376003616,
                            "process_1": 0.005674262702585576,
                            "process_3": 0.004876218640027707
                        },
                        "total_energy_joules": {
                            "process_2": 19474.082680532763,
                            "process_0": 13609.237095361303,
                            "process_1": 20427.345729308072,
                            "process_3": 17554.387104099744
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 678.557621069639,
                        "ram_power_avg": 0.6619337797164917,
                        "cpu_energy_total": 0.002276379219906005,
                        "gpu_energy_total": 0.017452158406159934,
                        "ram_energy_total": 1.1754765406807447e-05,
                        "total_energy_kwh": 0.019740292391472746,
                        "total_energy_joules": 71065.05260930189
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.230549326264137,
                        "joules_per_token": 4.337466589923211,
                        "flops_per_joule": 238513451.7009191,
                        "joules_per_flop": 4.19263564745999e-09
                    },
                    "per-process_emissions": [
                        0.0020607366103191547,
                        0.0014401219087438579,
                        0.0021616103765499752,
                        0.001857595490918555
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0119": {
            "setup": {
                "experiment_id": "0119",
                "date_time": "April 11, 2025 at 01:42:03 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_64",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 10.367802049000602,
                        "average_latency_ms_per_batch": 5183.901024500301,
                        "throughput_queries_per_sec": 12.34591472667425,
                        "throughput_tokens_per_sec": 1580.277085014304
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.7,
                        "cpu_memory_usage_bytes": 2015821824
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0119",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 624.8867592943449,
                            "process_3": 807.6256181421045,
                            "process_0": 1099.6186513763053,
                            "process_1": 471.7358266118246
                        },
                        "ram_power": {
                            "process_2": 0.6497969627380371,
                            "process_3": 0.6531343460083008,
                            "process_0": 0.7023439407348633,
                            "process_1": 0.6456198692321777
                        },
                        "cpu_energy": {
                            "process_2": 0.0005065485012188448,
                            "process_3": 0.0004460250966562286,
                            "process_0": 0.0003212725364687685,
                            "process_1": 0.0005635789367813686
                        },
                        "gpu_energy": {
                            "process_2": 0.00408224854357403,
                            "process_3": 0.003717694085263984,
                            "process_0": 0.00282757753983795,
                            "process_1": 0.004346112087997983
                        },
                        "ram_energy": {
                            "process_2": 2.6350869480689943e-06,
                            "process_3": 2.33241312949949e-06,
                            "process_0": 1.7687734622462276e-06,
                            "process_1": 2.9201764158901453e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.004591432131740944,
                            "process_3": 0.004166051595049711,
                            "process_0": 0.0031506188497689644,
                            "process_1": 0.004912611201195243
                        },
                        "total_energy_joules": {
                            "process_2": 16529.1556742674,
                            "process_3": 14997.785742178961,
                            "process_0": 11342.227859168272,
                            "process_1": 17685.400324302875
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 750.9667138561447,
                        "ram_power_avg": 0.6627237796783447,
                        "cpu_energy_total": 0.0018374250711252105,
                        "gpu_energy_total": 0.014973632256673947,
                        "ram_energy_total": 9.656449955704857e-06,
                        "total_energy_kwh": 0.01682071377775486,
                        "total_energy_joules": 60554.56959991751
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.27056587319914366,
                        "joules_per_token": 3.695957617182465,
                        "flops_per_joule": 279912335.35536665,
                        "joules_per_flop": 3.572547093111977e-09
                    },
                    "per-process_emissions": [
                        0.0017491060705867127,
                        0.0015870573551341877,
                        0.001200228250819487,
                        0.0018714592370953278
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0119": {
            "setup": {
                "experiment_id": "0119",
                "date_time": "April 11, 2025 at 01:42:03 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_64",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 10.367802049000602,
                        "average_latency_ms_per_batch": 5183.901024500301,
                        "throughput_queries_per_sec": 12.34591472667425,
                        "throughput_tokens_per_sec": 1580.277085014304
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.7,
                        "cpu_memory_usage_bytes": 2015821824
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0119",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 624.8867592943449,
                            "process_3": 807.6256181421045,
                            "process_0": 1099.6186513763053,
                            "process_1": 471.7358266118246
                        },
                        "ram_power": {
                            "process_2": 0.6497969627380371,
                            "process_3": 0.6531343460083008,
                            "process_0": 0.7023439407348633,
                            "process_1": 0.6456198692321777
                        },
                        "cpu_energy": {
                            "process_2": 0.0005065485012188448,
                            "process_3": 0.0004460250966562286,
                            "process_0": 0.0003212725364687685,
                            "process_1": 0.0005635789367813686
                        },
                        "gpu_energy": {
                            "process_2": 0.00408224854357403,
                            "process_3": 0.003717694085263984,
                            "process_0": 0.00282757753983795,
                            "process_1": 0.004346112087997983
                        },
                        "ram_energy": {
                            "process_2": 2.6350869480689943e-06,
                            "process_3": 2.33241312949949e-06,
                            "process_0": 1.7687734622462276e-06,
                            "process_1": 2.9201764158901453e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.004591432131740944,
                            "process_3": 0.004166051595049711,
                            "process_0": 0.0031506188497689644,
                            "process_1": 0.004912611201195243
                        },
                        "total_energy_joules": {
                            "process_2": 16529.1556742674,
                            "process_3": 14997.785742178961,
                            "process_0": 11342.227859168272,
                            "process_1": 17685.400324302875
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 750.9667138561447,
                        "ram_power_avg": 0.6627237796783447,
                        "cpu_energy_total": 0.0018374250711252105,
                        "gpu_energy_total": 0.014973632256673947,
                        "ram_energy_total": 9.656449955704857e-06,
                        "total_energy_kwh": 0.01682071377775486,
                        "total_energy_joules": 60554.56959991751
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.27056587319914366,
                        "joules_per_token": 3.695957617182465,
                        "flops_per_joule": 279912335.35536665,
                        "joules_per_flop": 3.572547093111977e-09
                    },
                    "per-process_emissions": [
                        0.0017491060705867127,
                        0.0015870573551341877,
                        0.001200228250819487,
                        0.0018714592370953278
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0120": {
            "setup": {
                "experiment_id": "0120",
                "date_time": "April 11, 2025 at 01:43:09 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precis_float32_quant_False_quant8_False_quant4_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.777636398999675,
                        "average_latency_ms_per_batch": 2972.2045498749594,
                        "throughput_queries_per_sec": 5.383209577777249,
                        "throughput_tokens_per_sec": 689.0508259554879
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.5,
                        "cpu_memory_usage_bytes": 2011570176
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0120",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 486.6157919648468,
                            "process_3": 672.9666416524083,
                            "process_0": 845.5226175191438,
                            "process_1": 528.4347892892555
                        },
                        "ram_power": {
                            "process_2": 0.6475839614868164,
                            "process_3": 0.6499171257019044,
                            "process_0": 0.7008218765258789,
                            "process_1": 0.656609058380127
                        },
                        "cpu_energy": {
                            "process_2": 0.0010531898863125663,
                            "process_3": 0.000938399515280878,
                            "process_0": 0.0007361432209685859,
                            "process_1": 0.0010976355177188565
                        },
                        "gpu_energy": {
                            "process_2": 0.007275110820084113,
                            "process_3": 0.006634228362934064,
                            "process_0": 0.005347137333262059,
                            "process_1": 0.007461373191315929
                        },
                        "ram_energy": {
                            "process_2": 5.333862244538439e-06,
                            "process_3": 4.752660211994232e-06,
                            "process_0": 3.924762510152729e-06,
                            "process_1": 5.656693138499367e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.008333634568641217,
                            "process_3": 0.007577380538426935,
                            "process_0": 0.006087205316740795,
                            "process_1": 0.008564665402173282
                        },
                        "total_energy_joules": {
                            "process_2": 30001.08444710838,
                            "process_3": 27278.569938336965,
                            "process_0": 21913.939140266863,
                            "process_1": 30832.795447823813
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 633.3849601064136,
                        "ram_power_avg": 0.6637330055236816,
                        "cpu_energy_total": 0.0038253681402808867,
                        "gpu_energy_total": 0.026717849707596164,
                        "ram_energy_total": 1.966797810518477e-05,
                        "total_energy_kwh": 0.030562885825982232,
                        "total_energy_joules": 110026.38897353601
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1489097311367798,
                        "joules_per_token": 6.715477842623047,
                        "flops_per_joule": 154053687.9496143,
                        "joules_per_flop": 6.491243496404097e-09
                    },
                    "per-process_emissions": [
                        0.003174698088923872,
                        0.0028866031161137408,
                        0.002318920865412406,
                        0.003262709284957912
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0120": {
            "setup": {
                "experiment_id": "0120",
                "date_time": "April 11, 2025 at 01:43:09 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precis_float32_quant_False_quant8_False_quant4_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.777636398999675,
                        "average_latency_ms_per_batch": 2972.2045498749594,
                        "throughput_queries_per_sec": 5.383209577777249,
                        "throughput_tokens_per_sec": 689.0508259554879
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.5,
                        "cpu_memory_usage_bytes": 2011570176
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0120",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 486.6157919648468,
                            "process_3": 672.9666416524083,
                            "process_0": 845.5226175191438,
                            "process_1": 528.4347892892555
                        },
                        "ram_power": {
                            "process_2": 0.6475839614868164,
                            "process_3": 0.6499171257019044,
                            "process_0": 0.7008218765258789,
                            "process_1": 0.656609058380127
                        },
                        "cpu_energy": {
                            "process_2": 0.0010531898863125663,
                            "process_3": 0.000938399515280878,
                            "process_0": 0.0007361432209685859,
                            "process_1": 0.0010976355177188565
                        },
                        "gpu_energy": {
                            "process_2": 0.007275110820084113,
                            "process_3": 0.006634228362934064,
                            "process_0": 0.005347137333262059,
                            "process_1": 0.007461373191315929
                        },
                        "ram_energy": {
                            "process_2": 5.333862244538439e-06,
                            "process_3": 4.752660211994232e-06,
                            "process_0": 3.924762510152729e-06,
                            "process_1": 5.656693138499367e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.008333634568641217,
                            "process_3": 0.007577380538426935,
                            "process_0": 0.006087205316740795,
                            "process_1": 0.008564665402173282
                        },
                        "total_energy_joules": {
                            "process_2": 30001.08444710838,
                            "process_3": 27278.569938336965,
                            "process_0": 21913.939140266863,
                            "process_1": 30832.795447823813
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 633.3849601064136,
                        "ram_power_avg": 0.6637330055236816,
                        "cpu_energy_total": 0.0038253681402808867,
                        "gpu_energy_total": 0.026717849707596164,
                        "ram_energy_total": 1.966797810518477e-05,
                        "total_energy_kwh": 0.030562885825982232,
                        "total_energy_joules": 110026.38897353601
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1489097311367798,
                        "joules_per_token": 6.715477842623047,
                        "flops_per_joule": 154053687.9496143,
                        "joules_per_flop": 6.491243496404097e-09
                    },
                    "per-process_emissions": [
                        0.003174698088923872,
                        0.0028866031161137408,
                        0.002318920865412406,
                        0.003262709284957912
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0121": {
            "setup": {
                "experiment_id": "0121",
                "date_time": "April 11, 2025 at 01:44:07 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precis_float16_quant_False_quant8_False_quant4_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.51897817099598,
                        "average_latency_ms_per_batch": 2939.8722713744974,
                        "throughput_queries_per_sec": 5.442413316997414,
                        "throughput_tokens_per_sec": 696.628904575669
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419853312,
                        "gpu_max_memory_allocated_bytes": 4419853312,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.5,
                        "cpu_memory_usage_bytes": 3105521664
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0121",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 722.9123415351803,
                            "process_1": 498.91715820133163,
                            "process_2": 494.1064886545817,
                            "process_0": 590.6071427931262
                        },
                        "ram_power": {
                            "process_3": 0.8724031448364259,
                            "process_1": 0.8529124259948732,
                            "process_2": 0.8570151329040527,
                            "process_0": 1.0832476615905762
                        },
                        "cpu_energy": {
                            "process_3": 0.0007936921227502011,
                            "process_1": 0.0008365868269374915,
                            "process_2": 0.0008377072849063437,
                            "process_0": 0.0007271904629063783
                        },
                        "gpu_energy": {
                            "process_3": 0.0042366381115298835,
                            "process_1": 0.004443333276885869,
                            "process_2": 0.00442791937566589,
                            "process_0": 0.0038799142150399146
                        },
                        "ram_energy": {
                            "process_3": 5.374088171357257e-06,
                            "process_1": 5.545075789399197e-06,
                            "process_2": 5.55348169581517e-06,
                            "process_0": 5.993833526246146e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005035704322451443,
                            "process_1": 0.005285465179612759,
                            "process_2": 0.0052711801422680475,
                            "process_0": 0.004613098511472539
                        },
                        "total_energy_joules": {
                            "process_3": 18128.535560825196,
                            "process_1": 19027.674646605934,
                            "process_2": 18976.24851216497,
                            "process_0": 16607.154641301142
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 576.635782796055,
                        "ram_power_avg": 0.9163945913314819,
                        "cpu_energy_total": 0.0031951766975004143,
                        "gpu_energy_total": 0.016987804979121557,
                        "ram_energy_total": 2.2466479182817772e-05,
                        "total_energy_kwh": 0.020205448155804787,
                        "total_energy_joules": 72739.61336089723
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2252417801385727,
                        "joules_per_token": 4.43967366704695,
                        "flops_per_joule": 233022561.0226274,
                        "joules_per_flop": 4.291429961165417e-09
                    },
                    "per-process_emissions": [
                        0.0019183515616378775,
                        0.0020134979601734806,
                        0.002008056075197013,
                        0.001757359877945464
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0121": {
            "setup": {
                "experiment_id": "0121",
                "date_time": "April 11, 2025 at 01:44:07 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precis_float16_quant_False_quant8_False_quant4_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.51897817099598,
                        "average_latency_ms_per_batch": 2939.8722713744974,
                        "throughput_queries_per_sec": 5.442413316997414,
                        "throughput_tokens_per_sec": 696.628904575669
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419853312,
                        "gpu_max_memory_allocated_bytes": 4419853312,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.5,
                        "cpu_memory_usage_bytes": 3105521664
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0121",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 722.9123415351803,
                            "process_1": 498.91715820133163,
                            "process_2": 494.1064886545817,
                            "process_0": 590.6071427931262
                        },
                        "ram_power": {
                            "process_3": 0.8724031448364259,
                            "process_1": 0.8529124259948732,
                            "process_2": 0.8570151329040527,
                            "process_0": 1.0832476615905762
                        },
                        "cpu_energy": {
                            "process_3": 0.0007936921227502011,
                            "process_1": 0.0008365868269374915,
                            "process_2": 0.0008377072849063437,
                            "process_0": 0.0007271904629063783
                        },
                        "gpu_energy": {
                            "process_3": 0.0042366381115298835,
                            "process_1": 0.004443333276885869,
                            "process_2": 0.00442791937566589,
                            "process_0": 0.0038799142150399146
                        },
                        "ram_energy": {
                            "process_3": 5.374088171357257e-06,
                            "process_1": 5.545075789399197e-06,
                            "process_2": 5.55348169581517e-06,
                            "process_0": 5.993833526246146e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005035704322451443,
                            "process_1": 0.005285465179612759,
                            "process_2": 0.0052711801422680475,
                            "process_0": 0.004613098511472539
                        },
                        "total_energy_joules": {
                            "process_3": 18128.535560825196,
                            "process_1": 19027.674646605934,
                            "process_2": 18976.24851216497,
                            "process_0": 16607.154641301142
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 576.635782796055,
                        "ram_power_avg": 0.9163945913314819,
                        "cpu_energy_total": 0.0031951766975004143,
                        "gpu_energy_total": 0.016987804979121557,
                        "ram_energy_total": 2.2466479182817772e-05,
                        "total_energy_kwh": 0.020205448155804787,
                        "total_energy_joules": 72739.61336089723
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2252417801385727,
                        "joules_per_token": 4.43967366704695,
                        "flops_per_joule": 233022561.0226274,
                        "joules_per_flop": 4.291429961165417e-09
                    },
                    "per-process_emissions": [
                        0.0019183515616378775,
                        0.0020134979601734806,
                        0.002008056075197013,
                        0.001757359877945464
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0122": {
            "setup": {
                "experiment_id": "0122",
                "date_time": "April 11, 2025 at 01:47:13 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precis_float16_quant_True_quant8_True_quant4_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 74.51146055599929,
                        "average_latency_ms_per_batch": 9313.932569499912,
                        "throughput_queries_per_sec": 1.7178565424013028,
                        "throughput_tokens_per_sec": 219.88563742736676
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576560640,
                        "gpu_max_memory_allocated_bytes": 1576560640,
                        "gpu_current_memory_reserved_bytes": 2883584000,
                        "gpu_max_memory_reserved_bytes": 2883584000
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 32.9,
                        "cpu_memory_usage_bytes": 2712805376
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0122",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 328.13377023779583,
                            "process_2": 541.8323302320946,
                            "process_3": 504.4283241280552,
                            "process_0": 566.7352606591663
                        },
                        "ram_power": {
                            "process_1": 0.9914517402648926,
                            "process_2": 0.9904131889343262,
                            "process_3": 0.9893217086791993,
                            "process_0": 0.9470300674438477
                        },
                        "cpu_energy": {
                            "process_1": 0.0047936341938748794,
                            "process_2": 0.004220705831125143,
                            "process_3": 0.003396908146937562,
                            "process_0": 0.0022894508404066867
                        },
                        "gpu_energy": {
                            "process_1": 0.02156976753357795,
                            "process_2": 0.01936243465660198,
                            "process_3": 0.015643170847860044,
                            "process_0": 0.01046701670694003
                        },
                        "ram_energy": {
                            "process_1": 3.75411757607639e-05,
                            "process_2": 3.276591607232212e-05,
                            "process_3": 2.585795126892437e-05,
                            "process_0": 1.570739720097907e-05
                        },
                        "total_energy_kwh": {
                            "process_1": 0.026400942903213605,
                            "process_2": 0.023615906403799444,
                            "process_3": 0.019065936946066536,
                            "process_0": 0.012772174944547693
                        },
                        "total_energy_joules": {
                            "process_1": 95043.39445156898,
                            "process_2": 85017.263053678,
                            "process_3": 68637.37300583953,
                            "process_0": 45979.82980037169
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 485.28242131427794,
                        "ram_power_avg": 0.9795541763305664,
                        "cpu_energy_total": 0.01470069901234427,
                        "gpu_energy_total": 0.06704238974498,
                        "ram_energy_total": 0.00011187244030298946,
                        "total_energy_kwh": 0.08185496119762728,
                        "total_energy_joules": 294677.86031145824
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.055599697862211354,
                        "joules_per_token": 17.985709247525527,
                        "flops_per_joule": 3510762.9969436587,
                        "joules_per_flop": 2.848383672923986e-07
                    },
                    "per-process_emissions": [
                        0.010057439198979223,
                        0.008996479544527399,
                        0.007263168679604047,
                        0.004865560045125444
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0122": {
            "setup": {
                "experiment_id": "0122",
                "date_time": "April 11, 2025 at 01:47:13 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precis_float16_quant_True_quant8_True_quant4_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 74.51146055599929,
                        "average_latency_ms_per_batch": 9313.932569499912,
                        "throughput_queries_per_sec": 1.7178565424013028,
                        "throughput_tokens_per_sec": 219.88563742736676
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576560640,
                        "gpu_max_memory_allocated_bytes": 1576560640,
                        "gpu_current_memory_reserved_bytes": 2883584000,
                        "gpu_max_memory_reserved_bytes": 2883584000
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 32.9,
                        "cpu_memory_usage_bytes": 2712805376
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0122",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 328.13377023779583,
                            "process_2": 541.8323302320946,
                            "process_3": 504.4283241280552,
                            "process_0": 566.7352606591663
                        },
                        "ram_power": {
                            "process_1": 0.9914517402648926,
                            "process_2": 0.9904131889343262,
                            "process_3": 0.9893217086791993,
                            "process_0": 0.9470300674438477
                        },
                        "cpu_energy": {
                            "process_1": 0.0047936341938748794,
                            "process_2": 0.004220705831125143,
                            "process_3": 0.003396908146937562,
                            "process_0": 0.0022894508404066867
                        },
                        "gpu_energy": {
                            "process_1": 0.02156976753357795,
                            "process_2": 0.01936243465660198,
                            "process_3": 0.015643170847860044,
                            "process_0": 0.01046701670694003
                        },
                        "ram_energy": {
                            "process_1": 3.75411757607639e-05,
                            "process_2": 3.276591607232212e-05,
                            "process_3": 2.585795126892437e-05,
                            "process_0": 1.570739720097907e-05
                        },
                        "total_energy_kwh": {
                            "process_1": 0.026400942903213605,
                            "process_2": 0.023615906403799444,
                            "process_3": 0.019065936946066536,
                            "process_0": 0.012772174944547693
                        },
                        "total_energy_joules": {
                            "process_1": 95043.39445156898,
                            "process_2": 85017.263053678,
                            "process_3": 68637.37300583953,
                            "process_0": 45979.82980037169
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 485.28242131427794,
                        "ram_power_avg": 0.9795541763305664,
                        "cpu_energy_total": 0.01470069901234427,
                        "gpu_energy_total": 0.06704238974498,
                        "ram_energy_total": 0.00011187244030298946,
                        "total_energy_kwh": 0.08185496119762728,
                        "total_energy_joules": 294677.86031145824
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.055599697862211354,
                        "joules_per_token": 17.985709247525527,
                        "flops_per_joule": 3510762.9969436587,
                        "joules_per_flop": 2.848383672923986e-07
                    },
                    "per-process_emissions": [
                        0.010057439198979223,
                        0.008996479544527399,
                        0.007263168679604047,
                        0.004865560045125444
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0123": {
            "setup": {
                "experiment_id": "0123",
                "date_time": "April 11, 2025 at 01:48:28 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precis_float16_quant_True_quant8_False_quant4_True",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.632225758999994,
                        "average_latency_ms_per_batch": 4704.028219874999,
                        "throughput_queries_per_sec": 3.401340139159533,
                        "throughput_tokens_per_sec": 435.37153781242023
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.0,
                        "cpu_memory_usage_bytes": 2653773824
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0123",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 487.38897126180734,
                            "process_2": 715.6385166256223,
                            "process_3": 676.823882633681,
                            "process_1": 506.2097790768871
                        },
                        "ram_power": {
                            "process_0": 0.926630973815918,
                            "process_2": 0.976651668548584,
                            "process_3": 0.9675006866455078,
                            "process_1": 0.9801349639892579
                        },
                        "cpu_energy": {
                            "process_0": 0.0011348883941874985,
                            "process_2": 0.0012209695992187904,
                            "process_3": 0.0011745620718126592,
                            "process_1": 0.0011268923265624262
                        },
                        "gpu_energy": {
                            "process_0": 0.005306622856406096,
                            "process_2": 0.00569730066894808,
                            "process_3": 0.005513312743980092,
                            "process_1": 0.0053041211877380146
                        },
                        "ram_energy": {
                            "process_0": 7.892380917624673e-06,
                            "process_2": 9.06806972831638e-06,
                            "process_3": 8.705309070087195e-06,
                            "process_1": 8.285938064791328e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0064494036315112194,
                            "process_2": 0.006927338337895186,
                            "process_3": 0.006696580124862838,
                            "process_1": 0.006439299452365234
                        },
                        "total_energy_joules": {
                            "process_0": 23217.85307344039,
                            "process_2": 24938.418016422667,
                            "process_3": 24107.688449506215,
                            "process_1": 23181.47802851484
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 596.5152873994995,
                        "ram_power_avg": 0.9627295732498169,
                        "cpu_energy_total": 0.004657312391781374,
                        "gpu_energy_total": 0.021821357457072282,
                        "ram_energy_total": 3.3951697780819576e-05,
                        "total_energy_kwh": 0.026512621546634477,
                        "total_energy_joules": 95445.43756788412
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.17165828370106354,
                        "joules_per_token": 5.825527195305427,
                        "flops_per_joule": 10839115.565520838,
                        "joules_per_flop": 9.225844986661035e-08
                    },
                    "per-process_emissions": [
                        0.002456900313424199,
                        0.002638969539821171,
                        0.002551062198566498,
                        0.002453051126378536
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0123": {
            "setup": {
                "experiment_id": "0123",
                "date_time": "April 11, 2025 at 01:48:28 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precis_float16_quant_True_quant8_False_quant4_True",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.632225758999994,
                        "average_latency_ms_per_batch": 4704.028219874999,
                        "throughput_queries_per_sec": 3.401340139159533,
                        "throughput_tokens_per_sec": 435.37153781242023
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1929379840,
                        "gpu_max_memory_reserved_bytes": 1929379840
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 6.0,
                        "cpu_memory_usage_bytes": 2653773824
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0123",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 487.38897126180734,
                            "process_2": 715.6385166256223,
                            "process_3": 676.823882633681,
                            "process_1": 506.2097790768871
                        },
                        "ram_power": {
                            "process_0": 0.926630973815918,
                            "process_2": 0.976651668548584,
                            "process_3": 0.9675006866455078,
                            "process_1": 0.9801349639892579
                        },
                        "cpu_energy": {
                            "process_0": 0.0011348883941874985,
                            "process_2": 0.0012209695992187904,
                            "process_3": 0.0011745620718126592,
                            "process_1": 0.0011268923265624262
                        },
                        "gpu_energy": {
                            "process_0": 0.005306622856406096,
                            "process_2": 0.00569730066894808,
                            "process_3": 0.005513312743980092,
                            "process_1": 0.0053041211877380146
                        },
                        "ram_energy": {
                            "process_0": 7.892380917624673e-06,
                            "process_2": 9.06806972831638e-06,
                            "process_3": 8.705309070087195e-06,
                            "process_1": 8.285938064791328e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0064494036315112194,
                            "process_2": 0.006927338337895186,
                            "process_3": 0.006696580124862838,
                            "process_1": 0.006439299452365234
                        },
                        "total_energy_joules": {
                            "process_0": 23217.85307344039,
                            "process_2": 24938.418016422667,
                            "process_3": 24107.688449506215,
                            "process_1": 23181.47802851484
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 596.5152873994995,
                        "ram_power_avg": 0.9627295732498169,
                        "cpu_energy_total": 0.004657312391781374,
                        "gpu_energy_total": 0.021821357457072282,
                        "ram_energy_total": 3.3951697780819576e-05,
                        "total_energy_kwh": 0.026512621546634477,
                        "total_energy_joules": 95445.43756788412
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.17165828370106354,
                        "joules_per_token": 5.825527195305427,
                        "flops_per_joule": 10839115.565520838,
                        "joules_per_flop": 9.225844986661035e-08
                    },
                    "per-process_emissions": [
                        0.002456900313424199,
                        0.002638969539821171,
                        0.002551062198566498,
                        0.002453051126378536
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0124": {
            "setup": {
                "experiment_id": "0124",
                "date_time": "April 11, 2025 at 01:49:39 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.034821987002942,
                        "average_latency_ms_per_batch": 2879.352748375368,
                        "throughput_queries_per_sec": 5.556804392594052,
                        "throughput_tokens_per_sec": 711.2709622520387
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 12016680960,
                        "gpu_max_memory_reserved_bytes": 12016680960
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.9,
                        "cpu_memory_usage_bytes": 1998827520
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0124",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 452.2257271901062,
                            "process_0": 0.0,
                            "process_2": 532.3565967744776,
                            "process_3": 11.318236634116907
                        },
                        "ram_power": {
                            "process_1": 0.6196131706237793,
                            "process_0": 0.6956791877746582,
                            "process_2": 0.6175174713134766,
                            "process_3": 0.6392884254455566
                        },
                        "cpu_energy": {
                            "process_1": 0.0010328415149375588,
                            "process_0": 0.0008388075806249164,
                            "process_2": 0.0009382870071560775,
                            "process_3": 0.0008628248467498451
                        },
                        "gpu_energy": {
                            "process_1": 0.006462476558866143,
                            "process_0": 0.005058052657550105,
                            "process_2": 0.0059936189615580615,
                            "process_3": 0.005520964138990031
                        },
                        "ram_energy": {
                            "process_1": 5.041676195539272e-06,
                            "process_0": 4.503793218874494e-06,
                            "process_2": 4.5029379621354024e-06,
                            "process_3": 4.280008090891816e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.007500359749999242,
                            "process_0": 0.005901364031393896,
                            "process_2": 0.006936408906676275,
                            "process_3": 0.006388068993830767
                        },
                        "total_energy_joules": {
                            "process_1": 27001.29509999727,
                            "process_0": 21244.910513018025,
                            "process_2": 24971.07206403459,
                            "process_3": 22997.04837779076
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 248.97514014967518,
                        "ram_power_avg": 0.6430245637893677,
                        "cpu_energy_total": 0.0036727609494683977,
                        "gpu_energy_total": 0.02303511231696434,
                        "ram_energy_total": 1.8328415467440987e-05,
                        "total_energy_kwh": 0.02672620168190018,
                        "total_energy_joules": 96214.32605484065
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.17028649133457918,
                        "joules_per_token": 5.872456424245645,
                        "flops_per_joule": 176168889.68791178,
                        "joules_per_flop": 5.676371133243381e-09
                    },
                    "per-process_emissions": [
                        0.0028572620467622115,
                        0.0022481246277595048,
                        0.002642424972998327,
                        0.0024335348831998308
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0124": {
            "setup": {
                "experiment_id": "0124",
                "date_time": "April 11, 2025 at 01:49:39 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.034821987002942,
                        "average_latency_ms_per_batch": 2879.352748375368,
                        "throughput_queries_per_sec": 5.556804392594052,
                        "throughput_tokens_per_sec": 711.2709622520387
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 12016680960,
                        "gpu_max_memory_reserved_bytes": 12016680960
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.9,
                        "cpu_memory_usage_bytes": 1998827520
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0124",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 452.2257271901062,
                            "process_0": 0.0,
                            "process_2": 532.3565967744776,
                            "process_3": 11.318236634116907
                        },
                        "ram_power": {
                            "process_1": 0.6196131706237793,
                            "process_0": 0.6956791877746582,
                            "process_2": 0.6175174713134766,
                            "process_3": 0.6392884254455566
                        },
                        "cpu_energy": {
                            "process_1": 0.0010328415149375588,
                            "process_0": 0.0008388075806249164,
                            "process_2": 0.0009382870071560775,
                            "process_3": 0.0008628248467498451
                        },
                        "gpu_energy": {
                            "process_1": 0.006462476558866143,
                            "process_0": 0.005058052657550105,
                            "process_2": 0.0059936189615580615,
                            "process_3": 0.005520964138990031
                        },
                        "ram_energy": {
                            "process_1": 5.041676195539272e-06,
                            "process_0": 4.503793218874494e-06,
                            "process_2": 4.5029379621354024e-06,
                            "process_3": 4.280008090891816e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.007500359749999242,
                            "process_0": 0.005901364031393896,
                            "process_2": 0.006936408906676275,
                            "process_3": 0.006388068993830767
                        },
                        "total_energy_joules": {
                            "process_1": 27001.29509999727,
                            "process_0": 21244.910513018025,
                            "process_2": 24971.07206403459,
                            "process_3": 22997.04837779076
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 248.97514014967518,
                        "ram_power_avg": 0.6430245637893677,
                        "cpu_energy_total": 0.0036727609494683977,
                        "gpu_energy_total": 0.02303511231696434,
                        "ram_energy_total": 1.8328415467440987e-05,
                        "total_energy_kwh": 0.02672620168190018,
                        "total_energy_joules": 96214.32605484065
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.17028649133457918,
                        "joules_per_token": 5.872456424245645,
                        "flops_per_joule": 176168889.68791178,
                        "joules_per_flop": 5.676371133243381e-09
                    },
                    "per-process_emissions": [
                        0.0028572620467622115,
                        0.0022481246277595048,
                        0.002642424972998327,
                        0.0024335348831998308
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0125": {
            "setup": {
                "experiment_id": "0125",
                "date_time": "April 11, 2025 at 01:50:49 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.5743614160001,
                        "average_latency_ms_per_batch": 2946.7951770000127,
                        "throughput_queries_per_sec": 5.42962745591596,
                        "throughput_tokens_per_sec": 694.9923143572429
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.2,
                        "cpu_memory_usage_bytes": 2012307456
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0125",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 9.292454238687876,
                            "process_0": 2132.583274972605,
                            "process_2": 5.057340908576486,
                            "process_3": 136.9446755587152
                        },
                        "ram_power": {
                            "process_1": 0.6507124900817871,
                            "process_0": 0.7010250091552734,
                            "process_2": 0.6518039703369141,
                            "process_3": 0.6527080535888672
                        },
                        "cpu_energy": {
                            "process_1": 0.0009646041164687063,
                            "process_0": 0.0006719615624375022,
                            "process_2": 0.0008896667306563017,
                            "process_3": 0.0007779958651875065
                        },
                        "gpu_energy": {
                            "process_1": 0.006655854213568013,
                            "process_0": 0.005265880601589945,
                            "process_2": 0.006106161829369905,
                            "process_3": 0.005729860972773948
                        },
                        "ram_energy": {
                            "process_1": 5.495618779944095e-06,
                            "process_0": 4.242785923156107e-06,
                            "process_2": 5.170335896526262e-06,
                            "process_3": 4.52793140446593e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0076259539488166645,
                            "process_0": 0.005942084949950603,
                            "process_2": 0.0070009988959227306,
                            "process_3": 0.0065123847693659195
                        },
                        "total_energy_joules": {
                            "process_1": 27453.43421573999,
                            "process_0": 21391.50581982217,
                            "process_2": 25203.59602532183,
                            "process_3": 23444.58516971731
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 570.9694364196461,
                        "ram_power_avg": 0.6640623807907104,
                        "cpu_energy_total": 0.0033042282747500168,
                        "gpu_energy_total": 0.02375775761730181,
                        "ram_energy_total": 1.9436672004092392e-05,
                        "total_energy_kwh": 0.027081422564055914,
                        "total_energy_joules": 97493.12123060129
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.16805288202074062,
                        "joules_per_token": 5.950507887609942,
                        "flops_per_joule": 173858122.28803396,
                        "joules_per_flop": 5.751816405466991e-09
                    },
                    "per-process_emissions": [
                        0.0029051071568017084,
                        0.0022636372616836823,
                        0.002667030529401764,
                        0.002480892977889947
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0125": {
            "setup": {
                "experiment_id": "0125",
                "date_time": "April 11, 2025 at 01:50:49 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.5743614160001,
                        "average_latency_ms_per_batch": 2946.7951770000127,
                        "throughput_queries_per_sec": 5.42962745591596,
                        "throughput_tokens_per_sec": 694.9923143572429
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.2,
                        "cpu_memory_usage_bytes": 2012307456
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0125",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 9.292454238687876,
                            "process_0": 2132.583274972605,
                            "process_2": 5.057340908576486,
                            "process_3": 136.9446755587152
                        },
                        "ram_power": {
                            "process_1": 0.6507124900817871,
                            "process_0": 0.7010250091552734,
                            "process_2": 0.6518039703369141,
                            "process_3": 0.6527080535888672
                        },
                        "cpu_energy": {
                            "process_1": 0.0009646041164687063,
                            "process_0": 0.0006719615624375022,
                            "process_2": 0.0008896667306563017,
                            "process_3": 0.0007779958651875065
                        },
                        "gpu_energy": {
                            "process_1": 0.006655854213568013,
                            "process_0": 0.005265880601589945,
                            "process_2": 0.006106161829369905,
                            "process_3": 0.005729860972773948
                        },
                        "ram_energy": {
                            "process_1": 5.495618779944095e-06,
                            "process_0": 4.242785923156107e-06,
                            "process_2": 5.170335896526262e-06,
                            "process_3": 4.52793140446593e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0076259539488166645,
                            "process_0": 0.005942084949950603,
                            "process_2": 0.0070009988959227306,
                            "process_3": 0.0065123847693659195
                        },
                        "total_energy_joules": {
                            "process_1": 27453.43421573999,
                            "process_0": 21391.50581982217,
                            "process_2": 25203.59602532183,
                            "process_3": 23444.58516971731
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 570.9694364196461,
                        "ram_power_avg": 0.6640623807907104,
                        "cpu_energy_total": 0.0033042282747500168,
                        "gpu_energy_total": 0.02375775761730181,
                        "ram_energy_total": 1.9436672004092392e-05,
                        "total_energy_kwh": 0.027081422564055914,
                        "total_energy_joules": 97493.12123060129
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.16805288202074062,
                        "joules_per_token": 5.950507887609942,
                        "flops_per_joule": 173858122.28803396,
                        "joules_per_flop": 5.751816405466991e-09
                    },
                    "per-process_emissions": [
                        0.0029051071568017084,
                        0.0022636372616836823,
                        0.002667030529401764,
                        0.002480892977889947
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0126": {
            "setup": {
                "experiment_id": "0126",
                "date_time": "April 11, 2025 at 01:52:03 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.30218928499926,
                        "average_latency_ms_per_batch": 2912.7736606249073,
                        "throughput_queries_per_sec": 5.493046101140367,
                        "throughput_tokens_per_sec": 703.109900945967
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.1,
                        "cpu_memory_usage_bytes": 1992527872
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0126",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 11.212883879817413,
                            "process_3": 675.2676998335459,
                            "process_2": 752.5689573137257,
                            "process_0": 0.0
                        },
                        "ram_power": {
                            "process_1": 0.6532902717590332,
                            "process_3": 0.6531786918640137,
                            "process_2": 0.6470074653625488,
                            "process_0": 0.6938109397888184
                        },
                        "cpu_energy": {
                            "process_1": 0.0010880901969687216,
                            "process_3": 0.0008418836083125144,
                            "process_2": 0.0009466695074997348,
                            "process_0": 0.0007601655227499577
                        },
                        "gpu_energy": {
                            "process_1": 0.00680115321869601,
                            "process_3": 0.005704661230392011,
                            "process_2": 0.006108060442000018,
                            "process_0": 0.005040186532146024
                        },
                        "ram_energy": {
                            "process_1": 5.558624618247985e-06,
                            "process_3": 4.212816845920381e-06,
                            "process_2": 4.721320871859632e-06,
                            "process_0": 4.467374614346211e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.007894802040282981,
                            "process_3": 0.006550757655550445,
                            "process_2": 0.007059451270371614,
                            "process_0": 0.005804819429510329
                        },
                        "total_energy_joules": {
                            "process_1": 28421.287345018733,
                            "process_3": 23582.727559981602,
                            "process_2": 25414.02457333781,
                            "process_0": 20897.349946237187
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 359.7623852567723,
                        "ram_power_avg": 0.6618218421936035,
                        "cpu_energy_total": 0.0036368088355309284,
                        "gpu_energy_total": 0.023654061423234063,
                        "ram_energy_total": 1.8960136950374206e-05,
                        "total_energy_kwh": 0.02730983039571537,
                        "total_energy_joules": 98315.38942457533
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.16664735903395186,
                        "joules_per_token": 6.000695155308553,
                        "flops_per_joule": 172404046.73528266,
                        "joules_per_flop": 5.800327886360158e-09
                    },
                    "per-process_emissions": [
                        0.003007524837245802,
                        0.0024955111288819417,
                        0.0026892979614480668,
                        0.00221134596167196
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0126": {
            "setup": {
                "experiment_id": "0126",
                "date_time": "April 11, 2025 at 01:52:03 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.30218928499926,
                        "average_latency_ms_per_batch": 2912.7736606249073,
                        "throughput_queries_per_sec": 5.493046101140367,
                        "throughput_tokens_per_sec": 703.109900945967
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.1,
                        "cpu_memory_usage_bytes": 1992527872
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0126",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 11.212883879817413,
                            "process_3": 675.2676998335459,
                            "process_2": 752.5689573137257,
                            "process_0": 0.0
                        },
                        "ram_power": {
                            "process_1": 0.6532902717590332,
                            "process_3": 0.6531786918640137,
                            "process_2": 0.6470074653625488,
                            "process_0": 0.6938109397888184
                        },
                        "cpu_energy": {
                            "process_1": 0.0010880901969687216,
                            "process_3": 0.0008418836083125144,
                            "process_2": 0.0009466695074997348,
                            "process_0": 0.0007601655227499577
                        },
                        "gpu_energy": {
                            "process_1": 0.00680115321869601,
                            "process_3": 0.005704661230392011,
                            "process_2": 0.006108060442000018,
                            "process_0": 0.005040186532146024
                        },
                        "ram_energy": {
                            "process_1": 5.558624618247985e-06,
                            "process_3": 4.212816845920381e-06,
                            "process_2": 4.721320871859632e-06,
                            "process_0": 4.467374614346211e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.007894802040282981,
                            "process_3": 0.006550757655550445,
                            "process_2": 0.007059451270371614,
                            "process_0": 0.005804819429510329
                        },
                        "total_energy_joules": {
                            "process_1": 28421.287345018733,
                            "process_3": 23582.727559981602,
                            "process_2": 25414.02457333781,
                            "process_0": 20897.349946237187
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 359.7623852567723,
                        "ram_power_avg": 0.6618218421936035,
                        "cpu_energy_total": 0.0036368088355309284,
                        "gpu_energy_total": 0.023654061423234063,
                        "ram_energy_total": 1.8960136950374206e-05,
                        "total_energy_kwh": 0.02730983039571537,
                        "total_energy_joules": 98315.38942457533
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.16664735903395186,
                        "joules_per_token": 6.000695155308553,
                        "flops_per_joule": 172404046.73528266,
                        "joules_per_flop": 5.800327886360158e-09
                    },
                    "per-process_emissions": [
                        0.003007524837245802,
                        0.0024955111288819417,
                        0.0026892979614480668,
                        0.00221134596167196
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0127": {
            "setup": {
                "experiment_id": "0127",
                "date_time": "April 11, 2025 at 01:53:15 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 24.047178585999063,
                        "average_latency_ms_per_batch": 3005.897323249883,
                        "throughput_queries_per_sec": 5.322869772112274,
                        "throughput_tokens_per_sec": 681.3273308303711
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.1,
                        "cpu_memory_usage_bytes": 2027139072
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0127",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 27.024433239600942,
                            "process_0": 15.275659941683134,
                            "process_3": 23.198431756489995,
                            "process_1": 597.7241985384861
                        },
                        "ram_power": {
                            "process_2": 0.6517782211303711,
                            "process_0": 0.705604076385498,
                            "process_3": 0.6574287414550781,
                            "process_1": 0.6535005569458008
                        },
                        "cpu_energy": {
                            "process_2": 0.0009003121925625806,
                            "process_0": 0.0006703108479999285,
                            "process_3": 0.0009074311812811399,
                            "process_1": 0.0009094868724062621
                        },
                        "gpu_energy": {
                            "process_2": 0.00619947245957389,
                            "process_0": 0.005265961434988031,
                            "process_3": 0.006184107169503961,
                            "process_1": 0.006833754911444084
                        },
                        "ram_energy": {
                            "process_2": 5.217500786004036e-06,
                            "process_0": 4.201737887787978e-06,
                            "process_3": 5.305998410242266e-06,
                            "process_1": 5.150561741506247e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.007105002152922477,
                            "process_0": 0.005940474020875747,
                            "process_3": 0.007096844349195344,
                            "process_1": 0.007748392345591853
                        },
                        "total_energy_joules": {
                            "process_2": 25578.007750520916,
                            "process_0": 21385.70647515269,
                            "process_3": 25548.63965710324,
                            "process_1": 27894.21244413067
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 165.80568086906504,
                        "ram_power_avg": 0.667077898979187,
                        "cpu_energy_total": 0.0033875410942499113,
                        "gpu_energy_total": 0.024483295975509967,
                        "ram_energy_total": 1.9875798825540525e-05,
                        "total_energy_kwh": 0.02789071286858542,
                        "total_energy_joules": 100406.5663269075
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.163176578976482,
                        "joules_per_token": 6.128330464288788,
                        "flops_per_joule": 168813371.6072477,
                        "joules_per_flop": 5.923701366065641e-09
                    },
                    "per-process_emissions": [
                        0.0027066505701558176,
                        0.002263023578252616,
                        0.0027035428548259663,
                        0.0029517500640532162
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0127": {
            "setup": {
                "experiment_id": "0127",
                "date_time": "April 11, 2025 at 01:53:15 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 24.047178585999063,
                        "average_latency_ms_per_batch": 3005.897323249883,
                        "throughput_queries_per_sec": 5.322869772112274,
                        "throughput_tokens_per_sec": 681.3273308303711
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.1,
                        "cpu_memory_usage_bytes": 2027139072
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0127",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 27.024433239600942,
                            "process_0": 15.275659941683134,
                            "process_3": 23.198431756489995,
                            "process_1": 597.7241985384861
                        },
                        "ram_power": {
                            "process_2": 0.6517782211303711,
                            "process_0": 0.705604076385498,
                            "process_3": 0.6574287414550781,
                            "process_1": 0.6535005569458008
                        },
                        "cpu_energy": {
                            "process_2": 0.0009003121925625806,
                            "process_0": 0.0006703108479999285,
                            "process_3": 0.0009074311812811399,
                            "process_1": 0.0009094868724062621
                        },
                        "gpu_energy": {
                            "process_2": 0.00619947245957389,
                            "process_0": 0.005265961434988031,
                            "process_3": 0.006184107169503961,
                            "process_1": 0.006833754911444084
                        },
                        "ram_energy": {
                            "process_2": 5.217500786004036e-06,
                            "process_0": 4.201737887787978e-06,
                            "process_3": 5.305998410242266e-06,
                            "process_1": 5.150561741506247e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.007105002152922477,
                            "process_0": 0.005940474020875747,
                            "process_3": 0.007096844349195344,
                            "process_1": 0.007748392345591853
                        },
                        "total_energy_joules": {
                            "process_2": 25578.007750520916,
                            "process_0": 21385.70647515269,
                            "process_3": 25548.63965710324,
                            "process_1": 27894.21244413067
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 165.80568086906504,
                        "ram_power_avg": 0.667077898979187,
                        "cpu_energy_total": 0.0033875410942499113,
                        "gpu_energy_total": 0.024483295975509967,
                        "ram_energy_total": 1.9875798825540525e-05,
                        "total_energy_kwh": 0.02789071286858542,
                        "total_energy_joules": 100406.5663269075
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.163176578976482,
                        "joules_per_token": 6.128330464288788,
                        "flops_per_joule": 168813371.6072477,
                        "joules_per_flop": 5.923701366065641e-09
                    },
                    "per-process_emissions": [
                        0.0027066505701558176,
                        0.002263023578252616,
                        0.0027035428548259663,
                        0.0029517500640532162
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0128": {
            "setup": {
                "experiment_id": "0128",
                "date_time": "April 11, 2025 at 01:54:27 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 24.122499768001944,
                        "average_latency_ms_per_batch": 3015.312471000243,
                        "throughput_queries_per_sec": 5.3062494032973175,
                        "throughput_tokens_per_sec": 679.1999236220566
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.5,
                        "cpu_memory_usage_bytes": 1993207808
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0128",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 0.0,
                            "process_1": 498.84821827185505,
                            "process_0": 0.0,
                            "process_3": 1029.1310202175264
                        },
                        "ram_power": {
                            "process_2": 0.6545605659484863,
                            "process_1": 0.6532573699951172,
                            "process_0": 0.6938138008117677,
                            "process_3": 0.6505594253540039
                        },
                        "cpu_energy": {
                            "process_2": 0.0008955987656875094,
                            "process_1": 0.000965277695000225,
                            "process_0": 0.000819535222875004,
                            "process_3": 0.0008471590601564003
                        },
                        "gpu_energy": {
                            "process_2": 0.006156068535961984,
                            "process_1": 0.0068295843525520705,
                            "process_0": 0.00536495512529403,
                            "process_3": 0.005801293252141987
                        },
                        "ram_energy": {
                            "process_2": 4.83345449113777e-06,
                            "process_1": 5.519671833755312e-06,
                            "process_0": 4.308621055340632e-06,
                            "process_3": 4.235508513657156e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.00705650075614063,
                            "process_1": 0.00780038171938605,
                            "process_0": 0.006188798969224376,
                            "process_3": 0.006652687820812045
                        },
                        "total_energy_joules": {
                            "process_2": 25403.402722106268,
                            "process_1": 28081.37418978978,
                            "process_0": 22279.676289207753,
                            "process_3": 23949.676154923363
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 381.9948096223454,
                        "ram_power_avg": 0.6630477905273438,
                        "cpu_energy_total": 0.0035275707437191386,
                        "gpu_energy_total": 0.02415190126595007,
                        "ram_energy_total": 1.889725589389087e-05,
                        "total_energy_kwh": 0.0276983692655631,
                        "total_energy_joules": 99714.12935602717
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.16430971323533575,
                        "joules_per_token": 6.086067465577831,
                        "flops_per_joule": 169985649.0009805,
                        "joules_per_flop": 5.882849557460183e-09
                    },
                    "per-process_emissions": [
                        0.002688173963051773,
                        0.0029715554160001157,
                        0.002357622967326026,
                        0.0025343414253383486
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0128": {
            "setup": {
                "experiment_id": "0128",
                "date_time": "April 11, 2025 at 01:54:27 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 24.122499768001944,
                        "average_latency_ms_per_batch": 3015.312471000243,
                        "throughput_queries_per_sec": 5.3062494032973175,
                        "throughput_tokens_per_sec": 679.1999236220566
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.5,
                        "cpu_memory_usage_bytes": 1993207808
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0128",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 0.0,
                            "process_1": 498.84821827185505,
                            "process_0": 0.0,
                            "process_3": 1029.1310202175264
                        },
                        "ram_power": {
                            "process_2": 0.6545605659484863,
                            "process_1": 0.6532573699951172,
                            "process_0": 0.6938138008117677,
                            "process_3": 0.6505594253540039
                        },
                        "cpu_energy": {
                            "process_2": 0.0008955987656875094,
                            "process_1": 0.000965277695000225,
                            "process_0": 0.000819535222875004,
                            "process_3": 0.0008471590601564003
                        },
                        "gpu_energy": {
                            "process_2": 0.006156068535961984,
                            "process_1": 0.0068295843525520705,
                            "process_0": 0.00536495512529403,
                            "process_3": 0.005801293252141987
                        },
                        "ram_energy": {
                            "process_2": 4.83345449113777e-06,
                            "process_1": 5.519671833755312e-06,
                            "process_0": 4.308621055340632e-06,
                            "process_3": 4.235508513657156e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.00705650075614063,
                            "process_1": 0.00780038171938605,
                            "process_0": 0.006188798969224376,
                            "process_3": 0.006652687820812045
                        },
                        "total_energy_joules": {
                            "process_2": 25403.402722106268,
                            "process_1": 28081.37418978978,
                            "process_0": 22279.676289207753,
                            "process_3": 23949.676154923363
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 381.9948096223454,
                        "ram_power_avg": 0.6630477905273438,
                        "cpu_energy_total": 0.0035275707437191386,
                        "gpu_energy_total": 0.02415190126595007,
                        "ram_energy_total": 1.889725589389087e-05,
                        "total_energy_kwh": 0.0276983692655631,
                        "total_energy_joules": 99714.12935602717
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.16430971323533575,
                        "joules_per_token": 6.086067465577831,
                        "flops_per_joule": 169985649.0009805,
                        "joules_per_flop": 5.882849557460183e-09
                    },
                    "per-process_emissions": [
                        0.002688173963051773,
                        0.0029715554160001157,
                        0.002357622967326026,
                        0.0025343414253383486
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0129": {
            "setup": {
                "experiment_id": "0129",
                "date_time": "April 11, 2025 at 02:07:40 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.358511439999347,
                        "average_latency_ms_per_batch": 2919.8139299999184,
                        "throughput_queries_per_sec": 5.47980124199231,
                        "throughput_tokens_per_sec": 701.4145589750157
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.6,
                        "cpu_memory_usage_bytes": 2013167616
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0129",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 451.6823519951004,
                            "process_3": 773.0542318010312,
                            "process_0": 1009.1281413520077,
                            "process_2": 62.15754546504467
                        },
                        "ram_power": {
                            "process_1": 0.6505966186523438,
                            "process_3": 0.6483249664306641,
                            "process_0": 0.7012124061584473,
                            "process_2": 0.6476454734802246
                        },
                        "cpu_energy": {
                            "process_1": 0.01920413359000095,
                            "process_3": 0.01945451635356205,
                            "process_0": 0.018917432935312662,
                            "process_2": 0.019364159280562638
                        },
                        "gpu_energy": {
                            "process_1": 0.08847786633779212,
                            "process_3": 0.087116457470888,
                            "process_0": 0.08661857762813996,
                            "process_2": 0.08806192572726201
                        },
                        "ram_energy": {
                            "process_1": 0.00011095237706922414,
                            "process_3": 0.00011053850846133131,
                            "process_0": 0.00011611414483261811,
                            "process_2": 0.00010957454039212523
                        },
                        "total_energy_kwh": {
                            "process_1": 0.10779295230486215,
                            "process_3": 0.10668151233291133,
                            "process_0": 0.10565212470828525,
                            "process_2": 0.10753565954821681
                        },
                        "total_energy_joules": {
                            "process_1": 388054.62829750375,
                            "process_3": 384053.44439848076,
                            "process_0": 380347.6489498269,
                            "process_2": 387128.3743735805
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 574.005567653296,
                        "ram_power_avg": 0.6619448661804199,
                        "cpu_energy_total": 0.07694024215943829,
                        "gpu_energy_total": 0.3502748271640821,
                        "ram_energy_total": 0.0004471795707552988,
                        "total_energy_kwh": 0.42766224889427557,
                        "total_energy_joules": 1539584.096019392
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.010641835052960714,
                        "joules_per_token": 93.96875586055859,
                        "flops_per_joule": 11009447.965185076,
                        "joules_per_flop": 9.083107556003506e-08
                    },
                    "per-process_emissions": [
                        0.04106372518053723,
                        0.04064032212322257,
                        0.040248176907621266,
                        0.04096570950489319
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0129": {
            "setup": {
                "experiment_id": "0129",
                "date_time": "April 11, 2025 at 02:07:40 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.358511439999347,
                        "average_latency_ms_per_batch": 2919.8139299999184,
                        "throughput_queries_per_sec": 5.47980124199231,
                        "throughput_tokens_per_sec": 701.4145589750157
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.6,
                        "cpu_memory_usage_bytes": 2013167616
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0129",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 451.6823519951004,
                            "process_3": 773.0542318010312,
                            "process_0": 1009.1281413520077,
                            "process_2": 62.15754546504467
                        },
                        "ram_power": {
                            "process_1": 0.6505966186523438,
                            "process_3": 0.6483249664306641,
                            "process_0": 0.7012124061584473,
                            "process_2": 0.6476454734802246
                        },
                        "cpu_energy": {
                            "process_1": 0.01920413359000095,
                            "process_3": 0.01945451635356205,
                            "process_0": 0.018917432935312662,
                            "process_2": 0.019364159280562638
                        },
                        "gpu_energy": {
                            "process_1": 0.08847786633779212,
                            "process_3": 0.087116457470888,
                            "process_0": 0.08661857762813996,
                            "process_2": 0.08806192572726201
                        },
                        "ram_energy": {
                            "process_1": 0.00011095237706922414,
                            "process_3": 0.00011053850846133131,
                            "process_0": 0.00011611414483261811,
                            "process_2": 0.00010957454039212523
                        },
                        "total_energy_kwh": {
                            "process_1": 0.10779295230486215,
                            "process_3": 0.10668151233291133,
                            "process_0": 0.10565212470828525,
                            "process_2": 0.10753565954821681
                        },
                        "total_energy_joules": {
                            "process_1": 388054.62829750375,
                            "process_3": 384053.44439848076,
                            "process_0": 380347.6489498269,
                            "process_2": 387128.3743735805
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 574.005567653296,
                        "ram_power_avg": 0.6619448661804199,
                        "cpu_energy_total": 0.07694024215943829,
                        "gpu_energy_total": 0.3502748271640821,
                        "ram_energy_total": 0.0004471795707552988,
                        "total_energy_kwh": 0.42766224889427557,
                        "total_energy_joules": 1539584.096019392
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.010641835052960714,
                        "joules_per_token": 93.96875586055859,
                        "flops_per_joule": 11009447.965185076,
                        "joules_per_flop": 9.083107556003506e-08
                    },
                    "per-process_emissions": [
                        0.04106372518053723,
                        0.04064032212322257,
                        0.040248176907621266,
                        0.04096570950489319
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0130": {
            "setup": {
                "experiment_id": "0130",
                "date_time": "April 11, 2025 at 02:08:53 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.30423044300369,
                        "average_latency_ms_per_batch": 2913.028805375461,
                        "throughput_queries_per_sec": 5.492564979266573,
                        "throughput_tokens_per_sec": 703.0483173461214
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.9,
                        "cpu_memory_usage_bytes": 2011959296
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0130",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 840.6428238881521,
                            "process_0": 1201.7009860503501,
                            "process_1": 13.321082137965714,
                            "process_2": 1290.999517383447
                        },
                        "ram_power": {
                            "process_3": 0.6599321365356445,
                            "process_0": 0.700521469116211,
                            "process_1": 0.6554460525512695,
                            "process_2": 0.6500501632690431
                        },
                        "cpu_energy": {
                            "process_3": 0.0008463404789061997,
                            "process_0": 0.0007525448261876023,
                            "process_1": 0.0010711728677498514,
                            "process_2": 0.0008886942703438199
                        },
                        "gpu_energy": {
                            "process_3": 0.005747851264943926,
                            "process_0": 0.005062816550249982,
                            "process_1": 0.006817637954105976,
                            "process_2": 0.006170897158935956
                        },
                        "ram_energy": {
                            "process_3": 4.628044154846935e-06,
                            "process_0": 4.415437482378411e-06,
                            "process_1": 5.794607098775412e-06,
                            "process_2": 5.162480897917238e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.006598819788004972,
                            "process_0": 0.005819776813919962,
                            "process_1": 0.007894605428954603,
                            "process_2": 0.0070647539101776945
                        },
                        "total_energy_joules": {
                            "process_3": 23755.751236817898,
                            "process_0": 20951.196530111865,
                            "process_1": 28420.57954423657,
                            "process_2": 25433.1140766397
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 836.6661023649788,
                        "ram_power_avg": 0.666487455368042,
                        "cpu_energy_total": 0.003558752443187474,
                        "gpu_energy_total": 0.02379920292823584,
                        "ram_energy_total": 2.0000569633917996e-05,
                        "total_energy_kwh": 0.02737795594105723,
                        "total_energy_joules": 98560.64138780604
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.16623268446005704,
                        "joules_per_token": 6.015664147204959,
                        "flops_per_joule": 171975047.58982885,
                        "joules_per_flop": 5.8147970534950045e-09
                    },
                    "per-process_emissions": [
                        0.002513820398240494,
                        0.00221704397726281,
                        0.0030074499381602562,
                        0.0026913180020821928
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0130": {
            "setup": {
                "experiment_id": "0130",
                "date_time": "April 11, 2025 at 02:08:53 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.30423044300369,
                        "average_latency_ms_per_batch": 2913.028805375461,
                        "throughput_queries_per_sec": 5.492564979266573,
                        "throughput_tokens_per_sec": 703.0483173461214
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.9,
                        "cpu_memory_usage_bytes": 2011959296
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0130",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 840.6428238881521,
                            "process_0": 1201.7009860503501,
                            "process_1": 13.321082137965714,
                            "process_2": 1290.999517383447
                        },
                        "ram_power": {
                            "process_3": 0.6599321365356445,
                            "process_0": 0.700521469116211,
                            "process_1": 0.6554460525512695,
                            "process_2": 0.6500501632690431
                        },
                        "cpu_energy": {
                            "process_3": 0.0008463404789061997,
                            "process_0": 0.0007525448261876023,
                            "process_1": 0.0010711728677498514,
                            "process_2": 0.0008886942703438199
                        },
                        "gpu_energy": {
                            "process_3": 0.005747851264943926,
                            "process_0": 0.005062816550249982,
                            "process_1": 0.006817637954105976,
                            "process_2": 0.006170897158935956
                        },
                        "ram_energy": {
                            "process_3": 4.628044154846935e-06,
                            "process_0": 4.415437482378411e-06,
                            "process_1": 5.794607098775412e-06,
                            "process_2": 5.162480897917238e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.006598819788004972,
                            "process_0": 0.005819776813919962,
                            "process_1": 0.007894605428954603,
                            "process_2": 0.0070647539101776945
                        },
                        "total_energy_joules": {
                            "process_3": 23755.751236817898,
                            "process_0": 20951.196530111865,
                            "process_1": 28420.57954423657,
                            "process_2": 25433.1140766397
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 836.6661023649788,
                        "ram_power_avg": 0.666487455368042,
                        "cpu_energy_total": 0.003558752443187474,
                        "gpu_energy_total": 0.02379920292823584,
                        "ram_energy_total": 2.0000569633917996e-05,
                        "total_energy_kwh": 0.02737795594105723,
                        "total_energy_joules": 98560.64138780604
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.16623268446005704,
                        "joules_per_token": 6.015664147204959,
                        "flops_per_joule": 171975047.58982885,
                        "joules_per_flop": 5.8147970534950045e-09
                    },
                    "per-process_emissions": [
                        0.002513820398240494,
                        0.00221704397726281,
                        0.0030074499381602562,
                        0.0026913180020821928
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0132": {
            "setup": {
                "experiment_id": "0132",
                "date_time": "April 11, 2025 at 02:10:30 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.158800468001573,
                        "average_latency_ms_per_batch": 2894.8500585001966,
                        "throughput_queries_per_sec": 5.527056557910118,
                        "throughput_tokens_per_sec": 707.4632394124951
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 2001289216
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0132",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 1252.8297111531988,
                            "process_3": 57.2544055462669,
                            "process_2": 418.0610628743318,
                            "process_0": 1061.801104402552
                        },
                        "ram_power": {
                            "process_1": 0.6089358329772949,
                            "process_3": 0.6336665153503418,
                            "process_2": 0.6282620429992676,
                            "process_0": 0.6972870826721191
                        },
                        "cpu_energy": {
                            "process_1": 0.0006324847938750508,
                            "process_3": 0.0007959783547813118,
                            "process_2": 0.0008362435093748103,
                            "process_0": 0.0007216129968751375
                        },
                        "gpu_energy": {
                            "process_1": 0.0049559956314600695,
                            "process_3": 0.005311692304906079,
                            "process_2": 0.005387974588154154,
                            "process_0": 0.004824784137602067
                        },
                        "ram_energy": {
                            "process_1": 3.4796422092951e-06,
                            "process_3": 4.162911128687534e-06,
                            "process_2": 3.977976446570834e-06,
                            "process_0": 3.800220920839246e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005591960067544417,
                            "process_3": 0.006111833570816081,
                            "process_2": 0.006228196073975538,
                            "process_0": 0.005550197355398042
                        },
                        "total_energy_joules": {
                            "process_1": 20131.056243159903,
                            "process_3": 22002.60085493789,
                            "process_2": 22421.505866311934,
                            "process_0": 19980.710479432953
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 697.4865709940874,
                        "ram_power_avg": 0.6420378684997559,
                        "cpu_energy_total": 0.0029863196549063105,
                        "gpu_energy_total": 0.02048044666212237,
                        "ram_energy_total": 1.5420750705392714e-05,
                        "total_energy_kwh": 0.023482187067734077,
                        "total_energy_joules": 84535.87344384268
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.19381121093974285,
                        "joules_per_token": 5.159660244375163,
                        "flops_per_joule": 200506250.21828035,
                        "joules_per_flop": 4.987375699816609e-09
                    },
                    "per-process_emissions": [
                        0.0021302571877310455,
                        0.002328302998802386,
                        0.002372631294380981,
                        0.0021143476825388843
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0132": {
            "setup": {
                "experiment_id": "0132",
                "date_time": "April 11, 2025 at 02:10:30 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.158800468001573,
                        "average_latency_ms_per_batch": 2894.8500585001966,
                        "throughput_queries_per_sec": 5.527056557910118,
                        "throughput_tokens_per_sec": 707.4632394124951
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 2001289216
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0132",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 1252.8297111531988,
                            "process_3": 57.2544055462669,
                            "process_2": 418.0610628743318,
                            "process_0": 1061.801104402552
                        },
                        "ram_power": {
                            "process_1": 0.6089358329772949,
                            "process_3": 0.6336665153503418,
                            "process_2": 0.6282620429992676,
                            "process_0": 0.6972870826721191
                        },
                        "cpu_energy": {
                            "process_1": 0.0006324847938750508,
                            "process_3": 0.0007959783547813118,
                            "process_2": 0.0008362435093748103,
                            "process_0": 0.0007216129968751375
                        },
                        "gpu_energy": {
                            "process_1": 0.0049559956314600695,
                            "process_3": 0.005311692304906079,
                            "process_2": 0.005387974588154154,
                            "process_0": 0.004824784137602067
                        },
                        "ram_energy": {
                            "process_1": 3.4796422092951e-06,
                            "process_3": 4.162911128687534e-06,
                            "process_2": 3.977976446570834e-06,
                            "process_0": 3.800220920839246e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005591960067544417,
                            "process_3": 0.006111833570816081,
                            "process_2": 0.006228196073975538,
                            "process_0": 0.005550197355398042
                        },
                        "total_energy_joules": {
                            "process_1": 20131.056243159903,
                            "process_3": 22002.60085493789,
                            "process_2": 22421.505866311934,
                            "process_0": 19980.710479432953
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 697.4865709940874,
                        "ram_power_avg": 0.6420378684997559,
                        "cpu_energy_total": 0.0029863196549063105,
                        "gpu_energy_total": 0.02048044666212237,
                        "ram_energy_total": 1.5420750705392714e-05,
                        "total_energy_kwh": 0.023482187067734077,
                        "total_energy_joules": 84535.87344384268
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.19381121093974285,
                        "joules_per_token": 5.159660244375163,
                        "flops_per_joule": 200506250.21828035,
                        "joules_per_flop": 4.987375699816609e-09
                    },
                    "per-process_emissions": [
                        0.0021302571877310455,
                        0.002328302998802386,
                        0.002372631294380981,
                        0.0021143476825388843
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0133": {
            "setup": {
                "experiment_id": "0133",
                "date_time": "April 11, 2025 at 02:11:31 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.033007491005264,
                        "average_latency_ms_per_batch": 2879.125936375658,
                        "throughput_queries_per_sec": 5.557242146948718,
                        "throughput_tokens_per_sec": 711.3269948094359
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 1999810560
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0133",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 19.49086183449907,
                            "process_2": 1856.5490414943677,
                            "process_0": 2092.3935111820215,
                            "process_3": 13.165969564181015
                        },
                        "ram_power": {
                            "process_1": 0.6200723648071289,
                            "process_2": 0.6056070327758789,
                            "process_0": 0.6970853805541992,
                            "process_3": 0.6207017898559571
                        },
                        "cpu_energy": {
                            "process_1": 0.0006922648168123829,
                            "process_2": 0.000670683956187247,
                            "process_0": 0.0006585297720624226,
                            "process_3": 0.000787335929468668
                        },
                        "gpu_energy": {
                            "process_1": 0.00465576372460802,
                            "process_2": 0.004747841298270106,
                            "process_0": 0.00466093845097007,
                            "process_3": 0.005017582069618082
                        },
                        "ram_energy": {
                            "process_1": 3.823116872527485e-06,
                            "process_2": 3.331931863439469e-06,
                            "process_0": 4.313314533223849e-06,
                            "process_3": 4.36100579913946e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005351851658292929,
                            "process_2": 0.0054218571863207925,
                            "process_0": 0.005323781537565718,
                            "process_3": 0.00580927900488589
                        },
                        "total_energy_joules": {
                            "process_1": 19266.665969854545,
                            "process_2": 19518.685870754853,
                            "process_0": 19165.613535236585,
                            "process_3": 20913.404417589205
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 995.3998460187673,
                        "ram_power_avg": 0.635866641998291,
                        "cpu_energy_total": 0.0028088144745307205,
                        "gpu_energy_total": 0.019082125543466277,
                        "ram_energy_total": 1.5829369068330263e-05,
                        "total_energy_kwh": 0.02190676938706533,
                        "total_energy_joules": 78864.36979343518
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20774907658444047,
                        "joules_per_token": 4.8134991329000965,
                        "flops_per_joule": 214925587.2778552,
                        "joules_per_flop": 4.652773141930294e-09
                    },
                    "per-process_emissions": [
                        0.0020387878892266917,
                        0.002065456495128906,
                        0.0020280945767356603,
                        0.00221304483691128
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0133": {
            "setup": {
                "experiment_id": "0133",
                "date_time": "April 11, 2025 at 02:11:31 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.033007491005264,
                        "average_latency_ms_per_batch": 2879.125936375658,
                        "throughput_queries_per_sec": 5.557242146948718,
                        "throughput_tokens_per_sec": 711.3269948094359
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 1999810560
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0133",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 19.49086183449907,
                            "process_2": 1856.5490414943677,
                            "process_0": 2092.3935111820215,
                            "process_3": 13.165969564181015
                        },
                        "ram_power": {
                            "process_1": 0.6200723648071289,
                            "process_2": 0.6056070327758789,
                            "process_0": 0.6970853805541992,
                            "process_3": 0.6207017898559571
                        },
                        "cpu_energy": {
                            "process_1": 0.0006922648168123829,
                            "process_2": 0.000670683956187247,
                            "process_0": 0.0006585297720624226,
                            "process_3": 0.000787335929468668
                        },
                        "gpu_energy": {
                            "process_1": 0.00465576372460802,
                            "process_2": 0.004747841298270106,
                            "process_0": 0.00466093845097007,
                            "process_3": 0.005017582069618082
                        },
                        "ram_energy": {
                            "process_1": 3.823116872527485e-06,
                            "process_2": 3.331931863439469e-06,
                            "process_0": 4.313314533223849e-06,
                            "process_3": 4.36100579913946e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005351851658292929,
                            "process_2": 0.0054218571863207925,
                            "process_0": 0.005323781537565718,
                            "process_3": 0.00580927900488589
                        },
                        "total_energy_joules": {
                            "process_1": 19266.665969854545,
                            "process_2": 19518.685870754853,
                            "process_0": 19165.613535236585,
                            "process_3": 20913.404417589205
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 995.3998460187673,
                        "ram_power_avg": 0.635866641998291,
                        "cpu_energy_total": 0.0028088144745307205,
                        "gpu_energy_total": 0.019082125543466277,
                        "ram_energy_total": 1.5829369068330263e-05,
                        "total_energy_kwh": 0.02190676938706533,
                        "total_energy_joules": 78864.36979343518
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20774907658444047,
                        "joules_per_token": 4.8134991329000965,
                        "flops_per_joule": 214925587.2778552,
                        "joules_per_flop": 4.652773141930294e-09
                    },
                    "per-process_emissions": [
                        0.0020387878892266917,
                        0.002065456495128906,
                        0.0020280945767356603,
                        0.00221304483691128
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0134": {
            "setup": {
                "experiment_id": "0134",
                "date_time": "April 11, 2025 at 02:12:30 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.66479991799679,
                        "average_latency_ms_per_batch": 2833.0999897495985,
                        "throughput_queries_per_sec": 5.647523934167303,
                        "throughput_tokens_per_sec": 722.8830635734148
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            10.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.1,
                        "cpu_memory_usage_bytes": 1995395072
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0134",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 72.4940171708217,
                            "process_2": 69.41548309203304,
                            "process_0": 74.89972690334207,
                            "process_3": 349.1931427270656
                        },
                        "ram_power": {
                            "process_1": 0.632380485534668,
                            "process_2": 0.6182928085327148,
                            "process_0": 0.6956691741943359,
                            "process_3": 0.6187019348144531
                        },
                        "cpu_energy": {
                            "process_1": 0.0007206657930623806,
                            "process_2": 0.0007230659757188391,
                            "process_0": 0.0007190182851875306,
                            "process_3": 0.0007626157558437969
                        },
                        "gpu_energy": {
                            "process_1": 0.004665144287668038,
                            "process_2": 0.0046669187335320605,
                            "process_0": 0.004668275956840079,
                            "process_3": 0.00500980011894811
                        },
                        "ram_energy": {
                            "process_1": 3.8045979102607847e-06,
                            "process_2": 3.7441972638883097e-06,
                            "process_0": 4.209147376345436e-06,
                            "process_3": 3.935630058534177e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005389614678640679,
                            "process_2": 0.005393728906514786,
                            "process_0": 0.005391503389403954,
                            "process_3": 0.005776351504850438
                        },
                        "total_energy_joules": {
                            "process_1": 19402.612843106446,
                            "process_2": 19417.424063453233,
                            "process_0": 19409.412201854237,
                            "process_3": 20794.865417461577
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 141.5005924733156,
                        "ram_power_avg": 0.641261100769043,
                        "cpu_energy_total": 0.002925365809812547,
                        "gpu_energy_total": 0.019010139096988288,
                        "ram_energy_total": 1.569357260902871e-05,
                        "total_energy_kwh": 0.021951198479409857,
                        "total_energy_joules": 79024.3145258755
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20732859371573883,
                        "joules_per_token": 4.8232613846359556,
                        "flops_per_joule": 214490579.19511533,
                        "joules_per_flop": 4.662209425479389e-09
                    },
                    "per-process_emissions": [
                        0.002053173711828167,
                        0.002054741026936808,
                        0.002053893216193436,
                        0.0022005011057727743
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0134": {
            "setup": {
                "experiment_id": "0134",
                "date_time": "April 11, 2025 at 02:12:30 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.66479991799679,
                        "average_latency_ms_per_batch": 2833.0999897495985,
                        "throughput_queries_per_sec": 5.647523934167303,
                        "throughput_tokens_per_sec": 722.8830635734148
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            10.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.1,
                        "cpu_memory_usage_bytes": 1995395072
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0134",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 72.4940171708217,
                            "process_2": 69.41548309203304,
                            "process_0": 74.89972690334207,
                            "process_3": 349.1931427270656
                        },
                        "ram_power": {
                            "process_1": 0.632380485534668,
                            "process_2": 0.6182928085327148,
                            "process_0": 0.6956691741943359,
                            "process_3": 0.6187019348144531
                        },
                        "cpu_energy": {
                            "process_1": 0.0007206657930623806,
                            "process_2": 0.0007230659757188391,
                            "process_0": 0.0007190182851875306,
                            "process_3": 0.0007626157558437969
                        },
                        "gpu_energy": {
                            "process_1": 0.004665144287668038,
                            "process_2": 0.0046669187335320605,
                            "process_0": 0.004668275956840079,
                            "process_3": 0.00500980011894811
                        },
                        "ram_energy": {
                            "process_1": 3.8045979102607847e-06,
                            "process_2": 3.7441972638883097e-06,
                            "process_0": 4.209147376345436e-06,
                            "process_3": 3.935630058534177e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005389614678640679,
                            "process_2": 0.005393728906514786,
                            "process_0": 0.005391503389403954,
                            "process_3": 0.005776351504850438
                        },
                        "total_energy_joules": {
                            "process_1": 19402.612843106446,
                            "process_2": 19417.424063453233,
                            "process_0": 19409.412201854237,
                            "process_3": 20794.865417461577
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 141.5005924733156,
                        "ram_power_avg": 0.641261100769043,
                        "cpu_energy_total": 0.002925365809812547,
                        "gpu_energy_total": 0.019010139096988288,
                        "ram_energy_total": 1.569357260902871e-05,
                        "total_energy_kwh": 0.021951198479409857,
                        "total_energy_joules": 79024.3145258755
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20732859371573883,
                        "joules_per_token": 4.8232613846359556,
                        "flops_per_joule": 214490579.19511533,
                        "joules_per_flop": 4.662209425479389e-09
                    },
                    "per-process_emissions": [
                        0.002053173711828167,
                        0.002054741026936808,
                        0.002053893216193436,
                        0.0022005011057727743
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0135": {
            "setup": {
                "experiment_id": "0135",
                "date_time": "April 11, 2025 at 02:13:34 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.77387880400238,
                        "average_latency_ms_per_batch": 2846.7348505002974,
                        "throughput_queries_per_sec": 5.620474276762407,
                        "throughput_tokens_per_sec": 719.4207074255881
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            5.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 1999060992
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0135",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 784.5413198973698,
                            "process_3": 13.512987061407134,
                            "process_2": 793.9970616557735,
                            "process_1": 788.8413560617138
                        },
                        "ram_power": {
                            "process_0": 0.696042537689209,
                            "process_3": 0.6204400062561035,
                            "process_2": 0.6194057464599609,
                            "process_1": 0.6252193450927734
                        },
                        "cpu_energy": {
                            "process_0": 0.000603648209781113,
                            "process_3": 0.0008028562660312561,
                            "process_2": 0.0006054091344688005,
                            "process_1": 0.00065916028512504
                        },
                        "gpu_energy": {
                            "process_0": 0.004607664519461874,
                            "process_3": 0.005003678169606091,
                            "process_2": 0.004625142033444024,
                            "process_1": 0.004634511763161986
                        },
                        "ram_energy": {
                            "process_0": 3.86425158672148e-06,
                            "process_3": 4.458788448286871e-06,
                            "process_2": 3.395811692357707e-06,
                            "process_1": 3.7299843372720845e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005215176980829706,
                            "process_3": 0.005810993224085633,
                            "process_2": 0.005233946979605182,
                            "process_1": 0.005297402032624298
                        },
                        "total_energy_joules": {
                            "process_0": 18774.637130986943,
                            "process_3": 20919.57560670828,
                            "process_2": 18842.209126578655,
                            "process_1": 19070.647317447474
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 595.223181169066,
                        "ram_power_avg": 0.6402769088745117,
                        "cpu_energy_total": 0.0026710738954062096,
                        "gpu_energy_total": 0.018870996485673974,
                        "ram_energy_total": 1.544883606463814e-05,
                        "total_energy_kwh": 0.02155751921714482,
                        "total_energy_joules": 77607.06918172137
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2111147885463363,
                        "joules_per_token": 4.736759593610923,
                        "flops_per_joule": 218407564.8245739,
                        "joules_per_flop": 4.578595987749808e-09
                    },
                    "per-process_emissions": [
                        0.0019867216708470766,
                        0.0022136978687154222,
                        0.001993872101880594,
                        0.0020180453043282262
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0135": {
            "setup": {
                "experiment_id": "0135",
                "date_time": "April 11, 2025 at 02:13:34 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.77387880400238,
                        "average_latency_ms_per_batch": 2846.7348505002974,
                        "throughput_queries_per_sec": 5.620474276762407,
                        "throughput_tokens_per_sec": 719.4207074255881
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            5.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 1999060992
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0135",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 784.5413198973698,
                            "process_3": 13.512987061407134,
                            "process_2": 793.9970616557735,
                            "process_1": 788.8413560617138
                        },
                        "ram_power": {
                            "process_0": 0.696042537689209,
                            "process_3": 0.6204400062561035,
                            "process_2": 0.6194057464599609,
                            "process_1": 0.6252193450927734
                        },
                        "cpu_energy": {
                            "process_0": 0.000603648209781113,
                            "process_3": 0.0008028562660312561,
                            "process_2": 0.0006054091344688005,
                            "process_1": 0.00065916028512504
                        },
                        "gpu_energy": {
                            "process_0": 0.004607664519461874,
                            "process_3": 0.005003678169606091,
                            "process_2": 0.004625142033444024,
                            "process_1": 0.004634511763161986
                        },
                        "ram_energy": {
                            "process_0": 3.86425158672148e-06,
                            "process_3": 4.458788448286871e-06,
                            "process_2": 3.395811692357707e-06,
                            "process_1": 3.7299843372720845e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005215176980829706,
                            "process_3": 0.005810993224085633,
                            "process_2": 0.005233946979605182,
                            "process_1": 0.005297402032624298
                        },
                        "total_energy_joules": {
                            "process_0": 18774.637130986943,
                            "process_3": 20919.57560670828,
                            "process_2": 18842.209126578655,
                            "process_1": 19070.647317447474
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 595.223181169066,
                        "ram_power_avg": 0.6402769088745117,
                        "cpu_energy_total": 0.0026710738954062096,
                        "gpu_energy_total": 0.018870996485673974,
                        "ram_energy_total": 1.544883606463814e-05,
                        "total_energy_kwh": 0.02155751921714482,
                        "total_energy_joules": 77607.06918172137
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2111147885463363,
                        "joules_per_token": 4.736759593610923,
                        "flops_per_joule": 218407564.8245739,
                        "joules_per_flop": 4.578595987749808e-09
                    },
                    "per-process_emissions": [
                        0.0019867216708470766,
                        0.0022136978687154222,
                        0.001993872101880594,
                        0.0020180453043282262
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0136": {
            "setup": {
                "experiment_id": "0136",
                "date_time": "April 11, 2025 at 02:14:36 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.60634716399727,
                        "average_latency_ms_per_batch": 2825.793395499659,
                        "throughput_queries_per_sec": 5.662126617424155,
                        "throughput_tokens_per_sec": 724.7522070302919
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            8.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.0,
                        "cpu_memory_usage_bytes": 1997430784
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0136",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 157.8604187194188,
                            "process_0": 151.4145681134888,
                            "process_2": 0.0,
                            "process_3": 650.7882497661321
                        },
                        "ram_power": {
                            "process_1": 0.6315779685974121,
                            "process_0": 0.695559024810791,
                            "process_2": 0.6380653381347657,
                            "process_3": 0.6175274848937988
                        },
                        "cpu_energy": {
                            "process_1": 0.0007469751612811138,
                            "process_0": 0.0007363189046875505,
                            "process_2": 0.000821647945562404,
                            "process_3": 0.000819306909531349
                        },
                        "gpu_energy": {
                            "process_1": 0.004679484299140085,
                            "process_0": 0.004679484299140085,
                            "process_2": 0.004679484299140085,
                            "process_3": 0.004984428709762151
                        },
                        "ram_energy": {
                            "process_1": 3.5962784079508422e-06,
                            "process_0": 3.912002147851651e-06,
                            "process_2": 4.027285782343191e-06,
                            "process_3": 3.887395479722526e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005430055738829148,
                            "process_0": 0.005419715205975489,
                            "process_2": 0.005505159530484832,
                            "process_3": 0.005807623014773225
                        },
                        "total_energy_joules": {
                            "process_1": 19548.200659784932,
                            "process_0": 19510.97474151176,
                            "process_2": 19818.574309745396,
                            "process_3": 20907.44285318361
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 240.01580914975995,
                        "ram_power_avg": 0.6456824541091919,
                        "cpu_energy_total": 0.003124248921062417,
                        "gpu_energy_total": 0.019022881607182407,
                        "ram_energy_total": 1.542296181786821e-05,
                        "total_energy_kwh": 0.022162553490062693,
                        "total_energy_joules": 79785.1925642257
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2053513875624373,
                        "joules_per_token": 4.869701694593854,
                        "flops_per_joule": 212445072.1793717,
                        "joules_per_flop": 4.707099062084526e-09
                    },
                    "per-process_emissions": [
                        0.0020685797337069638,
                        0.0020646405077163624,
                        0.002097190523138197,
                        0.00221241398747786
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0136": {
            "setup": {
                "experiment_id": "0136",
                "date_time": "April 11, 2025 at 02:14:36 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.60634716399727,
                        "average_latency_ms_per_batch": 2825.793395499659,
                        "throughput_queries_per_sec": 5.662126617424155,
                        "throughput_tokens_per_sec": 724.7522070302919
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            8.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.0,
                        "cpu_memory_usage_bytes": 1997430784
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0136",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 157.8604187194188,
                            "process_0": 151.4145681134888,
                            "process_2": 0.0,
                            "process_3": 650.7882497661321
                        },
                        "ram_power": {
                            "process_1": 0.6315779685974121,
                            "process_0": 0.695559024810791,
                            "process_2": 0.6380653381347657,
                            "process_3": 0.6175274848937988
                        },
                        "cpu_energy": {
                            "process_1": 0.0007469751612811138,
                            "process_0": 0.0007363189046875505,
                            "process_2": 0.000821647945562404,
                            "process_3": 0.000819306909531349
                        },
                        "gpu_energy": {
                            "process_1": 0.004679484299140085,
                            "process_0": 0.004679484299140085,
                            "process_2": 0.004679484299140085,
                            "process_3": 0.004984428709762151
                        },
                        "ram_energy": {
                            "process_1": 3.5962784079508422e-06,
                            "process_0": 3.912002147851651e-06,
                            "process_2": 4.027285782343191e-06,
                            "process_3": 3.887395479722526e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005430055738829148,
                            "process_0": 0.005419715205975489,
                            "process_2": 0.005505159530484832,
                            "process_3": 0.005807623014773225
                        },
                        "total_energy_joules": {
                            "process_1": 19548.200659784932,
                            "process_0": 19510.97474151176,
                            "process_2": 19818.574309745396,
                            "process_3": 20907.44285318361
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 240.01580914975995,
                        "ram_power_avg": 0.6456824541091919,
                        "cpu_energy_total": 0.003124248921062417,
                        "gpu_energy_total": 0.019022881607182407,
                        "ram_energy_total": 1.542296181786821e-05,
                        "total_energy_kwh": 0.022162553490062693,
                        "total_energy_joules": 79785.1925642257
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2053513875624373,
                        "joules_per_token": 4.869701694593854,
                        "flops_per_joule": 212445072.1793717,
                        "joules_per_flop": 4.707099062084526e-09
                    },
                    "per-process_emissions": [
                        0.0020685797337069638,
                        0.0020646405077163624,
                        0.002097190523138197,
                        0.00221241398747786
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0137": {
            "setup": {
                "experiment_id": "0137",
                "date_time": "April 11, 2025 at 02:15:38 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.664170010995804,
                        "average_latency_ms_per_batch": 2958.0212513744755,
                        "throughput_queries_per_sec": 5.409021315369331,
                        "throughput_tokens_per_sec": 692.3547283672743
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 2021974016
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0137",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 501.32335290236654,
                            "process_0": 1161.348552683999,
                            "process_1": 998.4349034881761,
                            "process_2": 1060.0370735447354
                        },
                        "ram_power": {
                            "process_3": 0.6694049835205078,
                            "process_0": 0.704542636871338,
                            "process_1": 0.6591510772705078,
                            "process_2": 0.6775088310241699
                        },
                        "cpu_energy": {
                            "process_3": 0.0007724389415312203,
                            "process_0": 0.0006906106445936189,
                            "process_1": 0.0006879601333124583,
                            "process_2": 0.0006736379683126189
                        },
                        "gpu_energy": {
                            "process_3": 0.005130717715681937,
                            "process_0": 0.004789715776213982,
                            "process_1": 0.004807074401212003,
                            "process_2": 0.004743952961825948
                        },
                        "ram_energy": {
                            "process_3": 4.576332020416307e-06,
                            "process_0": 3.990490024727617e-06,
                            "process_1": 4.0637830399958625e-06,
                            "process_2": 4.119777318241846e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005907732989233574,
                            "process_0": 0.0054843169108323275,
                            "process_1": 0.0054990983175644565,
                            "process_2": 0.005421710707456807
                        },
                        "total_energy_joules": {
                            "process_3": 21267.838761240866,
                            "process_0": 19743.54087899638,
                            "process_1": 19796.753943232045,
                            "process_2": 19518.158546844505
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 930.2859706548193,
                        "ram_power_avg": 0.6776518821716309,
                        "cpu_energy_total": 0.0028246476877499165,
                        "gpu_energy_total": 0.01947146085493387,
                        "ram_energy_total": 1.6750382403381632e-05,
                        "total_energy_kwh": 0.022312858925087168,
                        "total_energy_joules": 80326.29213031378
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20396808523690035,
                        "joules_per_token": 4.902727791156847,
                        "flops_per_joule": 211013984.88123876,
                        "joules_per_flop": 4.7390223949508e-09
                    },
                    "per-process_emissions": [
                        0.0022505508822485303,
                        0.0020892505271815753,
                        0.00209488150407618,
                        0.002065400694005671
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0137": {
            "setup": {
                "experiment_id": "0137",
                "date_time": "April 11, 2025 at 02:15:38 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.664170010995804,
                        "average_latency_ms_per_batch": 2958.0212513744755,
                        "throughput_queries_per_sec": 5.409021315369331,
                        "throughput_tokens_per_sec": 692.3547283672743
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 2021974016
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0137",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 501.32335290236654,
                            "process_0": 1161.348552683999,
                            "process_1": 998.4349034881761,
                            "process_2": 1060.0370735447354
                        },
                        "ram_power": {
                            "process_3": 0.6694049835205078,
                            "process_0": 0.704542636871338,
                            "process_1": 0.6591510772705078,
                            "process_2": 0.6775088310241699
                        },
                        "cpu_energy": {
                            "process_3": 0.0007724389415312203,
                            "process_0": 0.0006906106445936189,
                            "process_1": 0.0006879601333124583,
                            "process_2": 0.0006736379683126189
                        },
                        "gpu_energy": {
                            "process_3": 0.005130717715681937,
                            "process_0": 0.004789715776213982,
                            "process_1": 0.004807074401212003,
                            "process_2": 0.004743952961825948
                        },
                        "ram_energy": {
                            "process_3": 4.576332020416307e-06,
                            "process_0": 3.990490024727617e-06,
                            "process_1": 4.0637830399958625e-06,
                            "process_2": 4.119777318241846e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005907732989233574,
                            "process_0": 0.0054843169108323275,
                            "process_1": 0.0054990983175644565,
                            "process_2": 0.005421710707456807
                        },
                        "total_energy_joules": {
                            "process_3": 21267.838761240866,
                            "process_0": 19743.54087899638,
                            "process_1": 19796.753943232045,
                            "process_2": 19518.158546844505
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 930.2859706548193,
                        "ram_power_avg": 0.6776518821716309,
                        "cpu_energy_total": 0.0028246476877499165,
                        "gpu_energy_total": 0.01947146085493387,
                        "ram_energy_total": 1.6750382403381632e-05,
                        "total_energy_kwh": 0.022312858925087168,
                        "total_energy_joules": 80326.29213031378
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20396808523690035,
                        "joules_per_token": 4.902727791156847,
                        "flops_per_joule": 211013984.88123876,
                        "joules_per_flop": 4.7390223949508e-09
                    },
                    "per-process_emissions": [
                        0.0022505508822485303,
                        0.0020892505271815753,
                        0.00209488150407618,
                        0.002065400694005671
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0138": {
            "setup": {
                "experiment_id": "0138",
                "date_time": "April 11, 2025 at 02:16:46 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.90777740699923,
                        "average_latency_ms_per_batch": 2988.4721758749038,
                        "throughput_queries_per_sec": 5.3539062967236255,
                        "throughput_tokens_per_sec": 685.3000059806241
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.0,
                        "cpu_memory_usage_bytes": 2016477184
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0138",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 0.0,
                            "process_1": 1979.2217777673447,
                            "process_3": 442.5034238909419,
                            "process_0": 850.5690254385306
                        },
                        "ram_power": {
                            "process_2": 0.6697711944580078,
                            "process_1": 0.6665997505187988,
                            "process_3": 0.6692476272583009,
                            "process_0": 0.7018589973449707
                        },
                        "cpu_energy": {
                            "process_2": 0.0007810545352812141,
                            "process_1": 0.0007202502751562747,
                            "process_3": 0.0008156110272498154,
                            "process_0": 0.0007447960221874154
                        },
                        "gpu_energy": {
                            "process_2": 0.004819741078012085,
                            "process_1": 0.004796646615091965,
                            "process_3": 0.005118373816918015,
                            "process_0": 0.004793891890665952
                        },
                        "ram_energy": {
                            "process_2": 4.263711955326052e-06,
                            "process_1": 3.994235323491134e-06,
                            "process_3": 4.146772202833309e-06,
                            "process_0": 4.294750628623333e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005605059325248626,
                            "process_1": 0.005520891125571733,
                            "process_3": 0.005938131616370662,
                            "process_0": 0.00554298266348199
                        },
                        "total_energy_joules": {
                            "process_2": 20178.21357089505,
                            "process_1": 19875.20805205824,
                            "process_3": 21377.273818934384,
                            "process_0": 19954.737588535165
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 818.0735567742042,
                        "ram_power_avg": 0.6768693923950195,
                        "cpu_energy_total": 0.0030617118598747196,
                        "gpu_energy_total": 0.019528653400688017,
                        "ram_energy_total": 1.669947011027383e-05,
                        "total_energy_kwh": 0.02260706473067301,
                        "total_energy_joules": 81385.43303042284
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20131366744556692,
                        "joules_per_token": 4.967372621485769,
                        "flops_per_joule": 208267872.541956,
                        "joules_per_flop": 4.801508690681746e-09
                    },
                    "per-process_emissions": [
                        0.002135247349953464,
                        0.0021031834742865517,
                        0.002262131239256404,
                        0.002111599245653464
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0138": {
            "setup": {
                "experiment_id": "0138",
                "date_time": "April 11, 2025 at 02:16:46 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.90777740699923,
                        "average_latency_ms_per_batch": 2988.4721758749038,
                        "throughput_queries_per_sec": 5.3539062967236255,
                        "throughput_tokens_per_sec": 685.3000059806241
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.0,
                        "cpu_memory_usage_bytes": 2016477184
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0138",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 0.0,
                            "process_1": 1979.2217777673447,
                            "process_3": 442.5034238909419,
                            "process_0": 850.5690254385306
                        },
                        "ram_power": {
                            "process_2": 0.6697711944580078,
                            "process_1": 0.6665997505187988,
                            "process_3": 0.6692476272583009,
                            "process_0": 0.7018589973449707
                        },
                        "cpu_energy": {
                            "process_2": 0.0007810545352812141,
                            "process_1": 0.0007202502751562747,
                            "process_3": 0.0008156110272498154,
                            "process_0": 0.0007447960221874154
                        },
                        "gpu_energy": {
                            "process_2": 0.004819741078012085,
                            "process_1": 0.004796646615091965,
                            "process_3": 0.005118373816918015,
                            "process_0": 0.004793891890665952
                        },
                        "ram_energy": {
                            "process_2": 4.263711955326052e-06,
                            "process_1": 3.994235323491134e-06,
                            "process_3": 4.146772202833309e-06,
                            "process_0": 4.294750628623333e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005605059325248626,
                            "process_1": 0.005520891125571733,
                            "process_3": 0.005938131616370662,
                            "process_0": 0.00554298266348199
                        },
                        "total_energy_joules": {
                            "process_2": 20178.21357089505,
                            "process_1": 19875.20805205824,
                            "process_3": 21377.273818934384,
                            "process_0": 19954.737588535165
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 818.0735567742042,
                        "ram_power_avg": 0.6768693923950195,
                        "cpu_energy_total": 0.0030617118598747196,
                        "gpu_energy_total": 0.019528653400688017,
                        "ram_energy_total": 1.669947011027383e-05,
                        "total_energy_kwh": 0.02260706473067301,
                        "total_energy_joules": 81385.43303042284
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20131366744556692,
                        "joules_per_token": 4.967372621485769,
                        "flops_per_joule": 208267872.541956,
                        "joules_per_flop": 4.801508690681746e-09
                    },
                    "per-process_emissions": [
                        0.002135247349953464,
                        0.0021031834742865517,
                        0.002262131239256404,
                        0.002111599245653464
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0139": {
            "setup": {
                "experiment_id": "0139",
                "date_time": "April 11, 2025 at 02:17:47 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.56195289599782,
                        "average_latency_ms_per_batch": 2945.2441119997275,
                        "throughput_queries_per_sec": 5.432486881074352,
                        "throughput_tokens_per_sec": 695.3583207775171
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 2019176448
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0139",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 466.270993515815,
                            "process_1": 730.472629893447,
                            "process_0": 823.3372130627316,
                            "process_2": 702.7595213118028
                        },
                        "ram_power": {
                            "process_3": 0.6761155128479004,
                            "process_1": 0.6682147979736328,
                            "process_0": 0.7031807899475099,
                            "process_2": 0.6620821952819824
                        },
                        "cpu_energy": {
                            "process_3": 0.0008470391947186046,
                            "process_1": 0.0006956208704686446,
                            "process_0": 0.0006910844844375674,
                            "process_2": 0.0007012743510937299
                        },
                        "gpu_energy": {
                            "process_3": 0.005135158552568048,
                            "process_1": 0.004750874078473993,
                            "process_0": 0.0047425176829000715,
                            "process_2": 0.004789706609539923
                        },
                        "ram_energy": {
                            "process_3": 4.762172969120463e-06,
                            "process_1": 4.195458873005194e-06,
                            "process_0": 4.380354930979813e-06,
                            "process_2": 4.1719215404475015e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0059869599202557735,
                            "process_1": 0.0054506904078156435,
                            "process_0": 0.005437982522268619,
                            "process_2": 0.005495152882174099
                        },
                        "total_energy_joules": {
                            "process_3": 21553.055712920785,
                            "process_1": 19622.485468136318,
                            "process_0": 19576.73708016703,
                            "process_2": 19782.550375826755
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 680.7100894459492,
                        "ram_power_avg": 0.6773983240127563,
                        "cpu_energy_total": 0.002935018900718547,
                        "gpu_energy_total": 0.019418256923482036,
                        "ram_energy_total": 1.750990831355297e-05,
                        "total_energy_kwh": 0.022370785732514137,
                        "total_energy_joules": 80534.82863705089
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2034399312356936,
                        "joules_per_token": 4.915455849429375,
                        "flops_per_joule": 210467586.2606106,
                        "joules_per_flop": 4.751325454750805e-09
                    },
                    "per-process_emissions": [
                        0.002280732381621437,
                        0.0020764405108573696,
                        0.0020715994418582305,
                        0.002093378490464223
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0139": {
            "setup": {
                "experiment_id": "0139",
                "date_time": "April 11, 2025 at 02:17:47 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.56195289599782,
                        "average_latency_ms_per_batch": 2945.2441119997275,
                        "throughput_queries_per_sec": 5.432486881074352,
                        "throughput_tokens_per_sec": 695.3583207775171
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 2019176448
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0139",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 466.270993515815,
                            "process_1": 730.472629893447,
                            "process_0": 823.3372130627316,
                            "process_2": 702.7595213118028
                        },
                        "ram_power": {
                            "process_3": 0.6761155128479004,
                            "process_1": 0.6682147979736328,
                            "process_0": 0.7031807899475099,
                            "process_2": 0.6620821952819824
                        },
                        "cpu_energy": {
                            "process_3": 0.0008470391947186046,
                            "process_1": 0.0006956208704686446,
                            "process_0": 0.0006910844844375674,
                            "process_2": 0.0007012743510937299
                        },
                        "gpu_energy": {
                            "process_3": 0.005135158552568048,
                            "process_1": 0.004750874078473993,
                            "process_0": 0.0047425176829000715,
                            "process_2": 0.004789706609539923
                        },
                        "ram_energy": {
                            "process_3": 4.762172969120463e-06,
                            "process_1": 4.195458873005194e-06,
                            "process_0": 4.380354930979813e-06,
                            "process_2": 4.1719215404475015e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0059869599202557735,
                            "process_1": 0.0054506904078156435,
                            "process_0": 0.005437982522268619,
                            "process_2": 0.005495152882174099
                        },
                        "total_energy_joules": {
                            "process_3": 21553.055712920785,
                            "process_1": 19622.485468136318,
                            "process_0": 19576.73708016703,
                            "process_2": 19782.550375826755
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 680.7100894459492,
                        "ram_power_avg": 0.6773983240127563,
                        "cpu_energy_total": 0.002935018900718547,
                        "gpu_energy_total": 0.019418256923482036,
                        "ram_energy_total": 1.750990831355297e-05,
                        "total_energy_kwh": 0.022370785732514137,
                        "total_energy_joules": 80534.82863705089
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2034399312356936,
                        "joules_per_token": 4.915455849429375,
                        "flops_per_joule": 210467586.2606106,
                        "joules_per_flop": 4.751325454750805e-09
                    },
                    "per-process_emissions": [
                        0.002280732381621437,
                        0.0020764405108573696,
                        0.0020715994418582305,
                        0.002093378490464223
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0140": {
            "setup": {
                "experiment_id": "0140",
                "date_time": "April 11, 2025 at 02:18:50 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.84014259899959,
                        "average_latency_ms_per_batch": 2980.017824874949,
                        "throughput_queries_per_sec": 5.369095401525462,
                        "throughput_tokens_per_sec": 687.2442113952592
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 2020286464
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0140",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 52.69694783633006,
                            "process_1": 805.7945652894658,
                            "process_0": 865.1945091036393,
                            "process_2": 799.4593066425557
                        },
                        "ram_power": {
                            "process_3": 0.6635355949401855,
                            "process_1": 0.6631579399108887,
                            "process_0": 0.7031164169311525,
                            "process_2": 0.6694693565368652
                        },
                        "cpu_energy": {
                            "process_3": 0.0008889588664061989,
                            "process_1": 0.0007738253625935269,
                            "process_0": 0.0007111750614686798,
                            "process_2": 0.0007128017772501494
                        },
                        "gpu_energy": {
                            "process_3": 0.005144687726858083,
                            "process_1": 0.004759636029928099,
                            "process_0": 0.004759636029928099,
                            "process_2": 0.004759636029928099
                        },
                        "ram_energy": {
                            "process_3": 4.578020309892218e-06,
                            "process_1": 3.890997022726399e-06,
                            "process_0": 4.131662849189254e-06,
                            "process_2": 3.9235782133287755e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.006038224613574173,
                            "process_1": 0.005537352389544353,
                            "process_0": 0.005474942754245968,
                            "process_2": 0.00547636138539158
                        },
                        "total_energy_joules": {
                            "process_3": 21737.608608867024,
                            "process_1": 19934.46860235967,
                            "process_0": 19709.793915285485,
                            "process_2": 19714.90098740969
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 630.7863322179978,
                        "ram_power_avg": 0.674819827079773,
                        "cpu_energy_total": 0.003086761067718555,
                        "gpu_energy_total": 0.01942359581664238,
                        "ram_energy_total": 1.6524258395136647e-05,
                        "total_energy_kwh": 0.022526881142756073,
                        "total_energy_joules": 81096.77211392188
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20203023588885063,
                        "joules_per_token": 4.949754157343865,
                        "flops_per_joule": 209009194.21726528,
                        "joules_per_flop": 4.78447851897127e-09
                    },
                    "per-process_emissions": [
                        0.0023002616665410815,
                        0.0021094543927969215,
                        0.0020856794422300013,
                        0.0020862198697649222
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0140": {
            "setup": {
                "experiment_id": "0140",
                "date_time": "April 11, 2025 at 02:18:50 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.84014259899959,
                        "average_latency_ms_per_batch": 2980.017824874949,
                        "throughput_queries_per_sec": 5.369095401525462,
                        "throughput_tokens_per_sec": 687.2442113952592
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 2020286464
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0140",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 52.69694783633006,
                            "process_1": 805.7945652894658,
                            "process_0": 865.1945091036393,
                            "process_2": 799.4593066425557
                        },
                        "ram_power": {
                            "process_3": 0.6635355949401855,
                            "process_1": 0.6631579399108887,
                            "process_0": 0.7031164169311525,
                            "process_2": 0.6694693565368652
                        },
                        "cpu_energy": {
                            "process_3": 0.0008889588664061989,
                            "process_1": 0.0007738253625935269,
                            "process_0": 0.0007111750614686798,
                            "process_2": 0.0007128017772501494
                        },
                        "gpu_energy": {
                            "process_3": 0.005144687726858083,
                            "process_1": 0.004759636029928099,
                            "process_0": 0.004759636029928099,
                            "process_2": 0.004759636029928099
                        },
                        "ram_energy": {
                            "process_3": 4.578020309892218e-06,
                            "process_1": 3.890997022726399e-06,
                            "process_0": 4.131662849189254e-06,
                            "process_2": 3.9235782133287755e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.006038224613574173,
                            "process_1": 0.005537352389544353,
                            "process_0": 0.005474942754245968,
                            "process_2": 0.00547636138539158
                        },
                        "total_energy_joules": {
                            "process_3": 21737.608608867024,
                            "process_1": 19934.46860235967,
                            "process_0": 19709.793915285485,
                            "process_2": 19714.90098740969
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 630.7863322179978,
                        "ram_power_avg": 0.674819827079773,
                        "cpu_energy_total": 0.003086761067718555,
                        "gpu_energy_total": 0.01942359581664238,
                        "ram_energy_total": 1.6524258395136647e-05,
                        "total_energy_kwh": 0.022526881142756073,
                        "total_energy_joules": 81096.77211392188
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20203023588885063,
                        "joules_per_token": 4.949754157343865,
                        "flops_per_joule": 209009194.21726528,
                        "joules_per_flop": 4.78447851897127e-09
                    },
                    "per-process_emissions": [
                        0.0023002616665410815,
                        0.0021094543927969215,
                        0.0020856794422300013,
                        0.0020862198697649222
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0141": {
            "setup": {
                "experiment_id": "0141",
                "date_time": "April 11, 2025 at 02:19:51 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.885973160000503,
                        "average_latency_ms_per_batch": 2985.746645000063,
                        "throughput_queries_per_sec": 5.358793595830923,
                        "throughput_tokens_per_sec": 685.9255802663581
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 2017255424
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0141",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 0.0,
                            "process_2": 933.9578987464005,
                            "process_0": 932.9717104495447,
                            "process_3": 471.7603998465799
                        },
                        "ram_power": {
                            "process_1": 0.6644368171691895,
                            "process_2": 0.6651692390441895,
                            "process_0": 0.7021679878234863,
                            "process_3": 0.6641721725463868
                        },
                        "cpu_energy": {
                            "process_1": 0.0007293373627186385,
                            "process_2": 0.0006854742505937566,
                            "process_0": 0.0006838813139999615,
                            "process_3": 0.000770020609812434
                        },
                        "gpu_energy": {
                            "process_1": 0.004787191607527969,
                            "process_2": 0.004748861576863966,
                            "process_0": 0.004772869651626044,
                            "process_3": 0.005097577411392024
                        },
                        "ram_energy": {
                            "process_1": 4.269822508152762e-06,
                            "process_2": 4.0802009774111404e-06,
                            "process_0": 4.3032814863919974e-06,
                            "process_3": 4.551125788617844e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005520798792754761,
                            "process_2": 0.005438416028435133,
                            "process_0": 0.005461054247112397,
                            "process_3": 0.005872149146993074
                        },
                        "total_energy_joules": {
                            "process_1": 19874.875653917137,
                            "process_2": 19578.29770236648,
                            "process_0": 19659.79528960463,
                            "process_3": 21139.736929175066
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 584.6725022606313,
                        "ram_power_avg": 0.673986554145813,
                        "cpu_energy_total": 0.00286871353712479,
                        "gpu_energy_total": 0.019406500247410002,
                        "ram_energy_total": 1.7204430760573746e-05,
                        "total_energy_kwh": 0.022292418215295366,
                        "total_energy_joules": 80252.70557506332
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20415511081648754,
                        "joules_per_token": 4.8982364242592356,
                        "flops_per_joule": 211207471.0963865,
                        "joules_per_flop": 4.734680997830994e-09
                    },
                    "per-process_emissions": [
                        0.0021031483000999262,
                        0.002071764586032364,
                        0.0020803886154374678,
                        0.0022369952175470115
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0141": {
            "setup": {
                "experiment_id": "0141",
                "date_time": "April 11, 2025 at 02:19:51 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.885973160000503,
                        "average_latency_ms_per_batch": 2985.746645000063,
                        "throughput_queries_per_sec": 5.358793595830923,
                        "throughput_tokens_per_sec": 685.9255802663581
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 2017255424
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0141",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 0.0,
                            "process_2": 933.9578987464005,
                            "process_0": 932.9717104495447,
                            "process_3": 471.7603998465799
                        },
                        "ram_power": {
                            "process_1": 0.6644368171691895,
                            "process_2": 0.6651692390441895,
                            "process_0": 0.7021679878234863,
                            "process_3": 0.6641721725463868
                        },
                        "cpu_energy": {
                            "process_1": 0.0007293373627186385,
                            "process_2": 0.0006854742505937566,
                            "process_0": 0.0006838813139999615,
                            "process_3": 0.000770020609812434
                        },
                        "gpu_energy": {
                            "process_1": 0.004787191607527969,
                            "process_2": 0.004748861576863966,
                            "process_0": 0.004772869651626044,
                            "process_3": 0.005097577411392024
                        },
                        "ram_energy": {
                            "process_1": 4.269822508152762e-06,
                            "process_2": 4.0802009774111404e-06,
                            "process_0": 4.3032814863919974e-06,
                            "process_3": 4.551125788617844e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005520798792754761,
                            "process_2": 0.005438416028435133,
                            "process_0": 0.005461054247112397,
                            "process_3": 0.005872149146993074
                        },
                        "total_energy_joules": {
                            "process_1": 19874.875653917137,
                            "process_2": 19578.29770236648,
                            "process_0": 19659.79528960463,
                            "process_3": 21139.736929175066
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 584.6725022606313,
                        "ram_power_avg": 0.673986554145813,
                        "cpu_energy_total": 0.00286871353712479,
                        "gpu_energy_total": 0.019406500247410002,
                        "ram_energy_total": 1.7204430760573746e-05,
                        "total_energy_kwh": 0.022292418215295366,
                        "total_energy_joules": 80252.70557506332
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20415511081648754,
                        "joules_per_token": 4.8982364242592356,
                        "flops_per_joule": 211207471.0963865,
                        "joules_per_flop": 4.734680997830994e-09
                    },
                    "per-process_emissions": [
                        0.0021031483000999262,
                        0.002071764586032364,
                        0.0020803886154374678,
                        0.0022369952175470115
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0142": {
            "setup": {
                "experiment_id": "0142",
                "date_time": "April 11, 2025 at 02:20:54 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.299355080003807,
                        "average_latency_ms_per_batch": 2912.419385000476,
                        "throughput_queries_per_sec": 5.493714292111603,
                        "throughput_tokens_per_sec": 703.1954293902852
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 2019057664
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0142",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 480.6203702165769,
                            "process_1": 24.60414420295019,
                            "process_2": 655.2690691798326,
                            "process_0": 737.5308576890952
                        },
                        "ram_power": {
                            "process_3": 0.6694822311401367,
                            "process_1": 0.6707267761230469,
                            "process_2": 0.6671061515808107,
                            "process_0": 0.703070640563965
                        },
                        "cpu_energy": {
                            "process_3": 0.0007946000165937335,
                            "process_1": 0.0007646459147812836,
                            "process_2": 0.0007416559067811476,
                            "process_0": 0.0006914449573751541
                        },
                        "gpu_energy": {
                            "process_3": 0.005104343250137949,
                            "process_1": 0.004785480217269961,
                            "process_2": 0.004795913003393881,
                            "process_0": 0.004654521501392034
                        },
                        "ram_energy": {
                            "process_3": 4.419582637131168e-06,
                            "process_1": 4.191254633849063e-06,
                            "process_2": 4.063569672040781e-06,
                            "process_0": 4.039593251352526e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0059033628493688145,
                            "process_1": 0.005554317386685093,
                            "process_2": 0.0055416324798470695,
                            "process_0": 0.0053500060520185405
                        },
                        "total_energy_joules": {
                            "process_3": 21252.106257727733,
                            "process_1": 19995.542592066337,
                            "process_2": 19949.87692744945,
                            "process_0": 19260.021787266745
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 474.5061103221137,
                        "ram_power_avg": 0.6775964498519897,
                        "cpu_energy_total": 0.0029923467955313187,
                        "gpu_energy_total": 0.019340257972193825,
                        "ram_energy_total": 1.671400019437354e-05,
                        "total_energy_kwh": 0.022349318767919516,
                        "total_energy_joules": 80457.54756451027
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20363533933051375,
                        "joules_per_token": 4.910738987091691,
                        "flops_per_joule": 210669744.55767047,
                        "joules_per_flop": 4.746766091636152e-09
                    },
                    "per-process_emissions": [
                        0.00224888607746705,
                        0.002115917208457686,
                        0.002111084893197741,
                        0.002038084805516463
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0142": {
            "setup": {
                "experiment_id": "0142",
                "date_time": "April 11, 2025 at 02:20:54 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.299355080003807,
                        "average_latency_ms_per_batch": 2912.419385000476,
                        "throughput_queries_per_sec": 5.493714292111603,
                        "throughput_tokens_per_sec": 703.1954293902852
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 2019057664
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0142",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 480.6203702165769,
                            "process_1": 24.60414420295019,
                            "process_2": 655.2690691798326,
                            "process_0": 737.5308576890952
                        },
                        "ram_power": {
                            "process_3": 0.6694822311401367,
                            "process_1": 0.6707267761230469,
                            "process_2": 0.6671061515808107,
                            "process_0": 0.703070640563965
                        },
                        "cpu_energy": {
                            "process_3": 0.0007946000165937335,
                            "process_1": 0.0007646459147812836,
                            "process_2": 0.0007416559067811476,
                            "process_0": 0.0006914449573751541
                        },
                        "gpu_energy": {
                            "process_3": 0.005104343250137949,
                            "process_1": 0.004785480217269961,
                            "process_2": 0.004795913003393881,
                            "process_0": 0.004654521501392034
                        },
                        "ram_energy": {
                            "process_3": 4.419582637131168e-06,
                            "process_1": 4.191254633849063e-06,
                            "process_2": 4.063569672040781e-06,
                            "process_0": 4.039593251352526e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0059033628493688145,
                            "process_1": 0.005554317386685093,
                            "process_2": 0.0055416324798470695,
                            "process_0": 0.0053500060520185405
                        },
                        "total_energy_joules": {
                            "process_3": 21252.106257727733,
                            "process_1": 19995.542592066337,
                            "process_2": 19949.87692744945,
                            "process_0": 19260.021787266745
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 474.5061103221137,
                        "ram_power_avg": 0.6775964498519897,
                        "cpu_energy_total": 0.0029923467955313187,
                        "gpu_energy_total": 0.019340257972193825,
                        "ram_energy_total": 1.671400019437354e-05,
                        "total_energy_kwh": 0.022349318767919516,
                        "total_energy_joules": 80457.54756451027
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20363533933051375,
                        "joules_per_token": 4.910738987091691,
                        "flops_per_joule": 210669744.55767047,
                        "joules_per_flop": 4.746766091636152e-09
                    },
                    "per-process_emissions": [
                        0.00224888607746705,
                        0.002115917208457686,
                        0.002111084893197741,
                        0.002038084805516463
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0143": {
            "setup": {
                "experiment_id": "0143",
                "date_time": "April 11, 2025 at 02:21:57 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.477821705997485,
                        "average_latency_ms_per_batch": 2934.7277132496856,
                        "throughput_queries_per_sec": 5.451953831274815,
                        "throughput_tokens_per_sec": 697.8500904031763
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 2020814848
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0143",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 833.9718633050586,
                            "process_3": 498.3228012931064,
                            "process_2": 613.8322133749165,
                            "process_1": 721.5325218608932
                        },
                        "ram_power": {
                            "process_0": 0.7041034698486328,
                            "process_3": 0.6649289131164551,
                            "process_2": 0.6718897819519043,
                            "process_1": 0.6676239967346191
                        },
                        "cpu_energy": {
                            "process_0": 0.0007540727435937242,
                            "process_3": 0.0007901908371562742,
                            "process_2": 0.0007841080919998264,
                            "process_1": 0.0007751250458437084
                        },
                        "gpu_energy": {
                            "process_0": 0.004696069312408041,
                            "process_3": 0.0050928743520739506,
                            "process_2": 0.004791618555513966,
                            "process_1": 0.004759231585159979
                        },
                        "ram_energy": {
                            "process_0": 4.06819300593836e-06,
                            "process_3": 4.36688987342613e-06,
                            "process_2": 4.13703782777828e-06,
                            "process_1": 3.933244505764589e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005454210249007703,
                            "process_3": 0.0058874320791036504,
                            "process_2": 0.005579863685341569,
                            "process_1": 0.00553828987550945
                        },
                        "total_energy_joules": {
                            "process_0": 19635.15689642773,
                            "process_3": 21194.75548477314,
                            "process_2": 20087.50926722965,
                            "process_1": 19937.84355183402
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 666.9148499584937,
                        "ram_power_avg": 0.6771365404129028,
                        "cpu_energy_total": 0.003103496718593533,
                        "gpu_energy_total": 0.019339793805155936,
                        "ram_energy_total": 1.650536521290736e-05,
                        "total_energy_kwh": 0.022459795888962374,
                        "total_energy_joules": 80855.26520026455
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20263368080507382,
                        "joules_per_token": 4.935013745133334,
                        "flops_per_joule": 209633484.61191544,
                        "joules_per_flop": 4.770230299092021e-09
                    },
                    "per-process_emissions": [
                        0.002077781394359484,
                        0.0022428172505345356,
                        0.002125649070930871,
                        0.002109811528075325
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0143": {
            "setup": {
                "experiment_id": "0143",
                "date_time": "April 11, 2025 at 02:21:57 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.477821705997485,
                        "average_latency_ms_per_batch": 2934.7277132496856,
                        "throughput_queries_per_sec": 5.451953831274815,
                        "throughput_tokens_per_sec": 697.8500904031763
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 2020814848
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0143",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 833.9718633050586,
                            "process_3": 498.3228012931064,
                            "process_2": 613.8322133749165,
                            "process_1": 721.5325218608932
                        },
                        "ram_power": {
                            "process_0": 0.7041034698486328,
                            "process_3": 0.6649289131164551,
                            "process_2": 0.6718897819519043,
                            "process_1": 0.6676239967346191
                        },
                        "cpu_energy": {
                            "process_0": 0.0007540727435937242,
                            "process_3": 0.0007901908371562742,
                            "process_2": 0.0007841080919998264,
                            "process_1": 0.0007751250458437084
                        },
                        "gpu_energy": {
                            "process_0": 0.004696069312408041,
                            "process_3": 0.0050928743520739506,
                            "process_2": 0.004791618555513966,
                            "process_1": 0.004759231585159979
                        },
                        "ram_energy": {
                            "process_0": 4.06819300593836e-06,
                            "process_3": 4.36688987342613e-06,
                            "process_2": 4.13703782777828e-06,
                            "process_1": 3.933244505764589e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005454210249007703,
                            "process_3": 0.0058874320791036504,
                            "process_2": 0.005579863685341569,
                            "process_1": 0.00553828987550945
                        },
                        "total_energy_joules": {
                            "process_0": 19635.15689642773,
                            "process_3": 21194.75548477314,
                            "process_2": 20087.50926722965,
                            "process_1": 19937.84355183402
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 666.9148499584937,
                        "ram_power_avg": 0.6771365404129028,
                        "cpu_energy_total": 0.003103496718593533,
                        "gpu_energy_total": 0.019339793805155936,
                        "ram_energy_total": 1.650536521290736e-05,
                        "total_energy_kwh": 0.022459795888962374,
                        "total_energy_joules": 80855.26520026455
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20263368080507382,
                        "joules_per_token": 4.935013745133334,
                        "flops_per_joule": 209633484.61191544,
                        "joules_per_flop": 4.770230299092021e-09
                    },
                    "per-process_emissions": [
                        0.002077781394359484,
                        0.0022428172505345356,
                        0.002125649070930871,
                        0.002109811528075325
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0144": {
            "setup": {
                "experiment_id": "0144",
                "date_time": "April 11, 2025 at 02:22:59 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.895986305000406,
                        "average_latency_ms_per_batch": 2986.9982881250507,
                        "throughput_queries_per_sec": 5.35654809833964,
                        "throughput_tokens_per_sec": 685.6381565874739
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 12633243648,
                        "gpu_max_memory_reserved_bytes": 12633243648
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.7,
                        "cpu_memory_usage_bytes": 2017898496
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0144",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 840.1448879077376,
                            "process_2": 1723.878013194361,
                            "process_1": 1161.2924604305088,
                            "process_3": 451.21778318626957
                        },
                        "ram_power": {
                            "process_0": 0.7033324241638185,
                            "process_2": 0.6689243316650391,
                            "process_1": 0.6685080528259277,
                            "process_3": 0.6679587364196777
                        },
                        "cpu_energy": {
                            "process_0": 0.0006851915824061054,
                            "process_2": 0.0006626490940003009,
                            "process_1": 0.0006319188219376885,
                            "process_3": 0.0007637089564061059
                        },
                        "gpu_energy": {
                            "process_0": 0.004805256621980003,
                            "process_2": 0.004775129097877973,
                            "process_1": 0.004774742986457914,
                            "process_3": 0.005120597985364045
                        },
                        "ram_energy": {
                            "process_0": 4.333011358137573e-06,
                            "process_2": 4.030072742216119e-06,
                            "process_1": 3.821206670363545e-06,
                            "process_3": 4.551182382362415e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005494781215744247,
                            "process_2": 0.005441808264620489,
                            "process_1": 0.005410483015065966,
                            "process_3": 0.005888858124152511
                        },
                        "total_energy_joules": {
                            "process_0": 19781.21237667929,
                            "process_2": 19590.50975263376,
                            "process_1": 19477.738854237476,
                            "process_3": 21199.88924694904
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1044.133286179719,
                        "ram_power_avg": 0.6771808862686157,
                        "cpu_energy_total": 0.0027434684547502004,
                        "gpu_energy_total": 0.019475726691679934,
                        "ram_energy_total": 1.6735473153079652e-05,
                        "total_energy_kwh": 0.022235930619583212,
                        "total_energy_joules": 80049.35023049956
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20467374129612284,
                        "joules_per_token": 4.885824599029514,
                        "flops_per_joule": 211744017.21369502,
                        "joules_per_flop": 4.722683611838657e-09
                    },
                    "per-process_emissions": [
                        0.002093236904137771,
                        0.0020730568584071754,
                        0.0020611235045893796,
                        0.002243360502395899
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0144": {
            "setup": {
                "experiment_id": "0144",
                "date_time": "April 11, 2025 at 02:22:59 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.895986305000406,
                        "average_latency_ms_per_batch": 2986.9982881250507,
                        "throughput_queries_per_sec": 5.35654809833964,
                        "throughput_tokens_per_sec": 685.6381565874739
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 12633243648,
                        "gpu_max_memory_reserved_bytes": 12633243648
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.7,
                        "cpu_memory_usage_bytes": 2017898496
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0144",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 840.1448879077376,
                            "process_2": 1723.878013194361,
                            "process_1": 1161.2924604305088,
                            "process_3": 451.21778318626957
                        },
                        "ram_power": {
                            "process_0": 0.7033324241638185,
                            "process_2": 0.6689243316650391,
                            "process_1": 0.6685080528259277,
                            "process_3": 0.6679587364196777
                        },
                        "cpu_energy": {
                            "process_0": 0.0006851915824061054,
                            "process_2": 0.0006626490940003009,
                            "process_1": 0.0006319188219376885,
                            "process_3": 0.0007637089564061059
                        },
                        "gpu_energy": {
                            "process_0": 0.004805256621980003,
                            "process_2": 0.004775129097877973,
                            "process_1": 0.004774742986457914,
                            "process_3": 0.005120597985364045
                        },
                        "ram_energy": {
                            "process_0": 4.333011358137573e-06,
                            "process_2": 4.030072742216119e-06,
                            "process_1": 3.821206670363545e-06,
                            "process_3": 4.551182382362415e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005494781215744247,
                            "process_2": 0.005441808264620489,
                            "process_1": 0.005410483015065966,
                            "process_3": 0.005888858124152511
                        },
                        "total_energy_joules": {
                            "process_0": 19781.21237667929,
                            "process_2": 19590.50975263376,
                            "process_1": 19477.738854237476,
                            "process_3": 21199.88924694904
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1044.133286179719,
                        "ram_power_avg": 0.6771808862686157,
                        "cpu_energy_total": 0.0027434684547502004,
                        "gpu_energy_total": 0.019475726691679934,
                        "ram_energy_total": 1.6735473153079652e-05,
                        "total_energy_kwh": 0.022235930619583212,
                        "total_energy_joules": 80049.35023049956
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20467374129612284,
                        "joules_per_token": 4.885824599029514,
                        "flops_per_joule": 211744017.21369502,
                        "joules_per_flop": 4.722683611838657e-09
                    },
                    "per-process_emissions": [
                        0.002093236904137771,
                        0.0020730568584071754,
                        0.0020611235045893796,
                        0.002243360502395899
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0145": {
            "setup": {
                "experiment_id": "0145",
                "date_time": "April 11, 2025 at 02:24:04 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.826980337998975,
                        "average_latency_ms_per_batch": 2978.372542249872,
                        "throughput_queries_per_sec": 5.372061343244035,
                        "throughput_tokens_per_sec": 687.6238519352365
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.9,
                        "cpu_memory_usage_bytes": 2019971072
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0145",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1024.9737455743289,
                            "process_3": 397.3480746057014,
                            "process_2": 1029.9789163259445,
                            "process_1": 0.0
                        },
                        "ram_power": {
                            "process_0": 0.7034282684326173,
                            "process_3": 0.6645913124084473,
                            "process_2": 0.6647143363952637,
                            "process_1": 0.6637845039367676
                        },
                        "cpu_energy": {
                            "process_0": 0.0006455158581250658,
                            "process_3": 0.0007364789607499914,
                            "process_2": 0.0006465250825311273,
                            "process_1": 0.0006865342317499311
                        },
                        "gpu_energy": {
                            "process_0": 0.004779702990426016,
                            "process_3": 0.005120571596453932,
                            "process_2": 0.004779807712731954,
                            "process_1": 0.004792760223093995
                        },
                        "ram_energy": {
                            "process_0": 4.069508696813071e-06,
                            "process_3": 4.366533063564484e-06,
                            "process_2": 3.857524507717949e-06,
                            "process_1": 4.0159582587003685e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005429288357247894,
                            "process_3": 0.005861417090267488,
                            "process_2": 0.005430190319770799,
                            "process_1": 0.005483310413102626
                        },
                        "total_energy_joules": {
                            "process_0": 19545.438086092418,
                            "process_3": 21101.101524962956,
                            "process_2": 19548.685151174876,
                            "process_1": 19739.917487169452
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 613.0751841264937,
                        "ram_power_avg": 0.6741296052932739,
                        "cpu_energy_total": 0.0027150541331561156,
                        "gpu_energy_total": 0.019472842522705897,
                        "ram_energy_total": 1.630952452679587e-05,
                        "total_energy_kwh": 0.022204206180388805,
                        "total_energy_joules": 79935.1422493997
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20496617055964572,
                        "joules_per_token": 4.878853897058087,
                        "flops_per_joule": 212046548.19112796,
                        "joules_per_flop": 4.715945666319694e-09
                    },
                    "per-process_emissions": [
                        0.0020682873996935853,
                        0.0022329068405373994,
                        0.002068631002316686,
                        0.002088867101871445
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0145": {
            "setup": {
                "experiment_id": "0145",
                "date_time": "April 11, 2025 at 02:24:04 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.826980337998975,
                        "average_latency_ms_per_batch": 2978.372542249872,
                        "throughput_queries_per_sec": 5.372061343244035,
                        "throughput_tokens_per_sec": 687.6238519352365
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.9,
                        "cpu_memory_usage_bytes": 2019971072
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0145",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1024.9737455743289,
                            "process_3": 397.3480746057014,
                            "process_2": 1029.9789163259445,
                            "process_1": 0.0
                        },
                        "ram_power": {
                            "process_0": 0.7034282684326173,
                            "process_3": 0.6645913124084473,
                            "process_2": 0.6647143363952637,
                            "process_1": 0.6637845039367676
                        },
                        "cpu_energy": {
                            "process_0": 0.0006455158581250658,
                            "process_3": 0.0007364789607499914,
                            "process_2": 0.0006465250825311273,
                            "process_1": 0.0006865342317499311
                        },
                        "gpu_energy": {
                            "process_0": 0.004779702990426016,
                            "process_3": 0.005120571596453932,
                            "process_2": 0.004779807712731954,
                            "process_1": 0.004792760223093995
                        },
                        "ram_energy": {
                            "process_0": 4.069508696813071e-06,
                            "process_3": 4.366533063564484e-06,
                            "process_2": 3.857524507717949e-06,
                            "process_1": 4.0159582587003685e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005429288357247894,
                            "process_3": 0.005861417090267488,
                            "process_2": 0.005430190319770799,
                            "process_1": 0.005483310413102626
                        },
                        "total_energy_joules": {
                            "process_0": 19545.438086092418,
                            "process_3": 21101.101524962956,
                            "process_2": 19548.685151174876,
                            "process_1": 19739.917487169452
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 613.0751841264937,
                        "ram_power_avg": 0.6741296052932739,
                        "cpu_energy_total": 0.0027150541331561156,
                        "gpu_energy_total": 0.019472842522705897,
                        "ram_energy_total": 1.630952452679587e-05,
                        "total_energy_kwh": 0.022204206180388805,
                        "total_energy_joules": 79935.1422493997
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20496617055964572,
                        "joules_per_token": 4.878853897058087,
                        "flops_per_joule": 212046548.19112796,
                        "joules_per_flop": 4.715945666319694e-09
                    },
                    "per-process_emissions": [
                        0.0020682873996935853,
                        0.0022329068405373994,
                        0.002068631002316686,
                        0.002088867101871445
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0146": {
            "setup": {
                "experiment_id": "0146",
                "date_time": "April 11, 2025 at 02:25:08 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 24.004102631000933,
                        "average_latency_ms_per_batch": 3000.5128288751166,
                        "throughput_queries_per_sec": 5.332421793376685,
                        "throughput_tokens_per_sec": 682.5499895522157
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.9,
                        "cpu_memory_usage_bytes": 2018074624
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0146",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 0.0,
                            "process_3": 395.6761373427577,
                            "process_2": 8.759802809501618,
                            "process_1": 98.37984908075603
                        },
                        "ram_power": {
                            "process_0": 0.7034740447998048,
                            "process_3": 0.6650018692016603,
                            "process_2": 0.664698600769043,
                            "process_1": 0.6718826293945312
                        },
                        "cpu_energy": {
                            "process_0": 0.0007898428050623352,
                            "process_3": 0.000816569330499874,
                            "process_2": 0.0007975650880624699,
                            "process_1": 0.0007155958250621097
                        },
                        "gpu_energy": {
                            "process_0": 0.00477967299040194,
                            "process_3": 0.005071850724143984,
                            "process_2": 0.004783711326965923,
                            "process_1": 0.004773653541141898
                        },
                        "ram_energy": {
                            "process_0": 4.562099946205827e-06,
                            "process_3": 4.497835428280775e-06,
                            "process_2": 4.337585807044994e-06,
                            "process_1": 3.9869732473855865e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005574077895410481,
                            "process_3": 0.005892917890072138,
                            "process_2": 0.005585614000835438,
                            "process_1": 0.0054932363394513935
                        },
                        "total_energy_joules": {
                            "process_0": 20066.680423477734,
                            "process_3": 21214.504404259696,
                            "process_2": 20108.210403007575,
                            "process_1": 19775.650822025018
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 125.70394730825384,
                        "ram_power_avg": 0.6762642860412598,
                        "cpu_energy_total": 0.003119573048686789,
                        "gpu_energy_total": 0.019408888582653744,
                        "ram_energy_total": 1.7384494428917182e-05,
                        "total_energy_kwh": 0.02254584612576945,
                        "total_energy_joules": 81165.04605277002
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20186029327634247,
                        "joules_per_token": 4.953921267869264,
                        "flops_per_joule": 208833381.085398,
                        "joules_per_flop": 4.788506486858397e-09
                    },
                    "per-process_emissions": [
                        0.002123444974256623,
                        0.002244907070222981,
                        0.00212783965361826,
                        0.002092648383514008
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0146": {
            "setup": {
                "experiment_id": "0146",
                "date_time": "April 11, 2025 at 02:25:08 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 24.004102631000933,
                        "average_latency_ms_per_batch": 3000.5128288751166,
                        "throughput_queries_per_sec": 5.332421793376685,
                        "throughput_tokens_per_sec": 682.5499895522157
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.9,
                        "cpu_memory_usage_bytes": 2018074624
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0146",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 0.0,
                            "process_3": 395.6761373427577,
                            "process_2": 8.759802809501618,
                            "process_1": 98.37984908075603
                        },
                        "ram_power": {
                            "process_0": 0.7034740447998048,
                            "process_3": 0.6650018692016603,
                            "process_2": 0.664698600769043,
                            "process_1": 0.6718826293945312
                        },
                        "cpu_energy": {
                            "process_0": 0.0007898428050623352,
                            "process_3": 0.000816569330499874,
                            "process_2": 0.0007975650880624699,
                            "process_1": 0.0007155958250621097
                        },
                        "gpu_energy": {
                            "process_0": 0.00477967299040194,
                            "process_3": 0.005071850724143984,
                            "process_2": 0.004783711326965923,
                            "process_1": 0.004773653541141898
                        },
                        "ram_energy": {
                            "process_0": 4.562099946205827e-06,
                            "process_3": 4.497835428280775e-06,
                            "process_2": 4.337585807044994e-06,
                            "process_1": 3.9869732473855865e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005574077895410481,
                            "process_3": 0.005892917890072138,
                            "process_2": 0.005585614000835438,
                            "process_1": 0.0054932363394513935
                        },
                        "total_energy_joules": {
                            "process_0": 20066.680423477734,
                            "process_3": 21214.504404259696,
                            "process_2": 20108.210403007575,
                            "process_1": 19775.650822025018
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 125.70394730825384,
                        "ram_power_avg": 0.6762642860412598,
                        "cpu_energy_total": 0.003119573048686789,
                        "gpu_energy_total": 0.019408888582653744,
                        "ram_energy_total": 1.7384494428917182e-05,
                        "total_energy_kwh": 0.02254584612576945,
                        "total_energy_joules": 81165.04605277002
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20186029327634247,
                        "joules_per_token": 4.953921267869264,
                        "flops_per_joule": 208833381.085398,
                        "joules_per_flop": 4.788506486858397e-09
                    },
                    "per-process_emissions": [
                        0.002123444974256623,
                        0.002244907070222981,
                        0.00212783965361826,
                        0.002092648383514008
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0147": {
            "setup": {
                "experiment_id": "0147",
                "date_time": "April 11, 2025 at 02:26:16 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 24.247810759001368,
                        "average_latency_ms_per_batch": 3030.976344875171,
                        "throughput_queries_per_sec": 5.278827077305663,
                        "throughput_tokens_per_sec": 675.6898658951249
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 2020343808
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0147",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1702.061510309675,
                            "process_2": 0.0,
                            "process_1": 101.9133936883801,
                            "process_3": 413.21371280763583
                        },
                        "ram_power": {
                            "process_0": 0.7042779922485352,
                            "process_2": 0.6682734489440918,
                            "process_1": 0.6630535125732422,
                            "process_3": 0.6714363098144531
                        },
                        "cpu_energy": {
                            "process_0": 0.0007444192353438553,
                            "process_2": 0.000815028731187681,
                            "process_1": 0.0007446210749379247,
                            "process_3": 0.0008236226127813212
                        },
                        "gpu_energy": {
                            "process_0": 0.0047691490930940605,
                            "process_2": 0.0047691490930940605,
                            "process_1": 0.004752262968474058,
                            "process_3": 0.0051154596479199566
                        },
                        "ram_energy": {
                            "process_0": 4.419038420093876e-06,
                            "process_2": 4.126853876836875e-06,
                            "process_1": 3.7450753097215163e-06,
                            "process_3": 4.187407953864989e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.00551798736685801,
                            "process_2": 0.005588304678158578,
                            "process_1": 0.005500629118721705,
                            "process_3": 0.005943269668655142
                        },
                        "total_energy_joules": {
                            "process_0": 19864.75452068884,
                            "process_2": 20117.89684137088,
                            "process_1": 19802.26482739814,
                            "process_3": 21395.77080715851
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 554.2971542014227,
                        "ram_power_avg": 0.6767603158950806,
                        "cpu_energy_total": 0.003127691654250782,
                        "gpu_energy_total": 0.019406020802582136,
                        "ram_energy_total": 1.6478375560517256e-05,
                        "total_energy_kwh": 0.022550190832393437,
                        "total_energy_joules": 81180.68699661636
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20182140119955982,
                        "joules_per_token": 4.954875915320823,
                        "flops_per_joule": 208793145.51573676,
                        "joules_per_flop": 4.78942925798601e-09
                    },
                    "per-process_emissions": [
                        0.0021020772874045593,
                        0.00212886466714451,
                        0.002095464662777034,
                        0.0022640885802741764
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0147": {
            "setup": {
                "experiment_id": "0147",
                "date_time": "April 11, 2025 at 02:26:16 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 24.247810759001368,
                        "average_latency_ms_per_batch": 3030.976344875171,
                        "throughput_queries_per_sec": 5.278827077305663,
                        "throughput_tokens_per_sec": 675.6898658951249
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 2020343808
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0147",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1702.061510309675,
                            "process_2": 0.0,
                            "process_1": 101.9133936883801,
                            "process_3": 413.21371280763583
                        },
                        "ram_power": {
                            "process_0": 0.7042779922485352,
                            "process_2": 0.6682734489440918,
                            "process_1": 0.6630535125732422,
                            "process_3": 0.6714363098144531
                        },
                        "cpu_energy": {
                            "process_0": 0.0007444192353438553,
                            "process_2": 0.000815028731187681,
                            "process_1": 0.0007446210749379247,
                            "process_3": 0.0008236226127813212
                        },
                        "gpu_energy": {
                            "process_0": 0.0047691490930940605,
                            "process_2": 0.0047691490930940605,
                            "process_1": 0.004752262968474058,
                            "process_3": 0.0051154596479199566
                        },
                        "ram_energy": {
                            "process_0": 4.419038420093876e-06,
                            "process_2": 4.126853876836875e-06,
                            "process_1": 3.7450753097215163e-06,
                            "process_3": 4.187407953864989e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.00551798736685801,
                            "process_2": 0.005588304678158578,
                            "process_1": 0.005500629118721705,
                            "process_3": 0.005943269668655142
                        },
                        "total_energy_joules": {
                            "process_0": 19864.75452068884,
                            "process_2": 20117.89684137088,
                            "process_1": 19802.26482739814,
                            "process_3": 21395.77080715851
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 554.2971542014227,
                        "ram_power_avg": 0.6767603158950806,
                        "cpu_energy_total": 0.003127691654250782,
                        "gpu_energy_total": 0.019406020802582136,
                        "ram_energy_total": 1.6478375560517256e-05,
                        "total_energy_kwh": 0.022550190832393437,
                        "total_energy_joules": 81180.68699661636
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20182140119955982,
                        "joules_per_token": 4.954875915320823,
                        "flops_per_joule": 208793145.51573676,
                        "joules_per_flop": 4.78942925798601e-09
                    },
                    "per-process_emissions": [
                        0.0021020772874045593,
                        0.00212886466714451,
                        0.002095464662777034,
                        0.0022640885802741764
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0148": {
            "setup": {
                "experiment_id": "0148",
                "date_time": "April 11, 2025 at 02:27:18 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.55748002800101,
                        "average_latency_ms_per_batch": 2944.685003500126,
                        "throughput_queries_per_sec": 5.433518349494768,
                        "throughput_tokens_per_sec": 695.4903487353303
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.7,
                        "cpu_memory_usage_bytes": 2019336192
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0148",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 907.5763655033014,
                            "process_1": 1262.4580644440275,
                            "process_2": 806.913316888455,
                            "process_3": 10.513688989979231
                        },
                        "ram_power": {
                            "process_0": 0.70279598236084,
                            "process_1": 0.6635198593139648,
                            "process_2": 0.6673135757446289,
                            "process_3": 0.6697139739990234
                        },
                        "cpu_energy": {
                            "process_0": 0.0006738967212190233,
                            "process_1": 0.0006758987798127125,
                            "process_2": 0.0006771589169376853,
                            "process_3": 0.0007897958674688538
                        },
                        "gpu_energy": {
                            "process_0": 0.004718789608362028,
                            "process_1": 0.004713977104512018,
                            "process_2": 0.004701635150194117,
                            "process_3": 0.005067234609340027
                        },
                        "ram_energy": {
                            "process_0": 4.286200803764874e-06,
                            "process_1": 4.040728900536282e-06,
                            "process_2": 3.705135030405404e-06,
                            "process_3": 4.678735767484316e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005396972530384817,
                            "process_1": 0.005393916613225269,
                            "process_2": 0.005382499202162207,
                            "process_3": 0.005861709212576366
                        },
                        "total_energy_joules": {
                            "process_0": 19429.10110938534,
                            "process_1": 19418.099807610968,
                            "process_2": 19376.997127783943,
                            "process_3": 21102.153165274918
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 746.8653589564408,
                        "ram_power_avg": 0.6758358478546143,
                        "cpu_energy_total": 0.0028167502854382753,
                        "gpu_energy_total": 0.01920163647240819,
                        "ram_energy_total": 1.6710800502190875e-05,
                        "total_energy_kwh": 0.02203509755834866,
                        "total_energy_joules": 79326.35121005517
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20653918590829137,
                        "joules_per_token": 4.841696240848094,
                        "flops_per_joule": 213673901.9833232,
                        "joules_per_flop": 4.6800287293768236e-09
                    },
                    "per-process_emissions": [
                        0.002055976685450096,
                        0.002054812533808166,
                        0.002050463071063693,
                        0.0022330181245309665
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0148": {
            "setup": {
                "experiment_id": "0148",
                "date_time": "April 11, 2025 at 02:27:18 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.55748002800101,
                        "average_latency_ms_per_batch": 2944.685003500126,
                        "throughput_queries_per_sec": 5.433518349494768,
                        "throughput_tokens_per_sec": 695.4903487353303
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.7,
                        "cpu_memory_usage_bytes": 2019336192
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0148",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 907.5763655033014,
                            "process_1": 1262.4580644440275,
                            "process_2": 806.913316888455,
                            "process_3": 10.513688989979231
                        },
                        "ram_power": {
                            "process_0": 0.70279598236084,
                            "process_1": 0.6635198593139648,
                            "process_2": 0.6673135757446289,
                            "process_3": 0.6697139739990234
                        },
                        "cpu_energy": {
                            "process_0": 0.0006738967212190233,
                            "process_1": 0.0006758987798127125,
                            "process_2": 0.0006771589169376853,
                            "process_3": 0.0007897958674688538
                        },
                        "gpu_energy": {
                            "process_0": 0.004718789608362028,
                            "process_1": 0.004713977104512018,
                            "process_2": 0.004701635150194117,
                            "process_3": 0.005067234609340027
                        },
                        "ram_energy": {
                            "process_0": 4.286200803764874e-06,
                            "process_1": 4.040728900536282e-06,
                            "process_2": 3.705135030405404e-06,
                            "process_3": 4.678735767484316e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005396972530384817,
                            "process_1": 0.005393916613225269,
                            "process_2": 0.005382499202162207,
                            "process_3": 0.005861709212576366
                        },
                        "total_energy_joules": {
                            "process_0": 19429.10110938534,
                            "process_1": 19418.099807610968,
                            "process_2": 19376.997127783943,
                            "process_3": 21102.153165274918
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 746.8653589564408,
                        "ram_power_avg": 0.6758358478546143,
                        "cpu_energy_total": 0.0028167502854382753,
                        "gpu_energy_total": 0.01920163647240819,
                        "ram_energy_total": 1.6710800502190875e-05,
                        "total_energy_kwh": 0.02203509755834866,
                        "total_energy_joules": 79326.35121005517
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20653918590829137,
                        "joules_per_token": 4.841696240848094,
                        "flops_per_joule": 213673901.9833232,
                        "joules_per_flop": 4.6800287293768236e-09
                    },
                    "per-process_emissions": [
                        0.002055976685450096,
                        0.002054812533808166,
                        0.002050463071063693,
                        0.0022330181245309665
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0149": {
            "setup": {
                "experiment_id": "0149",
                "date_time": "April 11, 2025 at 02:28:19 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 24.153399310000168,
                        "average_latency_ms_per_batch": 3019.174913750021,
                        "throughput_queries_per_sec": 5.299461096848778,
                        "throughput_tokens_per_sec": 678.3310203966436
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 2019917824
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0149",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 0.0,
                            "process_2": 124.54659387628226,
                            "process_3": 474.4127268022985,
                            "process_1": 113.35143215986727
                        },
                        "ram_power": {
                            "process_0": 0.7040705680847169,
                            "process_2": 0.6661605834960938,
                            "process_3": 0.670773983001709,
                            "process_1": 0.662325382232666
                        },
                        "cpu_energy": {
                            "process_0": 0.000796285532656327,
                            "process_2": 0.0007197166464063683,
                            "process_3": 0.000763849890312656,
                            "process_1": 0.0007201264689374511
                        },
                        "gpu_energy": {
                            "process_0": 0.004774493819591941,
                            "process_2": 0.004774493819591941,
                            "process_3": 0.005039318475896087,
                            "process_1": 0.004776093265315917
                        },
                        "ram_energy": {
                            "process_0": 4.617191524916113e-06,
                            "process_2": 3.953890736186758e-06,
                            "process_3": 4.223051588478084e-06,
                            "process_1": 3.929097921729456e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005575396543773183,
                            "process_2": 0.005498164356734497,
                            "process_3": 0.005807391417797222,
                            "process_1": 0.005500148832175097
                        },
                        "total_energy_joules": {
                            "process_0": 20071.42755758346,
                            "process_2": 19793.391684244187,
                            "process_3": 20906.609104069998,
                            "process_1": 19800.53579583035
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 178.077688209612,
                        "ram_power_avg": 0.6758326292037964,
                        "cpu_energy_total": 0.0029999785383128027,
                        "gpu_energy_total": 0.019364399380395886,
                        "ram_energy_total": 1.6723231771310412e-05,
                        "total_energy_kwh": 0.022381101150479997,
                        "total_energy_joules": 80571.96414172799
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20334616605821046,
                        "joules_per_token": 4.917722420759765,
                        "flops_per_joule": 210370582.04683456,
                        "joules_per_flop": 4.753516343731802e-09
                    },
                    "per-process_emissions": [
                        0.0021239473133503944,
                        0.002094525711698007,
                        0.002212325760609852,
                        0.002095281697617103
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0149": {
            "setup": {
                "experiment_id": "0149",
                "date_time": "April 11, 2025 at 02:28:19 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 24.153399310000168,
                        "average_latency_ms_per_batch": 3019.174913750021,
                        "throughput_queries_per_sec": 5.299461096848778,
                        "throughput_tokens_per_sec": 678.3310203966436
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 2019917824
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0149",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 0.0,
                            "process_2": 124.54659387628226,
                            "process_3": 474.4127268022985,
                            "process_1": 113.35143215986727
                        },
                        "ram_power": {
                            "process_0": 0.7040705680847169,
                            "process_2": 0.6661605834960938,
                            "process_3": 0.670773983001709,
                            "process_1": 0.662325382232666
                        },
                        "cpu_energy": {
                            "process_0": 0.000796285532656327,
                            "process_2": 0.0007197166464063683,
                            "process_3": 0.000763849890312656,
                            "process_1": 0.0007201264689374511
                        },
                        "gpu_energy": {
                            "process_0": 0.004774493819591941,
                            "process_2": 0.004774493819591941,
                            "process_3": 0.005039318475896087,
                            "process_1": 0.004776093265315917
                        },
                        "ram_energy": {
                            "process_0": 4.617191524916113e-06,
                            "process_2": 3.953890736186758e-06,
                            "process_3": 4.223051588478084e-06,
                            "process_1": 3.929097921729456e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005575396543773183,
                            "process_2": 0.005498164356734497,
                            "process_3": 0.005807391417797222,
                            "process_1": 0.005500148832175097
                        },
                        "total_energy_joules": {
                            "process_0": 20071.42755758346,
                            "process_2": 19793.391684244187,
                            "process_3": 20906.609104069998,
                            "process_1": 19800.53579583035
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 178.077688209612,
                        "ram_power_avg": 0.6758326292037964,
                        "cpu_energy_total": 0.0029999785383128027,
                        "gpu_energy_total": 0.019364399380395886,
                        "ram_energy_total": 1.6723231771310412e-05,
                        "total_energy_kwh": 0.022381101150479997,
                        "total_energy_joules": 80571.96414172799
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20334616605821046,
                        "joules_per_token": 4.917722420759765,
                        "flops_per_joule": 210370582.04683456,
                        "joules_per_flop": 4.753516343731802e-09
                    },
                    "per-process_emissions": [
                        0.0021239473133503944,
                        0.002094525711698007,
                        0.002212325760609852,
                        0.002095281697617103
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0150": {
            "setup": {
                "experiment_id": "0150",
                "date_time": "April 11, 2025 at 02:29:23 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 24.076160004000485,
                        "average_latency_ms_per_batch": 3009.5200005000606,
                        "throughput_queries_per_sec": 5.316462425018424,
                        "throughput_tokens_per_sec": 680.5071904023582
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 2017144832
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0150",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 536.5659698218622,
                            "process_0": 0.0,
                            "process_1": 94.05382910941394,
                            "process_2": 1490.4211229655557
                        },
                        "ram_power": {
                            "process_3": 0.6805987358093262,
                            "process_0": 0.703132152557373,
                            "process_1": 0.6693592071533203,
                            "process_2": 0.6618232727050781
                        },
                        "cpu_energy": {
                            "process_3": 0.0007982660774375746,
                            "process_0": 0.0008040958335938056,
                            "process_1": 0.0007259043331564499,
                            "process_2": 0.00061023932009374
                        },
                        "gpu_energy": {
                            "process_3": 0.005175484695940008,
                            "process_0": 0.004761922142868025,
                            "process_1": 0.004767992425502021,
                            "process_2": 0.00471315182607393
                        },
                        "ram_energy": {
                            "process_3": 4.406547902091263e-06,
                            "process_0": 4.64834815505094e-06,
                            "process_1": 3.999502097978299e-06,
                            "process_2": 4.0108423442707194e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0059781573212796734,
                            "process_0": 0.005570666324616882,
                            "process_1": 0.005497896260756447,
                            "process_2": 0.0053274019885119405
                        },
                        "total_energy_joules": {
                            "process_3": 21521.366356606824,
                            "process_0": 20054.398768620777,
                            "process_1": 19792.42653872321,
                            "process_2": 19178.647158642987
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 530.2602304742079,
                        "ram_power_avg": 0.6787283420562744,
                        "cpu_energy_total": 0.00293850556428157,
                        "gpu_energy_total": 0.019418551090383984,
                        "ram_energy_total": 1.7065240499391222e-05,
                        "total_energy_kwh": 0.022374121895164942,
                        "total_energy_joules": 80546.8388225938
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20340959669548453,
                        "joules_per_token": 4.916188892980578,
                        "flops_per_joule": 210436203.8401617,
                        "joules_per_flop": 4.752034021482145e-09
                    },
                    "per-process_emissions": [
                        0.0022773790315414916,
                        0.0021221453363628015,
                        0.0020944235805351688,
                        0.002029473787523624
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0150": {
            "setup": {
                "experiment_id": "0150",
                "date_time": "April 11, 2025 at 02:29:23 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 24.076160004000485,
                        "average_latency_ms_per_batch": 3009.5200005000606,
                        "throughput_queries_per_sec": 5.316462425018424,
                        "throughput_tokens_per_sec": 680.5071904023582
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 2017144832
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0150",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 536.5659698218622,
                            "process_0": 0.0,
                            "process_1": 94.05382910941394,
                            "process_2": 1490.4211229655557
                        },
                        "ram_power": {
                            "process_3": 0.6805987358093262,
                            "process_0": 0.703132152557373,
                            "process_1": 0.6693592071533203,
                            "process_2": 0.6618232727050781
                        },
                        "cpu_energy": {
                            "process_3": 0.0007982660774375746,
                            "process_0": 0.0008040958335938056,
                            "process_1": 0.0007259043331564499,
                            "process_2": 0.00061023932009374
                        },
                        "gpu_energy": {
                            "process_3": 0.005175484695940008,
                            "process_0": 0.004761922142868025,
                            "process_1": 0.004767992425502021,
                            "process_2": 0.00471315182607393
                        },
                        "ram_energy": {
                            "process_3": 4.406547902091263e-06,
                            "process_0": 4.64834815505094e-06,
                            "process_1": 3.999502097978299e-06,
                            "process_2": 4.0108423442707194e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0059781573212796734,
                            "process_0": 0.005570666324616882,
                            "process_1": 0.005497896260756447,
                            "process_2": 0.0053274019885119405
                        },
                        "total_energy_joules": {
                            "process_3": 21521.366356606824,
                            "process_0": 20054.398768620777,
                            "process_1": 19792.42653872321,
                            "process_2": 19178.647158642987
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 530.2602304742079,
                        "ram_power_avg": 0.6787283420562744,
                        "cpu_energy_total": 0.00293850556428157,
                        "gpu_energy_total": 0.019418551090383984,
                        "ram_energy_total": 1.7065240499391222e-05,
                        "total_energy_kwh": 0.022374121895164942,
                        "total_energy_joules": 80546.8388225938
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20340959669548453,
                        "joules_per_token": 4.916188892980578,
                        "flops_per_joule": 210436203.8401617,
                        "joules_per_flop": 4.752034021482145e-09
                    },
                    "per-process_emissions": [
                        0.0022773790315414916,
                        0.0021221453363628015,
                        0.0020944235805351688,
                        0.002029473787523624
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0151": {
            "setup": {
                "experiment_id": "0151",
                "date_time": "April 11, 2025 at 02:30:26 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.53547077699841,
                        "average_latency_ms_per_batch": 2941.933847124801,
                        "throughput_queries_per_sec": 5.438599516993578,
                        "throughput_tokens_per_sec": 696.140738175178
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 2019106816
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0151",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 323.1360951803033,
                            "process_0": 1296.061967250814,
                            "process_2": 47.808284286348645,
                            "process_1": 1646.5127989900955
                        },
                        "ram_power": {
                            "process_3": 0.6634654998779297,
                            "process_0": 0.7039518356323242,
                            "process_2": 0.6689958572387695,
                            "process_1": 0.66302490234375
                        },
                        "cpu_energy": {
                            "process_3": 0.0007642631705936082,
                            "process_0": 0.0006390137906564065,
                            "process_2": 0.0007234821808127094,
                            "process_1": 0.0006689789390934492
                        },
                        "gpu_energy": {
                            "process_3": 0.005069088499711966,
                            "process_0": 0.0047642124224780424,
                            "process_2": 0.004760970753217908,
                            "process_1": 0.004736418511353979
                        },
                        "ram_energy": {
                            "process_3": 4.522659717947312e-06,
                            "process_0": 4.058938041944021e-06,
                            "process_2": 4.004692439079775e-06,
                            "process_1": 4.031431481231386e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005837874330023522,
                            "process_0": 0.005407285151176392,
                            "process_2": 0.005488457626469697,
                            "process_1": 0.005409428881928663
                        },
                        "total_energy_joules": {
                            "process_3": 21016.34758808468,
                            "process_0": 19466.22654423501,
                            "process_2": 19758.44745529091,
                            "process_1": 19473.943974943188
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 828.3797864268904,
                        "ram_power_avg": 0.6748595237731934,
                        "cpu_energy_total": 0.0027957380811561737,
                        "gpu_energy_total": 0.019330690186761895,
                        "ram_energy_total": 1.6617721680202496e-05,
                        "total_energy_kwh": 0.022143045989598275,
                        "total_energy_joules": 79714.96556255379
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20553229728416778,
                        "joules_per_token": 4.865415378573839,
                        "flops_per_joule": 212632231.2696861,
                        "joules_per_flop": 4.702955869054857e-09
                    },
                    "per-process_emissions": [
                        0.0022239382260224607,
                        0.0020599052783406464,
                        0.002090827932803631,
                        0.0020607219325707244
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0151": {
            "setup": {
                "experiment_id": "0151",
                "date_time": "April 11, 2025 at 02:30:26 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.53547077699841,
                        "average_latency_ms_per_batch": 2941.933847124801,
                        "throughput_queries_per_sec": 5.438599516993578,
                        "throughput_tokens_per_sec": 696.140738175178
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 2019106816
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0151",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 323.1360951803033,
                            "process_0": 1296.061967250814,
                            "process_2": 47.808284286348645,
                            "process_1": 1646.5127989900955
                        },
                        "ram_power": {
                            "process_3": 0.6634654998779297,
                            "process_0": 0.7039518356323242,
                            "process_2": 0.6689958572387695,
                            "process_1": 0.66302490234375
                        },
                        "cpu_energy": {
                            "process_3": 0.0007642631705936082,
                            "process_0": 0.0006390137906564065,
                            "process_2": 0.0007234821808127094,
                            "process_1": 0.0006689789390934492
                        },
                        "gpu_energy": {
                            "process_3": 0.005069088499711966,
                            "process_0": 0.0047642124224780424,
                            "process_2": 0.004760970753217908,
                            "process_1": 0.004736418511353979
                        },
                        "ram_energy": {
                            "process_3": 4.522659717947312e-06,
                            "process_0": 4.058938041944021e-06,
                            "process_2": 4.004692439079775e-06,
                            "process_1": 4.031431481231386e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005837874330023522,
                            "process_0": 0.005407285151176392,
                            "process_2": 0.005488457626469697,
                            "process_1": 0.005409428881928663
                        },
                        "total_energy_joules": {
                            "process_3": 21016.34758808468,
                            "process_0": 19466.22654423501,
                            "process_2": 19758.44745529091,
                            "process_1": 19473.943974943188
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 828.3797864268904,
                        "ram_power_avg": 0.6748595237731934,
                        "cpu_energy_total": 0.0027957380811561737,
                        "gpu_energy_total": 0.019330690186761895,
                        "ram_energy_total": 1.6617721680202496e-05,
                        "total_energy_kwh": 0.022143045989598275,
                        "total_energy_joules": 79714.96556255379
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20553229728416778,
                        "joules_per_token": 4.865415378573839,
                        "flops_per_joule": 212632231.2696861,
                        "joules_per_flop": 4.702955869054857e-09
                    },
                    "per-process_emissions": [
                        0.0022239382260224607,
                        0.0020599052783406464,
                        0.002090827932803631,
                        0.0020607219325707244
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0152": {
            "setup": {
                "experiment_id": "0152",
                "date_time": "April 11, 2025 at 02:31:29 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.584798756000964,
                        "average_latency_ms_per_batch": 2948.0998445001205,
                        "throughput_queries_per_sec": 5.427224600228205,
                        "throughput_tokens_per_sec": 694.6847488292102
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            94.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 2018009088
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0152",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 1950.2694234040005,
                            "process_0": 1246.937855691088,
                            "process_3": 442.84665354601924,
                            "process_2": 875.2490878156675
                        },
                        "ram_power": {
                            "process_1": 0.6749510765075685,
                            "process_0": 0.7034683227539062,
                            "process_3": 0.6695737838745117,
                            "process_2": 0.6733274459838867
                        },
                        "cpu_energy": {
                            "process_1": 0.0006616895142813065,
                            "process_0": 0.0006276569604685847,
                            "process_3": 0.0007504126027187112,
                            "process_2": 0.000672840511531149
                        },
                        "gpu_energy": {
                            "process_1": 0.004746852964145926,
                            "process_0": 0.00475948352980593,
                            "process_3": 0.005076249894330004,
                            "process_2": 0.004767305202729888
                        },
                        "ram_energy": {
                            "process_1": 4.080317123107009e-06,
                            "process_0": 4.036597071889363e-06,
                            "process_3": 4.516695810512982e-06,
                            "process_2": 4.0889671536434e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005412622795550339,
                            "process_0": 0.005391177087346405,
                            "process_3": 0.005831179192859231,
                            "process_2": 0.00544423468141468
                        },
                        "total_energy_joules": {
                            "process_1": 19485.44206398122,
                            "process_0": 19408.237514447057,
                            "process_3": 20992.24509429323,
                            "process_2": 19599.24485309285
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1128.825755114194,
                        "ram_power_avg": 0.6803301572799683,
                        "cpu_energy_total": 0.0027125995889997513,
                        "gpu_energy_total": 0.019349891591011747,
                        "ram_energy_total": 1.6722577159152753e-05,
                        "total_energy_kwh": 0.022079213757170656,
                        "total_energy_joules": 79485.16952581436
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20612650256321058,
                        "joules_per_token": 4.851389741565818,
                        "flops_per_joule": 213246962.85194644,
                        "joules_per_flop": 4.689398557550769e-09
                    },
                    "per-process_emissions": [
                        0.0020619386539649015,
                        0.002053768911424613,
                        0.0022213877135197242,
                        0.0020739812018849223
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0152": {
            "setup": {
                "experiment_id": "0152",
                "date_time": "April 11, 2025 at 02:31:29 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.584798756000964,
                        "average_latency_ms_per_batch": 2948.0998445001205,
                        "throughput_queries_per_sec": 5.427224600228205,
                        "throughput_tokens_per_sec": 694.6847488292102
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            94.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 2018009088
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0152",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 1950.2694234040005,
                            "process_0": 1246.937855691088,
                            "process_3": 442.84665354601924,
                            "process_2": 875.2490878156675
                        },
                        "ram_power": {
                            "process_1": 0.6749510765075685,
                            "process_0": 0.7034683227539062,
                            "process_3": 0.6695737838745117,
                            "process_2": 0.6733274459838867
                        },
                        "cpu_energy": {
                            "process_1": 0.0006616895142813065,
                            "process_0": 0.0006276569604685847,
                            "process_3": 0.0007504126027187112,
                            "process_2": 0.000672840511531149
                        },
                        "gpu_energy": {
                            "process_1": 0.004746852964145926,
                            "process_0": 0.00475948352980593,
                            "process_3": 0.005076249894330004,
                            "process_2": 0.004767305202729888
                        },
                        "ram_energy": {
                            "process_1": 4.080317123107009e-06,
                            "process_0": 4.036597071889363e-06,
                            "process_3": 4.516695810512982e-06,
                            "process_2": 4.0889671536434e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005412622795550339,
                            "process_0": 0.005391177087346405,
                            "process_3": 0.005831179192859231,
                            "process_2": 0.00544423468141468
                        },
                        "total_energy_joules": {
                            "process_1": 19485.44206398122,
                            "process_0": 19408.237514447057,
                            "process_3": 20992.24509429323,
                            "process_2": 19599.24485309285
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1128.825755114194,
                        "ram_power_avg": 0.6803301572799683,
                        "cpu_energy_total": 0.0027125995889997513,
                        "gpu_energy_total": 0.019349891591011747,
                        "ram_energy_total": 1.6722577159152753e-05,
                        "total_energy_kwh": 0.022079213757170656,
                        "total_energy_joules": 79485.16952581436
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20612650256321058,
                        "joules_per_token": 4.851389741565818,
                        "flops_per_joule": 213246962.85194644,
                        "joules_per_flop": 4.689398557550769e-09
                    },
                    "per-process_emissions": [
                        0.0020619386539649015,
                        0.002053768911424613,
                        0.0022213877135197242,
                        0.0020739812018849223
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0153": {
            "setup": {
                "experiment_id": "0153",
                "date_time": "April 11, 2025 at 02:32:31 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 24.0445669799974,
                        "average_latency_ms_per_batch": 3005.570872499675,
                        "throughput_queries_per_sec": 5.323447916798951,
                        "throughput_tokens_per_sec": 681.4013333502658
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 2020339712
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0153",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 136.96088462526717,
                            "process_3": 466.8434378265527,
                            "process_0": 0.0,
                            "process_1": 129.71428376157488
                        },
                        "ram_power": {
                            "process_2": 0.6639976501464845,
                            "process_3": 0.664515495300293,
                            "process_0": 0.7041778564453125,
                            "process_1": 0.6626043319702148
                        },
                        "cpu_energy": {
                            "process_2": 0.0006796726652813165,
                            "process_3": 0.0007546969420000096,
                            "process_0": 0.0007563446889998885,
                            "process_1": 0.0006722841060625342
                        },
                        "gpu_energy": {
                            "process_2": 0.004766471035395903,
                            "process_3": 0.005035427083893873,
                            "process_0": 0.004766471035395903,
                            "process_1": 0.004766471035395903
                        },
                        "ram_energy": {
                            "process_2": 3.7227265242377524e-06,
                            "process_3": 4.468307118517619e-06,
                            "process_0": 4.7174484311829735e-06,
                            "process_1": 4.008713528678028e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005449866427201456,
                            "process_3": 0.005794592333012403,
                            "process_0": 0.005527533172826975,
                            "process_1": 0.005442763854987114
                        },
                        "total_energy_joules": {
                            "process_2": 19619.519137925243,
                            "process_3": 20860.53239884465,
                            "process_0": 19899.11942217711,
                            "process_1": 19593.94987795361
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 183.37965155334868,
                        "ram_power_avg": 0.6738238334655762,
                        "cpu_energy_total": 0.002862998402343749,
                        "gpu_energy_total": 0.01933484019008158,
                        "ram_energy_total": 1.6917195602616374e-05,
                        "total_energy_kwh": 0.02221475578802795,
                        "total_energy_joules": 79973.12083690062
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20486883378496606,
                        "joules_per_token": 4.88117192608036,
                        "flops_per_joule": 211945849.00244465,
                        "joules_per_flop": 4.718186294785446e-09
                    },
                    "per-process_emissions": [
                        0.0020761266154423947,
                        0.002207449949261075,
                        0.0021057137621884364,
                        0.0020734208905573413
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0153": {
            "setup": {
                "experiment_id": "0153",
                "date_time": "April 11, 2025 at 02:32:31 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 24.0445669799974,
                        "average_latency_ms_per_batch": 3005.570872499675,
                        "throughput_queries_per_sec": 5.323447916798951,
                        "throughput_tokens_per_sec": 681.4013333502658
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 2020339712
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0153",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 136.96088462526717,
                            "process_3": 466.8434378265527,
                            "process_0": 0.0,
                            "process_1": 129.71428376157488
                        },
                        "ram_power": {
                            "process_2": 0.6639976501464845,
                            "process_3": 0.664515495300293,
                            "process_0": 0.7041778564453125,
                            "process_1": 0.6626043319702148
                        },
                        "cpu_energy": {
                            "process_2": 0.0006796726652813165,
                            "process_3": 0.0007546969420000096,
                            "process_0": 0.0007563446889998885,
                            "process_1": 0.0006722841060625342
                        },
                        "gpu_energy": {
                            "process_2": 0.004766471035395903,
                            "process_3": 0.005035427083893873,
                            "process_0": 0.004766471035395903,
                            "process_1": 0.004766471035395903
                        },
                        "ram_energy": {
                            "process_2": 3.7227265242377524e-06,
                            "process_3": 4.468307118517619e-06,
                            "process_0": 4.7174484311829735e-06,
                            "process_1": 4.008713528678028e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005449866427201456,
                            "process_3": 0.005794592333012403,
                            "process_0": 0.005527533172826975,
                            "process_1": 0.005442763854987114
                        },
                        "total_energy_joules": {
                            "process_2": 19619.519137925243,
                            "process_3": 20860.53239884465,
                            "process_0": 19899.11942217711,
                            "process_1": 19593.94987795361
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 183.37965155334868,
                        "ram_power_avg": 0.6738238334655762,
                        "cpu_energy_total": 0.002862998402343749,
                        "gpu_energy_total": 0.01933484019008158,
                        "ram_energy_total": 1.6917195602616374e-05,
                        "total_energy_kwh": 0.02221475578802795,
                        "total_energy_joules": 79973.12083690062
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20486883378496606,
                        "joules_per_token": 4.88117192608036,
                        "flops_per_joule": 211945849.00244465,
                        "joules_per_flop": 4.718186294785446e-09
                    },
                    "per-process_emissions": [
                        0.0020761266154423947,
                        0.002207449949261075,
                        0.0021057137621884364,
                        0.0020734208905573413
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0154": {
            "setup": {
                "experiment_id": "0154",
                "date_time": "April 11, 2025 at 02:33:34 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.701610989000983,
                        "average_latency_ms_per_batch": 2962.701373625123,
                        "throughput_queries_per_sec": 5.400476788662169,
                        "throughput_tokens_per_sec": 691.2610289487576
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            2.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.7,
                        "cpu_memory_usage_bytes": 2021003264
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0154",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 774.1425475169659,
                            "process_1": 1568.7923559559947,
                            "process_2": 0.0,
                            "process_3": 0.0
                        },
                        "ram_power": {
                            "process_0": 0.7038230895996094,
                            "process_1": 0.6694865226745605,
                            "process_2": 0.6680903434753418,
                            "process_3": 0.6651577949523927
                        },
                        "cpu_energy": {
                            "process_0": 0.0006838431082190367,
                            "process_1": 0.0007056635218127099,
                            "process_2": 0.000729211754812468,
                            "process_3": 0.0007750466129374444
                        },
                        "gpu_energy": {
                            "process_0": 0.004688817084383956,
                            "process_1": 0.004854628050365989,
                            "process_2": 0.004758190473215962,
                            "process_3": 0.0051002793579980055
                        },
                        "ram_energy": {
                            "process_0": 4.343539762547348e-06,
                            "process_1": 4.183507511804419e-06,
                            "process_2": 4.3136428005097985e-06,
                            "process_3": 4.6143916651423904e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0053770037323655415,
                            "process_1": 0.0055644750796905035,
                            "process_2": 0.005491715870828939,
                            "process_3": 0.005879940362600592
                        },
                        "total_energy_joules": {
                            "process_0": 19357.21343651595,
                            "process_1": 20032.110286885814,
                            "process_2": 19770.17713498418,
                            "process_3": 21167.785305362133
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 585.7337258682402,
                        "ram_power_avg": 0.6766394376754761,
                        "cpu_energy_total": 0.002893764997781659,
                        "gpu_energy_total": 0.019401914965963912,
                        "ram_energy_total": 1.7455081740003954e-05,
                        "total_energy_kwh": 0.022313135045485575,
                        "total_energy_joules": 80327.28616374808
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2039655611743316,
                        "joules_per_token": 4.902788462142827,
                        "flops_per_joule": 211011373.62712955,
                        "joules_per_flop": 4.739081040091532e-09
                    },
                    "per-process_emissions": [
                        0.002048369571844653,
                        0.0021197867816080975,
                        0.0020920691609922844,
                        0.0022399632811326954
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0154": {
            "setup": {
                "experiment_id": "0154",
                "date_time": "April 11, 2025 at 02:33:34 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.701610989000983,
                        "average_latency_ms_per_batch": 2962.701373625123,
                        "throughput_queries_per_sec": 5.400476788662169,
                        "throughput_tokens_per_sec": 691.2610289487576
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            2.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.7,
                        "cpu_memory_usage_bytes": 2021003264
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0154",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 774.1425475169659,
                            "process_1": 1568.7923559559947,
                            "process_2": 0.0,
                            "process_3": 0.0
                        },
                        "ram_power": {
                            "process_0": 0.7038230895996094,
                            "process_1": 0.6694865226745605,
                            "process_2": 0.6680903434753418,
                            "process_3": 0.6651577949523927
                        },
                        "cpu_energy": {
                            "process_0": 0.0006838431082190367,
                            "process_1": 0.0007056635218127099,
                            "process_2": 0.000729211754812468,
                            "process_3": 0.0007750466129374444
                        },
                        "gpu_energy": {
                            "process_0": 0.004688817084383956,
                            "process_1": 0.004854628050365989,
                            "process_2": 0.004758190473215962,
                            "process_3": 0.0051002793579980055
                        },
                        "ram_energy": {
                            "process_0": 4.343539762547348e-06,
                            "process_1": 4.183507511804419e-06,
                            "process_2": 4.3136428005097985e-06,
                            "process_3": 4.6143916651423904e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0053770037323655415,
                            "process_1": 0.0055644750796905035,
                            "process_2": 0.005491715870828939,
                            "process_3": 0.005879940362600592
                        },
                        "total_energy_joules": {
                            "process_0": 19357.21343651595,
                            "process_1": 20032.110286885814,
                            "process_2": 19770.17713498418,
                            "process_3": 21167.785305362133
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 585.7337258682402,
                        "ram_power_avg": 0.6766394376754761,
                        "cpu_energy_total": 0.002893764997781659,
                        "gpu_energy_total": 0.019401914965963912,
                        "ram_energy_total": 1.7455081740003954e-05,
                        "total_energy_kwh": 0.022313135045485575,
                        "total_energy_joules": 80327.28616374808
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2039655611743316,
                        "joules_per_token": 4.902788462142827,
                        "flops_per_joule": 211011373.62712955,
                        "joules_per_flop": 4.739081040091532e-09
                    },
                    "per-process_emissions": [
                        0.002048369571844653,
                        0.0021197867816080975,
                        0.0020920691609922844,
                        0.0022399632811326954
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0155": {
            "setup": {
                "experiment_id": "0155",
                "date_time": "April 11, 2025 at 02:34:37 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.352319478000936,
                        "average_latency_ms_per_batch": 2919.039934750117,
                        "throughput_queries_per_sec": 5.481254233464152,
                        "throughput_tokens_per_sec": 701.6005418834114
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            5.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 2019926016
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0155",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 915.9127520987195,
                            "process_2": 722.772004676543,
                            "process_3": 10.899223835239338,
                            "process_1": 738.7194962853114
                        },
                        "ram_power": {
                            "process_0": 0.7040591239929199,
                            "process_2": 0.6684651374816895,
                            "process_3": 0.6639561653137207,
                            "process_1": 0.6684994697570801
                        },
                        "cpu_energy": {
                            "process_0": 0.0006869990584373227,
                            "process_2": 0.0006494763911563837,
                            "process_3": 0.0007984232537499452,
                            "process_1": 0.0006986175140623913
                        },
                        "gpu_energy": {
                            "process_0": 0.00464699038425595,
                            "process_2": 0.004715739883700054,
                            "process_3": 0.005088413515171947,
                            "process_1": 0.004702212095100106
                        },
                        "ram_energy": {
                            "process_0": 4.064549677642608e-06,
                            "process_2": 3.581802005732858e-06,
                            "process_3": 4.726295692667151e-06,
                            "process_1": 4.221664088160991e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005338053992370916,
                            "process_2": 0.0053687980768621696,
                            "process_3": 0.005891563064614559,
                            "process_1": 0.005405051273250657
                        },
                        "total_energy_joules": {
                            "process_0": 19216.994372535297,
                            "process_2": 19327.67307670381,
                            "process_3": 21209.627032612414,
                            "process_1": 19458.184583702365
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 597.0758692239533,
                        "ram_power_avg": 0.6762449741363525,
                        "cpu_energy_total": 0.002833516217406043,
                        "gpu_energy_total": 0.019153355878228057,
                        "ram_energy_total": 1.6594311464203608e-05,
                        "total_energy_kwh": 0.022003466407098302,
                        "total_energy_joules": 79212.47906555388
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2068360969543838,
                        "joules_per_token": 4.834746036715935,
                        "flops_per_joule": 213981069.56259644,
                        "joules_per_flop": 4.673310597260415e-09
                    },
                    "per-process_emissions": [
                        0.0020335316683937005,
                        0.0020452436273806435,
                        0.0022443909494649166,
                        0.0020590542825448376
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0155": {
            "setup": {
                "experiment_id": "0155",
                "date_time": "April 11, 2025 at 02:34:37 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.352319478000936,
                        "average_latency_ms_per_batch": 2919.039934750117,
                        "throughput_queries_per_sec": 5.481254233464152,
                        "throughput_tokens_per_sec": 701.6005418834114
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            5.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 2019926016
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0155",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 915.9127520987195,
                            "process_2": 722.772004676543,
                            "process_3": 10.899223835239338,
                            "process_1": 738.7194962853114
                        },
                        "ram_power": {
                            "process_0": 0.7040591239929199,
                            "process_2": 0.6684651374816895,
                            "process_3": 0.6639561653137207,
                            "process_1": 0.6684994697570801
                        },
                        "cpu_energy": {
                            "process_0": 0.0006869990584373227,
                            "process_2": 0.0006494763911563837,
                            "process_3": 0.0007984232537499452,
                            "process_1": 0.0006986175140623913
                        },
                        "gpu_energy": {
                            "process_0": 0.00464699038425595,
                            "process_2": 0.004715739883700054,
                            "process_3": 0.005088413515171947,
                            "process_1": 0.004702212095100106
                        },
                        "ram_energy": {
                            "process_0": 4.064549677642608e-06,
                            "process_2": 3.581802005732858e-06,
                            "process_3": 4.726295692667151e-06,
                            "process_1": 4.221664088160991e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005338053992370916,
                            "process_2": 0.0053687980768621696,
                            "process_3": 0.005891563064614559,
                            "process_1": 0.005405051273250657
                        },
                        "total_energy_joules": {
                            "process_0": 19216.994372535297,
                            "process_2": 19327.67307670381,
                            "process_3": 21209.627032612414,
                            "process_1": 19458.184583702365
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 597.0758692239533,
                        "ram_power_avg": 0.6762449741363525,
                        "cpu_energy_total": 0.002833516217406043,
                        "gpu_energy_total": 0.019153355878228057,
                        "ram_energy_total": 1.6594311464203608e-05,
                        "total_energy_kwh": 0.022003466407098302,
                        "total_energy_joules": 79212.47906555388
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2068360969543838,
                        "joules_per_token": 4.834746036715935,
                        "flops_per_joule": 213981069.56259644,
                        "joules_per_flop": 4.673310597260415e-09
                    },
                    "per-process_emissions": [
                        0.0020335316683937005,
                        0.0020452436273806435,
                        0.0022443909494649166,
                        0.0020590542825448376
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0156": {
            "setup": {
                "experiment_id": "0156",
                "date_time": "April 11, 2025 at 02:35:41 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.586959439997372,
                        "average_latency_ms_per_batch": 2948.3699299996715,
                        "throughput_queries_per_sec": 5.426727439185958,
                        "throughput_tokens_per_sec": 694.6211122158027
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 2000257024
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0156",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 928.2590692923889,
                            "process_0": 1168.832823259911,
                            "process_1": 1283.376749660682,
                            "process_3": 2326.1094944746146
                        },
                        "ram_power": {
                            "process_2": 0.6710028648376465,
                            "process_0": 0.6973814964294434,
                            "process_1": 0.6690373420715332,
                            "process_3": 0.6717495918273926
                        },
                        "cpu_energy": {
                            "process_2": 0.00063798706312474,
                            "process_0": 0.0006418080791874559,
                            "process_1": 0.0006907217246248932,
                            "process_3": 0.0006517952940001239
                        },
                        "gpu_energy": {
                            "process_2": 0.004760852142011995,
                            "process_0": 0.004750823800656057,
                            "process_1": 0.004753039913540036,
                            "process_3": 0.005060523492860014
                        },
                        "ram_energy": {
                            "process_2": 3.882510073312558e-06,
                            "process_0": 4.058269289795716e-06,
                            "process_1": 3.807671682725713e-06,
                            "process_3": 4.000433788511038e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0054027217152100475,
                            "process_0": 0.00539669014913331,
                            "process_1": 0.0054475693098476555,
                            "process_3": 0.005716319220648649
                        },
                        "total_energy_joules": {
                            "process_2": 19449.79817475617,
                            "process_0": 19428.084536879916,
                            "process_1": 19611.24951545156,
                            "process_3": 20578.749194335134
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1426.644534171899,
                        "ram_power_avg": 0.6772928237915039,
                        "cpu_energy_total": 0.002622312160937213,
                        "gpu_energy_total": 0.019325239349068102,
                        "ram_energy_total": 1.5748884834345024e-05,
                        "total_energy_kwh": 0.021963300394839663,
                        "total_energy_joules": 79067.88142142279
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20721435436818078,
                        "joules_per_token": 4.825920496913012,
                        "flops_per_joule": 214372393.54891258,
                        "joules_per_flop": 4.664779748199404e-09
                    },
                    "per-process_emissions": [
                        0.0020581668374092675,
                        0.0020558691123123344,
                        0.0020752515285864643,
                        0.0021776318071061026
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0156": {
            "setup": {
                "experiment_id": "0156",
                "date_time": "April 11, 2025 at 02:35:41 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.586959439997372,
                        "average_latency_ms_per_batch": 2948.3699299996715,
                        "throughput_queries_per_sec": 5.426727439185958,
                        "throughput_tokens_per_sec": 694.6211122158027
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 2000257024
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0156",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 928.2590692923889,
                            "process_0": 1168.832823259911,
                            "process_1": 1283.376749660682,
                            "process_3": 2326.1094944746146
                        },
                        "ram_power": {
                            "process_2": 0.6710028648376465,
                            "process_0": 0.6973814964294434,
                            "process_1": 0.6690373420715332,
                            "process_3": 0.6717495918273926
                        },
                        "cpu_energy": {
                            "process_2": 0.00063798706312474,
                            "process_0": 0.0006418080791874559,
                            "process_1": 0.0006907217246248932,
                            "process_3": 0.0006517952940001239
                        },
                        "gpu_energy": {
                            "process_2": 0.004760852142011995,
                            "process_0": 0.004750823800656057,
                            "process_1": 0.004753039913540036,
                            "process_3": 0.005060523492860014
                        },
                        "ram_energy": {
                            "process_2": 3.882510073312558e-06,
                            "process_0": 4.058269289795716e-06,
                            "process_1": 3.807671682725713e-06,
                            "process_3": 4.000433788511038e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0054027217152100475,
                            "process_0": 0.00539669014913331,
                            "process_1": 0.0054475693098476555,
                            "process_3": 0.005716319220648649
                        },
                        "total_energy_joules": {
                            "process_2": 19449.79817475617,
                            "process_0": 19428.084536879916,
                            "process_1": 19611.24951545156,
                            "process_3": 20578.749194335134
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1426.644534171899,
                        "ram_power_avg": 0.6772928237915039,
                        "cpu_energy_total": 0.002622312160937213,
                        "gpu_energy_total": 0.019325239349068102,
                        "ram_energy_total": 1.5748884834345024e-05,
                        "total_energy_kwh": 0.021963300394839663,
                        "total_energy_joules": 79067.88142142279
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20721435436818078,
                        "joules_per_token": 4.825920496913012,
                        "flops_per_joule": 214372393.54891258,
                        "joules_per_flop": 4.664779748199404e-09
                    },
                    "per-process_emissions": [
                        0.0020581668374092675,
                        0.0020558691123123344,
                        0.0020752515285864643,
                        0.0021776318071061026
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0157": {
            "setup": {
                "experiment_id": "0157",
                "date_time": "April 11, 2025 at 02:36:45 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.279664626999875,
                        "average_latency_ms_per_batch": 2909.9580783749843,
                        "throughput_queries_per_sec": 5.498360996641891,
                        "throughput_tokens_per_sec": 703.790207570162
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            7.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 2020048896
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0157",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 54.41939934916443,
                            "process_1": 737.9452789558944,
                            "process_0": 882.6920970373335,
                            "process_2": 762.6637167520363
                        },
                        "ram_power": {
                            "process_3": 0.6646528244018555,
                            "process_1": 0.6642379760742188,
                            "process_0": 0.7035741806030273,
                            "process_2": 0.6750211715698242
                        },
                        "cpu_energy": {
                            "process_3": 0.0008408646790314835,
                            "process_1": 0.000774786252843853,
                            "process_0": 0.0007540671238126605,
                            "process_2": 0.000770370671624903
                        },
                        "gpu_energy": {
                            "process_3": 0.00513040243765206,
                            "process_1": 0.004711079602194024,
                            "process_0": 0.004653585111753983,
                            "process_2": 0.004720313776247903
                        },
                        "ram_energy": {
                            "process_3": 4.651301837262658e-06,
                            "process_1": 3.9428633288262314e-06,
                            "process_0": 4.082705912795454e-06,
                            "process_2": 3.975878414204154e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005975918418520806,
                            "process_1": 0.0054898087183667035,
                            "process_0": 0.005411734941479439,
                            "process_2": 0.0054946603262870115
                        },
                        "total_energy_joules": {
                            "process_3": 21513.306306674902,
                            "process_1": 19763.31138612013,
                            "process_0": 19482.24578932598,
                            "process_2": 19780.77717463324
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 609.4301230236072,
                        "ram_power_avg": 0.6768715381622314,
                        "cpu_energy_total": 0.0031400887273129,
                        "gpu_energy_total": 0.01921538092784797,
                        "ram_energy_total": 1.6652749493088495e-05,
                        "total_energy_kwh": 0.022372122404653957,
                        "total_energy_joules": 80539.64065675426
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20342777626517752,
                        "joules_per_token": 4.915749551803849,
                        "flops_per_joule": 210455011.40723717,
                        "joules_per_flop": 4.751609350204391e-09
                    },
                    "per-process_emissions": [
                        0.002276526121535501,
                        0.002091342631261796,
                        0.0020616004259565923,
                        0.002093190851299037
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0157": {
            "setup": {
                "experiment_id": "0157",
                "date_time": "April 11, 2025 at 02:36:45 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.279664626999875,
                        "average_latency_ms_per_batch": 2909.9580783749843,
                        "throughput_queries_per_sec": 5.498360996641891,
                        "throughput_tokens_per_sec": 703.790207570162
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            7.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 2020048896
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0157",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 54.41939934916443,
                            "process_1": 737.9452789558944,
                            "process_0": 882.6920970373335,
                            "process_2": 762.6637167520363
                        },
                        "ram_power": {
                            "process_3": 0.6646528244018555,
                            "process_1": 0.6642379760742188,
                            "process_0": 0.7035741806030273,
                            "process_2": 0.6750211715698242
                        },
                        "cpu_energy": {
                            "process_3": 0.0008408646790314835,
                            "process_1": 0.000774786252843853,
                            "process_0": 0.0007540671238126605,
                            "process_2": 0.000770370671624903
                        },
                        "gpu_energy": {
                            "process_3": 0.00513040243765206,
                            "process_1": 0.004711079602194024,
                            "process_0": 0.004653585111753983,
                            "process_2": 0.004720313776247903
                        },
                        "ram_energy": {
                            "process_3": 4.651301837262658e-06,
                            "process_1": 3.9428633288262314e-06,
                            "process_0": 4.082705912795454e-06,
                            "process_2": 3.975878414204154e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005975918418520806,
                            "process_1": 0.0054898087183667035,
                            "process_0": 0.005411734941479439,
                            "process_2": 0.0054946603262870115
                        },
                        "total_energy_joules": {
                            "process_3": 21513.306306674902,
                            "process_1": 19763.31138612013,
                            "process_0": 19482.24578932598,
                            "process_2": 19780.77717463324
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 609.4301230236072,
                        "ram_power_avg": 0.6768715381622314,
                        "cpu_energy_total": 0.0031400887273129,
                        "gpu_energy_total": 0.01921538092784797,
                        "ram_energy_total": 1.6652749493088495e-05,
                        "total_energy_kwh": 0.022372122404653957,
                        "total_energy_joules": 80539.64065675426
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20342777626517752,
                        "joules_per_token": 4.915749551803849,
                        "flops_per_joule": 210455011.40723717,
                        "joules_per_flop": 4.751609350204391e-09
                    },
                    "per-process_emissions": [
                        0.002276526121535501,
                        0.002091342631261796,
                        0.0020616004259565923,
                        0.002093190851299037
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0158": {
            "setup": {
                "experiment_id": "0158",
                "date_time": "April 11, 2025 at 02:37:48 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.32810169699769,
                        "average_latency_ms_per_batch": 2916.0127121247115,
                        "throughput_queries_per_sec": 5.486944529930333,
                        "throughput_tokens_per_sec": 702.3288998310826
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            9.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 2019524608
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0158",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 3150.571003926759,
                            "process_1": 660.2546985296069,
                            "process_3": 506.29923542304266,
                            "process_0": 603.25973707153
                        },
                        "ram_power": {
                            "process_2": 0.6709842681884766,
                            "process_1": 0.6629905700683594,
                            "process_3": 0.6691689491271973,
                            "process_0": 0.7036614418029785
                        },
                        "cpu_energy": {
                            "process_2": 0.0006941383765935711,
                            "process_1": 0.0006942057525627093,
                            "process_3": 0.0007376409203123446,
                            "process_0": 0.0006956877355626715
                        },
                        "gpu_energy": {
                            "process_2": 0.004632667039464078,
                            "process_1": 0.004672095404340076,
                            "process_3": 0.005082654899454003,
                            "process_0": 0.004672095404340076
                        },
                        "ram_energy": {
                            "process_2": 4.256386112434273e-06,
                            "process_1": 3.855619940548696e-06,
                            "process_3": 4.449547713212175e-06,
                            "process_0": 4.1306690038165216e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005331061802170083,
                            "process_1": 0.0053701567768433345,
                            "process_3": 0.00582474536747956,
                            "process_0": 0.005371913808906563
                        },
                        "total_energy_joules": {
                            "process_2": 19191.822487812296,
                            "process_1": 19332.564396636004,
                            "process_3": 20969.083322926417,
                            "process_0": 19338.88971206363
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1230.0961687377348,
                        "ram_power_avg": 0.6767013072967529,
                        "cpu_energy_total": 0.0028216727850312964,
                        "gpu_energy_total": 0.019059512747598234,
                        "ram_energy_total": 1.6692222770011668e-05,
                        "total_energy_kwh": 0.02189787775539954,
                        "total_energy_joules": 78832.35991943834
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20783343308183855,
                        "joules_per_token": 4.811545405239157,
                        "flops_per_joule": 215012857.79689702,
                        "joules_per_flop": 4.650884650557077e-09
                    },
                    "per-process_emissions": [
                        0.002030867993536693,
                        0.0020457612241384683,
                        0.0022189367477413384,
                        0.0020464305655029554
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0158": {
            "setup": {
                "experiment_id": "0158",
                "date_time": "April 11, 2025 at 02:37:48 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.32810169699769,
                        "average_latency_ms_per_batch": 2916.0127121247115,
                        "throughput_queries_per_sec": 5.486944529930333,
                        "throughput_tokens_per_sec": 702.3288998310826
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            9.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 2019524608
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0158",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 3150.571003926759,
                            "process_1": 660.2546985296069,
                            "process_3": 506.29923542304266,
                            "process_0": 603.25973707153
                        },
                        "ram_power": {
                            "process_2": 0.6709842681884766,
                            "process_1": 0.6629905700683594,
                            "process_3": 0.6691689491271973,
                            "process_0": 0.7036614418029785
                        },
                        "cpu_energy": {
                            "process_2": 0.0006941383765935711,
                            "process_1": 0.0006942057525627093,
                            "process_3": 0.0007376409203123446,
                            "process_0": 0.0006956877355626715
                        },
                        "gpu_energy": {
                            "process_2": 0.004632667039464078,
                            "process_1": 0.004672095404340076,
                            "process_3": 0.005082654899454003,
                            "process_0": 0.004672095404340076
                        },
                        "ram_energy": {
                            "process_2": 4.256386112434273e-06,
                            "process_1": 3.855619940548696e-06,
                            "process_3": 4.449547713212175e-06,
                            "process_0": 4.1306690038165216e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005331061802170083,
                            "process_1": 0.0053701567768433345,
                            "process_3": 0.00582474536747956,
                            "process_0": 0.005371913808906563
                        },
                        "total_energy_joules": {
                            "process_2": 19191.822487812296,
                            "process_1": 19332.564396636004,
                            "process_3": 20969.083322926417,
                            "process_0": 19338.88971206363
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1230.0961687377348,
                        "ram_power_avg": 0.6767013072967529,
                        "cpu_energy_total": 0.0028216727850312964,
                        "gpu_energy_total": 0.019059512747598234,
                        "ram_energy_total": 1.6692222770011668e-05,
                        "total_energy_kwh": 0.02189787775539954,
                        "total_energy_joules": 78832.35991943834
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20783343308183855,
                        "joules_per_token": 4.811545405239157,
                        "flops_per_joule": 215012857.79689702,
                        "joules_per_flop": 4.650884650557077e-09
                    },
                    "per-process_emissions": [
                        0.002030867993536693,
                        0.0020457612241384683,
                        0.0022189367477413384,
                        0.0020464305655029554
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0159": {
            "setup": {
                "experiment_id": "0159",
                "date_time": "April 11, 2025 at 02:38:49 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.584233726000093,
                        "average_latency_ms_per_batch": 2948.0292157500116,
                        "throughput_queries_per_sec": 5.427354625428778,
                        "throughput_tokens_per_sec": 694.7013920548836
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.0,
                        "cpu_memory_usage_bytes": 2017579008
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0159",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1010.7004073801058,
                            "process_0": 1374.7873632681785,
                            "process_3": 625.2458449109866,
                            "process_1": 0.0
                        },
                        "ram_power": {
                            "process_2": 0.6649389266967773,
                            "process_0": 0.7033281326293945,
                            "process_3": 0.6650419235229492,
                            "process_1": 0.6693406105041504
                        },
                        "cpu_energy": {
                            "process_2": 0.0006970255460312842,
                            "process_0": 0.0006914202169375583,
                            "process_3": 0.0007741404214687578,
                            "process_1": 0.0007376371566249417
                        },
                        "gpu_energy": {
                            "process_2": 0.004778779378575931,
                            "process_0": 0.004762046587411944,
                            "process_3": 0.005049968762193907,
                            "process_1": 0.004778779378575931
                        },
                        "ram_energy": {
                            "process_2": 3.816727205136295e-06,
                            "process_0": 4.023314758573005e-06,
                            "process_3": 4.272744935697129e-06,
                            "process_1": 4.023248207383874e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005479621651812352,
                            "process_0": 0.0054574901191080764,
                            "process_3": 0.00582838192859836,
                            "process_1": 0.005520439783408255
                        },
                        "total_energy_joules": {
                            "process_2": 19726.637946524468,
                            "process_0": 19646.964428789077,
                            "process_3": 20982.174942954098,
                            "process_1": 19873.58322026972
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 752.6834038898178,
                        "ram_power_avg": 0.6756623983383179,
                        "cpu_energy_total": 0.0029002233410625423,
                        "gpu_energy_total": 0.019369574106757714,
                        "ram_energy_total": 1.6136035106790304e-05,
                        "total_energy_kwh": 0.022285933482927046,
                        "total_energy_joules": 80229.36053853737
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20421451560903456,
                        "joules_per_token": 4.896811556307212,
                        "flops_per_joule": 211268927.97569105,
                        "joules_per_flop": 4.7333037071833946e-09
                    },
                    "per-process_emissions": [
                        0.0020874618682579157,
                        0.0020790308608742217,
                        0.0022203220956995453,
                        0.0021030115354893747
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0159": {
            "setup": {
                "experiment_id": "0159",
                "date_time": "April 11, 2025 at 02:38:49 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.584233726000093,
                        "average_latency_ms_per_batch": 2948.0292157500116,
                        "throughput_queries_per_sec": 5.427354625428778,
                        "throughput_tokens_per_sec": 694.7013920548836
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.0,
                        "cpu_memory_usage_bytes": 2017579008
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0159",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1010.7004073801058,
                            "process_0": 1374.7873632681785,
                            "process_3": 625.2458449109866,
                            "process_1": 0.0
                        },
                        "ram_power": {
                            "process_2": 0.6649389266967773,
                            "process_0": 0.7033281326293945,
                            "process_3": 0.6650419235229492,
                            "process_1": 0.6693406105041504
                        },
                        "cpu_energy": {
                            "process_2": 0.0006970255460312842,
                            "process_0": 0.0006914202169375583,
                            "process_3": 0.0007741404214687578,
                            "process_1": 0.0007376371566249417
                        },
                        "gpu_energy": {
                            "process_2": 0.004778779378575931,
                            "process_0": 0.004762046587411944,
                            "process_3": 0.005049968762193907,
                            "process_1": 0.004778779378575931
                        },
                        "ram_energy": {
                            "process_2": 3.816727205136295e-06,
                            "process_0": 4.023314758573005e-06,
                            "process_3": 4.272744935697129e-06,
                            "process_1": 4.023248207383874e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005479621651812352,
                            "process_0": 0.0054574901191080764,
                            "process_3": 0.00582838192859836,
                            "process_1": 0.005520439783408255
                        },
                        "total_energy_joules": {
                            "process_2": 19726.637946524468,
                            "process_0": 19646.964428789077,
                            "process_3": 20982.174942954098,
                            "process_1": 19873.58322026972
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 752.6834038898178,
                        "ram_power_avg": 0.6756623983383179,
                        "cpu_energy_total": 0.0029002233410625423,
                        "gpu_energy_total": 0.019369574106757714,
                        "ram_energy_total": 1.6136035106790304e-05,
                        "total_energy_kwh": 0.022285933482927046,
                        "total_energy_joules": 80229.36053853737
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20421451560903456,
                        "joules_per_token": 4.896811556307212,
                        "flops_per_joule": 211268927.97569105,
                        "joules_per_flop": 4.7333037071833946e-09
                    },
                    "per-process_emissions": [
                        0.0020874618682579157,
                        0.0020790308608742217,
                        0.0022203220956995453,
                        0.0021030115354893747
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0160": {
            "setup": {
                "experiment_id": "0160",
                "date_time": "April 11, 2025 at 02:39:50 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.233561777999057,
                        "average_latency_ms_per_batch": 2904.195222249882,
                        "throughput_queries_per_sec": 5.509271510888579,
                        "throughput_tokens_per_sec": 705.1867533937381
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 2031259648
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0160",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 673.4696573841757,
                            "process_1": 1069.004616669678,
                            "process_0": 784.9690262484944,
                            "process_2": 0.0
                        },
                        "ram_power": {
                            "process_3": 0.6682605743408203,
                            "process_1": 0.6717109680175781,
                            "process_0": 0.7082033157348633,
                            "process_2": 0.6656570434570312
                        },
                        "cpu_energy": {
                            "process_3": 0.0006876423253435178,
                            "process_1": 0.0006749032647812215,
                            "process_0": 0.000742558302000134,
                            "process_2": 0.0007899742619374025
                        },
                        "gpu_energy": {
                            "process_3": 0.005027147910604185,
                            "process_1": 0.004701700983580165,
                            "process_0": 0.0046302742597721935,
                            "process_2": 0.004786264106786231
                        },
                        "ram_energy": {
                            "process_3": 4.144676053536594e-06,
                            "process_1": 3.784992798193952e-06,
                            "process_0": 4.051952799669096e-06,
                            "process_2": 3.953233609168892e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0057189349120012395,
                            "process_1": 0.00538038924115958,
                            "process_0": 0.005376884514571998,
                            "process_2": 0.005580191602332802
                        },
                        "total_energy_joules": {
                            "process_3": 20588.16568320446,
                            "process_1": 19369.40126817449,
                            "process_0": 19356.784252459194,
                            "process_2": 20088.689768398086
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 631.860825075587,
                        "ram_power_avg": 0.6784579753875732,
                        "cpu_energy_total": 0.0028950781540622757,
                        "gpu_energy_total": 0.019145387260742774,
                        "ram_energy_total": 1.5934855260568537e-05,
                        "total_energy_kwh": 0.02205640027006562,
                        "total_energy_joules": 79403.04097223622
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20633970436634497,
                        "joules_per_token": 4.84637701246559,
                        "flops_per_joule": 213467529.52545816,
                        "joules_per_flop": 4.684553206864841e-09
                    },
                    "per-process_emissions": [
                        0.002178628254726872,
                        0.0020496592814197422,
                        0.002048324155826203,
                        0.0021257739909086807
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0160": {
            "setup": {
                "experiment_id": "0160",
                "date_time": "April 11, 2025 at 02:39:50 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.233561777999057,
                        "average_latency_ms_per_batch": 2904.195222249882,
                        "throughput_queries_per_sec": 5.509271510888579,
                        "throughput_tokens_per_sec": 705.1867533937381
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 2031259648
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0160",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 673.4696573841757,
                            "process_1": 1069.004616669678,
                            "process_0": 784.9690262484944,
                            "process_2": 0.0
                        },
                        "ram_power": {
                            "process_3": 0.6682605743408203,
                            "process_1": 0.6717109680175781,
                            "process_0": 0.7082033157348633,
                            "process_2": 0.6656570434570312
                        },
                        "cpu_energy": {
                            "process_3": 0.0006876423253435178,
                            "process_1": 0.0006749032647812215,
                            "process_0": 0.000742558302000134,
                            "process_2": 0.0007899742619374025
                        },
                        "gpu_energy": {
                            "process_3": 0.005027147910604185,
                            "process_1": 0.004701700983580165,
                            "process_0": 0.0046302742597721935,
                            "process_2": 0.004786264106786231
                        },
                        "ram_energy": {
                            "process_3": 4.144676053536594e-06,
                            "process_1": 3.784992798193952e-06,
                            "process_0": 4.051952799669096e-06,
                            "process_2": 3.953233609168892e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0057189349120012395,
                            "process_1": 0.00538038924115958,
                            "process_0": 0.005376884514571998,
                            "process_2": 0.005580191602332802
                        },
                        "total_energy_joules": {
                            "process_3": 20588.16568320446,
                            "process_1": 19369.40126817449,
                            "process_0": 19356.784252459194,
                            "process_2": 20088.689768398086
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 631.860825075587,
                        "ram_power_avg": 0.6784579753875732,
                        "cpu_energy_total": 0.0028950781540622757,
                        "gpu_energy_total": 0.019145387260742774,
                        "ram_energy_total": 1.5934855260568537e-05,
                        "total_energy_kwh": 0.02205640027006562,
                        "total_energy_joules": 79403.04097223622
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20633970436634497,
                        "joules_per_token": 4.84637701246559,
                        "flops_per_joule": 213467529.52545816,
                        "joules_per_flop": 4.684553206864841e-09
                    },
                    "per-process_emissions": [
                        0.002178628254726872,
                        0.0020496592814197422,
                        0.002048324155826203,
                        0.0021257739909086807
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0161": {
            "setup": {
                "experiment_id": "0161",
                "date_time": "April 11, 2025 at 02:40:54 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.68791321400022,
                        "average_latency_ms_per_batch": 2960.9891517500273,
                        "throughput_queries_per_sec": 5.40359966889563,
                        "throughput_tokens_per_sec": 691.6607576186407
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 2020073472
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0161",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 909.304739038698,
                            "process_0": 1038.9668730776411,
                            "process_1": 2230.7317139282404,
                            "process_3": 320.67228862258133
                        },
                        "ram_power": {
                            "process_2": 0.6749668121337891,
                            "process_0": 0.7042551040649414,
                            "process_1": 0.6767091751098633,
                            "process_3": 0.681427001953125
                        },
                        "cpu_energy": {
                            "process_2": 0.0006439559951249977,
                            "process_0": 0.0006891365797497997,
                            "process_1": 0.0007161525032187227,
                            "process_3": 0.0007677719566560199
                        },
                        "gpu_energy": {
                            "process_2": 0.004767297424946082,
                            "process_0": 0.004763151032740109,
                            "process_1": 0.004734011009427885,
                            "process_3": 0.005074369059492045
                        },
                        "ram_energy": {
                            "process_2": 3.948944295451907e-06,
                            "process_0": 3.987635785311717e-06,
                            "process_1": 4.041975183027991e-06,
                            "process_3": 4.667239889681642e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.00541520236436653,
                            "process_0": 0.005456275248275219,
                            "process_1": 0.0054542054878296345,
                            "process_3": 0.005846808256037746
                        },
                        "total_energy_joules": {
                            "process_2": 19494.72851171951,
                            "process_0": 19642.590893790788,
                            "process_1": 19635.139756186683,
                            "process_3": 21048.509721735885
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1124.9189036667904,
                        "ram_power_avg": 0.6843395233154297,
                        "cpu_energy_total": 0.0028170170347495403,
                        "gpu_energy_total": 0.019338828526606122,
                        "ram_energy_total": 1.6645795153473258e-05,
                        "total_energy_kwh": 0.022172491356509127,
                        "total_energy_joules": 79820.96888343286
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2052593476273947,
                        "joules_per_token": 4.871885307826712,
                        "flops_per_joule": 212349852.80503193,
                        "joules_per_flop": 4.7092097629949644e-09
                    },
                    "per-process_emissions": [
                        0.0020629213407054297,
                        0.0020785680558304447,
                        0.002077779580588699,
                        0.0022273416051375792
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0161": {
            "setup": {
                "experiment_id": "0161",
                "date_time": "April 11, 2025 at 02:40:54 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.68791321400022,
                        "average_latency_ms_per_batch": 2960.9891517500273,
                        "throughput_queries_per_sec": 5.40359966889563,
                        "throughput_tokens_per_sec": 691.6607576186407
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 2020073472
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0161",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 909.304739038698,
                            "process_0": 1038.9668730776411,
                            "process_1": 2230.7317139282404,
                            "process_3": 320.67228862258133
                        },
                        "ram_power": {
                            "process_2": 0.6749668121337891,
                            "process_0": 0.7042551040649414,
                            "process_1": 0.6767091751098633,
                            "process_3": 0.681427001953125
                        },
                        "cpu_energy": {
                            "process_2": 0.0006439559951249977,
                            "process_0": 0.0006891365797497997,
                            "process_1": 0.0007161525032187227,
                            "process_3": 0.0007677719566560199
                        },
                        "gpu_energy": {
                            "process_2": 0.004767297424946082,
                            "process_0": 0.004763151032740109,
                            "process_1": 0.004734011009427885,
                            "process_3": 0.005074369059492045
                        },
                        "ram_energy": {
                            "process_2": 3.948944295451907e-06,
                            "process_0": 3.987635785311717e-06,
                            "process_1": 4.041975183027991e-06,
                            "process_3": 4.667239889681642e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.00541520236436653,
                            "process_0": 0.005456275248275219,
                            "process_1": 0.0054542054878296345,
                            "process_3": 0.005846808256037746
                        },
                        "total_energy_joules": {
                            "process_2": 19494.72851171951,
                            "process_0": 19642.590893790788,
                            "process_1": 19635.139756186683,
                            "process_3": 21048.509721735885
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1124.9189036667904,
                        "ram_power_avg": 0.6843395233154297,
                        "cpu_energy_total": 0.0028170170347495403,
                        "gpu_energy_total": 0.019338828526606122,
                        "ram_energy_total": 1.6645795153473258e-05,
                        "total_energy_kwh": 0.022172491356509127,
                        "total_energy_joules": 79820.96888343286
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2052593476273947,
                        "joules_per_token": 4.871885307826712,
                        "flops_per_joule": 212349852.80503193,
                        "joules_per_flop": 4.7092097629949644e-09
                    },
                    "per-process_emissions": [
                        0.0020629213407054297,
                        0.0020785680558304447,
                        0.002077779580588699,
                        0.0022273416051375792
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0162": {
            "setup": {
                "experiment_id": "0162",
                "date_time": "April 11, 2025 at 02:41:56 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.430405542998415,
                        "average_latency_ms_per_batch": 2928.800692874802,
                        "throughput_queries_per_sec": 5.462986962180413,
                        "throughput_tokens_per_sec": 699.2623311590928
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.0,
                        "cpu_memory_usage_bytes": 2023170048
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0162",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 53.497819401686115,
                            "process_1": 45.50851076850377,
                            "process_2": 71.43771178101241,
                            "process_3": 259.8771861270169
                        },
                        "ram_power": {
                            "process_0": 0.7047357559204102,
                            "process_1": 0.6712203025817871,
                            "process_2": 0.6682648658752441,
                            "process_3": 0.6764130592346191
                        },
                        "cpu_energy": {
                            "process_0": 0.000721068874250136,
                            "process_1": 0.0006698763614687098,
                            "process_2": 0.0006787239354999314,
                            "process_3": 0.0008134208749062281
                        },
                        "gpu_energy": {
                            "process_0": 0.004758585751310135,
                            "process_1": 0.004757293805832202,
                            "process_2": 0.004758961307166104,
                            "process_3": 0.0050657534970439955
                        },
                        "ram_energy": {
                            "process_0": 4.234598520496146e-06,
                            "process_1": 4.072604579405363e-06,
                            "process_2": 4.079299996372033e-06,
                            "process_3": 4.57329662892077e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0054838892240807674,
                            "process_1": 0.005431242771880316,
                            "process_2": 0.005441764542662408,
                            "process_3": 0.0058837476685791425
                        },
                        "total_energy_joules": {
                            "process_0": 19742.001206690762,
                            "process_1": 19552.473978769136,
                            "process_2": 19590.35235358467,
                            "process_3": 21181.491606884912
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 107.5803070195548,
                        "ram_power_avg": 0.6801584959030151,
                        "cpu_energy_total": 0.0028830900461250053,
                        "gpu_energy_total": 0.019340594361352437,
                        "ram_energy_total": 1.6959799725194312e-05,
                        "total_energy_kwh": 0.02224064420720263,
                        "total_energy_joules": 80066.31914592948
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20463036361317422,
                        "joules_per_token": 4.886860299434172,
                        "flops_per_joule": 211699141.08651423,
                        "joules_per_flop": 4.723684729506456e-09
                    },
                    "per-process_emissions": [
                        0.0020890875999135684,
                        0.0020690319339478065,
                        0.0020730402025272445,
                        0.0022414136743452245
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0162": {
            "setup": {
                "experiment_id": "0162",
                "date_time": "April 11, 2025 at 02:41:56 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.430405542998415,
                        "average_latency_ms_per_batch": 2928.800692874802,
                        "throughput_queries_per_sec": 5.462986962180413,
                        "throughput_tokens_per_sec": 699.2623311590928
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.0,
                        "cpu_memory_usage_bytes": 2023170048
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0162",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 53.497819401686115,
                            "process_1": 45.50851076850377,
                            "process_2": 71.43771178101241,
                            "process_3": 259.8771861270169
                        },
                        "ram_power": {
                            "process_0": 0.7047357559204102,
                            "process_1": 0.6712203025817871,
                            "process_2": 0.6682648658752441,
                            "process_3": 0.6764130592346191
                        },
                        "cpu_energy": {
                            "process_0": 0.000721068874250136,
                            "process_1": 0.0006698763614687098,
                            "process_2": 0.0006787239354999314,
                            "process_3": 0.0008134208749062281
                        },
                        "gpu_energy": {
                            "process_0": 0.004758585751310135,
                            "process_1": 0.004757293805832202,
                            "process_2": 0.004758961307166104,
                            "process_3": 0.0050657534970439955
                        },
                        "ram_energy": {
                            "process_0": 4.234598520496146e-06,
                            "process_1": 4.072604579405363e-06,
                            "process_2": 4.079299996372033e-06,
                            "process_3": 4.57329662892077e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0054838892240807674,
                            "process_1": 0.005431242771880316,
                            "process_2": 0.005441764542662408,
                            "process_3": 0.0058837476685791425
                        },
                        "total_energy_joules": {
                            "process_0": 19742.001206690762,
                            "process_1": 19552.473978769136,
                            "process_2": 19590.35235358467,
                            "process_3": 21181.491606884912
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 107.5803070195548,
                        "ram_power_avg": 0.6801584959030151,
                        "cpu_energy_total": 0.0028830900461250053,
                        "gpu_energy_total": 0.019340594361352437,
                        "ram_energy_total": 1.6959799725194312e-05,
                        "total_energy_kwh": 0.02224064420720263,
                        "total_energy_joules": 80066.31914592948
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20463036361317422,
                        "joules_per_token": 4.886860299434172,
                        "flops_per_joule": 211699141.08651423,
                        "joules_per_flop": 4.723684729506456e-09
                    },
                    "per-process_emissions": [
                        0.0020890875999135684,
                        0.0020690319339478065,
                        0.0020730402025272445,
                        0.0022414136743452245
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0163": {
            "setup": {
                "experiment_id": "0163",
                "date_time": "April 11, 2025 at 02:42:58 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.54016885099918,
                        "average_latency_ms_per_batch": 2942.5211063748975,
                        "throughput_queries_per_sec": 5.437514098144073,
                        "throughput_tokens_per_sec": 696.0018045624413
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.0,
                        "cpu_memory_usage_bytes": 2024202240
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0163",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 930.6941661887469,
                            "process_3": 438.45454351786873,
                            "process_0": 1303.0294783257818,
                            "process_2": 1187.4096040019624
                        },
                        "ram_power": {
                            "process_1": 0.6643924713134766,
                            "process_3": 0.6615457534790039,
                            "process_0": 0.7050046920776367,
                            "process_2": 0.6640934944152832
                        },
                        "cpu_energy": {
                            "process_1": 0.0006948537006874745,
                            "process_3": 0.0007710482446250354,
                            "process_0": 0.0006319230407188457,
                            "process_2": 0.0006812358458126369
                        },
                        "gpu_energy": {
                            "process_1": 0.0047634321440759575,
                            "process_3": 0.005064310718111975,
                            "process_0": 0.0047487929656979955,
                            "process_2": 0.004751461856722011
                        },
                        "ram_energy": {
                            "process_1": 4.145071516297734e-06,
                            "process_3": 4.544858111877072e-06,
                            "process_0": 4.064219226685627e-06,
                            "process_2": 4.111790697226554e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005462430916279729,
                            "process_3": 0.005839903820848888,
                            "process_0": 0.005384780225643528,
                            "process_2": 0.005436809493231874
                        },
                        "total_energy_joules": {
                            "process_1": 19664.751298607025,
                            "process_3": 21023.653755055995,
                            "process_0": 19385.2088123167,
                            "process_2": 19572.514175634744
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 964.89694800859,
                        "ram_power_avg": 0.6737591028213501,
                        "cpu_energy_total": 0.0027790608318439924,
                        "gpu_energy_total": 0.01932799768460794,
                        "ram_energy_total": 1.6865939552086985e-05,
                        "total_energy_kwh": 0.022123924456004015,
                        "total_energy_joules": 79646.12804161447
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20570993722933387,
                        "joules_per_token": 4.861213869727446,
                        "flops_per_joule": 212816007.63185593,
                        "joules_per_flop": 4.698894651429935e-09
                    },
                    "per-process_emissions": [
                        0.002080913057556763,
                        0.002224711360552384,
                        0.002051332026958902,
                        0.002071152576446682
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0163": {
            "setup": {
                "experiment_id": "0163",
                "date_time": "April 11, 2025 at 02:42:58 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.54016885099918,
                        "average_latency_ms_per_batch": 2942.5211063748975,
                        "throughput_queries_per_sec": 5.437514098144073,
                        "throughput_tokens_per_sec": 696.0018045624413
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.0,
                        "cpu_memory_usage_bytes": 2024202240
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0163",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 930.6941661887469,
                            "process_3": 438.45454351786873,
                            "process_0": 1303.0294783257818,
                            "process_2": 1187.4096040019624
                        },
                        "ram_power": {
                            "process_1": 0.6643924713134766,
                            "process_3": 0.6615457534790039,
                            "process_0": 0.7050046920776367,
                            "process_2": 0.6640934944152832
                        },
                        "cpu_energy": {
                            "process_1": 0.0006948537006874745,
                            "process_3": 0.0007710482446250354,
                            "process_0": 0.0006319230407188457,
                            "process_2": 0.0006812358458126369
                        },
                        "gpu_energy": {
                            "process_1": 0.0047634321440759575,
                            "process_3": 0.005064310718111975,
                            "process_0": 0.0047487929656979955,
                            "process_2": 0.004751461856722011
                        },
                        "ram_energy": {
                            "process_1": 4.145071516297734e-06,
                            "process_3": 4.544858111877072e-06,
                            "process_0": 4.064219226685627e-06,
                            "process_2": 4.111790697226554e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005462430916279729,
                            "process_3": 0.005839903820848888,
                            "process_0": 0.005384780225643528,
                            "process_2": 0.005436809493231874
                        },
                        "total_energy_joules": {
                            "process_1": 19664.751298607025,
                            "process_3": 21023.653755055995,
                            "process_0": 19385.2088123167,
                            "process_2": 19572.514175634744
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 964.89694800859,
                        "ram_power_avg": 0.6737591028213501,
                        "cpu_energy_total": 0.0027790608318439924,
                        "gpu_energy_total": 0.01932799768460794,
                        "ram_energy_total": 1.6865939552086985e-05,
                        "total_energy_kwh": 0.022123924456004015,
                        "total_energy_joules": 79646.12804161447
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20570993722933387,
                        "joules_per_token": 4.861213869727446,
                        "flops_per_joule": 212816007.63185593,
                        "joules_per_flop": 4.698894651429935e-09
                    },
                    "per-process_emissions": [
                        0.002080913057556763,
                        0.002224711360552384,
                        0.002051332026958902,
                        0.002071152576446682
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0164": {
            "setup": {
                "experiment_id": "0164",
                "date_time": "April 11, 2025 at 02:44:00 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.653556199999002,
                        "average_latency_ms_per_batch": 2956.6945249998753,
                        "throughput_queries_per_sec": 5.411448448500332,
                        "throughput_tokens_per_sec": 692.6654014080425
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 2019373056
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0164",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1178.7105485583231,
                            "process_1": 1180.3269234637485,
                            "process_0": 1022.6959624907024,
                            "process_3": 0.0
                        },
                        "ram_power": {
                            "process_2": 0.6654925346374513,
                            "process_1": 0.6640448570251465,
                            "process_0": 0.7029862403869629,
                            "process_3": 0.6646685600280762
                        },
                        "cpu_energy": {
                            "process_2": 0.0006282609452811699,
                            "process_1": 0.0006359071095002378,
                            "process_0": 0.0006275894238438584,
                            "process_3": 0.0007189792709374957
                        },
                        "gpu_energy": {
                            "process_2": 0.004738566846405945,
                            "process_1": 0.004740484347939999,
                            "process_0": 0.004741778793419971,
                            "process_3": 0.005057380990345972
                        },
                        "ram_energy": {
                            "process_2": 3.804852606660694e-06,
                            "process_1": 3.856559463822499e-06,
                            "process_0": 4.0161562525851065e-06,
                            "process_3": 4.366270677469484e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0053706326442937765,
                            "process_1": 0.0053802480169040596,
                            "process_0": 0.005373384373516412,
                            "process_3": 0.005780726531960938
                        },
                        "total_energy_joules": {
                            "process_2": 19334.277519457595,
                            "process_1": 19368.892860854616,
                            "process_0": 19344.183744659083,
                            "process_3": 20810.615515059377
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 845.4333586281934,
                        "ram_power_avg": 0.6742980480194092,
                        "cpu_energy_total": 0.002610736749562762,
                        "gpu_energy_total": 0.019278210978111887,
                        "ram_energy_total": 1.6043839000537785e-05,
                        "total_energy_kwh": 0.021904991566675188,
                        "total_energy_joules": 78857.96964003067
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2077659376064254,
                        "joules_per_token": 4.813108498537028,
                        "flops_per_joule": 214943030.74914175,
                        "joules_per_flop": 4.6523955511127585e-09
                    },
                    "per-process_emissions": [
                        0.0020459425058437144,
                        0.0020496054820396017,
                        0.0020469907770910775,
                        0.002202167772350519
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0164": {
            "setup": {
                "experiment_id": "0164",
                "date_time": "April 11, 2025 at 02:44:00 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.653556199999002,
                        "average_latency_ms_per_batch": 2956.6945249998753,
                        "throughput_queries_per_sec": 5.411448448500332,
                        "throughput_tokens_per_sec": 692.6654014080425
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 2019373056
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0164",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1178.7105485583231,
                            "process_1": 1180.3269234637485,
                            "process_0": 1022.6959624907024,
                            "process_3": 0.0
                        },
                        "ram_power": {
                            "process_2": 0.6654925346374513,
                            "process_1": 0.6640448570251465,
                            "process_0": 0.7029862403869629,
                            "process_3": 0.6646685600280762
                        },
                        "cpu_energy": {
                            "process_2": 0.0006282609452811699,
                            "process_1": 0.0006359071095002378,
                            "process_0": 0.0006275894238438584,
                            "process_3": 0.0007189792709374957
                        },
                        "gpu_energy": {
                            "process_2": 0.004738566846405945,
                            "process_1": 0.004740484347939999,
                            "process_0": 0.004741778793419971,
                            "process_3": 0.005057380990345972
                        },
                        "ram_energy": {
                            "process_2": 3.804852606660694e-06,
                            "process_1": 3.856559463822499e-06,
                            "process_0": 4.0161562525851065e-06,
                            "process_3": 4.366270677469484e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0053706326442937765,
                            "process_1": 0.0053802480169040596,
                            "process_0": 0.005373384373516412,
                            "process_3": 0.005780726531960938
                        },
                        "total_energy_joules": {
                            "process_2": 19334.277519457595,
                            "process_1": 19368.892860854616,
                            "process_0": 19344.183744659083,
                            "process_3": 20810.615515059377
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 845.4333586281934,
                        "ram_power_avg": 0.6742980480194092,
                        "cpu_energy_total": 0.002610736749562762,
                        "gpu_energy_total": 0.019278210978111887,
                        "ram_energy_total": 1.6043839000537785e-05,
                        "total_energy_kwh": 0.021904991566675188,
                        "total_energy_joules": 78857.96964003067
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2077659376064254,
                        "joules_per_token": 4.813108498537028,
                        "flops_per_joule": 214943030.74914175,
                        "joules_per_flop": 4.6523955511127585e-09
                    },
                    "per-process_emissions": [
                        0.0020459425058437144,
                        0.0020496054820396017,
                        0.0020469907770910775,
                        0.002202167772350519
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0165": {
            "setup": {
                "experiment_id": "0165",
                "date_time": "April 11, 2025 at 02:45:01 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.88322816300024,
                        "average_latency_ms_per_batch": 2985.40352037503,
                        "throughput_queries_per_sec": 5.359409503875061,
                        "throughput_tokens_per_sec": 686.0044164960078
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            10.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 2017050624
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0165",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 958.9757083911462,
                            "process_0": 989.9922628418183,
                            "process_2": 1408.042542485698,
                            "process_3": 373.10911881670813
                        },
                        "ram_power": {
                            "process_1": 0.6628704071044922,
                            "process_0": 0.702568531036377,
                            "process_2": 0.6627531051635742,
                            "process_3": 0.667862892150879
                        },
                        "cpu_energy": {
                            "process_1": 0.0006786579915313384,
                            "process_0": 0.0006415848183750654,
                            "process_2": 0.0006382395514999643,
                            "process_3": 0.000721659710937672
                        },
                        "gpu_energy": {
                            "process_1": 0.00478217299240194,
                            "process_0": 0.00478217299240194,
                            "process_2": 0.004773061874002088,
                            "process_3": 0.005074118225958046
                        },
                        "ram_energy": {
                            "process_1": 4.030249937418704e-06,
                            "process_0": 4.072022513635014e-06,
                            "process_2": 3.83821012164804e-06,
                            "process_3": 4.321315583036433e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005464861233870695,
                            "process_0": 0.00542782983329064,
                            "process_2": 0.005415139635623702,
                            "process_3": 0.0058000992524787544
                        },
                        "total_energy_joules": {
                            "process_1": 19673.500441934502,
                            "process_0": 19540.187399846305,
                            "process_2": 19494.502688245328,
                            "process_3": 20880.357308923514
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 932.5299081338426,
                        "ram_power_avg": 0.6740137338638306,
                        "cpu_energy_total": 0.00268014207234404,
                        "gpu_energy_total": 0.019411526084764014,
                        "ram_energy_total": 1.626179815573819e-05,
                        "total_energy_kwh": 0.02210792995526379,
                        "total_energy_joules": 79588.54783894964
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20585876291088545,
                        "joules_per_token": 4.857699453060891,
                        "flops_per_joule": 212969974.36680076,
                        "joules_per_flop": 4.69549758351235e-09
                    },
                    "per-process_emissions": [
                        0.0020818388870430414,
                        0.0020677317749920694,
                        0.0020628974441908496,
                        0.0022095478102317815
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0165": {
            "setup": {
                "experiment_id": "0165",
                "date_time": "April 11, 2025 at 02:45:01 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.88322816300024,
                        "average_latency_ms_per_batch": 2985.40352037503,
                        "throughput_queries_per_sec": 5.359409503875061,
                        "throughput_tokens_per_sec": 686.0044164960078
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            10.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 2017050624
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0165",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 958.9757083911462,
                            "process_0": 989.9922628418183,
                            "process_2": 1408.042542485698,
                            "process_3": 373.10911881670813
                        },
                        "ram_power": {
                            "process_1": 0.6628704071044922,
                            "process_0": 0.702568531036377,
                            "process_2": 0.6627531051635742,
                            "process_3": 0.667862892150879
                        },
                        "cpu_energy": {
                            "process_1": 0.0006786579915313384,
                            "process_0": 0.0006415848183750654,
                            "process_2": 0.0006382395514999643,
                            "process_3": 0.000721659710937672
                        },
                        "gpu_energy": {
                            "process_1": 0.00478217299240194,
                            "process_0": 0.00478217299240194,
                            "process_2": 0.004773061874002088,
                            "process_3": 0.005074118225958046
                        },
                        "ram_energy": {
                            "process_1": 4.030249937418704e-06,
                            "process_0": 4.072022513635014e-06,
                            "process_2": 3.83821012164804e-06,
                            "process_3": 4.321315583036433e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005464861233870695,
                            "process_0": 0.00542782983329064,
                            "process_2": 0.005415139635623702,
                            "process_3": 0.0058000992524787544
                        },
                        "total_energy_joules": {
                            "process_1": 19673.500441934502,
                            "process_0": 19540.187399846305,
                            "process_2": 19494.502688245328,
                            "process_3": 20880.357308923514
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 932.5299081338426,
                        "ram_power_avg": 0.6740137338638306,
                        "cpu_energy_total": 0.00268014207234404,
                        "gpu_energy_total": 0.019411526084764014,
                        "ram_energy_total": 1.626179815573819e-05,
                        "total_energy_kwh": 0.02210792995526379,
                        "total_energy_joules": 79588.54783894964
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20585876291088545,
                        "joules_per_token": 4.857699453060891,
                        "flops_per_joule": 212969974.36680076,
                        "joules_per_flop": 4.69549758351235e-09
                    },
                    "per-process_emissions": [
                        0.0020818388870430414,
                        0.0020677317749920694,
                        0.0020628974441908496,
                        0.0022095478102317815
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0166": {
            "setup": {
                "experiment_id": "0166",
                "date_time": "April 11, 2025 at 02:46:03 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.846678295001766,
                        "average_latency_ms_per_batch": 2980.8347868752207,
                        "throughput_queries_per_sec": 5.36762388524479,
                        "throughput_tokens_per_sec": 687.0558573113332
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 2018820096
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0166",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 933.3839138574134,
                            "process_1": 1006.1024225051572,
                            "process_3": 412.4784675095533,
                            "process_2": 961.6080016143594
                        },
                        "ram_power": {
                            "process_0": 0.7033853530883789,
                            "process_1": 0.6719083786010742,
                            "process_3": 0.6703505516052246,
                            "process_2": 0.6658186912536621
                        },
                        "cpu_energy": {
                            "process_0": 0.000679177683281239,
                            "process_1": 0.0006694923799062736,
                            "process_3": 0.0007538027192815094,
                            "process_2": 0.0006327010349686476
                        },
                        "gpu_energy": {
                            "process_0": 0.004761979087358009,
                            "process_1": 0.004753656302921877,
                            "process_3": 0.005059559325421986,
                            "process_2": 0.0046880981949201095
                        },
                        "ram_energy": {
                            "process_0": 4.322890878366633e-06,
                            "process_1": 4.078039029894516e-06,
                            "process_3": 4.520813875547926e-06,
                            "process_2": 3.8421104118118936e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0054454796615176155,
                            "process_1": 0.005427226721858046,
                            "process_3": 0.005817882858579044,
                            "process_2": 0.0053246413403005705
                        },
                        "total_energy_joules": {
                            "process_0": 19603.726781463414,
                            "process_1": 19538.016198688965,
                            "process_3": 20944.378290884557,
                            "process_2": 19168.708825082052
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 828.3932013716208,
                        "ram_power_avg": 0.677865743637085,
                        "cpu_energy_total": 0.0027351738174376697,
                        "gpu_energy_total": 0.019263292910621982,
                        "ram_energy_total": 1.676385419562097e-05,
                        "total_energy_kwh": 0.022015230582255275,
                        "total_energy_joules": 79254.830096119
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20672557092267746,
                        "joules_per_token": 4.8373309384838254,
                        "flops_per_joule": 213866725.5055035,
                        "joules_per_flop": 4.675809187410346e-09
                    },
                    "per-process_emissions": [
                        0.0020744554770551356,
                        0.0020675020196918225,
                        0.0022163224749756866,
                        0.0020284221185875024
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0166": {
            "setup": {
                "experiment_id": "0166",
                "date_time": "April 11, 2025 at 02:46:03 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.846678295001766,
                        "average_latency_ms_per_batch": 2980.8347868752207,
                        "throughput_queries_per_sec": 5.36762388524479,
                        "throughput_tokens_per_sec": 687.0558573113332
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 2018820096
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0166",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 933.3839138574134,
                            "process_1": 1006.1024225051572,
                            "process_3": 412.4784675095533,
                            "process_2": 961.6080016143594
                        },
                        "ram_power": {
                            "process_0": 0.7033853530883789,
                            "process_1": 0.6719083786010742,
                            "process_3": 0.6703505516052246,
                            "process_2": 0.6658186912536621
                        },
                        "cpu_energy": {
                            "process_0": 0.000679177683281239,
                            "process_1": 0.0006694923799062736,
                            "process_3": 0.0007538027192815094,
                            "process_2": 0.0006327010349686476
                        },
                        "gpu_energy": {
                            "process_0": 0.004761979087358009,
                            "process_1": 0.004753656302921877,
                            "process_3": 0.005059559325421986,
                            "process_2": 0.0046880981949201095
                        },
                        "ram_energy": {
                            "process_0": 4.322890878366633e-06,
                            "process_1": 4.078039029894516e-06,
                            "process_3": 4.520813875547926e-06,
                            "process_2": 3.8421104118118936e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0054454796615176155,
                            "process_1": 0.005427226721858046,
                            "process_3": 0.005817882858579044,
                            "process_2": 0.0053246413403005705
                        },
                        "total_energy_joules": {
                            "process_0": 19603.726781463414,
                            "process_1": 19538.016198688965,
                            "process_3": 20944.378290884557,
                            "process_2": 19168.708825082052
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 828.3932013716208,
                        "ram_power_avg": 0.677865743637085,
                        "cpu_energy_total": 0.0027351738174376697,
                        "gpu_energy_total": 0.019263292910621982,
                        "ram_energy_total": 1.676385419562097e-05,
                        "total_energy_kwh": 0.022015230582255275,
                        "total_energy_joules": 79254.830096119
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20672557092267746,
                        "joules_per_token": 4.8373309384838254,
                        "flops_per_joule": 213866725.5055035,
                        "joules_per_flop": 4.675809187410346e-09
                    },
                    "per-process_emissions": [
                        0.0020744554770551356,
                        0.0020675020196918225,
                        0.0022163224749756866,
                        0.0020284221185875024
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0172": {
            "setup": {
                "experiment_id": "0172",
                "date_time": "April 11, 2025 at 02:50:00 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.873191131000567,
                        "average_latency_ms_per_batch": 2859.148891375071,
                        "throughput_queries_per_sec": 5.596070931550894,
                        "throughput_tokens_per_sec": 716.2970792385145
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 1976737792
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0172",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 577.7409622135535,
                            "process_1": 949.2192545862339,
                            "process_0": 38.68928474053624,
                            "process_3": 605.0472158828413
                        },
                        "ram_power": {
                            "process_2": 0.6267414093017578,
                            "process_1": 0.6199193000793457,
                            "process_0": 0.6891360282897949,
                            "process_3": 0.6203012466430664
                        },
                        "cpu_energy": {
                            "process_2": 0.0007286934662811858,
                            "process_1": 0.0007307294582499256,
                            "process_0": 0.0007770778524064211,
                            "process_3": 0.0008781448772499515
                        },
                        "gpu_energy": {
                            "process_2": 0.004667322900521881,
                            "process_1": 0.004667322900521881,
                            "process_0": 0.004655307890909899,
                            "process_3": 0.004987328434303967
                        },
                        "ram_energy": {
                            "process_2": 3.7934220222847556e-06,
                            "process_1": 3.7544635120926917e-06,
                            "process_0": 4.1307071933460895e-06,
                            "process_3": 4.248565787701497e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005399809788825352,
                            "process_1": 0.005401806822283897,
                            "process_0": 0.005436516450509665,
                            "process_3": 0.005869721877341622
                        },
                        "total_energy_joules": {
                            "process_2": 19439.315239771266,
                            "process_1": 19446.504560222027,
                            "process_0": 19571.459221834793,
                            "process_3": 21130.99875842984
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 542.6741793557912,
                        "ram_power_avg": 0.6390244960784912,
                        "cpu_energy_total": 0.0031146456541874842,
                        "gpu_energy_total": 0.018977282126257627,
                        "ram_energy_total": 1.5927158515425034e-05,
                        "total_energy_kwh": 0.022107854938960534,
                        "total_energy_joules": 79588.27778025792
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20585946143018682,
                        "joules_per_token": 4.857682969986445,
                        "flops_per_joule": 212970697.01584226,
                        "joules_per_flop": 4.695481650818906e-09
                    },
                    "per-process_emissions": [
                        0.0020570575390530177,
                        0.0020578183089490507,
                        0.002071040941821657,
                        0.0022360705491732908
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0172": {
            "setup": {
                "experiment_id": "0172",
                "date_time": "April 11, 2025 at 02:50:00 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.873191131000567,
                        "average_latency_ms_per_batch": 2859.148891375071,
                        "throughput_queries_per_sec": 5.596070931550894,
                        "throughput_tokens_per_sec": 716.2970792385145
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 1976737792
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0172",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 577.7409622135535,
                            "process_1": 949.2192545862339,
                            "process_0": 38.68928474053624,
                            "process_3": 605.0472158828413
                        },
                        "ram_power": {
                            "process_2": 0.6267414093017578,
                            "process_1": 0.6199193000793457,
                            "process_0": 0.6891360282897949,
                            "process_3": 0.6203012466430664
                        },
                        "cpu_energy": {
                            "process_2": 0.0007286934662811858,
                            "process_1": 0.0007307294582499256,
                            "process_0": 0.0007770778524064211,
                            "process_3": 0.0008781448772499515
                        },
                        "gpu_energy": {
                            "process_2": 0.004667322900521881,
                            "process_1": 0.004667322900521881,
                            "process_0": 0.004655307890909899,
                            "process_3": 0.004987328434303967
                        },
                        "ram_energy": {
                            "process_2": 3.7934220222847556e-06,
                            "process_1": 3.7544635120926917e-06,
                            "process_0": 4.1307071933460895e-06,
                            "process_3": 4.248565787701497e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005399809788825352,
                            "process_1": 0.005401806822283897,
                            "process_0": 0.005436516450509665,
                            "process_3": 0.005869721877341622
                        },
                        "total_energy_joules": {
                            "process_2": 19439.315239771266,
                            "process_1": 19446.504560222027,
                            "process_0": 19571.459221834793,
                            "process_3": 21130.99875842984
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 542.6741793557912,
                        "ram_power_avg": 0.6390244960784912,
                        "cpu_energy_total": 0.0031146456541874842,
                        "gpu_energy_total": 0.018977282126257627,
                        "ram_energy_total": 1.5927158515425034e-05,
                        "total_energy_kwh": 0.022107854938960534,
                        "total_energy_joules": 79588.27778025792
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20585946143018682,
                        "joules_per_token": 4.857682969986445,
                        "flops_per_joule": 212970697.01584226,
                        "joules_per_flop": 4.695481650818906e-09
                    },
                    "per-process_emissions": [
                        0.0020570575390530177,
                        0.0020578183089490507,
                        0.002071040941821657,
                        0.0022360705491732908
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0173": {
            "setup": {
                "experiment_id": "0173",
                "date_time": "April 11, 2025 at 02:51:04 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.07682016900253,
                        "average_latency_ms_per_batch": 2884.6025211253163,
                        "throughput_queries_per_sec": 5.54669140126738,
                        "throughput_tokens_per_sec": 709.9764993622247
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 1998172160
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0173",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 31000.47471049894,
                            "process_3": 1061.6503077568218,
                            "process_2": 147.94284811508837,
                            "process_0": 1465.047114279995
                        },
                        "ram_power": {
                            "process_1": 0.6312532424926758,
                            "process_3": 0.6450033187866211,
                            "process_2": 0.6196990013122559,
                            "process_0": 0.6960611343383789
                        },
                        "cpu_energy": {
                            "process_1": 0.0007188536351876566,
                            "process_3": 0.0007046686639687322,
                            "process_2": 0.0007604509601562768,
                            "process_0": 0.0007129269879999357
                        },
                        "gpu_energy": {
                            "process_1": 0.004635366208290137,
                            "process_3": 0.005046327648170146,
                            "process_2": 0.0046352048192719875,
                            "process_0": 0.004666419010909917
                        },
                        "ram_energy": {
                            "process_1": 3.7484448496758255e-06,
                            "process_3": 4.524126527397412e-06,
                            "process_2": 3.6276738837487105e-06,
                            "process_0": 3.7916434167189753e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005357968288327469,
                            "process_3": 0.005755520438666277,
                            "process_2": 0.0053992834533120124,
                            "process_0": 0.005383137642326572
                        },
                        "total_energy_joules": {
                            "process_1": 19288.685837978886,
                            "process_3": 20719.873579198596,
                            "process_2": 19437.420431923245,
                            "process_0": 19379.29551237566
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 8418.778745162712,
                        "ram_power_avg": 0.6480041742324829,
                        "cpu_energy_total": 0.0028969002473126014,
                        "gpu_energy_total": 0.018983317686642187,
                        "ram_energy_total": 1.569188867754092e-05,
                        "total_energy_kwh": 0.02189590982263233,
                        "total_energy_joules": 78825.2753614764
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20785211247111243,
                        "joules_per_token": 4.811112998136987,
                        "flops_per_joule": 215032182.44938496,
                        "joules_per_flop": 4.650466681820446e-09
                    },
                    "per-process_emissions": [
                        0.0020411180194383496,
                        0.0021925655111099183,
                        0.0020568570315392114,
                        0.0020507062848443077
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0173": {
            "setup": {
                "experiment_id": "0173",
                "date_time": "April 11, 2025 at 02:51:04 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.07682016900253,
                        "average_latency_ms_per_batch": 2884.6025211253163,
                        "throughput_queries_per_sec": 5.54669140126738,
                        "throughput_tokens_per_sec": 709.9764993622247
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 1998172160
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0173",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 31000.47471049894,
                            "process_3": 1061.6503077568218,
                            "process_2": 147.94284811508837,
                            "process_0": 1465.047114279995
                        },
                        "ram_power": {
                            "process_1": 0.6312532424926758,
                            "process_3": 0.6450033187866211,
                            "process_2": 0.6196990013122559,
                            "process_0": 0.6960611343383789
                        },
                        "cpu_energy": {
                            "process_1": 0.0007188536351876566,
                            "process_3": 0.0007046686639687322,
                            "process_2": 0.0007604509601562768,
                            "process_0": 0.0007129269879999357
                        },
                        "gpu_energy": {
                            "process_1": 0.004635366208290137,
                            "process_3": 0.005046327648170146,
                            "process_2": 0.0046352048192719875,
                            "process_0": 0.004666419010909917
                        },
                        "ram_energy": {
                            "process_1": 3.7484448496758255e-06,
                            "process_3": 4.524126527397412e-06,
                            "process_2": 3.6276738837487105e-06,
                            "process_0": 3.7916434167189753e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005357968288327469,
                            "process_3": 0.005755520438666277,
                            "process_2": 0.0053992834533120124,
                            "process_0": 0.005383137642326572
                        },
                        "total_energy_joules": {
                            "process_1": 19288.685837978886,
                            "process_3": 20719.873579198596,
                            "process_2": 19437.420431923245,
                            "process_0": 19379.29551237566
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 8418.778745162712,
                        "ram_power_avg": 0.6480041742324829,
                        "cpu_energy_total": 0.0028969002473126014,
                        "gpu_energy_total": 0.018983317686642187,
                        "ram_energy_total": 1.569188867754092e-05,
                        "total_energy_kwh": 0.02189590982263233,
                        "total_energy_joules": 78825.2753614764
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20785211247111243,
                        "joules_per_token": 4.811112998136987,
                        "flops_per_joule": 215032182.44938496,
                        "joules_per_flop": 4.650466681820446e-09
                    },
                    "per-process_emissions": [
                        0.0020411180194383496,
                        0.0021925655111099183,
                        0.0020568570315392114,
                        0.0020507062848443077
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0174": {
            "setup": {
                "experiment_id": "0174",
                "date_time": "April 11, 2025 at 02:52:05 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.701395044998208,
                        "average_latency_ms_per_batch": 2837.674380624776,
                        "throughput_queries_per_sec": 5.638420006624315,
                        "throughput_tokens_per_sec": 721.7177608479124
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 12903776256,
                        "gpu_max_memory_reserved_bytes": 12903776256
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 2010185728
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0174",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 0.0,
                            "process_0": 771.4246442060597,
                            "process_2": 488.13133787145694,
                            "process_1": 796.7459302283127
                        },
                        "ram_power": {
                            "process_3": 0.6204242706298828,
                            "process_0": 0.7002124786376953,
                            "process_2": 0.6320257186889648,
                            "process_1": 0.6329798698425293
                        },
                        "cpu_energy": {
                            "process_3": 0.0008274485890625558,
                            "process_0": 0.0007547858100002147,
                            "process_2": 0.0007622066893124496,
                            "process_1": 0.0007535878222811902
                        },
                        "gpu_energy": {
                            "process_3": 0.005023435685411914,
                            "process_0": 0.0046145425805199924,
                            "process_2": 0.004676043185275924,
                            "process_1": 0.004588226448355853
                        },
                        "ram_energy": {
                            "process_3": 4.282082338635801e-06,
                            "process_0": 4.0691183077301426e-06,
                            "process_2": 3.6849999309554835e-06,
                            "process_1": 3.669881430498872e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005855166356813107,
                            "process_0": 0.005373397508827939,
                            "process_2": 0.00544193487451933,
                            "process_1": 0.005345484152067542
                        },
                        "total_energy_joules": {
                            "process_3": 21078.598884527186,
                            "process_0": 19344.231031780582,
                            "process_2": 19590.96554826959,
                            "process_1": 19243.742947443152
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 514.0754780764573,
                        "ram_power_avg": 0.6464105844497681,
                        "cpu_energy_total": 0.0030980289106564106,
                        "gpu_energy_total": 0.018902247899563684,
                        "ram_energy_total": 1.57060820078203e-05,
                        "total_energy_kwh": 0.02201598289222792,
                        "total_energy_joules": 79257.53841202051
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20671850688609247,
                        "joules_per_token": 4.837496240968049,
                        "flops_per_joule": 213859417.4479345,
                        "joules_per_flop": 4.675968970332843e-09
                    },
                    "per-process_emissions": [
                        0.0022305256236279532,
                        0.0020469957809880037,
                        0.002073105090448139,
                        0.00203636218773013
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0174": {
            "setup": {
                "experiment_id": "0174",
                "date_time": "April 11, 2025 at 02:52:05 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.701395044998208,
                        "average_latency_ms_per_batch": 2837.674380624776,
                        "throughput_queries_per_sec": 5.638420006624315,
                        "throughput_tokens_per_sec": 721.7177608479124
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 12903776256,
                        "gpu_max_memory_reserved_bytes": 12903776256
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 2010185728
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0174",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 0.0,
                            "process_0": 771.4246442060597,
                            "process_2": 488.13133787145694,
                            "process_1": 796.7459302283127
                        },
                        "ram_power": {
                            "process_3": 0.6204242706298828,
                            "process_0": 0.7002124786376953,
                            "process_2": 0.6320257186889648,
                            "process_1": 0.6329798698425293
                        },
                        "cpu_energy": {
                            "process_3": 0.0008274485890625558,
                            "process_0": 0.0007547858100002147,
                            "process_2": 0.0007622066893124496,
                            "process_1": 0.0007535878222811902
                        },
                        "gpu_energy": {
                            "process_3": 0.005023435685411914,
                            "process_0": 0.0046145425805199924,
                            "process_2": 0.004676043185275924,
                            "process_1": 0.004588226448355853
                        },
                        "ram_energy": {
                            "process_3": 4.282082338635801e-06,
                            "process_0": 4.0691183077301426e-06,
                            "process_2": 3.6849999309554835e-06,
                            "process_1": 3.669881430498872e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005855166356813107,
                            "process_0": 0.005373397508827939,
                            "process_2": 0.00544193487451933,
                            "process_1": 0.005345484152067542
                        },
                        "total_energy_joules": {
                            "process_3": 21078.598884527186,
                            "process_0": 19344.231031780582,
                            "process_2": 19590.96554826959,
                            "process_1": 19243.742947443152
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 514.0754780764573,
                        "ram_power_avg": 0.6464105844497681,
                        "cpu_energy_total": 0.0030980289106564106,
                        "gpu_energy_total": 0.018902247899563684,
                        "ram_energy_total": 1.57060820078203e-05,
                        "total_energy_kwh": 0.02201598289222792,
                        "total_energy_joules": 79257.53841202051
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20671850688609247,
                        "joules_per_token": 4.837496240968049,
                        "flops_per_joule": 213859417.4479345,
                        "joules_per_flop": 4.675968970332843e-09
                    },
                    "per-process_emissions": [
                        0.0022305256236279532,
                        0.0020469957809880037,
                        0.002073105090448139,
                        0.00203636218773013
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0175": {
            "setup": {
                "experiment_id": "0175",
                "date_time": "April 11, 2025 at 02:53:06 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.59847955099758,
                        "average_latency_ms_per_batch": 2824.8099438746976,
                        "throughput_queries_per_sec": 5.664097874865639,
                        "throughput_tokens_per_sec": 725.0045279828018
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1999220736
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0175",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 797.1462510011953,
                            "process_3": 0.0,
                            "process_0": 886.2994776898252,
                            "process_1": 773.4050203693262
                        },
                        "ram_power": {
                            "process_2": 0.6203556060791016,
                            "process_3": 0.6198534965515138,
                            "process_0": 0.6970138549804689,
                            "process_1": 0.6306467056274415
                        },
                        "cpu_energy": {
                            "process_2": 0.0006352418873437956,
                            "process_3": 0.0007407351909686214,
                            "process_0": 0.0006292735428751824,
                            "process_1": 0.0006397281238751021
                        },
                        "gpu_energy": {
                            "process_2": 0.004638553988617955,
                            "process_3": 0.005020955127872012,
                            "process_0": 0.004610928966518074,
                            "process_1": 0.004648351774233894
                        },
                        "ram_energy": {
                            "process_2": 3.6211946378643997e-06,
                            "process_3": 4.1229237525648775e-06,
                            "process_0": 4.052441740944709e-06,
                            "process_1": 3.6810668469187263e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005277417070599614,
                            "process_3": 0.005765813242593202,
                            "process_0": 0.005244254951134201,
                            "process_1": 0.005291760964955914
                        },
                        "total_energy_joules": {
                            "process_2": 18998.70145415861,
                            "process_3": 20756.92767333553,
                            "process_0": 18879.317824083126,
                            "process_1": 19050.33947384129
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 614.2126872650867,
                        "ram_power_avg": 0.6419674158096313,
                        "cpu_energy_total": 0.0026449787450627013,
                        "gpu_energy_total": 0.018918789857241936,
                        "ram_energy_total": 1.5477626978292713e-05,
                        "total_energy_kwh": 0.02157924622928293,
                        "total_energy_joules": 77685.28642541856
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21090222812951065,
                        "joules_per_token": 4.7415335953014255,
                        "flops_per_joule": 218187661.69350165,
                        "joules_per_flop": 4.58321058229565e-09
                    },
                    "per-process_emissions": [
                        0.002010432033044923,
                        0.0021964865547658805,
                        0.001997798923634574,
                        0.0020158963395999555
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0175": {
            "setup": {
                "experiment_id": "0175",
                "date_time": "April 11, 2025 at 02:53:06 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.59847955099758,
                        "average_latency_ms_per_batch": 2824.8099438746976,
                        "throughput_queries_per_sec": 5.664097874865639,
                        "throughput_tokens_per_sec": 725.0045279828018
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1999220736
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0175",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 797.1462510011953,
                            "process_3": 0.0,
                            "process_0": 886.2994776898252,
                            "process_1": 773.4050203693262
                        },
                        "ram_power": {
                            "process_2": 0.6203556060791016,
                            "process_3": 0.6198534965515138,
                            "process_0": 0.6970138549804689,
                            "process_1": 0.6306467056274415
                        },
                        "cpu_energy": {
                            "process_2": 0.0006352418873437956,
                            "process_3": 0.0007407351909686214,
                            "process_0": 0.0006292735428751824,
                            "process_1": 0.0006397281238751021
                        },
                        "gpu_energy": {
                            "process_2": 0.004638553988617955,
                            "process_3": 0.005020955127872012,
                            "process_0": 0.004610928966518074,
                            "process_1": 0.004648351774233894
                        },
                        "ram_energy": {
                            "process_2": 3.6211946378643997e-06,
                            "process_3": 4.1229237525648775e-06,
                            "process_0": 4.052441740944709e-06,
                            "process_1": 3.6810668469187263e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005277417070599614,
                            "process_3": 0.005765813242593202,
                            "process_0": 0.005244254951134201,
                            "process_1": 0.005291760964955914
                        },
                        "total_energy_joules": {
                            "process_2": 18998.70145415861,
                            "process_3": 20756.92767333553,
                            "process_0": 18879.317824083126,
                            "process_1": 19050.33947384129
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 614.2126872650867,
                        "ram_power_avg": 0.6419674158096313,
                        "cpu_energy_total": 0.0026449787450627013,
                        "gpu_energy_total": 0.018918789857241936,
                        "ram_energy_total": 1.5477626978292713e-05,
                        "total_energy_kwh": 0.02157924622928293,
                        "total_energy_joules": 77685.28642541856
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21090222812951065,
                        "joules_per_token": 4.7415335953014255,
                        "flops_per_joule": 218187661.69350165,
                        "joules_per_flop": 4.58321058229565e-09
                    },
                    "per-process_emissions": [
                        0.002010432033044923,
                        0.0021964865547658805,
                        0.001997798923634574,
                        0.0020158963395999555
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0176": {
            "setup": {
                "experiment_id": "0176",
                "date_time": "April 11, 2025 at 02:54:09 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.28657142999691,
                        "average_latency_ms_per_batch": 2910.821428749614,
                        "throughput_queries_per_sec": 5.496730181374621,
                        "throughput_tokens_per_sec": 703.5814632159515
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 2013782016
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0176",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 728.3327076111658,
                            "process_0": 807.718125448761,
                            "process_2": 646.4420666597293,
                            "process_1": 820.3063822772825
                        },
                        "ram_power": {
                            "process_3": 0.6560454368591309,
                            "process_0": 0.7013211250305176,
                            "process_2": 0.6500730514526368,
                            "process_1": 0.6585516929626465
                        },
                        "cpu_energy": {
                            "process_3": 0.000749180107156235,
                            "process_0": 0.000661944705499991,
                            "process_2": 0.0006709256712812817,
                            "process_1": 0.000661630930093736
                        },
                        "gpu_energy": {
                            "process_3": 0.005085092123625817,
                            "process_0": 0.004711800991659826,
                            "process_2": 0.004738821013275801,
                            "process_1": 0.004691080419527732
                        },
                        "ram_energy": {
                            "process_3": 4.433402054997166e-06,
                            "process_0": 4.238676214109319e-06,
                            "process_2": 3.945804558167281e-06,
                            "process_1": 3.93379921086251e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0058387056328370514,
                            "process_0": 0.005377984373373927,
                            "process_2": 0.005413692489115248,
                            "process_1": 0.00535664514883233
                        },
                        "total_energy_joules": {
                            "process_3": 21019.340278213385,
                            "process_0": 19360.743744146137,
                            "process_2": 19489.29296081489,
                            "process_1": 19283.922535796388
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 750.6998204992346,
                        "ram_power_avg": 0.6664978265762329,
                        "cpu_energy_total": 0.0027436814140312437,
                        "gpu_energy_total": 0.019226794548089177,
                        "ram_energy_total": 1.6551682038136276e-05,
                        "total_energy_kwh": 0.021987027644158552,
                        "total_energy_joules": 79153.2995189708
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2069907394836171,
                        "joules_per_token": 4.831134003843433,
                        "flops_per_joule": 214141054.0831538,
                        "joules_per_flop": 4.6698191726080076e-09
                    },
                    "per-process_emissions": [
                        0.002224254910829275,
                        0.0020487431470367977,
                        0.0020623461537284536,
                        0.0020406139694476764
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0176": {
            "setup": {
                "experiment_id": "0176",
                "date_time": "April 11, 2025 at 02:54:09 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.28657142999691,
                        "average_latency_ms_per_batch": 2910.821428749614,
                        "throughput_queries_per_sec": 5.496730181374621,
                        "throughput_tokens_per_sec": 703.5814632159515
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 2013782016
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0176",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 728.3327076111658,
                            "process_0": 807.718125448761,
                            "process_2": 646.4420666597293,
                            "process_1": 820.3063822772825
                        },
                        "ram_power": {
                            "process_3": 0.6560454368591309,
                            "process_0": 0.7013211250305176,
                            "process_2": 0.6500730514526368,
                            "process_1": 0.6585516929626465
                        },
                        "cpu_energy": {
                            "process_3": 0.000749180107156235,
                            "process_0": 0.000661944705499991,
                            "process_2": 0.0006709256712812817,
                            "process_1": 0.000661630930093736
                        },
                        "gpu_energy": {
                            "process_3": 0.005085092123625817,
                            "process_0": 0.004711800991659826,
                            "process_2": 0.004738821013275801,
                            "process_1": 0.004691080419527732
                        },
                        "ram_energy": {
                            "process_3": 4.433402054997166e-06,
                            "process_0": 4.238676214109319e-06,
                            "process_2": 3.945804558167281e-06,
                            "process_1": 3.93379921086251e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0058387056328370514,
                            "process_0": 0.005377984373373927,
                            "process_2": 0.005413692489115248,
                            "process_1": 0.00535664514883233
                        },
                        "total_energy_joules": {
                            "process_3": 21019.340278213385,
                            "process_0": 19360.743744146137,
                            "process_2": 19489.29296081489,
                            "process_1": 19283.922535796388
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 750.6998204992346,
                        "ram_power_avg": 0.6664978265762329,
                        "cpu_energy_total": 0.0027436814140312437,
                        "gpu_energy_total": 0.019226794548089177,
                        "ram_energy_total": 1.6551682038136276e-05,
                        "total_energy_kwh": 0.021987027644158552,
                        "total_energy_joules": 79153.2995189708
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2069907394836171,
                        "joules_per_token": 4.831134003843433,
                        "flops_per_joule": 214141054.0831538,
                        "joules_per_flop": 4.6698191726080076e-09
                    },
                    "per-process_emissions": [
                        0.002224254910829275,
                        0.0020487431470367977,
                        0.0020623461537284536,
                        0.0020406139694476764
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0177": {
            "setup": {
                "experiment_id": "0177",
                "date_time": "April 11, 2025 at 02:55:09 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.371715217997917,
                        "average_latency_ms_per_batch": 2921.4644022497396,
                        "throughput_queries_per_sec": 5.476705445282454,
                        "throughput_tokens_per_sec": 701.0182969961542
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 2015571968
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0177",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1507.774547436249,
                            "process_1": 2644.588979333865,
                            "process_3": 13.140209582099454,
                            "process_2": 2546.422016219924
                        },
                        "ram_power": {
                            "process_0": 0.7024569511413574,
                            "process_1": 0.6567907333374023,
                            "process_3": 0.650914192199707,
                            "process_2": 0.6506180763244629
                        },
                        "cpu_energy": {
                            "process_0": 0.0006736214913435105,
                            "process_1": 0.0006678518196874849,
                            "process_3": 0.0008009673245937846,
                            "process_2": 0.0006722182127500675
                        },
                        "gpu_energy": {
                            "process_0": 0.004775413820327951,
                            "process_1": 0.004777279099597992,
                            "process_3": 0.005020720961018049,
                            "process_2": 0.004769344648806045
                        },
                        "ram_energy": {
                            "process_0": 4.302088245318447e-06,
                            "process_1": 3.970853721782088e-06,
                            "process_3": 4.655122344198728e-06,
                            "process_2": 3.968622426232401e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.00545333739991678,
                            "process_1": 0.005449101773007258,
                            "process_3": 0.005826343407956033,
                            "process_2": 0.0054455314839823455
                        },
                        "total_energy_joules": {
                            "process_0": 19632.01463970041,
                            "process_1": 19616.76638282613,
                            "process_3": 20974.83626864172,
                            "process_2": 19603.913342336444
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1677.9814381430342,
                        "ram_power_avg": 0.6651949882507324,
                        "cpu_energy_total": 0.0028146588483748476,
                        "gpu_energy_total": 0.019342758529750037,
                        "ram_energy_total": 1.6896686737531664e-05,
                        "total_energy_kwh": 0.022174314064862415,
                        "total_energy_joules": 79827.5306335047
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20524247549658528,
                        "joules_per_token": 4.872285805267621,
                        "flops_per_joule": 212332397.84117618,
                        "joules_per_flop": 4.709596887555502e-09
                    },
                    "per-process_emissions": [
                        0.0020774488824982975,
                        0.002075835320427115,
                        0.0022195455212608507,
                        0.0020744752188230748
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0177": {
            "setup": {
                "experiment_id": "0177",
                "date_time": "April 11, 2025 at 02:55:09 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.371715217997917,
                        "average_latency_ms_per_batch": 2921.4644022497396,
                        "throughput_queries_per_sec": 5.476705445282454,
                        "throughput_tokens_per_sec": 701.0182969961542
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 2015571968
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0177",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1507.774547436249,
                            "process_1": 2644.588979333865,
                            "process_3": 13.140209582099454,
                            "process_2": 2546.422016219924
                        },
                        "ram_power": {
                            "process_0": 0.7024569511413574,
                            "process_1": 0.6567907333374023,
                            "process_3": 0.650914192199707,
                            "process_2": 0.6506180763244629
                        },
                        "cpu_energy": {
                            "process_0": 0.0006736214913435105,
                            "process_1": 0.0006678518196874849,
                            "process_3": 0.0008009673245937846,
                            "process_2": 0.0006722182127500675
                        },
                        "gpu_energy": {
                            "process_0": 0.004775413820327951,
                            "process_1": 0.004777279099597992,
                            "process_3": 0.005020720961018049,
                            "process_2": 0.004769344648806045
                        },
                        "ram_energy": {
                            "process_0": 4.302088245318447e-06,
                            "process_1": 3.970853721782088e-06,
                            "process_3": 4.655122344198728e-06,
                            "process_2": 3.968622426232401e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.00545333739991678,
                            "process_1": 0.005449101773007258,
                            "process_3": 0.005826343407956033,
                            "process_2": 0.0054455314839823455
                        },
                        "total_energy_joules": {
                            "process_0": 19632.01463970041,
                            "process_1": 19616.76638282613,
                            "process_3": 20974.83626864172,
                            "process_2": 19603.913342336444
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1677.9814381430342,
                        "ram_power_avg": 0.6651949882507324,
                        "cpu_energy_total": 0.0028146588483748476,
                        "gpu_energy_total": 0.019342758529750037,
                        "ram_energy_total": 1.6896686737531664e-05,
                        "total_energy_kwh": 0.022174314064862415,
                        "total_energy_joules": 79827.5306335047
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20524247549658528,
                        "joules_per_token": 4.872285805267621,
                        "flops_per_joule": 212332397.84117618,
                        "joules_per_flop": 4.709596887555502e-09
                    },
                    "per-process_emissions": [
                        0.0020774488824982975,
                        0.002075835320427115,
                        0.0022195455212608507,
                        0.0020744752188230748
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0178": {
            "setup": {
                "experiment_id": "0178",
                "date_time": "April 11, 2025 at 02:56:10 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.444686383996668,
                        "average_latency_ms_per_batch": 2930.5857979995835,
                        "throughput_queries_per_sec": 5.45965929778326,
                        "throughput_tokens_per_sec": 698.8363901162572
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            3.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.0,
                        "cpu_memory_usage_bytes": 2014027776
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0178",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 3250.96622414159,
                            "process_0": 52.11142761814016,
                            "process_1": 1049.9266729050191,
                            "process_3": 450.4156934014709
                        },
                        "ram_power": {
                            "process_2": 0.6559181213378906,
                            "process_0": 0.7019248008728028,
                            "process_1": 0.6521615982055664,
                            "process_3": 0.6507754325866699
                        },
                        "cpu_energy": {
                            "process_2": 0.0006655698255626703,
                            "process_0": 0.0006661676490627997,
                            "process_1": 0.0006325367172189544,
                            "process_3": 0.0007637203689374133
                        },
                        "gpu_energy": {
                            "process_2": 0.004765836868222018,
                            "process_0": 0.004795792447742142,
                            "process_1": 0.004781029935932074,
                            "process_3": 0.005074829615415999
                        },
                        "ram_energy": {
                            "process_2": 3.631007003517781e-06,
                            "process_0": 3.8572559117538315e-06,
                            "process_1": 3.735801204744296e-06,
                            "process_3": 4.1160342691738795e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0054350377007882054,
                            "process_0": 0.005465817352716696,
                            "process_1": 0.005417302454355772,
                            "process_3": 0.005842666018622584
                        },
                        "total_energy_joules": {
                            "process_2": 19566.13572283754,
                            "process_0": 19676.942469780104,
                            "process_1": 19502.28883568078,
                            "process_3": 21033.597667041304
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1200.855004516555,
                        "ram_power_avg": 0.6651949882507324,
                        "cpu_energy_total": 0.0027279945607818377,
                        "gpu_energy_total": 0.019417488867312233,
                        "ram_energy_total": 1.5340098389189786e-05,
                        "total_energy_kwh": 0.022160823526483256,
                        "total_energy_joules": 79778.96469533973
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2053674181229011,
                        "joules_per_token": 4.869321575643294,
                        "flops_per_joule": 212461656.5015681,
                        "joules_per_flop": 4.706731635562764e-09
                    },
                    "per-process_emissions": [
                        0.0020704776121152667,
                        0.0020822031205174254,
                        0.0020637213699868317,
                        0.0022257636197942736
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0178": {
            "setup": {
                "experiment_id": "0178",
                "date_time": "April 11, 2025 at 02:56:10 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.444686383996668,
                        "average_latency_ms_per_batch": 2930.5857979995835,
                        "throughput_queries_per_sec": 5.45965929778326,
                        "throughput_tokens_per_sec": 698.8363901162572
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            3.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.0,
                        "cpu_memory_usage_bytes": 2014027776
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0178",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 3250.96622414159,
                            "process_0": 52.11142761814016,
                            "process_1": 1049.9266729050191,
                            "process_3": 450.4156934014709
                        },
                        "ram_power": {
                            "process_2": 0.6559181213378906,
                            "process_0": 0.7019248008728028,
                            "process_1": 0.6521615982055664,
                            "process_3": 0.6507754325866699
                        },
                        "cpu_energy": {
                            "process_2": 0.0006655698255626703,
                            "process_0": 0.0006661676490627997,
                            "process_1": 0.0006325367172189544,
                            "process_3": 0.0007637203689374133
                        },
                        "gpu_energy": {
                            "process_2": 0.004765836868222018,
                            "process_0": 0.004795792447742142,
                            "process_1": 0.004781029935932074,
                            "process_3": 0.005074829615415999
                        },
                        "ram_energy": {
                            "process_2": 3.631007003517781e-06,
                            "process_0": 3.8572559117538315e-06,
                            "process_1": 3.735801204744296e-06,
                            "process_3": 4.1160342691738795e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0054350377007882054,
                            "process_0": 0.005465817352716696,
                            "process_1": 0.005417302454355772,
                            "process_3": 0.005842666018622584
                        },
                        "total_energy_joules": {
                            "process_2": 19566.13572283754,
                            "process_0": 19676.942469780104,
                            "process_1": 19502.28883568078,
                            "process_3": 21033.597667041304
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1200.855004516555,
                        "ram_power_avg": 0.6651949882507324,
                        "cpu_energy_total": 0.0027279945607818377,
                        "gpu_energy_total": 0.019417488867312233,
                        "ram_energy_total": 1.5340098389189786e-05,
                        "total_energy_kwh": 0.022160823526483256,
                        "total_energy_joules": 79778.96469533973
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2053674181229011,
                        "joules_per_token": 4.869321575643294,
                        "flops_per_joule": 212461656.5015681,
                        "joules_per_flop": 4.706731635562764e-09
                    },
                    "per-process_emissions": [
                        0.0020704776121152667,
                        0.0020822031205174254,
                        0.0020637213699868317,
                        0.0022257636197942736
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0179": {
            "setup": {
                "experiment_id": "0179",
                "date_time": "April 11, 2025 at 02:57:12 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.23256201300137,
                        "average_latency_ms_per_batch": 2904.0702516251713,
                        "throughput_queries_per_sec": 5.5095085909323664,
                        "throughput_tokens_per_sec": 705.2170996393429
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 2016215040
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0179",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 636.2553243924808,
                            "process_0": 853.9947543349717,
                            "process_2": 833.8757215689196,
                            "process_1": 1769.7184800266793
                        },
                        "ram_power": {
                            "process_3": 0.6482019424438477,
                            "process_0": 0.7020406723022461,
                            "process_2": 0.6545963287353516,
                            "process_1": 0.6539225578308105
                        },
                        "cpu_energy": {
                            "process_3": 0.0008084011610313271,
                            "process_0": 0.0006625053259687094,
                            "process_2": 0.0007373068192814003,
                            "process_1": 0.0007368417880312566
                        },
                        "gpu_energy": {
                            "process_3": 0.005049455150672039,
                            "process_0": 0.00466111678444589,
                            "process_2": 0.0047782299336920075,
                            "process_1": 0.0047748790976779865
                        },
                        "ram_energy": {
                            "process_3": 3.999323606533268e-06,
                            "process_0": 4.256742630319822e-06,
                            "process_2": 3.652925340025344e-06,
                            "process_1": 4.0436634841213095e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0058618556353098985,
                            "process_0": 0.005327878853044921,
                            "process_2": 0.005519189678313435,
                            "process_1": 0.005515764549193364
                        },
                        "total_energy_joules": {
                            "process_3": 21102.680287115636,
                            "process_0": 19180.363870961715,
                            "process_2": 19869.082841928364,
                            "process_1": 19856.75237709611
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1023.461070080763,
                        "ram_power_avg": 0.664690375328064,
                        "cpu_energy_total": 0.0029450550943126933,
                        "gpu_energy_total": 0.019263680966487923,
                        "ram_energy_total": 1.5952655060999743e-05,
                        "total_energy_kwh": 0.022224688715861616,
                        "total_energy_joules": 80008.87937710182
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20477727131732615,
                        "joules_per_token": 4.8833544541688125,
                        "flops_per_joule": 211851123.58920258,
                        "joules_per_flop": 4.720295946785184e-09
                    },
                    "per-process_emissions": [
                        0.0022330739042713058,
                        0.0020296554490674625,
                        0.002102535307953503,
                        0.002101230505015212
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0179": {
            "setup": {
                "experiment_id": "0179",
                "date_time": "April 11, 2025 at 02:57:12 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.23256201300137,
                        "average_latency_ms_per_batch": 2904.0702516251713,
                        "throughput_queries_per_sec": 5.5095085909323664,
                        "throughput_tokens_per_sec": 705.2170996393429
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 2016215040
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0179",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 636.2553243924808,
                            "process_0": 853.9947543349717,
                            "process_2": 833.8757215689196,
                            "process_1": 1769.7184800266793
                        },
                        "ram_power": {
                            "process_3": 0.6482019424438477,
                            "process_0": 0.7020406723022461,
                            "process_2": 0.6545963287353516,
                            "process_1": 0.6539225578308105
                        },
                        "cpu_energy": {
                            "process_3": 0.0008084011610313271,
                            "process_0": 0.0006625053259687094,
                            "process_2": 0.0007373068192814003,
                            "process_1": 0.0007368417880312566
                        },
                        "gpu_energy": {
                            "process_3": 0.005049455150672039,
                            "process_0": 0.00466111678444589,
                            "process_2": 0.0047782299336920075,
                            "process_1": 0.0047748790976779865
                        },
                        "ram_energy": {
                            "process_3": 3.999323606533268e-06,
                            "process_0": 4.256742630319822e-06,
                            "process_2": 3.652925340025344e-06,
                            "process_1": 4.0436634841213095e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0058618556353098985,
                            "process_0": 0.005327878853044921,
                            "process_2": 0.005519189678313435,
                            "process_1": 0.005515764549193364
                        },
                        "total_energy_joules": {
                            "process_3": 21102.680287115636,
                            "process_0": 19180.363870961715,
                            "process_2": 19869.082841928364,
                            "process_1": 19856.75237709611
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1023.461070080763,
                        "ram_power_avg": 0.664690375328064,
                        "cpu_energy_total": 0.0029450550943126933,
                        "gpu_energy_total": 0.019263680966487923,
                        "ram_energy_total": 1.5952655060999743e-05,
                        "total_energy_kwh": 0.022224688715861616,
                        "total_energy_joules": 80008.87937710182
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20477727131732615,
                        "joules_per_token": 4.8833544541688125,
                        "flops_per_joule": 211851123.58920258,
                        "joules_per_flop": 4.720295946785184e-09
                    },
                    "per-process_emissions": [
                        0.0022330739042713058,
                        0.0020296554490674625,
                        0.002102535307953503,
                        0.002101230505015212
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0180": {
            "setup": {
                "experiment_id": "0180",
                "date_time": "April 11, 2025 at 02:58:15 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.270355494001706,
                        "average_latency_ms_per_batch": 2908.794436750213,
                        "throughput_queries_per_sec": 5.500560575148668,
                        "throughput_tokens_per_sec": 704.0717536190296
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 2011607040
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0180",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 657.723018640388,
                            "process_1": 637.2011770937162,
                            "process_2": 733.0640309480707,
                            "process_3": 538.970033170825
                        },
                        "ram_power": {
                            "process_0": 0.7011938095092773,
                            "process_1": 0.6508727073669434,
                            "process_2": 0.6490345001220703,
                            "process_3": 0.6567478179931641
                        },
                        "cpu_energy": {
                            "process_0": 0.0006748335689373448,
                            "process_1": 0.0006956569255312957,
                            "process_2": 0.0007258380810001769,
                            "process_3": 0.0007692793901250069
                        },
                        "gpu_energy": {
                            "process_0": 0.004674691517528112,
                            "process_1": 0.004717777663107964,
                            "process_2": 0.0046747248508880734,
                            "process_3": 0.005087402125474122
                        },
                        "ram_energy": {
                            "process_0": 4.3072106257579945e-06,
                            "process_1": 3.7581135560010614e-06,
                            "process_2": 3.92798769118201e-06,
                            "process_3": 4.557191093008676e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005353832297091212,
                            "process_1": 0.0054171927021952615,
                            "process_2": 0.005404490919579433,
                            "process_3": 0.0058612387066921375
                        },
                        "total_energy_joules": {
                            "process_0": 19273.796269528364,
                            "process_1": 19501.89372790294,
                            "process_2": 19456.16731048596,
                            "process_3": 21100.459344091694
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 641.7395649632499,
                        "ram_power_avg": 0.6644622087478638,
                        "cpu_energy_total": 0.002865607965593824,
                        "gpu_energy_total": 0.01915459615699827,
                        "ram_energy_total": 1.655050296594974e-05,
                        "total_energy_kwh": 0.022036754625558045,
                        "total_energy_joules": 79332.31665200896
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20652365506819095,
                        "joules_per_token": 4.842060342529844,
                        "flops_per_joule": 213657834.6438944,
                        "joules_per_flop": 4.680380673457212e-09
                    },
                    "per-process_emissions": [
                        0.002039542413576897,
                        0.0020636795599012847,
                        0.002058840815813785,
                        0.00223283888531437
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0180": {
            "setup": {
                "experiment_id": "0180",
                "date_time": "April 11, 2025 at 02:58:15 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.270355494001706,
                        "average_latency_ms_per_batch": 2908.794436750213,
                        "throughput_queries_per_sec": 5.500560575148668,
                        "throughput_tokens_per_sec": 704.0717536190296
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 2011607040
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0180",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 657.723018640388,
                            "process_1": 637.2011770937162,
                            "process_2": 733.0640309480707,
                            "process_3": 538.970033170825
                        },
                        "ram_power": {
                            "process_0": 0.7011938095092773,
                            "process_1": 0.6508727073669434,
                            "process_2": 0.6490345001220703,
                            "process_3": 0.6567478179931641
                        },
                        "cpu_energy": {
                            "process_0": 0.0006748335689373448,
                            "process_1": 0.0006956569255312957,
                            "process_2": 0.0007258380810001769,
                            "process_3": 0.0007692793901250069
                        },
                        "gpu_energy": {
                            "process_0": 0.004674691517528112,
                            "process_1": 0.004717777663107964,
                            "process_2": 0.0046747248508880734,
                            "process_3": 0.005087402125474122
                        },
                        "ram_energy": {
                            "process_0": 4.3072106257579945e-06,
                            "process_1": 3.7581135560010614e-06,
                            "process_2": 3.92798769118201e-06,
                            "process_3": 4.557191093008676e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005353832297091212,
                            "process_1": 0.0054171927021952615,
                            "process_2": 0.005404490919579433,
                            "process_3": 0.0058612387066921375
                        },
                        "total_energy_joules": {
                            "process_0": 19273.796269528364,
                            "process_1": 19501.89372790294,
                            "process_2": 19456.16731048596,
                            "process_3": 21100.459344091694
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 641.7395649632499,
                        "ram_power_avg": 0.6644622087478638,
                        "cpu_energy_total": 0.002865607965593824,
                        "gpu_energy_total": 0.01915459615699827,
                        "ram_energy_total": 1.655050296594974e-05,
                        "total_energy_kwh": 0.022036754625558045,
                        "total_energy_joules": 79332.31665200896
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20652365506819095,
                        "joules_per_token": 4.842060342529844,
                        "flops_per_joule": 213657834.6438944,
                        "joules_per_flop": 4.680380673457212e-09
                    },
                    "per-process_emissions": [
                        0.002039542413576897,
                        0.0020636795599012847,
                        0.002058840815813785,
                        0.00223283888531437
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0181": {
            "setup": {
                "experiment_id": "0181",
                "date_time": "April 11, 2025 at 02:59:16 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.287194367001575,
                        "average_latency_ms_per_batch": 2910.899295875197,
                        "throughput_queries_per_sec": 5.4965831427670215,
                        "throughput_tokens_per_sec": 703.5626422741788
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            5.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.9,
                        "cpu_memory_usage_bytes": 2015203328
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0181",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 2148.7947206477143,
                            "process_2": 1534.0528339894383,
                            "process_0": 2089.517057840385,
                            "process_3": 432.4646720446261
                        },
                        "ram_power": {
                            "process_1": 0.6451935768127441,
                            "process_2": 0.6534719467163086,
                            "process_0": 0.7013411521911621,
                            "process_3": 0.6523919105529785
                        },
                        "cpu_energy": {
                            "process_1": 0.0006712505279375024,
                            "process_2": 0.0006165172511249465,
                            "process_0": 0.0006750388638749315,
                            "process_3": 0.0007632065808437574
                        },
                        "gpu_energy": {
                            "process_1": 0.004752845746718193,
                            "process_2": 0.004752845746718193,
                            "process_0": 0.004759175196226151,
                            "process_3": 0.005064769329589935
                        },
                        "ram_energy": {
                            "process_1": 3.6207624645055892e-06,
                            "process_2": 3.6802073059391165e-06,
                            "process_0": 3.9514673800794416e-06,
                            "process_3": 4.144915106779874e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0054277170371202004,
                            "process_2": 0.005373043205149079,
                            "process_0": 0.005438165527481161,
                            "process_3": 0.0058321208255404725
                        },
                        "total_energy_joules": {
                            "process_1": 19539.78133363272,
                            "process_2": 19342.955538536684,
                            "process_0": 19577.39589893218,
                            "process_3": 20995.6349719457
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1551.207321130541,
                        "ram_power_avg": 0.6630996465682983,
                        "cpu_energy_total": 0.002726013223781138,
                        "gpu_energy_total": 0.019329636019252472,
                        "ram_energy_total": 1.539735225730402e-05,
                        "total_energy_kwh": 0.022071046595290914,
                        "total_energy_joules": 79455.7677430473
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20620277753761515,
                        "joules_per_token": 4.849595199160602,
                        "flops_per_joule": 213325872.67883006,
                        "joules_per_flop": 4.687663936129945e-09
                    },
                    "per-process_emissions": [
                        0.0020676888052909405,
                        0.0020468608090015417,
                        0.0020716691576939486,
                        0.002221746428489643
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0181": {
            "setup": {
                "experiment_id": "0181",
                "date_time": "April 11, 2025 at 02:59:16 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.287194367001575,
                        "average_latency_ms_per_batch": 2910.899295875197,
                        "throughput_queries_per_sec": 5.4965831427670215,
                        "throughput_tokens_per_sec": 703.5626422741788
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            5.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.9,
                        "cpu_memory_usage_bytes": 2015203328
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0181",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 2148.7947206477143,
                            "process_2": 1534.0528339894383,
                            "process_0": 2089.517057840385,
                            "process_3": 432.4646720446261
                        },
                        "ram_power": {
                            "process_1": 0.6451935768127441,
                            "process_2": 0.6534719467163086,
                            "process_0": 0.7013411521911621,
                            "process_3": 0.6523919105529785
                        },
                        "cpu_energy": {
                            "process_1": 0.0006712505279375024,
                            "process_2": 0.0006165172511249465,
                            "process_0": 0.0006750388638749315,
                            "process_3": 0.0007632065808437574
                        },
                        "gpu_energy": {
                            "process_1": 0.004752845746718193,
                            "process_2": 0.004752845746718193,
                            "process_0": 0.004759175196226151,
                            "process_3": 0.005064769329589935
                        },
                        "ram_energy": {
                            "process_1": 3.6207624645055892e-06,
                            "process_2": 3.6802073059391165e-06,
                            "process_0": 3.9514673800794416e-06,
                            "process_3": 4.144915106779874e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0054277170371202004,
                            "process_2": 0.005373043205149079,
                            "process_0": 0.005438165527481161,
                            "process_3": 0.0058321208255404725
                        },
                        "total_energy_joules": {
                            "process_1": 19539.78133363272,
                            "process_2": 19342.955538536684,
                            "process_0": 19577.39589893218,
                            "process_3": 20995.6349719457
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1551.207321130541,
                        "ram_power_avg": 0.6630996465682983,
                        "cpu_energy_total": 0.002726013223781138,
                        "gpu_energy_total": 0.019329636019252472,
                        "ram_energy_total": 1.539735225730402e-05,
                        "total_energy_kwh": 0.022071046595290914,
                        "total_energy_joules": 79455.7677430473
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20620277753761515,
                        "joules_per_token": 4.849595199160602,
                        "flops_per_joule": 213325872.67883006,
                        "joules_per_flop": 4.687663936129945e-09
                    },
                    "per-process_emissions": [
                        0.0020676888052909405,
                        0.0020468608090015417,
                        0.0020716691576939486,
                        0.002221746428489643
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0182": {
            "setup": {
                "experiment_id": "0182",
                "date_time": "April 11, 2025 at 03:00:18 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.24935098399874,
                        "average_latency_ms_per_batch": 2906.1688729998423,
                        "throughput_queries_per_sec": 5.505530029121907,
                        "throughput_tokens_per_sec": 704.7078437276041
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.9,
                        "cpu_memory_usage_bytes": 2009636864
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0182",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 985.5085684360741,
                            "process_3": 388.43613020113105,
                            "process_1": 1967.2733820821225,
                            "process_0": 2453.7558306374963
                        },
                        "ram_power": {
                            "process_2": 0.6534991264343262,
                            "process_3": 0.6511187553405763,
                            "process_1": 0.6562914848327637,
                            "process_0": 0.6997675895690918
                        },
                        "cpu_energy": {
                            "process_2": 0.0006873391187186827,
                            "process_3": 0.0007744786964685774,
                            "process_1": 0.0006809408363437227,
                            "process_0": 0.0006802442641875929
                        },
                        "gpu_energy": {
                            "process_2": 0.004752600468744095,
                            "process_3": 0.005040289310005941,
                            "process_1": 0.0047447410180119776,
                            "process_0": 0.004743371016915965
                        },
                        "ram_energy": {
                            "process_2": 3.7134916014754045e-06,
                            "process_3": 4.524817423345323e-06,
                            "process_1": 3.715157239548862e-06,
                            "process_0": 3.962099468972801e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005443653079064254,
                            "process_3": 0.005819292823897863,
                            "process_1": 0.005429397011595251,
                            "process_0": 0.00542757738057253
                        },
                        "total_energy_joules": {
                            "process_2": 19597.151084631314,
                            "process_3": 20949.45416603231,
                            "process_1": 19545.829241742904,
                            "process_0": 19539.27857006111
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1448.743477839206,
                        "ram_power_avg": 0.6651692390441895,
                        "cpu_energy_total": 0.0028230029157185754,
                        "gpu_energy_total": 0.019281001813677978,
                        "ram_energy_total": 1.5915565733342394e-05,
                        "total_energy_kwh": 0.0221199202951299,
                        "total_energy_joules": 79631.71306246764
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20574717496215936,
                        "joules_per_token": 4.860334049222878,
                        "flops_per_joule": 212854531.7096906,
                        "joules_per_flop": 4.698044208726956e-09
                    },
                    "per-process_emissions": [
                        0.0020737596404695278,
                        0.002216859601263891,
                        0.002068328791567211,
                        0.0020676356031291056
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0182": {
            "setup": {
                "experiment_id": "0182",
                "date_time": "April 11, 2025 at 03:00:18 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.24935098399874,
                        "average_latency_ms_per_batch": 2906.1688729998423,
                        "throughput_queries_per_sec": 5.505530029121907,
                        "throughput_tokens_per_sec": 704.7078437276041
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.9,
                        "cpu_memory_usage_bytes": 2009636864
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0182",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 985.5085684360741,
                            "process_3": 388.43613020113105,
                            "process_1": 1967.2733820821225,
                            "process_0": 2453.7558306374963
                        },
                        "ram_power": {
                            "process_2": 0.6534991264343262,
                            "process_3": 0.6511187553405763,
                            "process_1": 0.6562914848327637,
                            "process_0": 0.6997675895690918
                        },
                        "cpu_energy": {
                            "process_2": 0.0006873391187186827,
                            "process_3": 0.0007744786964685774,
                            "process_1": 0.0006809408363437227,
                            "process_0": 0.0006802442641875929
                        },
                        "gpu_energy": {
                            "process_2": 0.004752600468744095,
                            "process_3": 0.005040289310005941,
                            "process_1": 0.0047447410180119776,
                            "process_0": 0.004743371016915965
                        },
                        "ram_energy": {
                            "process_2": 3.7134916014754045e-06,
                            "process_3": 4.524817423345323e-06,
                            "process_1": 3.715157239548862e-06,
                            "process_0": 3.962099468972801e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005443653079064254,
                            "process_3": 0.005819292823897863,
                            "process_1": 0.005429397011595251,
                            "process_0": 0.00542757738057253
                        },
                        "total_energy_joules": {
                            "process_2": 19597.151084631314,
                            "process_3": 20949.45416603231,
                            "process_1": 19545.829241742904,
                            "process_0": 19539.27857006111
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1448.743477839206,
                        "ram_power_avg": 0.6651692390441895,
                        "cpu_energy_total": 0.0028230029157185754,
                        "gpu_energy_total": 0.019281001813677978,
                        "ram_energy_total": 1.5915565733342394e-05,
                        "total_energy_kwh": 0.0221199202951299,
                        "total_energy_joules": 79631.71306246764
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20574717496215936,
                        "joules_per_token": 4.860334049222878,
                        "flops_per_joule": 212854531.7096906,
                        "joules_per_flop": 4.698044208726956e-09
                    },
                    "per-process_emissions": [
                        0.0020737596404695278,
                        0.002216859601263891,
                        0.002068328791567211,
                        0.0020676356031291056
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0183": {
            "setup": {
                "experiment_id": "0183",
                "date_time": "April 11, 2025 at 03:01:19 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.336428720000185,
                        "average_latency_ms_per_batch": 2917.053590000023,
                        "throughput_queries_per_sec": 5.484986650519462,
                        "throughput_tokens_per_sec": 702.0782912664912
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 2014113792
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0183",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 667.7985395881038,
                            "process_2": 2485.634032910787,
                            "process_1": 940.5321978170101,
                            "process_0": 1847.0837229016665
                        },
                        "ram_power": {
                            "process_3": 0.6576404571533203,
                            "process_2": 0.6561026573181152,
                            "process_1": 0.6450090408325195,
                            "process_0": 0.7021465301513672
                        },
                        "cpu_energy": {
                            "process_3": 0.0007435628971563801,
                            "process_2": 0.0006677065542186826,
                            "process_1": 0.0006866825364375587,
                            "process_0": 0.0006767352035311092
                        },
                        "gpu_energy": {
                            "process_3": 0.005026074298633887,
                            "process_2": 0.004762054087418144,
                            "process_1": 0.004774587986334011,
                            "process_0": 0.004750656578299983
                        },
                        "ram_energy": {
                            "process_3": 4.401289722145764e-06,
                            "process_2": 3.985958319581277e-06,
                            "process_1": 3.975468016586486e-06,
                            "process_0": 4.295192884380029e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005774038485512413,
                            "process_2": 0.005433746599956408,
                            "process_1": 0.005465245990788155,
                            "process_0": 0.005431686974715473
                        },
                        "total_energy_joules": {
                            "process_3": 20786.538547844688,
                            "process_2": 19561.48775984307,
                            "process_1": 19674.885566837358,
                            "process_0": 19554.073108975703
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1485.262123304392,
                        "ram_power_avg": 0.6652246713638306,
                        "cpu_energy_total": 0.0027746871913437304,
                        "gpu_energy_total": 0.019313372950686025,
                        "ram_energy_total": 1.6657908942693555e-05,
                        "total_energy_kwh": 0.02210471805097245,
                        "total_energy_joules": 79576.98498350082
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20588867501573468,
                        "joules_per_token": 4.856993712371876,
                        "flops_per_joule": 213000919.75922862,
                        "joules_per_flop": 4.694815407982168e-09
                    },
                    "per-process_emissions": [
                        0.002199619961055954,
                        0.0020699857672533935,
                        0.0020819854601907477,
                        0.0020692011530178593
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0183": {
            "setup": {
                "experiment_id": "0183",
                "date_time": "April 11, 2025 at 03:01:19 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.336428720000185,
                        "average_latency_ms_per_batch": 2917.053590000023,
                        "throughput_queries_per_sec": 5.484986650519462,
                        "throughput_tokens_per_sec": 702.0782912664912
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 2014113792
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0183",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 667.7985395881038,
                            "process_2": 2485.634032910787,
                            "process_1": 940.5321978170101,
                            "process_0": 1847.0837229016665
                        },
                        "ram_power": {
                            "process_3": 0.6576404571533203,
                            "process_2": 0.6561026573181152,
                            "process_1": 0.6450090408325195,
                            "process_0": 0.7021465301513672
                        },
                        "cpu_energy": {
                            "process_3": 0.0007435628971563801,
                            "process_2": 0.0006677065542186826,
                            "process_1": 0.0006866825364375587,
                            "process_0": 0.0006767352035311092
                        },
                        "gpu_energy": {
                            "process_3": 0.005026074298633887,
                            "process_2": 0.004762054087418144,
                            "process_1": 0.004774587986334011,
                            "process_0": 0.004750656578299983
                        },
                        "ram_energy": {
                            "process_3": 4.401289722145764e-06,
                            "process_2": 3.985958319581277e-06,
                            "process_1": 3.975468016586486e-06,
                            "process_0": 4.295192884380029e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005774038485512413,
                            "process_2": 0.005433746599956408,
                            "process_1": 0.005465245990788155,
                            "process_0": 0.005431686974715473
                        },
                        "total_energy_joules": {
                            "process_3": 20786.538547844688,
                            "process_2": 19561.48775984307,
                            "process_1": 19674.885566837358,
                            "process_0": 19554.073108975703
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1485.262123304392,
                        "ram_power_avg": 0.6652246713638306,
                        "cpu_energy_total": 0.0027746871913437304,
                        "gpu_energy_total": 0.019313372950686025,
                        "ram_energy_total": 1.6657908942693555e-05,
                        "total_energy_kwh": 0.02210471805097245,
                        "total_energy_joules": 79576.98498350082
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20588867501573468,
                        "joules_per_token": 4.856993712371876,
                        "flops_per_joule": 213000919.75922862,
                        "joules_per_flop": 4.694815407982168e-09
                    },
                    "per-process_emissions": [
                        0.002199619961055954,
                        0.0020699857672533935,
                        0.0020819854601907477,
                        0.0020692011530178593
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0184": {
            "setup": {
                "experiment_id": "0184",
                "date_time": "April 11, 2025 at 03:02:21 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.317204624001533,
                        "average_latency_ms_per_batch": 2914.6505780001917,
                        "throughput_queries_per_sec": 5.489508801078298,
                        "throughput_tokens_per_sec": 702.6571265380221
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 12903776256,
                        "gpu_max_memory_reserved_bytes": 12903776256
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 2011889664
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0184",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 519.2699606211964,
                            "process_2": 1137.646687311636,
                            "process_1": 1400.7816667542947,
                            "process_0": 2041.9195491052556
                        },
                        "ram_power": {
                            "process_3": 0.649116039276123,
                            "process_2": 0.6545505523681641,
                            "process_1": 0.6491618156433105,
                            "process_0": 0.7008991241455078
                        },
                        "cpu_energy": {
                            "process_3": 0.0007500702119063476,
                            "process_2": 0.0006423283834375865,
                            "process_1": 0.0006258783585312245,
                            "process_0": 0.0006138656967811472
                        },
                        "gpu_energy": {
                            "process_3": 0.005047454593516021,
                            "process_2": 0.004762250198686091,
                            "process_1": 0.004768003536621934,
                            "process_0": 0.004762250198686091
                        },
                        "ram_energy": {
                            "process_3": 4.383386256297672e-06,
                            "process_2": 3.8087661607077273e-06,
                            "process_1": 3.709039135395832e-06,
                            "process_0": 3.938935590189198e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005801908191678667,
                            "process_2": 0.005408387348284386,
                            "process_1": 0.005397590934288554,
                            "process_0": 0.005380054831057428
                        },
                        "total_energy_joules": {
                            "process_3": 20886.8694900432,
                            "process_2": 19470.19445382379,
                            "process_1": 19431.327363438795,
                            "process_0": 19368.19739180674
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1274.9044659480955,
                        "ram_power_avg": 0.6634318828582764,
                        "cpu_energy_total": 0.0026321426506563063,
                        "gpu_energy_total": 0.019339958527510137,
                        "ram_energy_total": 1.5840127142590433e-05,
                        "total_energy_kwh": 0.021987941305309032,
                        "total_energy_joules": 79156.58869911253
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2069821384329526,
                        "joules_per_token": 4.8313347594673175,
                        "flops_per_joule": 214132155.91669422,
                        "joules_per_flop": 4.670013224865859e-09
                    },
                    "per-process_emissions": [
                        0.002210236925619988,
                        0.002060325160328937,
                        0.002056212266417225,
                        0.002049531887891327
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0184": {
            "setup": {
                "experiment_id": "0184",
                "date_time": "April 11, 2025 at 03:02:21 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.317204624001533,
                        "average_latency_ms_per_batch": 2914.6505780001917,
                        "throughput_queries_per_sec": 5.489508801078298,
                        "throughput_tokens_per_sec": 702.6571265380221
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 12903776256,
                        "gpu_max_memory_reserved_bytes": 12903776256
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 2011889664
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0184",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 519.2699606211964,
                            "process_2": 1137.646687311636,
                            "process_1": 1400.7816667542947,
                            "process_0": 2041.9195491052556
                        },
                        "ram_power": {
                            "process_3": 0.649116039276123,
                            "process_2": 0.6545505523681641,
                            "process_1": 0.6491618156433105,
                            "process_0": 0.7008991241455078
                        },
                        "cpu_energy": {
                            "process_3": 0.0007500702119063476,
                            "process_2": 0.0006423283834375865,
                            "process_1": 0.0006258783585312245,
                            "process_0": 0.0006138656967811472
                        },
                        "gpu_energy": {
                            "process_3": 0.005047454593516021,
                            "process_2": 0.004762250198686091,
                            "process_1": 0.004768003536621934,
                            "process_0": 0.004762250198686091
                        },
                        "ram_energy": {
                            "process_3": 4.383386256297672e-06,
                            "process_2": 3.8087661607077273e-06,
                            "process_1": 3.709039135395832e-06,
                            "process_0": 3.938935590189198e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005801908191678667,
                            "process_2": 0.005408387348284386,
                            "process_1": 0.005397590934288554,
                            "process_0": 0.005380054831057428
                        },
                        "total_energy_joules": {
                            "process_3": 20886.8694900432,
                            "process_2": 19470.19445382379,
                            "process_1": 19431.327363438795,
                            "process_0": 19368.19739180674
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1274.9044659480955,
                        "ram_power_avg": 0.6634318828582764,
                        "cpu_energy_total": 0.0026321426506563063,
                        "gpu_energy_total": 0.019339958527510137,
                        "ram_energy_total": 1.5840127142590433e-05,
                        "total_energy_kwh": 0.021987941305309032,
                        "total_energy_joules": 79156.58869911253
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2069821384329526,
                        "joules_per_token": 4.8313347594673175,
                        "flops_per_joule": 214132155.91669422,
                        "joules_per_flop": 4.670013224865859e-09
                    },
                    "per-process_emissions": [
                        0.002210236925619988,
                        0.002060325160328937,
                        0.002056212266417225,
                        0.002049531887891327
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0185": {
            "setup": {
                "experiment_id": "0185",
                "date_time": "April 11, 2025 at 03:03:22 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.15849438299483,
                        "average_latency_ms_per_batch": 2894.811797874354,
                        "throughput_queries_per_sec": 5.527129608822488,
                        "throughput_tokens_per_sec": 707.4725899292785
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 1994887168
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0185",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 671.2723177962525,
                            "process_2": 537.7662219929152,
                            "process_0": 580.6578424781501,
                            "process_3": 460.4587746955642
                        },
                        "ram_power": {
                            "process_1": 0.6558952331542969,
                            "process_2": 0.6550812721252441,
                            "process_0": 0.6954431533813477,
                            "process_3": 0.6509613990783691
                        },
                        "cpu_energy": {
                            "process_1": 0.0006889132572812286,
                            "process_2": 0.0007031895889375618,
                            "process_0": 0.0006873930034063277,
                            "process_3": 0.0007373927128127208
                        },
                        "gpu_energy": {
                            "process_1": 0.0046643720648280595,
                            "process_2": 0.004707852655167988,
                            "process_0": 0.0046643720648280595,
                            "process_3": 0.0050810476759459156
                        },
                        "ram_energy": {
                            "process_1": 3.7816815398677274e-06,
                            "process_2": 3.824374136618452e-06,
                            "process_0": 3.990647455654038e-06,
                            "process_3": 4.331520349563762e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005357067003649157,
                            "process_2": 0.0054148666182421675,
                            "process_0": 0.005355755715690039,
                            "process_3": 0.005822771909108199
                        },
                        "total_energy_joules": {
                            "process_1": 19285.441213136965,
                            "process_2": 19493.519825671803,
                            "process_0": 19280.72057648414,
                            "process_3": 20961.978872789517
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 562.5387892407205,
                        "ram_power_avg": 0.6643452644348145,
                        "cpu_energy_total": 0.002816888562437839,
                        "gpu_energy_total": 0.019117644460770022,
                        "ram_energy_total": 1.5928223481703982e-05,
                        "total_energy_kwh": 0.021950461246689562,
                        "total_energy_joules": 79021.66048808243
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20733555709666385,
                        "joules_per_token": 4.823099395024562,
                        "flops_per_joule": 214497783.1199623,
                        "joules_per_flop": 4.6620528448106586e-09
                    },
                    "per-process_emissions": [
                        0.002040774675040146,
                        0.002062793438219354,
                        0.0020402751398921203,
                        0.0022181849587747686
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0185": {
            "setup": {
                "experiment_id": "0185",
                "date_time": "April 11, 2025 at 03:03:22 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.15849438299483,
                        "average_latency_ms_per_batch": 2894.811797874354,
                        "throughput_queries_per_sec": 5.527129608822488,
                        "throughput_tokens_per_sec": 707.4725899292785
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 1994887168
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0185",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 671.2723177962525,
                            "process_2": 537.7662219929152,
                            "process_0": 580.6578424781501,
                            "process_3": 460.4587746955642
                        },
                        "ram_power": {
                            "process_1": 0.6558952331542969,
                            "process_2": 0.6550812721252441,
                            "process_0": 0.6954431533813477,
                            "process_3": 0.6509613990783691
                        },
                        "cpu_energy": {
                            "process_1": 0.0006889132572812286,
                            "process_2": 0.0007031895889375618,
                            "process_0": 0.0006873930034063277,
                            "process_3": 0.0007373927128127208
                        },
                        "gpu_energy": {
                            "process_1": 0.0046643720648280595,
                            "process_2": 0.004707852655167988,
                            "process_0": 0.0046643720648280595,
                            "process_3": 0.0050810476759459156
                        },
                        "ram_energy": {
                            "process_1": 3.7816815398677274e-06,
                            "process_2": 3.824374136618452e-06,
                            "process_0": 3.990647455654038e-06,
                            "process_3": 4.331520349563762e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005357067003649157,
                            "process_2": 0.0054148666182421675,
                            "process_0": 0.005355755715690039,
                            "process_3": 0.005822771909108199
                        },
                        "total_energy_joules": {
                            "process_1": 19285.441213136965,
                            "process_2": 19493.519825671803,
                            "process_0": 19280.72057648414,
                            "process_3": 20961.978872789517
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 562.5387892407205,
                        "ram_power_avg": 0.6643452644348145,
                        "cpu_energy_total": 0.002816888562437839,
                        "gpu_energy_total": 0.019117644460770022,
                        "ram_energy_total": 1.5928223481703982e-05,
                        "total_energy_kwh": 0.021950461246689562,
                        "total_energy_joules": 79021.66048808243
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20733555709666385,
                        "joules_per_token": 4.823099395024562,
                        "flops_per_joule": 214497783.1199623,
                        "joules_per_flop": 4.6620528448106586e-09
                    },
                    "per-process_emissions": [
                        0.002040774675040146,
                        0.002062793438219354,
                        0.0020402751398921203,
                        0.0022181849587747686
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0186": {
            "setup": {
                "experiment_id": "0186",
                "date_time": "April 11, 2025 at 03:04:26 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.202427624990378,
                        "average_latency_ms_per_batch": 2900.303453123797,
                        "throughput_queries_per_sec": 5.516664121048113,
                        "throughput_tokens_per_sec": 706.1330074941585
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1993248768
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0186",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 676.7994359341177,
                            "process_3": 555.574279164105,
                            "process_0": 481.2383394157799,
                            "process_1": 1468.601515626677
                        },
                        "ram_power": {
                            "process_2": 0.6488814353942871,
                            "process_3": 0.6496381759643555,
                            "process_0": 0.6943559646606445,
                            "process_1": 0.6617388725280762
                        },
                        "cpu_energy": {
                            "process_2": 0.0006724458041874187,
                            "process_3": 0.0007654988836872008,
                            "process_0": 0.0006719420435002801,
                            "process_1": 0.0007336998087500889
                        },
                        "gpu_energy": {
                            "process_2": 0.004706250153886038,
                            "process_3": 0.005097025188728066,
                            "process_0": 0.004691214308523983,
                            "process_1": 0.004709422934202012
                        },
                        "ram_energy": {
                            "process_2": 3.987170492604492e-06,
                            "process_3": 4.491730612299713e-06,
                            "process_0": 4.682878717449602e-06,
                            "process_1": 3.6751154389358076e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0053826831285660615,
                            "process_3": 0.005867015803027565,
                            "process_0": 0.005367839230741711,
                            "process_1": 0.005446797858391038
                        },
                        "total_energy_joules": {
                            "process_2": 19377.65926283782,
                            "process_3": 21121.256890899236,
                            "process_0": 19324.22123067016,
                            "process_1": 19608.472290207737
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 795.5533925351699,
                        "ram_power_avg": 0.6636536121368408,
                        "cpu_energy_total": 0.0028435865401249885,
                        "gpu_energy_total": 0.0192039125853401,
                        "ram_energy_total": 1.6836895261289614e-05,
                        "total_energy_kwh": 0.022064336020726376,
                        "total_energy_joules": 79431.60967461494
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2062654913719577,
                        "joules_per_token": 4.848120707679135,
                        "flops_per_joule": 213390752.9078935,
                        "joules_per_flop": 4.686238678916106e-09
                    },
                    "per-process_emissions": [
                        0.002050533137827241,
                        0.002235039670163351,
                        0.0020448783549510546,
                        0.0020749576441540657
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0186": {
            "setup": {
                "experiment_id": "0186",
                "date_time": "April 11, 2025 at 03:04:26 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.202427624990378,
                        "average_latency_ms_per_batch": 2900.303453123797,
                        "throughput_queries_per_sec": 5.516664121048113,
                        "throughput_tokens_per_sec": 706.1330074941585
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1993248768
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0186",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 676.7994359341177,
                            "process_3": 555.574279164105,
                            "process_0": 481.2383394157799,
                            "process_1": 1468.601515626677
                        },
                        "ram_power": {
                            "process_2": 0.6488814353942871,
                            "process_3": 0.6496381759643555,
                            "process_0": 0.6943559646606445,
                            "process_1": 0.6617388725280762
                        },
                        "cpu_energy": {
                            "process_2": 0.0006724458041874187,
                            "process_3": 0.0007654988836872008,
                            "process_0": 0.0006719420435002801,
                            "process_1": 0.0007336998087500889
                        },
                        "gpu_energy": {
                            "process_2": 0.004706250153886038,
                            "process_3": 0.005097025188728066,
                            "process_0": 0.004691214308523983,
                            "process_1": 0.004709422934202012
                        },
                        "ram_energy": {
                            "process_2": 3.987170492604492e-06,
                            "process_3": 4.491730612299713e-06,
                            "process_0": 4.682878717449602e-06,
                            "process_1": 3.6751154389358076e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0053826831285660615,
                            "process_3": 0.005867015803027565,
                            "process_0": 0.005367839230741711,
                            "process_1": 0.005446797858391038
                        },
                        "total_energy_joules": {
                            "process_2": 19377.65926283782,
                            "process_3": 21121.256890899236,
                            "process_0": 19324.22123067016,
                            "process_1": 19608.472290207737
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 795.5533925351699,
                        "ram_power_avg": 0.6636536121368408,
                        "cpu_energy_total": 0.0028435865401249885,
                        "gpu_energy_total": 0.0192039125853401,
                        "ram_energy_total": 1.6836895261289614e-05,
                        "total_energy_kwh": 0.022064336020726376,
                        "total_energy_joules": 79431.60967461494
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2062654913719577,
                        "joules_per_token": 4.848120707679135,
                        "flops_per_joule": 213390752.9078935,
                        "joules_per_flop": 4.686238678916106e-09
                    },
                    "per-process_emissions": [
                        0.002050533137827241,
                        0.002235039670163351,
                        0.0020448783549510546,
                        0.0020749576441540657
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0187": {
            "setup": {
                "experiment_id": "0187",
                "date_time": "April 11, 2025 at 03:05:27 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.640072765992954,
                        "average_latency_ms_per_batch": 2955.0090957491193,
                        "throughput_queries_per_sec": 5.414534941031668,
                        "throughput_tokens_per_sec": 693.0604724520535
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            5.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.9,
                        "cpu_memory_usage_bytes": 2012434432
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0187",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 2311.788444652306,
                            "process_0": 59.54012614052082,
                            "process_3": 463.83037606401456,
                            "process_2": 0.0
                        },
                        "ram_power": {
                            "process_1": 0.6498541831970215,
                            "process_0": 0.7009806632995605,
                            "process_3": 0.6493163108825684,
                            "process_2": 0.6543602943420411
                        },
                        "cpu_energy": {
                            "process_1": 0.0007138994315001808,
                            "process_0": 0.000707895407218871,
                            "process_3": 0.000799011025969321,
                            "process_2": 0.0006330877329063468
                        },
                        "gpu_energy": {
                            "process_1": 0.004754034914335847,
                            "process_0": 0.004768912426237948,
                            "process_3": 0.005061635993749813,
                            "process_2": 0.004757420750377828
                        },
                        "ram_energy": {
                            "process_1": 3.8733786809252495e-06,
                            "process_0": 4.11231580781383e-06,
                            "process_3": 4.3056780530599e-06,
                            "process_2": 3.772417072659044e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005471807724516955,
                            "process_0": 0.005480920149264633,
                            "process_3": 0.005864952697772196,
                            "process_2": 0.005394280900356833
                        },
                        "total_energy_joules": {
                            "process_1": 19698.507808261038,
                            "process_0": 19731.31253735268,
                            "process_3": 21113.829711979903,
                            "process_2": 19419.411241284597
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 708.7897367142104,
                        "ram_power_avg": 0.6636278629302979,
                        "cpu_energy_total": 0.0028538935975947195,
                        "gpu_energy_total": 0.019342004084701436,
                        "ram_energy_total": 1.6063789614458022e-05,
                        "total_energy_kwh": 0.022211961471910617,
                        "total_energy_joules": 79963.06129887821
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20489460675800628,
                        "joules_per_token": 4.880557940605359,
                        "flops_per_joule": 211972512.28036448,
                        "joules_per_flop": 4.717592810700637e-09
                    },
                    "per-process_emissions": [
                        0.002084485152654734,
                        0.002087956530862362,
                        0.002234253730216318,
                        0.0020549513089909354
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0187": {
            "setup": {
                "experiment_id": "0187",
                "date_time": "April 11, 2025 at 03:05:27 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.640072765992954,
                        "average_latency_ms_per_batch": 2955.0090957491193,
                        "throughput_queries_per_sec": 5.414534941031668,
                        "throughput_tokens_per_sec": 693.0604724520535
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            5.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 2.9,
                        "cpu_memory_usage_bytes": 2012434432
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0187",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 2311.788444652306,
                            "process_0": 59.54012614052082,
                            "process_3": 463.83037606401456,
                            "process_2": 0.0
                        },
                        "ram_power": {
                            "process_1": 0.6498541831970215,
                            "process_0": 0.7009806632995605,
                            "process_3": 0.6493163108825684,
                            "process_2": 0.6543602943420411
                        },
                        "cpu_energy": {
                            "process_1": 0.0007138994315001808,
                            "process_0": 0.000707895407218871,
                            "process_3": 0.000799011025969321,
                            "process_2": 0.0006330877329063468
                        },
                        "gpu_energy": {
                            "process_1": 0.004754034914335847,
                            "process_0": 0.004768912426237948,
                            "process_3": 0.005061635993749813,
                            "process_2": 0.004757420750377828
                        },
                        "ram_energy": {
                            "process_1": 3.8733786809252495e-06,
                            "process_0": 4.11231580781383e-06,
                            "process_3": 4.3056780530599e-06,
                            "process_2": 3.772417072659044e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005471807724516955,
                            "process_0": 0.005480920149264633,
                            "process_3": 0.005864952697772196,
                            "process_2": 0.005394280900356833
                        },
                        "total_energy_joules": {
                            "process_1": 19698.507808261038,
                            "process_0": 19731.31253735268,
                            "process_3": 21113.829711979903,
                            "process_2": 19419.411241284597
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 708.7897367142104,
                        "ram_power_avg": 0.6636278629302979,
                        "cpu_energy_total": 0.0028538935975947195,
                        "gpu_energy_total": 0.019342004084701436,
                        "ram_energy_total": 1.6063789614458022e-05,
                        "total_energy_kwh": 0.022211961471910617,
                        "total_energy_joules": 79963.06129887821
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20489460675800628,
                        "joules_per_token": 4.880557940605359,
                        "flops_per_joule": 211972512.28036448,
                        "joules_per_flop": 4.717592810700637e-09
                    },
                    "per-process_emissions": [
                        0.002084485152654734,
                        0.002087956530862362,
                        0.002234253730216318,
                        0.0020549513089909354
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0188": {
            "setup": {
                "experiment_id": "0188",
                "date_time": "April 11, 2025 at 03:06:29 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.7258511680011,
                        "average_latency_ms_per_batch": 2965.7313960001375,
                        "throughput_queries_per_sec": 5.394959240603884,
                        "throughput_tokens_per_sec": 690.5547827972972
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.7,
                        "cpu_memory_usage_bytes": 2016657408
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0188",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 82.86553161976956,
                            "process_0": 84.02525650339466,
                            "process_3": 534.1096405293149,
                            "process_1": 85.13377788252055
                        },
                        "ram_power": {
                            "process_2": 0.6551198959350586,
                            "process_0": 0.7030348777770996,
                            "process_3": 0.6554174423217773,
                            "process_1": 0.652763843536377
                        },
                        "cpu_energy": {
                            "process_2": 0.0007134681467191515,
                            "process_0": 0.0007198571896250316,
                            "process_3": 0.000761871837031208,
                            "process_1": 0.0007137098227815386
                        },
                        "gpu_energy": {
                            "process_2": 0.004761841309470105,
                            "process_0": 0.00476926520429799,
                            "process_3": 0.005046847648585873,
                            "process_1": 0.004763468532994031
                        },
                        "ram_energy": {
                            "process_2": 3.891134479846889e-06,
                            "process_0": 4.182450813750128e-06,
                            "process_3": 4.110269117942428e-06,
                            "process_1": 3.886549909370088e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005479200590669104,
                            "process_0": 0.005493304844736769,
                            "process_3": 0.005812829754735024,
                            "process_1": 0.005481064905684938
                        },
                        "total_energy_joules": {
                            "process_2": 19725.122126408773,
                            "process_0": 19775.89744105237,
                            "process_3": 20926.187117046087,
                            "process_1": 19731.833660465778
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 196.5335516337499,
                        "ram_power_avg": 0.6665840148925781,
                        "cpu_energy_total": 0.0029089069961569297,
                        "gpu_energy_total": 0.019341422695348,
                        "ram_energy_total": 1.6070404320909534e-05,
                        "total_energy_kwh": 0.022266400095825836,
                        "total_energy_joules": 80159.04034497301
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2043936645135683,
                        "joules_per_token": 4.892519552305481,
                        "flops_per_joule": 211454265.4229141,
                        "joules_per_flop": 4.729155016097565e-09
                    },
                    "per-process_emissions": [
                        0.0020873014650153953,
                        0.0020926744806024724,
                        0.0022143974950663073,
                        0.0020880116758206775
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0188": {
            "setup": {
                "experiment_id": "0188",
                "date_time": "April 11, 2025 at 03:06:29 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.7258511680011,
                        "average_latency_ms_per_batch": 2965.7313960001375,
                        "throughput_queries_per_sec": 5.394959240603884,
                        "throughput_tokens_per_sec": 690.5547827972972
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.7,
                        "cpu_memory_usage_bytes": 2016657408
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0188",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 82.86553161976956,
                            "process_0": 84.02525650339466,
                            "process_3": 534.1096405293149,
                            "process_1": 85.13377788252055
                        },
                        "ram_power": {
                            "process_2": 0.6551198959350586,
                            "process_0": 0.7030348777770996,
                            "process_3": 0.6554174423217773,
                            "process_1": 0.652763843536377
                        },
                        "cpu_energy": {
                            "process_2": 0.0007134681467191515,
                            "process_0": 0.0007198571896250316,
                            "process_3": 0.000761871837031208,
                            "process_1": 0.0007137098227815386
                        },
                        "gpu_energy": {
                            "process_2": 0.004761841309470105,
                            "process_0": 0.00476926520429799,
                            "process_3": 0.005046847648585873,
                            "process_1": 0.004763468532994031
                        },
                        "ram_energy": {
                            "process_2": 3.891134479846889e-06,
                            "process_0": 4.182450813750128e-06,
                            "process_3": 4.110269117942428e-06,
                            "process_1": 3.886549909370088e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005479200590669104,
                            "process_0": 0.005493304844736769,
                            "process_3": 0.005812829754735024,
                            "process_1": 0.005481064905684938
                        },
                        "total_energy_joules": {
                            "process_2": 19725.122126408773,
                            "process_0": 19775.89744105237,
                            "process_3": 20926.187117046087,
                            "process_1": 19731.833660465778
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 196.5335516337499,
                        "ram_power_avg": 0.6665840148925781,
                        "cpu_energy_total": 0.0029089069961569297,
                        "gpu_energy_total": 0.019341422695348,
                        "ram_energy_total": 1.6070404320909534e-05,
                        "total_energy_kwh": 0.022266400095825836,
                        "total_energy_joules": 80159.04034497301
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2043936645135683,
                        "joules_per_token": 4.892519552305481,
                        "flops_per_joule": 211454265.4229141,
                        "joules_per_flop": 4.729155016097565e-09
                    },
                    "per-process_emissions": [
                        0.0020873014650153953,
                        0.0020926744806024724,
                        0.0022143974950663073,
                        0.0020880116758206775
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0189": {
            "setup": {
                "experiment_id": "0189",
                "date_time": "April 11, 2025 at 03:07:33 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.440051502999268,
                        "average_latency_ms_per_batch": 2930.0064378749084,
                        "throughput_queries_per_sec": 5.460738854759845,
                        "throughput_tokens_per_sec": 698.9745734092602
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 2012508160
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0189",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1280.2263818392018,
                            "process_3": 836.5376698908619,
                            "process_1": 918.5834483316646,
                            "process_2": 724.68342658884
                        },
                        "ram_power": {
                            "process_0": 0.7008419036865234,
                            "process_3": 0.654261589050293,
                            "process_1": 0.6533117294311523,
                            "process_2": 0.653895378112793
                        },
                        "cpu_energy": {
                            "process_0": 0.0007261718540310085,
                            "process_3": 0.0008084362644065095,
                            "process_1": 0.0007154305753750807,
                            "process_2": 0.0007188068384060671
                        },
                        "gpu_energy": {
                            "process_0": 0.00477238187345802,
                            "process_3": 0.005060523770638092,
                            "process_1": 0.004669454846672083,
                            "process_2": 0.004682912079659984
                        },
                        "ram_energy": {
                            "process_0": 4.226933865956777e-06,
                            "process_3": 4.3916978876576265e-06,
                            "process_1": 3.8760199631547024e-06,
                            "process_2": 3.910422032469053e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005502780661354985,
                            "process_3": 0.005873351732932261,
                            "process_1": 0.0053887614420103196,
                            "process_2": 0.005405629340098523
                        },
                        "total_energy_joules": {
                            "process_0": 19810.010380877946,
                            "process_3": 21144.066238556137,
                            "process_1": 19399.54119123715,
                            "process_2": 19460.26562435468
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 940.0077316626421,
                        "ram_power_avg": 0.6655776500701904,
                        "cpu_energy_total": 0.0029688455322186657,
                        "gpu_energy_total": 0.01918527257042818,
                        "ram_energy_total": 1.6405073749238158e-05,
                        "total_energy_kwh": 0.02217052317639609,
                        "total_energy_joules": 79813.88343502591
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.205277569451156,
                        "joules_per_token": 4.871452846376093,
                        "flops_per_joule": 212368704.0858056,
                        "joules_per_flop": 4.7087917417246155e-09
                    },
                    "per-process_emissions": [
                        0.0020962842929431818,
                        0.002237453342660545,
                        0.0020528486713338313,
                        0.0020592744971105323
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0189": {
            "setup": {
                "experiment_id": "0189",
                "date_time": "April 11, 2025 at 03:07:33 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.440051502999268,
                        "average_latency_ms_per_batch": 2930.0064378749084,
                        "throughput_queries_per_sec": 5.460738854759845,
                        "throughput_tokens_per_sec": 698.9745734092602
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 2012508160
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0189",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1280.2263818392018,
                            "process_3": 836.5376698908619,
                            "process_1": 918.5834483316646,
                            "process_2": 724.68342658884
                        },
                        "ram_power": {
                            "process_0": 0.7008419036865234,
                            "process_3": 0.654261589050293,
                            "process_1": 0.6533117294311523,
                            "process_2": 0.653895378112793
                        },
                        "cpu_energy": {
                            "process_0": 0.0007261718540310085,
                            "process_3": 0.0008084362644065095,
                            "process_1": 0.0007154305753750807,
                            "process_2": 0.0007188068384060671
                        },
                        "gpu_energy": {
                            "process_0": 0.00477238187345802,
                            "process_3": 0.005060523770638092,
                            "process_1": 0.004669454846672083,
                            "process_2": 0.004682912079659984
                        },
                        "ram_energy": {
                            "process_0": 4.226933865956777e-06,
                            "process_3": 4.3916978876576265e-06,
                            "process_1": 3.8760199631547024e-06,
                            "process_2": 3.910422032469053e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005502780661354985,
                            "process_3": 0.005873351732932261,
                            "process_1": 0.0053887614420103196,
                            "process_2": 0.005405629340098523
                        },
                        "total_energy_joules": {
                            "process_0": 19810.010380877946,
                            "process_3": 21144.066238556137,
                            "process_1": 19399.54119123715,
                            "process_2": 19460.26562435468
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 940.0077316626421,
                        "ram_power_avg": 0.6655776500701904,
                        "cpu_energy_total": 0.0029688455322186657,
                        "gpu_energy_total": 0.01918527257042818,
                        "ram_energy_total": 1.6405073749238158e-05,
                        "total_energy_kwh": 0.02217052317639609,
                        "total_energy_joules": 79813.88343502591
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.205277569451156,
                        "joules_per_token": 4.871452846376093,
                        "flops_per_joule": 212368704.0858056,
                        "joules_per_flop": 4.7087917417246155e-09
                    },
                    "per-process_emissions": [
                        0.0020962842929431818,
                        0.002237453342660545,
                        0.0020528486713338313,
                        0.0020592744971105323
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0190": {
            "setup": {
                "experiment_id": "0190",
                "date_time": "April 11, 2025 at 03:08:35 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.25769509800739,
                        "average_latency_ms_per_batch": 2907.2118872509236,
                        "throughput_queries_per_sec": 5.503554821774512,
                        "throughput_tokens_per_sec": 704.4550171871375
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 2015485952
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0190",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 854.4650189596504,
                            "process_2": 623.092399420029,
                            "process_1": 786.069151790431,
                            "process_3": 375.30482830647236
                        },
                        "ram_power": {
                            "process_0": 0.7022109031677246,
                            "process_2": 0.6508884429931641,
                            "process_1": 0.6590352058410645,
                            "process_3": 0.6522588729858398
                        },
                        "cpu_energy": {
                            "process_0": 0.0007203055303122029,
                            "process_2": 0.0006865516977186414,
                            "process_1": 0.0007314132911562865,
                            "process_3": 0.0007492048364373432
                        },
                        "gpu_energy": {
                            "process_0": 0.004673195682997844,
                            "process_2": 0.004754242414501897,
                            "process_1": 0.004673195682997844,
                            "process_3": 0.005122312431179998
                        },
                        "ram_energy": {
                            "process_0": 4.220125828688862e-06,
                            "process_2": 4.00536393922587e-06,
                            "process_1": 4.0192196082902455e-06,
                            "process_3": 4.718391802631531e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005397721339138735,
                            "process_2": 0.005444799476159764,
                            "process_1": 0.00540862819376242,
                            "process_3": 0.005876235659419973
                        },
                        "total_energy_joules": {
                            "process_0": 19431.796820899446,
                            "process_2": 19601.278114175147,
                            "process_1": 19471.06149754471,
                            "process_3": 21154.448373911902
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 659.7328496191457,
                        "ram_power_avg": 0.6660983562469482,
                        "cpu_energy_total": 0.002887475355624474,
                        "gpu_energy_total": 0.019222946211677583,
                        "ram_energy_total": 1.696310117883651e-05,
                        "total_energy_kwh": 0.022127384668480892,
                        "total_energy_joules": 79658.5848065312
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20567776894094003,
                        "joules_per_token": 4.861974170320508,
                        "flops_per_joule": 212782728.1179903,
                        "joules_per_flop": 4.699629565072074e-09
                    },
                    "per-process_emissions": [
                        0.002056261944144901,
                        0.002074196360443062,
                        0.0020604169104137936,
                        0.002238551974456039
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0190": {
            "setup": {
                "experiment_id": "0190",
                "date_time": "April 11, 2025 at 03:08:35 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.25769509800739,
                        "average_latency_ms_per_batch": 2907.2118872509236,
                        "throughput_queries_per_sec": 5.503554821774512,
                        "throughput_tokens_per_sec": 704.4550171871375
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 2015485952
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0190",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 854.4650189596504,
                            "process_2": 623.092399420029,
                            "process_1": 786.069151790431,
                            "process_3": 375.30482830647236
                        },
                        "ram_power": {
                            "process_0": 0.7022109031677246,
                            "process_2": 0.6508884429931641,
                            "process_1": 0.6590352058410645,
                            "process_3": 0.6522588729858398
                        },
                        "cpu_energy": {
                            "process_0": 0.0007203055303122029,
                            "process_2": 0.0006865516977186414,
                            "process_1": 0.0007314132911562865,
                            "process_3": 0.0007492048364373432
                        },
                        "gpu_energy": {
                            "process_0": 0.004673195682997844,
                            "process_2": 0.004754242414501897,
                            "process_1": 0.004673195682997844,
                            "process_3": 0.005122312431179998
                        },
                        "ram_energy": {
                            "process_0": 4.220125828688862e-06,
                            "process_2": 4.00536393922587e-06,
                            "process_1": 4.0192196082902455e-06,
                            "process_3": 4.718391802631531e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005397721339138735,
                            "process_2": 0.005444799476159764,
                            "process_1": 0.00540862819376242,
                            "process_3": 0.005876235659419973
                        },
                        "total_energy_joules": {
                            "process_0": 19431.796820899446,
                            "process_2": 19601.278114175147,
                            "process_1": 19471.06149754471,
                            "process_3": 21154.448373911902
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 659.7328496191457,
                        "ram_power_avg": 0.6660983562469482,
                        "cpu_energy_total": 0.002887475355624474,
                        "gpu_energy_total": 0.019222946211677583,
                        "ram_energy_total": 1.696310117883651e-05,
                        "total_energy_kwh": 0.022127384668480892,
                        "total_energy_joules": 79658.5848065312
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20567776894094003,
                        "joules_per_token": 4.861974170320508,
                        "flops_per_joule": 212782728.1179903,
                        "joules_per_flop": 4.699629565072074e-09
                    },
                    "per-process_emissions": [
                        0.002056261944144901,
                        0.002074196360443062,
                        0.0020604169104137936,
                        0.002238551974456039
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0191": {
            "setup": {
                "experiment_id": "0191",
                "date_time": "April 11, 2025 at 03:09:37 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.40953261200775,
                        "average_latency_ms_per_batch": 2926.191576500969,
                        "throughput_queries_per_sec": 5.467857992788089,
                        "throughput_tokens_per_sec": 699.8858230768753
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 2012946432
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0191",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 403.85703822312837,
                            "process_0": 800.8478286853817,
                            "process_2": 1469.0011991716526,
                            "process_1": 851.9714453149419
                        },
                        "ram_power": {
                            "process_3": 0.6530499458312988,
                            "process_0": 0.7013010978698732,
                            "process_2": 0.6500658988952637,
                            "process_1": 0.6550698280334473
                        },
                        "cpu_energy": {
                            "process_3": 0.0007672065678124226,
                            "process_0": 0.0006701974951872669,
                            "process_2": 0.0006855596264375664,
                            "process_1": 0.0006995969115312164
                        },
                        "gpu_energy": {
                            "process_3": 0.005051975708244005,
                            "process_0": 0.004662877619188155,
                            "process_2": 0.004774423263980032,
                            "process_1": 0.004797991060611961
                        },
                        "ram_energy": {
                            "process_3": 4.4657068708563325e-06,
                            "process_0": 4.265640932522579e-06,
                            "process_2": 4.012153195996888e-06,
                            "process_1": 4.074013384377638e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005823647982927285,
                            "process_0": 0.005337340755307945,
                            "process_2": 0.005463995043613597,
                            "process_1": 0.005501661985527555
                        },
                        "total_energy_joules": {
                            "process_3": 20965.132738538225,
                            "process_0": 19214.426719108604,
                            "process_2": 19670.38215700895,
                            "process_1": 19805.9831478992
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 881.4193778487761,
                        "ram_power_avg": 0.6648716926574707,
                        "cpu_energy_total": 0.002822560600968472,
                        "gpu_energy_total": 0.019287267652024154,
                        "ram_energy_total": 1.6817514383753438e-05,
                        "total_energy_kwh": 0.022126645767376385,
                        "total_energy_joules": 79655.92476255498
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2056846373805689,
                        "joules_per_token": 4.861811814120787,
                        "flops_per_joule": 212789833.82187685,
                        "joules_per_flop": 4.699472630055648e-09
                    },
                    "per-process_emissions": [
                        0.0022185186990961495,
                        0.0020332599607345617,
                        0.0020815089118646,
                        0.002095858133386722
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0191": {
            "setup": {
                "experiment_id": "0191",
                "date_time": "April 11, 2025 at 03:09:37 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.40953261200775,
                        "average_latency_ms_per_batch": 2926.191576500969,
                        "throughput_queries_per_sec": 5.467857992788089,
                        "throughput_tokens_per_sec": 699.8858230768753
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 2012946432
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0191",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 403.85703822312837,
                            "process_0": 800.8478286853817,
                            "process_2": 1469.0011991716526,
                            "process_1": 851.9714453149419
                        },
                        "ram_power": {
                            "process_3": 0.6530499458312988,
                            "process_0": 0.7013010978698732,
                            "process_2": 0.6500658988952637,
                            "process_1": 0.6550698280334473
                        },
                        "cpu_energy": {
                            "process_3": 0.0007672065678124226,
                            "process_0": 0.0006701974951872669,
                            "process_2": 0.0006855596264375664,
                            "process_1": 0.0006995969115312164
                        },
                        "gpu_energy": {
                            "process_3": 0.005051975708244005,
                            "process_0": 0.004662877619188155,
                            "process_2": 0.004774423263980032,
                            "process_1": 0.004797991060611961
                        },
                        "ram_energy": {
                            "process_3": 4.4657068708563325e-06,
                            "process_0": 4.265640932522579e-06,
                            "process_2": 4.012153195996888e-06,
                            "process_1": 4.074013384377638e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005823647982927285,
                            "process_0": 0.005337340755307945,
                            "process_2": 0.005463995043613597,
                            "process_1": 0.005501661985527555
                        },
                        "total_energy_joules": {
                            "process_3": 20965.132738538225,
                            "process_0": 19214.426719108604,
                            "process_2": 19670.38215700895,
                            "process_1": 19805.9831478992
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 881.4193778487761,
                        "ram_power_avg": 0.6648716926574707,
                        "cpu_energy_total": 0.002822560600968472,
                        "gpu_energy_total": 0.019287267652024154,
                        "ram_energy_total": 1.6817514383753438e-05,
                        "total_energy_kwh": 0.022126645767376385,
                        "total_energy_joules": 79655.92476255498
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2056846373805689,
                        "joules_per_token": 4.861811814120787,
                        "flops_per_joule": 212789833.82187685,
                        "joules_per_flop": 4.699472630055648e-09
                    },
                    "per-process_emissions": [
                        0.0022185186990961495,
                        0.0020332599607345617,
                        0.0020815089118646,
                        0.002095858133386722
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0192": {
            "setup": {
                "experiment_id": "0192",
                "date_time": "April 11, 2025 at 03:10:39 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.33989140399717,
                        "average_latency_ms_per_batch": 2917.4864254996464,
                        "throughput_queries_per_sec": 5.48417290313865,
                        "throughput_tokens_per_sec": 701.9741316017472
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 2011414528
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0192",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 908.342508168056,
                            "process_3": 5386.906198450752,
                            "process_0": 1573.1571757713216,
                            "process_1": 944.4400215605858
                        },
                        "ram_power": {
                            "process_2": 0.6544718742370605,
                            "process_3": 0.6572427749633789,
                            "process_0": 0.7008390426635742,
                            "process_1": 0.649683952331543
                        },
                        "cpu_energy": {
                            "process_2": 0.0006092904614997678,
                            "process_3": 0.0006916198208436981,
                            "process_0": 0.0006772580893436954,
                            "process_1": 0.0006709861700313697
                        },
                        "gpu_energy": {
                            "process_2": 0.004657154559054022,
                            "process_3": 0.005014781511821942,
                            "process_0": 0.004753200191446094,
                            "process_1": 0.004663480119669994
                        },
                        "ram_energy": {
                            "process_2": 3.6555516487540145e-06,
                            "process_3": 4.390602819577557e-06,
                            "process_0": 3.957541162948164e-06,
                            "process_1": 3.6305018079849965e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005270100572202544,
                            "process_3": 0.005710791935485217,
                            "process_0": 0.005434415821952738,
                            "process_1": 0.005338096791509348
                        },
                        "total_energy_joules": {
                            "process_2": 18972.362059929157,
                            "process_3": 20558.85096774678,
                            "process_0": 19563.89695902986,
                            "process_1": 19217.148449433655
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 2203.211475987679,
                        "ram_power_avg": 0.6655594110488892,
                        "cpu_energy_total": 0.0026491545417185315,
                        "gpu_energy_total": 0.019088616381992052,
                        "ram_energy_total": 1.5634197439264732e-05,
                        "total_energy_kwh": 0.021753405121149847,
                        "total_energy_joules": 78312.25843613945
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2092137339310742,
                        "joules_per_token": 4.779800929940152,
                        "flops_per_joule": 216440839.93534717,
                        "joules_per_flop": 4.6202001447541466e-09
                    },
                    "per-process_emissions": [
                        0.002007644812980559,
                        0.0021755261878230936,
                        0.0020702407073728956,
                        0.0020335479727254862
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0192": {
            "setup": {
                "experiment_id": "0192",
                "date_time": "April 11, 2025 at 03:10:39 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.33989140399717,
                        "average_latency_ms_per_batch": 2917.4864254996464,
                        "throughput_queries_per_sec": 5.48417290313865,
                        "throughput_tokens_per_sec": 701.9741316017472
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 2011414528
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0192",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 908.342508168056,
                            "process_3": 5386.906198450752,
                            "process_0": 1573.1571757713216,
                            "process_1": 944.4400215605858
                        },
                        "ram_power": {
                            "process_2": 0.6544718742370605,
                            "process_3": 0.6572427749633789,
                            "process_0": 0.7008390426635742,
                            "process_1": 0.649683952331543
                        },
                        "cpu_energy": {
                            "process_2": 0.0006092904614997678,
                            "process_3": 0.0006916198208436981,
                            "process_0": 0.0006772580893436954,
                            "process_1": 0.0006709861700313697
                        },
                        "gpu_energy": {
                            "process_2": 0.004657154559054022,
                            "process_3": 0.005014781511821942,
                            "process_0": 0.004753200191446094,
                            "process_1": 0.004663480119669994
                        },
                        "ram_energy": {
                            "process_2": 3.6555516487540145e-06,
                            "process_3": 4.390602819577557e-06,
                            "process_0": 3.957541162948164e-06,
                            "process_1": 3.6305018079849965e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005270100572202544,
                            "process_3": 0.005710791935485217,
                            "process_0": 0.005434415821952738,
                            "process_1": 0.005338096791509348
                        },
                        "total_energy_joules": {
                            "process_2": 18972.362059929157,
                            "process_3": 20558.85096774678,
                            "process_0": 19563.89695902986,
                            "process_1": 19217.148449433655
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 2203.211475987679,
                        "ram_power_avg": 0.6655594110488892,
                        "cpu_energy_total": 0.0026491545417185315,
                        "gpu_energy_total": 0.019088616381992052,
                        "ram_energy_total": 1.5634197439264732e-05,
                        "total_energy_kwh": 0.021753405121149847,
                        "total_energy_joules": 78312.25843613945
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2092137339310742,
                        "joules_per_token": 4.779800929940152,
                        "flops_per_joule": 216440839.93534717,
                        "joules_per_flop": 4.6202001447541466e-09
                    },
                    "per-process_emissions": [
                        0.002007644812980559,
                        0.0021755261878230936,
                        0.0020702407073728956,
                        0.0020335479727254862
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0193": {
            "setup": {
                "experiment_id": "0193",
                "date_time": "April 11, 2025 at 03:11:41 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.37091388599947,
                        "average_latency_ms_per_batch": 2921.364235749934,
                        "throughput_queries_per_sec": 5.476893228239542,
                        "throughput_tokens_per_sec": 701.0423332146614
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 2013650944
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0193",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1872.834768576715,
                            "process_0": 1695.2506194038622,
                            "process_3": 2096.3632856340582,
                            "process_1": 985.8525497021242
                        },
                        "ram_power": {
                            "process_2": 0.6523175239562988,
                            "process_0": 0.7019147872924805,
                            "process_3": 0.6469874382019044,
                            "process_1": 0.6499400138854982
                        },
                        "cpu_energy": {
                            "process_2": 0.0006807093363440799,
                            "process_0": 0.0006303087346873326,
                            "process_3": 0.0006977815492496121,
                            "process_1": 0.0006853277419060078
                        },
                        "gpu_energy": {
                            "process_2": 0.0047678263142578725,
                            "process_0": 0.004761567698139846,
                            "process_3": 0.00502622874320191,
                            "process_1": 0.004769431315541861
                        },
                        "ram_energy": {
                            "process_2": 3.7118498280742984e-06,
                            "process_0": 4.029746330357861e-06,
                            "process_3": 3.971629736062082e-06,
                            "process_1": 3.6798343551096227e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005452247500430026,
                            "process_0": 0.005395906179157536,
                            "process_3": 0.005727981922187585,
                            "process_1": 0.005458438891802977
                        },
                        "total_energy_joules": {
                            "process_2": 19628.091001548095,
                            "process_0": 19425.26224496713,
                            "process_3": 20620.734919875307,
                            "process_1": 19650.380010490717
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1662.5753058291898,
                        "ram_power_avg": 0.6627899408340454,
                        "cpu_energy_total": 0.0026941273621870327,
                        "gpu_energy_total": 0.01932505407114149,
                        "ram_energy_total": 1.5393060249603863e-05,
                        "total_energy_kwh": 0.022034574493578128,
                        "total_energy_joules": 79324.46817688124
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20654408881086003,
                        "joules_per_token": 4.841581309624099,
                        "flops_per_joule": 213678974.25238574,
                        "joules_per_flop": 4.679917635784115e-09
                    },
                    "per-process_emissions": [
                        0.0020770336852888186,
                        0.0020555704589500633,
                        0.0021820747132573607,
                        0.002079392295832344
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0193": {
            "setup": {
                "experiment_id": "0193",
                "date_time": "April 11, 2025 at 03:11:41 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.37091388599947,
                        "average_latency_ms_per_batch": 2921.364235749934,
                        "throughput_queries_per_sec": 5.476893228239542,
                        "throughput_tokens_per_sec": 701.0423332146614
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 2013650944
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0193",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1872.834768576715,
                            "process_0": 1695.2506194038622,
                            "process_3": 2096.3632856340582,
                            "process_1": 985.8525497021242
                        },
                        "ram_power": {
                            "process_2": 0.6523175239562988,
                            "process_0": 0.7019147872924805,
                            "process_3": 0.6469874382019044,
                            "process_1": 0.6499400138854982
                        },
                        "cpu_energy": {
                            "process_2": 0.0006807093363440799,
                            "process_0": 0.0006303087346873326,
                            "process_3": 0.0006977815492496121,
                            "process_1": 0.0006853277419060078
                        },
                        "gpu_energy": {
                            "process_2": 0.0047678263142578725,
                            "process_0": 0.004761567698139846,
                            "process_3": 0.00502622874320191,
                            "process_1": 0.004769431315541861
                        },
                        "ram_energy": {
                            "process_2": 3.7118498280742984e-06,
                            "process_0": 4.029746330357861e-06,
                            "process_3": 3.971629736062082e-06,
                            "process_1": 3.6798343551096227e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005452247500430026,
                            "process_0": 0.005395906179157536,
                            "process_3": 0.005727981922187585,
                            "process_1": 0.005458438891802977
                        },
                        "total_energy_joules": {
                            "process_2": 19628.091001548095,
                            "process_0": 19425.26224496713,
                            "process_3": 20620.734919875307,
                            "process_1": 19650.380010490717
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1662.5753058291898,
                        "ram_power_avg": 0.6627899408340454,
                        "cpu_energy_total": 0.0026941273621870327,
                        "gpu_energy_total": 0.01932505407114149,
                        "ram_energy_total": 1.5393060249603863e-05,
                        "total_energy_kwh": 0.022034574493578128,
                        "total_energy_joules": 79324.46817688124
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20654408881086003,
                        "joules_per_token": 4.841581309624099,
                        "flops_per_joule": 213678974.25238574,
                        "joules_per_flop": 4.679917635784115e-09
                    },
                    "per-process_emissions": [
                        0.0020770336852888186,
                        0.0020555704589500633,
                        0.0021820747132573607,
                        0.002079392295832344
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0194": {
            "setup": {
                "experiment_id": "0194",
                "date_time": "April 11, 2025 at 03:12:44 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.218669691003015,
                        "average_latency_ms_per_batch": 2902.333711375377,
                        "throughput_queries_per_sec": 5.512805070378284,
                        "throughput_tokens_per_sec": 705.6390490084203
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 2017865728
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0194",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 518.7564640103095,
                            "process_0": 829.3610418172757,
                            "process_1": 639.7124441934726,
                            "process_2": 596.2544139700799
                        },
                        "ram_power": {
                            "process_3": 0.6507296562194824,
                            "process_0": 0.703132152557373,
                            "process_1": 0.6494593620300293,
                            "process_2": 0.6483364105224609
                        },
                        "cpu_energy": {
                            "process_3": 0.0008013886575937479,
                            "process_0": 0.0006988725287496892,
                            "process_1": 0.000697427084249739,
                            "process_2": 0.0007174172563754837
                        },
                        "gpu_energy": {
                            "process_3": 0.005097582133617828,
                            "process_0": 0.0046604442839079385,
                            "process_1": 0.0046799217994900055,
                            "process_2": 0.004758213528789967
                        },
                        "ram_energy": {
                            "process_3": 4.661811325812182e-06,
                            "process_0": 4.443783398955074e-06,
                            "process_1": 4.1081030176165815e-06,
                            "process_2": 4.1768339299485e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005903632602537387,
                            "process_0": 0.005363760596056583,
                            "process_1": 0.005381456986757362,
                            "process_2": 0.005479807619095399
                        },
                        "total_energy_joules": {
                            "process_3": 21253.077369134593,
                            "process_0": 19309.538145803697,
                            "process_1": 19373.245152326504,
                            "process_2": 19727.307428743436
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 646.0210909977844,
                        "ram_power_avg": 0.6629143953323364,
                        "cpu_energy_total": 0.00291510552696866,
                        "gpu_energy_total": 0.01919616174580574,
                        "ram_energy_total": 1.7390531672332338e-05,
                        "total_energy_kwh": 0.02212865780444673,
                        "total_energy_joules": 79663.16809600823
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2056659356084656,
                        "joules_per_token": 4.862253912109877,
                        "flops_per_joule": 212770486.0133642,
                        "joules_per_flop": 4.699899966093933e-09
                    },
                    "per-process_emissions": [
                        0.0022489888399366176,
                        0.0020433245990677555,
                        0.002050066039105217,
                        0.002087532712494392
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0194": {
            "setup": {
                "experiment_id": "0194",
                "date_time": "April 11, 2025 at 03:12:44 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.218669691003015,
                        "average_latency_ms_per_batch": 2902.333711375377,
                        "throughput_queries_per_sec": 5.512805070378284,
                        "throughput_tokens_per_sec": 705.6390490084203
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 2017865728
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0194",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 518.7564640103095,
                            "process_0": 829.3610418172757,
                            "process_1": 639.7124441934726,
                            "process_2": 596.2544139700799
                        },
                        "ram_power": {
                            "process_3": 0.6507296562194824,
                            "process_0": 0.703132152557373,
                            "process_1": 0.6494593620300293,
                            "process_2": 0.6483364105224609
                        },
                        "cpu_energy": {
                            "process_3": 0.0008013886575937479,
                            "process_0": 0.0006988725287496892,
                            "process_1": 0.000697427084249739,
                            "process_2": 0.0007174172563754837
                        },
                        "gpu_energy": {
                            "process_3": 0.005097582133617828,
                            "process_0": 0.0046604442839079385,
                            "process_1": 0.0046799217994900055,
                            "process_2": 0.004758213528789967
                        },
                        "ram_energy": {
                            "process_3": 4.661811325812182e-06,
                            "process_0": 4.443783398955074e-06,
                            "process_1": 4.1081030176165815e-06,
                            "process_2": 4.1768339299485e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005903632602537387,
                            "process_0": 0.005363760596056583,
                            "process_1": 0.005381456986757362,
                            "process_2": 0.005479807619095399
                        },
                        "total_energy_joules": {
                            "process_3": 21253.077369134593,
                            "process_0": 19309.538145803697,
                            "process_1": 19373.245152326504,
                            "process_2": 19727.307428743436
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 646.0210909977844,
                        "ram_power_avg": 0.6629143953323364,
                        "cpu_energy_total": 0.00291510552696866,
                        "gpu_energy_total": 0.01919616174580574,
                        "ram_energy_total": 1.7390531672332338e-05,
                        "total_energy_kwh": 0.02212865780444673,
                        "total_energy_joules": 79663.16809600823
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2056659356084656,
                        "joules_per_token": 4.862253912109877,
                        "flops_per_joule": 212770486.0133642,
                        "joules_per_flop": 4.699899966093933e-09
                    },
                    "per-process_emissions": [
                        0.0022489888399366176,
                        0.0020433245990677555,
                        0.002050066039105217,
                        0.002087532712494392
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0195": {
            "setup": {
                "experiment_id": "0195",
                "date_time": "April 11, 2025 at 03:13:47 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.55485256099928,
                        "average_latency_ms_per_batch": 2944.35657012491,
                        "throughput_queries_per_sec": 5.4341244407504705,
                        "throughput_tokens_per_sec": 695.5679284160602
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 2012020736
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0195",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 421.8796903839526,
                            "process_1": 2603.764888665296,
                            "process_0": 1183.4278421145393,
                            "process_2": 1863.4872965379673
                        },
                        "ram_power": {
                            "process_3": 0.6528997421264648,
                            "process_1": 0.6515307426452637,
                            "process_0": 0.7008090019226074,
                            "process_2": 0.6566162109375
                        },
                        "cpu_energy": {
                            "process_3": 0.0007194961673440049,
                            "process_1": 0.0006542080766564595,
                            "process_0": 0.0006839340629684332,
                            "process_2": 0.0006559117707500945
                        },
                        "gpu_energy": {
                            "process_3": 0.005040709588119918,
                            "process_1": 0.004741068792852077,
                            "process_0": 0.0047713188170519505,
                            "process_2": 0.004759140751753987
                        },
                        "ram_energy": {
                            "process_3": 4.25371402140622e-06,
                            "process_1": 3.8843170658409536e-06,
                            "process_0": 3.96288915500963e-06,
                            "process_2": 3.9376790792794205e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005764459469485327,
                            "process_1": 0.005399161186574375,
                            "process_0": 0.005459215769175393,
                            "process_2": 0.005418990201583361
                        },
                        "total_energy_joules": {
                            "process_3": 20752.054090147176,
                            "process_1": 19436.98027166775,
                            "process_0": 19653.176769031415,
                            "process_2": 19508.3647257001
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1518.1399294254388,
                        "ram_power_avg": 0.665463924407959,
                        "cpu_energy_total": 0.002713550077718992,
                        "gpu_energy_total": 0.019312237949777933,
                        "ram_energy_total": 1.6038599321536224e-05,
                        "total_energy_kwh": 0.02204182662681846,
                        "total_energy_joules": 79350.57585654643
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20647613231717105,
                        "joules_per_token": 4.843174795931789,
                        "flops_per_joule": 213608670.26088035,
                        "joules_per_flop": 4.681457914506465e-09
                    },
                    "per-process_emissions": [
                        0.0021959708349004355,
                        0.0020568104540255082,
                        0.002079688247267366,
                        0.0020643643172931817
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0195": {
            "setup": {
                "experiment_id": "0195",
                "date_time": "April 11, 2025 at 03:13:47 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.55485256099928,
                        "average_latency_ms_per_batch": 2944.35657012491,
                        "throughput_queries_per_sec": 5.4341244407504705,
                        "throughput_tokens_per_sec": 695.5679284160602
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 2012020736
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0195",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 421.8796903839526,
                            "process_1": 2603.764888665296,
                            "process_0": 1183.4278421145393,
                            "process_2": 1863.4872965379673
                        },
                        "ram_power": {
                            "process_3": 0.6528997421264648,
                            "process_1": 0.6515307426452637,
                            "process_0": 0.7008090019226074,
                            "process_2": 0.6566162109375
                        },
                        "cpu_energy": {
                            "process_3": 0.0007194961673440049,
                            "process_1": 0.0006542080766564595,
                            "process_0": 0.0006839340629684332,
                            "process_2": 0.0006559117707500945
                        },
                        "gpu_energy": {
                            "process_3": 0.005040709588119918,
                            "process_1": 0.004741068792852077,
                            "process_0": 0.0047713188170519505,
                            "process_2": 0.004759140751753987
                        },
                        "ram_energy": {
                            "process_3": 4.25371402140622e-06,
                            "process_1": 3.8843170658409536e-06,
                            "process_0": 3.96288915500963e-06,
                            "process_2": 3.9376790792794205e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005764459469485327,
                            "process_1": 0.005399161186574375,
                            "process_0": 0.005459215769175393,
                            "process_2": 0.005418990201583361
                        },
                        "total_energy_joules": {
                            "process_3": 20752.054090147176,
                            "process_1": 19436.98027166775,
                            "process_0": 19653.176769031415,
                            "process_2": 19508.3647257001
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1518.1399294254388,
                        "ram_power_avg": 0.665463924407959,
                        "cpu_energy_total": 0.002713550077718992,
                        "gpu_energy_total": 0.019312237949777933,
                        "ram_energy_total": 1.6038599321536224e-05,
                        "total_energy_kwh": 0.02204182662681846,
                        "total_energy_joules": 79350.57585654643
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20647613231717105,
                        "joules_per_token": 4.843174795931789,
                        "flops_per_joule": 213608670.26088035,
                        "joules_per_flop": 4.681457914506465e-09
                    },
                    "per-process_emissions": [
                        0.0021959708349004355,
                        0.0020568104540255082,
                        0.002079688247267366,
                        0.0020643643172931817
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0196": {
            "setup": {
                "experiment_id": "0196",
                "date_time": "April 11, 2025 at 03:14:49 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.547086257996853,
                        "average_latency_ms_per_batch": 2943.3857822496066,
                        "throughput_queries_per_sec": 5.435916724368807,
                        "throughput_tokens_per_sec": 695.7973407192073
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 2017513472
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0196",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 758.8126405215852,
                            "process_1": 793.9872730337526,
                            "process_3": 27.59374695845666,
                            "process_0": 1252.4239999544654
                        },
                        "ram_power": {
                            "process_2": 0.6506710052490234,
                            "process_1": 0.6483206748962402,
                            "process_3": 0.6498942375183107,
                            "process_0": 0.7031164169311525
                        },
                        "cpu_energy": {
                            "process_2": 0.0006986398934689076,
                            "process_1": 0.0007035520621877822,
                            "process_3": 0.0007608768421566767,
                            "process_0": 0.0007536885449062539
                        },
                        "gpu_energy": {
                            "process_2": 0.004654623168140026,
                            "process_1": 0.0046350384302500935,
                            "process_3": 0.0050181095700399525,
                            "process_0": 0.004779082434373971
                        },
                        "ram_energy": {
                            "process_2": 3.4199195355725226e-06,
                            "process_1": 3.456957249248866e-06,
                            "process_3": 4.107884630773363e-06,
                            "process_0": 4.040127171832096e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0053566829811445065,
                            "process_1": 0.005342047449687124,
                            "process_3": 0.0057830942968274054,
                            "process_0": 0.005536811106452056
                        },
                        "total_energy_joules": {
                            "process_2": 19284.058732120222,
                            "process_1": 19231.37081887365,
                            "process_3": 20819.139468578658,
                            "process_0": 19932.5199832274
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 708.204415117065,
                        "ram_power_avg": 0.6630005836486816,
                        "cpu_energy_total": 0.0029167573427196203,
                        "gpu_energy_total": 0.019086853602804044,
                        "ram_energy_total": 1.5024888587426845e-05,
                        "total_energy_kwh": 0.022018635834111094,
                        "total_energy_joules": 79267.08900279993
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20669360015757957,
                        "joules_per_token": 4.83807916276855,
                        "flops_per_joule": 213833650.33820382,
                        "joules_per_flop": 4.6765324279802496e-09
                    },
                    "per-process_emissions": [
                        0.0020406283816669997,
                        0.00203505297595831,
                        0.0022030697723764,
                        0.0021092481910029107
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0196": {
            "setup": {
                "experiment_id": "0196",
                "date_time": "April 11, 2025 at 03:14:49 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.547086257996853,
                        "average_latency_ms_per_batch": 2943.3857822496066,
                        "throughput_queries_per_sec": 5.435916724368807,
                        "throughput_tokens_per_sec": 695.7973407192073
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 2017513472
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0196",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 758.8126405215852,
                            "process_1": 793.9872730337526,
                            "process_3": 27.59374695845666,
                            "process_0": 1252.4239999544654
                        },
                        "ram_power": {
                            "process_2": 0.6506710052490234,
                            "process_1": 0.6483206748962402,
                            "process_3": 0.6498942375183107,
                            "process_0": 0.7031164169311525
                        },
                        "cpu_energy": {
                            "process_2": 0.0006986398934689076,
                            "process_1": 0.0007035520621877822,
                            "process_3": 0.0007608768421566767,
                            "process_0": 0.0007536885449062539
                        },
                        "gpu_energy": {
                            "process_2": 0.004654623168140026,
                            "process_1": 0.0046350384302500935,
                            "process_3": 0.0050181095700399525,
                            "process_0": 0.004779082434373971
                        },
                        "ram_energy": {
                            "process_2": 3.4199195355725226e-06,
                            "process_1": 3.456957249248866e-06,
                            "process_3": 4.107884630773363e-06,
                            "process_0": 4.040127171832096e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0053566829811445065,
                            "process_1": 0.005342047449687124,
                            "process_3": 0.0057830942968274054,
                            "process_0": 0.005536811106452056
                        },
                        "total_energy_joules": {
                            "process_2": 19284.058732120222,
                            "process_1": 19231.37081887365,
                            "process_3": 20819.139468578658,
                            "process_0": 19932.5199832274
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 708.204415117065,
                        "ram_power_avg": 0.6630005836486816,
                        "cpu_energy_total": 0.0029167573427196203,
                        "gpu_energy_total": 0.019086853602804044,
                        "ram_energy_total": 1.5024888587426845e-05,
                        "total_energy_kwh": 0.022018635834111094,
                        "total_energy_joules": 79267.08900279993
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20669360015757957,
                        "joules_per_token": 4.83807916276855,
                        "flops_per_joule": 213833650.33820382,
                        "joules_per_flop": 4.6765324279802496e-09
                    },
                    "per-process_emissions": [
                        0.0020406283816669997,
                        0.00203505297595831,
                        0.0022030697723764,
                        0.0021092481910029107
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0197": {
            "setup": {
                "experiment_id": "0197",
                "date_time": "April 11, 2025 at 03:15:52 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.347140455000044,
                        "average_latency_ms_per_batch": 2918.3925568750055,
                        "throughput_queries_per_sec": 5.482470122913378,
                        "throughput_tokens_per_sec": 701.7561757329124
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            9.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 2031509504
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0197",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 758.3198753001615,
                            "process_3": 533.7966354778259,
                            "process_2": 1208.6116246460292,
                            "process_1": 590.2115714017713
                        },
                        "ram_power": {
                            "process_0": 0.7081046104431152,
                            "process_3": 0.6495823860168458,
                            "process_2": 0.6491274833679199,
                            "process_1": 0.6510071754455566
                        },
                        "cpu_energy": {
                            "process_0": 0.0006886708861875378,
                            "process_3": 0.0007456077217500478,
                            "process_2": 0.0007375065516874884,
                            "process_1": 0.000703484795718623
                        },
                        "gpu_energy": {
                            "process_0": 0.004671750404064312,
                            "process_3": 0.005094094353050194,
                            "process_2": 0.004656309280600179,
                            "process_1": 0.004741395737558163
                        },
                        "ram_energy": {
                            "process_0": 4.0702746482952225e-06,
                            "process_3": 4.332546937919091e-06,
                            "process_2": 3.984752355206689e-06,
                            "process_1": 4.109221318051914e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005364491564900145,
                            "process_3": 0.005844034621738163,
                            "process_2": 0.0053978005846428735,
                            "process_1": 0.005448989754594837
                        },
                        "total_energy_joules": {
                            "process_0": 19312.169633640522,
                            "process_3": 21038.524638257386,
                            "process_2": 19432.082104714344,
                            "process_1": 19616.36311654141
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 772.734926706447,
                        "ram_power_avg": 0.6644554138183594,
                        "cpu_energy_total": 0.002875269955343697,
                        "gpu_energy_total": 0.019163549775272848,
                        "ram_energy_total": 1.6496795259472918e-05,
                        "total_energy_kwh": 0.022055316525876018,
                        "total_energy_joules": 79399.13949315366
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20634984339361437,
                        "joules_per_token": 4.84613888508018,
                        "flops_per_joule": 213478018.79658332,
                        "joules_per_flop": 4.684323030713853e-09
                    },
                    "per-process_emissions": [
                        0.00204360306164871,
                        0.002226284989151153,
                        0.0020562921327197028,
                        0.0020757926470129032
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0197": {
            "setup": {
                "experiment_id": "0197",
                "date_time": "April 11, 2025 at 03:15:52 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.347140455000044,
                        "average_latency_ms_per_batch": 2918.3925568750055,
                        "throughput_queries_per_sec": 5.482470122913378,
                        "throughput_tokens_per_sec": 701.7561757329124
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            9.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 2031509504
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0197",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 758.3198753001615,
                            "process_3": 533.7966354778259,
                            "process_2": 1208.6116246460292,
                            "process_1": 590.2115714017713
                        },
                        "ram_power": {
                            "process_0": 0.7081046104431152,
                            "process_3": 0.6495823860168458,
                            "process_2": 0.6491274833679199,
                            "process_1": 0.6510071754455566
                        },
                        "cpu_energy": {
                            "process_0": 0.0006886708861875378,
                            "process_3": 0.0007456077217500478,
                            "process_2": 0.0007375065516874884,
                            "process_1": 0.000703484795718623
                        },
                        "gpu_energy": {
                            "process_0": 0.004671750404064312,
                            "process_3": 0.005094094353050194,
                            "process_2": 0.004656309280600179,
                            "process_1": 0.004741395737558163
                        },
                        "ram_energy": {
                            "process_0": 4.0702746482952225e-06,
                            "process_3": 4.332546937919091e-06,
                            "process_2": 3.984752355206689e-06,
                            "process_1": 4.109221318051914e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005364491564900145,
                            "process_3": 0.005844034621738163,
                            "process_2": 0.0053978005846428735,
                            "process_1": 0.005448989754594837
                        },
                        "total_energy_joules": {
                            "process_0": 19312.169633640522,
                            "process_3": 21038.524638257386,
                            "process_2": 19432.082104714344,
                            "process_1": 19616.36311654141
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 772.734926706447,
                        "ram_power_avg": 0.6644554138183594,
                        "cpu_energy_total": 0.002875269955343697,
                        "gpu_energy_total": 0.019163549775272848,
                        "ram_energy_total": 1.6496795259472918e-05,
                        "total_energy_kwh": 0.022055316525876018,
                        "total_energy_joules": 79399.13949315366
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20634984339361437,
                        "joules_per_token": 4.84613888508018,
                        "flops_per_joule": 213478018.79658332,
                        "joules_per_flop": 4.684323030713853e-09
                    },
                    "per-process_emissions": [
                        0.00204360306164871,
                        0.002226284989151153,
                        0.0020562921327197028,
                        0.0020757926470129032
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0198": {
            "setup": {
                "experiment_id": "0198",
                "date_time": "April 11, 2025 at 03:16:56 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.360484459000872,
                        "average_latency_ms_per_batch": 2920.060557375109,
                        "throughput_queries_per_sec": 5.479338419742454,
                        "throughput_tokens_per_sec": 701.3553177270342
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 2011983872
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0198",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 51.06922713191012,
                            "process_0": 91.65493748469669,
                            "process_1": 72.91180191469147,
                            "process_3": 358.23167610382853
                        },
                        "ram_power": {
                            "process_2": 0.6497840881347657,
                            "process_0": 0.7007904052734375,
                            "process_1": 0.6588363647460938,
                            "process_3": 0.6493306159973146
                        },
                        "cpu_energy": {
                            "process_2": 0.0007307912490315401,
                            "process_0": 0.0007268784654692128,
                            "process_1": 0.0007243685854688238,
                            "process_3": 0.0008252124333753275
                        },
                        "gpu_energy": {
                            "process_2": 0.004765643534733965,
                            "process_0": 0.00476653381322395,
                            "process_1": 0.004768159370079961,
                            "process_3": 0.005084936567945952
                        },
                        "ram_energy": {
                            "process_2": 3.944376337444518e-06,
                            "process_0": 4.24871286186756e-06,
                            "process_1": 3.945643138874959e-06,
                            "process_3": 4.410760315639823e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005500379160102949,
                            "process_0": 0.0054976609915550305,
                            "process_1": 0.005496473598687659,
                            "process_3": 0.005914559761636918
                        },
                        "total_energy_joules": {
                            "process_2": 19801.364976370616,
                            "process_0": 19791.57956959811,
                            "process_1": 19787.304955275573,
                            "process_3": 21292.415141892907
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 143.4669106587817,
                        "ram_power_avg": 0.6646853685379028,
                        "cpu_energy_total": 0.0030072507333449043,
                        "gpu_energy_total": 0.01938527328598383,
                        "ram_energy_total": 1.654949265382686e-05,
                        "total_energy_kwh": 0.022409073511982555,
                        "total_energy_joules": 80672.6646431372
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2030923370695154,
                        "joules_per_token": 4.92386869159773,
                        "flops_per_joule": 210107984.7570639,
                        "joules_per_flop": 4.759457386430335e-09
                    },
                    "per-process_emissions": [
                        0.0020953694410412184,
                        0.002094333954732889,
                        0.002093881617420064,
                        0.002253151541195584
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0198": {
            "setup": {
                "experiment_id": "0198",
                "date_time": "April 11, 2025 at 03:16:56 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.360484459000872,
                        "average_latency_ms_per_batch": 2920.060557375109,
                        "throughput_queries_per_sec": 5.479338419742454,
                        "throughput_tokens_per_sec": 701.3553177270342
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 2011983872
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0198",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 51.06922713191012,
                            "process_0": 91.65493748469669,
                            "process_1": 72.91180191469147,
                            "process_3": 358.23167610382853
                        },
                        "ram_power": {
                            "process_2": 0.6497840881347657,
                            "process_0": 0.7007904052734375,
                            "process_1": 0.6588363647460938,
                            "process_3": 0.6493306159973146
                        },
                        "cpu_energy": {
                            "process_2": 0.0007307912490315401,
                            "process_0": 0.0007268784654692128,
                            "process_1": 0.0007243685854688238,
                            "process_3": 0.0008252124333753275
                        },
                        "gpu_energy": {
                            "process_2": 0.004765643534733965,
                            "process_0": 0.00476653381322395,
                            "process_1": 0.004768159370079961,
                            "process_3": 0.005084936567945952
                        },
                        "ram_energy": {
                            "process_2": 3.944376337444518e-06,
                            "process_0": 4.24871286186756e-06,
                            "process_1": 3.945643138874959e-06,
                            "process_3": 4.410760315639823e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005500379160102949,
                            "process_0": 0.0054976609915550305,
                            "process_1": 0.005496473598687659,
                            "process_3": 0.005914559761636918
                        },
                        "total_energy_joules": {
                            "process_2": 19801.364976370616,
                            "process_0": 19791.57956959811,
                            "process_1": 19787.304955275573,
                            "process_3": 21292.415141892907
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 143.4669106587817,
                        "ram_power_avg": 0.6646853685379028,
                        "cpu_energy_total": 0.0030072507333449043,
                        "gpu_energy_total": 0.01938527328598383,
                        "ram_energy_total": 1.654949265382686e-05,
                        "total_energy_kwh": 0.022409073511982555,
                        "total_energy_joules": 80672.6646431372
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2030923370695154,
                        "joules_per_token": 4.92386869159773,
                        "flops_per_joule": 210107984.7570639,
                        "joules_per_flop": 4.759457386430335e-09
                    },
                    "per-process_emissions": [
                        0.0020953694410412184,
                        0.002094333954732889,
                        0.002093881617420064,
                        0.002253151541195584
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0199": {
            "setup": {
                "experiment_id": "0199",
                "date_time": "April 11, 2025 at 03:18:00 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.647898751009052,
                        "average_latency_ms_per_batch": 2955.9873438761315,
                        "throughput_queries_per_sec": 5.412743066423111,
                        "throughput_tokens_per_sec": 692.8311125021582
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 2016174080
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0199",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1156.9694162967278,
                            "process_1": 2278.4883648484474,
                            "process_0": 963.5893203021967,
                            "process_3": 538.2578390592178
                        },
                        "ram_power": {
                            "process_2": 0.6530213356018068,
                            "process_1": 0.6538181304931641,
                            "process_0": 0.7024884223937989,
                            "process_3": 0.6622467041015625
                        },
                        "cpu_energy": {
                            "process_2": 0.0006360206005936107,
                            "process_1": 0.0006622620218441851,
                            "process_0": 0.0006315458203123398,
                            "process_3": 0.0007506199693746113
                        },
                        "gpu_energy": {
                            "process_2": 0.004752374357451905,
                            "process_1": 0.004747160742169909,
                            "process_0": 0.004759010196093927,
                            "process_3": 0.0050210995724317975
                        },
                        "ram_energy": {
                            "process_2": 3.7454053714375474e-06,
                            "process_1": 3.916678215929463e-06,
                            "process_0": 4.02693019992514e-06,
                            "process_3": 4.450812516499455e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005392140363416953,
                            "process_1": 0.005413339442230023,
                            "process_0": 0.005394582946606191,
                            "process_3": 0.005776170354322908
                        },
                        "total_energy_joules": {
                            "process_2": 19411.70530830103,
                            "process_1": 19488.021992028083,
                            "process_0": 19420.498607782287,
                            "process_3": 20794.21327556247
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1234.3262351266476,
                        "ram_power_avg": 0.667893648147583,
                        "cpu_energy_total": 0.002680448412124747,
                        "gpu_energy_total": 0.01927964486814754,
                        "ram_energy_total": 1.6139826303791608e-05,
                        "total_energy_kwh": 0.021976233106576076,
                        "total_energy_joules": 79114.43918367388
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20709241156298325,
                        "joules_per_token": 4.828762157206658,
                        "flops_per_joule": 214246238.33584362,
                        "joules_per_flop": 4.667526523534294e-09
                    },
                    "per-process_emissions": [
                        0.0020541358714436884,
                        0.0020622116605175274,
                        0.0020550663735096285,
                        0.002200432096479312
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0199": {
            "setup": {
                "experiment_id": "0199",
                "date_time": "April 11, 2025 at 03:18:00 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.647898751009052,
                        "average_latency_ms_per_batch": 2955.9873438761315,
                        "throughput_queries_per_sec": 5.412743066423111,
                        "throughput_tokens_per_sec": 692.8311125021582
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 2016174080
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0199",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1156.9694162967278,
                            "process_1": 2278.4883648484474,
                            "process_0": 963.5893203021967,
                            "process_3": 538.2578390592178
                        },
                        "ram_power": {
                            "process_2": 0.6530213356018068,
                            "process_1": 0.6538181304931641,
                            "process_0": 0.7024884223937989,
                            "process_3": 0.6622467041015625
                        },
                        "cpu_energy": {
                            "process_2": 0.0006360206005936107,
                            "process_1": 0.0006622620218441851,
                            "process_0": 0.0006315458203123398,
                            "process_3": 0.0007506199693746113
                        },
                        "gpu_energy": {
                            "process_2": 0.004752374357451905,
                            "process_1": 0.004747160742169909,
                            "process_0": 0.004759010196093927,
                            "process_3": 0.0050210995724317975
                        },
                        "ram_energy": {
                            "process_2": 3.7454053714375474e-06,
                            "process_1": 3.916678215929463e-06,
                            "process_0": 4.02693019992514e-06,
                            "process_3": 4.450812516499455e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005392140363416953,
                            "process_1": 0.005413339442230023,
                            "process_0": 0.005394582946606191,
                            "process_3": 0.005776170354322908
                        },
                        "total_energy_joules": {
                            "process_2": 19411.70530830103,
                            "process_1": 19488.021992028083,
                            "process_0": 19420.498607782287,
                            "process_3": 20794.21327556247
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1234.3262351266476,
                        "ram_power_avg": 0.667893648147583,
                        "cpu_energy_total": 0.002680448412124747,
                        "gpu_energy_total": 0.01927964486814754,
                        "ram_energy_total": 1.6139826303791608e-05,
                        "total_energy_kwh": 0.021976233106576076,
                        "total_energy_joules": 79114.43918367388
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20709241156298325,
                        "joules_per_token": 4.828762157206658,
                        "flops_per_joule": 214246238.33584362,
                        "joules_per_flop": 4.667526523534294e-09
                    },
                    "per-process_emissions": [
                        0.0020541358714436884,
                        0.0020622116605175274,
                        0.0020550663735096285,
                        0.002200432096479312
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0204": {
            "setup": {
                "experiment_id": "0204",
                "date_time": "April 11, 2025 at 03:21:23 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.879527494005742,
                        "average_latency_ms_per_batch": 2984.940936750718,
                        "throughput_queries_per_sec": 5.360240064721995,
                        "throughput_tokens_per_sec": 686.1107282844154
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.1,
                        "cpu_memory_usage_bytes": 2015072256
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0204",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1501.975818580537,
                            "process_3": 455.15143652374445,
                            "process_1": 1793.700060218678,
                            "process_0": 761.1241525942042
                        },
                        "ram_power": {
                            "process_2": 0.6503691673278809,
                            "process_3": 0.6573042869567871,
                            "process_1": 0.6486339569091797,
                            "process_0": 0.7022194862365723
                        },
                        "cpu_energy": {
                            "process_2": 0.0006807586578435121,
                            "process_3": 0.0007790753750933846,
                            "process_1": 0.0006723067835315533,
                            "process_0": 0.0006822459270313174
                        },
                        "gpu_energy": {
                            "process_2": 0.004683878747100079,
                            "process_3": 0.005015803734862101,
                            "process_1": 0.004679386799062102,
                            "process_0": 0.0047219904442558636
                        },
                        "ram_energy": {
                            "process_2": 3.6554124299742487e-06,
                            "process_3": 4.663252722050512e-06,
                            "process_1": 3.6457616377054137e-06,
                            "process_0": 4.310251999461175e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005368292817373564,
                            "process_3": 0.005799542362677536,
                            "process_1": 0.00535533934423136,
                            "process_0": 0.005408546623286642
                        },
                        "total_energy_joules": {
                            "process_2": 19325.85414254483,
                            "process_3": 20878.35250563913,
                            "process_1": 19279.221639232896,
                            "process_0": 19470.767843831913
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1127.9878669792909,
                        "ram_power_avg": 0.664631724357605,
                        "cpu_energy_total": 0.0028143867434997677,
                        "gpu_energy_total": 0.019101059725280145,
                        "ram_energy_total": 1.627467878919135e-05,
                        "total_energy_kwh": 0.0219317211475691,
                        "total_energy_joules": 78954.19613124877
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20751272006828125,
                        "joules_per_token": 4.818981697463914,
                        "flops_per_joule": 214681066.03194812,
                        "joules_per_flop": 4.658072639955977e-09
                    },
                    "per-process_emissions": [
                        0.002045051148778459,
                        0.0022093356630620074,
                        0.0020401165231849366,
                        0.0020603858361410462
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0204": {
            "setup": {
                "experiment_id": "0204",
                "date_time": "April 11, 2025 at 03:21:23 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.879527494005742,
                        "average_latency_ms_per_batch": 2984.940936750718,
                        "throughput_queries_per_sec": 5.360240064721995,
                        "throughput_tokens_per_sec": 686.1107282844154
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.1,
                        "cpu_memory_usage_bytes": 2015072256
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0204",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1501.975818580537,
                            "process_3": 455.15143652374445,
                            "process_1": 1793.700060218678,
                            "process_0": 761.1241525942042
                        },
                        "ram_power": {
                            "process_2": 0.6503691673278809,
                            "process_3": 0.6573042869567871,
                            "process_1": 0.6486339569091797,
                            "process_0": 0.7022194862365723
                        },
                        "cpu_energy": {
                            "process_2": 0.0006807586578435121,
                            "process_3": 0.0007790753750933846,
                            "process_1": 0.0006723067835315533,
                            "process_0": 0.0006822459270313174
                        },
                        "gpu_energy": {
                            "process_2": 0.004683878747100079,
                            "process_3": 0.005015803734862101,
                            "process_1": 0.004679386799062102,
                            "process_0": 0.0047219904442558636
                        },
                        "ram_energy": {
                            "process_2": 3.6554124299742487e-06,
                            "process_3": 4.663252722050512e-06,
                            "process_1": 3.6457616377054137e-06,
                            "process_0": 4.310251999461175e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005368292817373564,
                            "process_3": 0.005799542362677536,
                            "process_1": 0.00535533934423136,
                            "process_0": 0.005408546623286642
                        },
                        "total_energy_joules": {
                            "process_2": 19325.85414254483,
                            "process_3": 20878.35250563913,
                            "process_1": 19279.221639232896,
                            "process_0": 19470.767843831913
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1127.9878669792909,
                        "ram_power_avg": 0.664631724357605,
                        "cpu_energy_total": 0.0028143867434997677,
                        "gpu_energy_total": 0.019101059725280145,
                        "ram_energy_total": 1.627467878919135e-05,
                        "total_energy_kwh": 0.0219317211475691,
                        "total_energy_joules": 78954.19613124877
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20751272006828125,
                        "joules_per_token": 4.818981697463914,
                        "flops_per_joule": 214681066.03194812,
                        "joules_per_flop": 4.658072639955977e-09
                    },
                    "per-process_emissions": [
                        0.002045051148778459,
                        0.0022093356630620074,
                        0.0020401165231849366,
                        0.0020603858361410462
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0205": {
            "setup": {
                "experiment_id": "0205",
                "date_time": "April 11, 2025 at 03:22:26 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_True_latency_0.05_latency_0.2_latency_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 24.329176611998264,
                        "average_latency_ms_per_batch": 3041.147076499783,
                        "throughput_queries_per_sec": 5.261172708034643,
                        "throughput_tokens_per_sec": 673.4301066284343
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 2015412224
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0205",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1624.941627963622,
                            "process_1": 1120.3090992290995,
                            "process_0": 1618.2697121135886,
                            "process_3": 673.3371960696992
                        },
                        "ram_power": {
                            "process_2": 0.6556663513183594,
                            "process_1": 0.6505951881408691,
                            "process_0": 0.7016000747680664,
                            "process_3": 0.6495552062988282
                        },
                        "cpu_energy": {
                            "process_2": 0.0006735851599685248,
                            "process_1": 0.0007386833343442731,
                            "process_0": 0.0007323949011562264,
                            "process_3": 0.0007901273392500345
                        },
                        "gpu_energy": {
                            "process_2": 0.004800128562322148,
                            "process_1": 0.004801725508044141,
                            "process_0": 0.004804007176536063,
                            "process_3": 0.005215804728196105
                        },
                        "ram_energy": {
                            "process_2": 3.949809054325826e-06,
                            "process_1": 3.94010931007149e-06,
                            "process_0": 4.23735814360497e-06,
                            "process_3": 4.574886690138548e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005477663531344997,
                            "process_1": 0.005544348951698487,
                            "process_0": 0.005540639435835897,
                            "process_3": 0.006010506954136279
                        },
                        "total_energy_joules": {
                            "process_2": 19719.58871284199,
                            "process_1": 19959.656226114552,
                            "process_0": 19946.30196900923,
                            "process_3": 21637.825034890604
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1259.2144088440025,
                        "ram_power_avg": 0.6643542051315308,
                        "cpu_energy_total": 0.002934790734719059,
                        "gpu_energy_total": 0.019621665975098457,
                        "ram_energy_total": 1.6702163198140837e-05,
                        "total_energy_kwh": 0.02257315887301566,
                        "total_energy_joules": 81263.37194285638
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20161604925182125,
                        "joules_per_token": 4.959922603934105,
                        "flops_per_joule": 208580699.86403045,
                        "joules_per_flop": 4.794307434253888e-09
                    },
                    "per-process_emissions": [
                        0.002086715922265877,
                        0.0021121197331495385,
                        0.002110706593081685,
                        0.0022897026241782154
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0205": {
            "setup": {
                "experiment_id": "0205",
                "date_time": "April 11, 2025 at 03:22:26 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_True_latency_0.05_latency_0.2_latency_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 24.329176611998264,
                        "average_latency_ms_per_batch": 3041.147076499783,
                        "throughput_queries_per_sec": 5.261172708034643,
                        "throughput_tokens_per_sec": 673.4301066284343
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 2015412224
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0205",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 1624.941627963622,
                            "process_1": 1120.3090992290995,
                            "process_0": 1618.2697121135886,
                            "process_3": 673.3371960696992
                        },
                        "ram_power": {
                            "process_2": 0.6556663513183594,
                            "process_1": 0.6505951881408691,
                            "process_0": 0.7016000747680664,
                            "process_3": 0.6495552062988282
                        },
                        "cpu_energy": {
                            "process_2": 0.0006735851599685248,
                            "process_1": 0.0007386833343442731,
                            "process_0": 0.0007323949011562264,
                            "process_3": 0.0007901273392500345
                        },
                        "gpu_energy": {
                            "process_2": 0.004800128562322148,
                            "process_1": 0.004801725508044141,
                            "process_0": 0.004804007176536063,
                            "process_3": 0.005215804728196105
                        },
                        "ram_energy": {
                            "process_2": 3.949809054325826e-06,
                            "process_1": 3.94010931007149e-06,
                            "process_0": 4.23735814360497e-06,
                            "process_3": 4.574886690138548e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005477663531344997,
                            "process_1": 0.005544348951698487,
                            "process_0": 0.005540639435835897,
                            "process_3": 0.006010506954136279
                        },
                        "total_energy_joules": {
                            "process_2": 19719.58871284199,
                            "process_1": 19959.656226114552,
                            "process_0": 19946.30196900923,
                            "process_3": 21637.825034890604
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1259.2144088440025,
                        "ram_power_avg": 0.6643542051315308,
                        "cpu_energy_total": 0.002934790734719059,
                        "gpu_energy_total": 0.019621665975098457,
                        "ram_energy_total": 1.6702163198140837e-05,
                        "total_energy_kwh": 0.02257315887301566,
                        "total_energy_joules": 81263.37194285638
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20161604925182125,
                        "joules_per_token": 4.959922603934105,
                        "flops_per_joule": 208580699.86403045,
                        "joules_per_flop": 4.794307434253888e-09
                    },
                    "per-process_emissions": [
                        0.002086715922265877,
                        0.0021121197331495385,
                        0.002110706593081685,
                        0.0022897026241782154
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0206": {
            "setup": {
                "experiment_id": "0206",
                "date_time": "April 11, 2025 at 03:23:31 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_True_latency_0.2_latency_0.6_latency_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.6,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 27.506127418993856,
                        "average_latency_ms_per_batch": 3438.265927374232,
                        "throughput_queries_per_sec": 4.653508581931891,
                        "throughput_tokens_per_sec": 595.649098487282
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 2012401664
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0206",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 50.77033614182556,
                            "process_1": 864.4809634627254,
                            "process_0": 592.3485640235789,
                            "process_2": 52.799130240647415
                        },
                        "ram_power": {
                            "process_3": 0.6593928337097168,
                            "process_1": 0.6519670486450195,
                            "process_0": 0.7015714645385743,
                            "process_2": 0.6463465690612793
                        },
                        "cpu_energy": {
                            "process_3": 0.0009316697127499083,
                            "process_1": 0.0008027906981559454,
                            "process_0": 0.0008284651039689378,
                            "process_2": 0.0008607361752189035
                        },
                        "gpu_energy": {
                            "process_3": 0.005316729808935883,
                            "process_1": 0.005074906837700066,
                            "process_0": 0.004947483957983945,
                            "process_2": 0.0048164202420220015
                        },
                        "ram_energy": {
                            "process_3": 5.113828267189979e-06,
                            "process_1": 4.298125103445194e-06,
                            "process_0": 4.778303717625594e-06,
                            "process_2": 4.3043563789347845e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.006253513349952979,
                            "process_1": 0.005881995660959458,
                            "process_0": 0.005780727365670507,
                            "process_2": 0.005681460773619839
                        },
                        "total_energy_joules": {
                            "process_3": 22512.648059830724,
                            "process_1": 21175.18437945405,
                            "process_0": 20810.618516413826,
                            "process_2": 20453.258785031423
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 390.0997484671943,
                        "ram_power_avg": 0.6648194789886475,
                        "cpu_energy_total": 0.003423661690093695,
                        "gpu_energy_total": 0.020155540846641895,
                        "ram_energy_total": 1.849461346719555e-05,
                        "total_energy_kwh": 0.02359769715020278,
                        "total_energy_joules": 84951.70974073003
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.19286251036033833,
                        "joules_per_token": 5.1850408777301045,
                        "flops_per_joule": 199524777.6046272,
                        "joules_per_flop": 5.011908856661264e-09
                    },
                    "per-process_emissions": [
                        0.0023822759106645877,
                        0.002240746247042506,
                        0.00220216808995218,
                        0.002164352481710478
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0206": {
            "setup": {
                "experiment_id": "0206",
                "date_time": "April 11, 2025 at 03:23:31 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_True_latency_0.2_latency_0.6_latency_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.6,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 27.506127418993856,
                        "average_latency_ms_per_batch": 3438.265927374232,
                        "throughput_queries_per_sec": 4.653508581931891,
                        "throughput_tokens_per_sec": 595.649098487282
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 2012401664
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0206",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 50.77033614182556,
                            "process_1": 864.4809634627254,
                            "process_0": 592.3485640235789,
                            "process_2": 52.799130240647415
                        },
                        "ram_power": {
                            "process_3": 0.6593928337097168,
                            "process_1": 0.6519670486450195,
                            "process_0": 0.7015714645385743,
                            "process_2": 0.6463465690612793
                        },
                        "cpu_energy": {
                            "process_3": 0.0009316697127499083,
                            "process_1": 0.0008027906981559454,
                            "process_0": 0.0008284651039689378,
                            "process_2": 0.0008607361752189035
                        },
                        "gpu_energy": {
                            "process_3": 0.005316729808935883,
                            "process_1": 0.005074906837700066,
                            "process_0": 0.004947483957983945,
                            "process_2": 0.0048164202420220015
                        },
                        "ram_energy": {
                            "process_3": 5.113828267189979e-06,
                            "process_1": 4.298125103445194e-06,
                            "process_0": 4.778303717625594e-06,
                            "process_2": 4.3043563789347845e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.006253513349952979,
                            "process_1": 0.005881995660959458,
                            "process_0": 0.005780727365670507,
                            "process_2": 0.005681460773619839
                        },
                        "total_energy_joules": {
                            "process_3": 22512.648059830724,
                            "process_1": 21175.18437945405,
                            "process_0": 20810.618516413826,
                            "process_2": 20453.258785031423
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 390.0997484671943,
                        "ram_power_avg": 0.6648194789886475,
                        "cpu_energy_total": 0.003423661690093695,
                        "gpu_energy_total": 0.020155540846641895,
                        "ram_energy_total": 1.849461346719555e-05,
                        "total_energy_kwh": 0.02359769715020278,
                        "total_energy_joules": 84951.70974073003
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.19286251036033833,
                        "joules_per_token": 5.1850408777301045,
                        "flops_per_joule": 199524777.6046272,
                        "joules_per_flop": 5.011908856661264e-09
                    },
                    "per-process_emissions": [
                        0.0023822759106645877,
                        0.002240746247042506,
                        0.00220216808995218,
                        0.002164352481710478
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0207": {
            "setup": {
                "experiment_id": "0207",
                "date_time": "April 11, 2025 at 03:24:40 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_True_latency_0.05_latency_0.2_latency_True_latency_4.0_latency_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 28.59574876500119,
                        "average_latency_ms_per_batch": 3574.4685956251487,
                        "throughput_queries_per_sec": 4.476189836884472,
                        "throughput_tokens_per_sec": 572.9522991212124
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 2018140160
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0207",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 489.0106633974402,
                            "process_0": 24.472879062013373,
                            "process_2": 635.8737957456489,
                            "process_1": 987.1035125278377
                        },
                        "ram_power": {
                            "process_3": 0.6534547805786133,
                            "process_0": 0.7033309936523438,
                            "process_2": 0.6524834632873535,
                            "process_1": 0.6544790267944336
                        },
                        "cpu_energy": {
                            "process_3": 0.0008793125634997524,
                            "process_0": 0.0008705332805623129,
                            "process_2": 0.0007920234862501728,
                            "process_1": 0.0007760334571562455
                        },
                        "gpu_energy": {
                            "process_3": 0.005475737991697871,
                            "process_0": 0.005022460406853946,
                            "process_2": 0.005078191284771938,
                            "process_1": 0.005047156259944008
                        },
                        "ram_energy": {
                            "process_3": 5.088022395628447e-06,
                            "process_0": 4.8000562178497755e-06,
                            "process_2": 4.557331072005112e-06,
                            "process_1": 4.21766530045908e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.006360138577593251,
                            "process_0": 0.00589779374363411,
                            "process_2": 0.005874772102094116,
                            "process_1": 0.005827407382400713
                        },
                        "total_energy_joules": {
                            "process_3": 22896.498879335704,
                            "process_0": 21232.057477082795,
                            "process_2": 21149.179567538817,
                            "process_1": 20978.66657664257
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 534.1152126832351,
                        "ram_power_avg": 0.665937066078186,
                        "cpu_energy_total": 0.0033179027874684834,
                        "gpu_energy_total": 0.020623545943267763,
                        "ram_energy_total": 1.8663074985942413e-05,
                        "total_energy_kwh": 0.02396011180572219,
                        "total_energy_joules": 86256.40250059988
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1899453202895409,
                        "joules_per_token": 5.264673004187005,
                        "flops_per_joule": 196506815.74662378,
                        "joules_per_flop": 5.0888820125679594e-09
                    },
                    "per-process_emissions": [
                        0.002422894791134149,
                        0.0022467645266374143,
                        0.0022379944322927533,
                        0.002219950842325552
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0207": {
            "setup": {
                "experiment_id": "0207",
                "date_time": "April 11, 2025 at 03:24:40 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_True_latency_0.05_latency_0.2_latency_True_latency_4.0_latency_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 28.59574876500119,
                        "average_latency_ms_per_batch": 3574.4685956251487,
                        "throughput_queries_per_sec": 4.476189836884472,
                        "throughput_tokens_per_sec": 572.9522991212124
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 2018140160
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0207",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 489.0106633974402,
                            "process_0": 24.472879062013373,
                            "process_2": 635.8737957456489,
                            "process_1": 987.1035125278377
                        },
                        "ram_power": {
                            "process_3": 0.6534547805786133,
                            "process_0": 0.7033309936523438,
                            "process_2": 0.6524834632873535,
                            "process_1": 0.6544790267944336
                        },
                        "cpu_energy": {
                            "process_3": 0.0008793125634997524,
                            "process_0": 0.0008705332805623129,
                            "process_2": 0.0007920234862501728,
                            "process_1": 0.0007760334571562455
                        },
                        "gpu_energy": {
                            "process_3": 0.005475737991697871,
                            "process_0": 0.005022460406853946,
                            "process_2": 0.005078191284771938,
                            "process_1": 0.005047156259944008
                        },
                        "ram_energy": {
                            "process_3": 5.088022395628447e-06,
                            "process_0": 4.8000562178497755e-06,
                            "process_2": 4.557331072005112e-06,
                            "process_1": 4.21766530045908e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.006360138577593251,
                            "process_0": 0.00589779374363411,
                            "process_2": 0.005874772102094116,
                            "process_1": 0.005827407382400713
                        },
                        "total_energy_joules": {
                            "process_3": 22896.498879335704,
                            "process_0": 21232.057477082795,
                            "process_2": 21149.179567538817,
                            "process_1": 20978.66657664257
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 534.1152126832351,
                        "ram_power_avg": 0.665937066078186,
                        "cpu_energy_total": 0.0033179027874684834,
                        "gpu_energy_total": 0.020623545943267763,
                        "ram_energy_total": 1.8663074985942413e-05,
                        "total_energy_kwh": 0.02396011180572219,
                        "total_energy_joules": 86256.40250059988
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1899453202895409,
                        "joules_per_token": 5.264673004187005,
                        "flops_per_joule": 196506815.74662378,
                        "joules_per_flop": 5.0888820125679594e-09
                    },
                    "per-process_emissions": [
                        0.002422894791134149,
                        0.0022467645266374143,
                        0.0022379944322927533,
                        0.002219950842325552
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0208": {
            "setup": {
                "experiment_id": "0208",
                "date_time": "April 11, 2025 at 03:25:52 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_True_latency_0.2_latency_0.6_latency_True_latency_5.0_latency_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.6,
                    "simulate_burst": true,
                    "burst_interval": 5.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 32.41438798600211,
                        "average_latency_ms_per_batch": 4051.798498250264,
                        "throughput_queries_per_sec": 3.9488636976664733,
                        "throughput_tokens_per_sec": 505.4545533013086
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 2025857024
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0208",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 696.2927497629445,
                            "process_2": 836.3950447780419,
                            "process_3": 532.7672312676534,
                            "process_1": 674.7365786514791
                        },
                        "ram_power": {
                            "process_0": 0.7054367065429689,
                            "process_2": 0.6576075553894043,
                            "process_3": 0.6579680442810059,
                            "process_1": 0.6581926345825195
                        },
                        "cpu_energy": {
                            "process_0": 0.0009936705203753037,
                            "process_2": 0.0009263843829686493,
                            "process_3": 0.0010035968579369412,
                            "process_1": 0.0009732284624064962
                        },
                        "gpu_energy": {
                            "process_0": 0.0053397553829118705,
                            "process_2": 0.0051982055474499855,
                            "process_3": 0.005680288710894044,
                            "process_1": 0.0053413262175018295
                        },
                        "ram_energy": {
                            "process_0": 5.435523468298103e-06,
                            "process_2": 4.691490847517647e-06,
                            "process_3": 5.418404265587928e-06,
                            "process_1": 4.9411973142484445e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.006338861426755472,
                            "process_2": 0.006129281421266153,
                            "process_3": 0.0066893039730965735,
                            "process_1": 0.0063194958772225755
                        },
                        "total_energy_joules": {
                            "process_0": 22819.901136319702,
                            "process_2": 22065.413116558153,
                            "process_3": 24081.494303147665,
                            "process_1": 22750.18515800127
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 685.0479011150298,
                        "ram_power_avg": 0.6698012351989746,
                        "cpu_energy_total": 0.003896880223687391,
                        "gpu_energy_total": 0.02155957585875773,
                        "ram_energy_total": 2.0486615895652122e-05,
                        "total_energy_kwh": 0.025476942698340777,
                        "total_energy_joules": 91716.9937140268
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.17863647004267544,
                        "joules_per_token": 5.597961042115894,
                        "flops_per_joule": 184807311.1292978,
                        "joules_per_flop": 5.411041337538667e-09
                    },
                    "per-process_emissions": [
                        0.0024147892605224973,
                        0.0023349497574313414,
                        0.0025482903485511397,
                        0.0024074119544279403
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0208": {
            "setup": {
                "experiment_id": "0208",
                "date_time": "April 11, 2025 at 03:25:52 PM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_True_latency_0.2_latency_0.6_latency_True_latency_5.0_latency_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.6,
                    "simulate_burst": true,
                    "burst_interval": 5.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 32.41438798600211,
                        "average_latency_ms_per_batch": 4051.798498250264,
                        "throughput_queries_per_sec": 3.9488636976664733,
                        "throughput_tokens_per_sec": 505.4545533013086
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.3,
                        "cpu_memory_usage_bytes": 2025857024
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0208",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 696.2927497629445,
                            "process_2": 836.3950447780419,
                            "process_3": 532.7672312676534,
                            "process_1": 674.7365786514791
                        },
                        "ram_power": {
                            "process_0": 0.7054367065429689,
                            "process_2": 0.6576075553894043,
                            "process_3": 0.6579680442810059,
                            "process_1": 0.6581926345825195
                        },
                        "cpu_energy": {
                            "process_0": 0.0009936705203753037,
                            "process_2": 0.0009263843829686493,
                            "process_3": 0.0010035968579369412,
                            "process_1": 0.0009732284624064962
                        },
                        "gpu_energy": {
                            "process_0": 0.0053397553829118705,
                            "process_2": 0.0051982055474499855,
                            "process_3": 0.005680288710894044,
                            "process_1": 0.0053413262175018295
                        },
                        "ram_energy": {
                            "process_0": 5.435523468298103e-06,
                            "process_2": 4.691490847517647e-06,
                            "process_3": 5.418404265587928e-06,
                            "process_1": 4.9411973142484445e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.006338861426755472,
                            "process_2": 0.006129281421266153,
                            "process_3": 0.0066893039730965735,
                            "process_1": 0.0063194958772225755
                        },
                        "total_energy_joules": {
                            "process_0": 22819.901136319702,
                            "process_2": 22065.413116558153,
                            "process_3": 24081.494303147665,
                            "process_1": 22750.18515800127
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 685.0479011150298,
                        "ram_power_avg": 0.6698012351989746,
                        "cpu_energy_total": 0.003896880223687391,
                        "gpu_energy_total": 0.02155957585875773,
                        "ram_energy_total": 2.0486615895652122e-05,
                        "total_energy_kwh": 0.025476942698340777,
                        "total_energy_joules": 91716.9937140268
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.17863647004267544,
                        "joules_per_token": 5.597961042115894,
                        "flops_per_joule": 184807311.1292978,
                        "joules_per_flop": 5.411041337538667e-09
                    },
                    "per-process_emissions": [
                        0.0024147892605224973,
                        0.0023349497574313414,
                        0.0025482903485511397,
                        0.0024074119544279403
                    ]
                },
                "local_energy_results": {}
            }
        }
    }
]