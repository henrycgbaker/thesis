import torch
import torchinfo
import ptflops
import pandas as pd


def get_inference_metrics(inference_results):
    inference_results = {
        "number_runs": inference_results["num_runs"],
        "total_token_inputted": inference_results["total_input_tokens"],
        "total_tokens_outputted": inference_results["total_generated_tokens"],
    }
    
    inference_performance = {
        "total_inference_time_sec": inference_results["total_time"],
        "average_latency_ms_per_batch": inference_results["avg_latency_ms"],
        "throughput_queries_per_sec": inference_results["throughput_qps"],
        "throughput_tokens_per_sec": inference_results["tokens_per_sec"],
    }
    return inference_results, inference_performance

def get_energy_metrics(codecarbon_data, inference_results): # MOVE TO ENERGY .PY
    energy_kwh = codecarbon_data.energy_consumed
    energy_joules = energy_kwh * 3.6e6
    tokens_per_joule = (inference_results["total_generated_tokens"] / energy_joules) if energy_joules > 0 else 0
    
    power_energy_results = {
        "cpu_power": codecarbon_data.cpu_power,
        "gpu_power": codecarbon_data.gpu_power,
        "ram_power": codecarbon_data.ram_power,
        "cpu_energy": codecarbon_data.cpu_energy,
        "gpu_energy": codecarbon_data.gpu_energy,
        "ram_energy": codecarbon_data.ram_energy,
        "total_energy_kwh": energy_kwh,
        "total_energy_joules": energy_joules,
        "energy_efficiency_tokens_per_joule": tokens_per_joule,
        "final_emissions": codecarbon_data.emissions
    }
    return power_energy_results


# NEED TO COMBINE THESE TWO BELOW NEITHER WORK!!!
def get_compute_performance_metrics_new(model, tokenizer, device):
    # Run a dummy forward pass with torchinfo to get summary statistics.
    summary = torchinfo.summary(model, input_size=(1, 3, 224, 224), verbose=0)
    params_tinfo = summary.total_params
    macs_tinfo = summary.total_mult_adds

    macs, params = ptflops.get_model_complexity_info(model, (3, 224, 224), as_strings=False, print_per_layer_stat=False, verbose=False)
    
    return {
        "params_tinfo": params_tinfo,
        "macs_tinfo": macs_tinfo,
        "params_ptflops": params,
        "macs_ptflops": macs
    }

#OLD & DEGRADED
def get_compute_performance_metrics_old(model=None, tokenizer=None, device=None, input_length=128):
    compute_metrics = {}
    
    if torch.cuda.is_available() and device is not None:
        torch.cuda.reset_peak_memory_stats(device)
        compute_metrics["current_memory_allocated_bytes"] = torch.cuda.memory_allocated(device)
        compute_metrics["max_memory_allocated_bytes"] = torch.cuda.max_memory_allocated(device)
        compute_metrics["current_memory_reserved_bytes"] = torch.cuda.memory_reserved(device)
        compute_metrics["max_memory_reserved_bytes"] = torch.cuda.max_memory_reserved(device)
        try:
            result = subprocess.check_output(
                ["nvidia-smi", "--query-gpu=utilization.gpu", "--format=csv,noheader,nounits"]
            )
            lines = result.decode("utf-8").strip().splitlines()
            gpu_utils = [float(line.strip()) for line in lines if line.strip()]
            compute_metrics["gpu_utilization_percent"] = gpu_utils
        except Exception as e:
            compute_metrics["gpu_utilization_percent"] = f"Error: {str(e)}"
    
    try:
        process = psutil.Process(os.getpid())
        cpu_usage = process.cpu_percent(interval=1.0)
        compute_metrics["cpu_usage_percent"] = cpu_usage
        cpu_mem = process.memory_info().rss
        compute_metrics["cpu_memory_usage_bytes"] = cpu_mem
    
    if model is not None and tokenizer is not None and device is not None:
        try:
            model_to_trace = model.module if hasattr(model, "module") else model
            model_to_trace.eval()
            wrapped_model = ModelWrapper(model_to_trace)
            dummy_input_ids = torch.ones((1, input_length), dtype=torch.long).to(device)
            flops_analyzer = FlopCountAnalysis(wrapped_model, dummy_input_ids)
            compute_metrics["flops_forward_pass"] = flops_analyzer.total()
        except Exception as e:
            compute_metrics["flops_forward_pass"] = f"Error: {str(e)}"
    else:
        compute_metrics["flops_forward_pass"] = "model, tokenizer, or device not provided"
    
    return compute_metrics

