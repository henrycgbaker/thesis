import sys
import os
from datetime import datetime
import json
from transformers import AutoConfig

def get_cores_info(codecarbon_data):  
    """
    Pulls GPU / CPU / OS info from CodeCarbon's tracker info.
    """
    cores_info = {
        "gpu_model": codecarbon_data.gpu_model,
        "gpu_count": codecarbon_data.gpu_count,
        "cpu_model": codecarbon_data.cpu_model,
        "cpu_count": codecarbon_data.cpu_count,
        "os": codecarbon_data.os,  
    }  
    return cores_info

def get_region_info(codecarbon_data): 
    """
    Pulls country/region info from CodeCarbon's tracker info.
    """
    region_info = {
        "country_name": codecarbon_data.country_name,
        "region": codecarbon_data.region,
    }
    return region_info


def get_exp_setup(exp_config, 
                         model, 
                         cores_info, 
                         region_info):
    """
    Gets all experiment setup
    `exp_config` is expected to be an instance of ExperimentConfig.
    returns dict
    """     
    
    model_config = AutoConfig.from_pretrained(model)
    if hasattr(model_config, "is_encoder_decoder"):
        is_encoder_decoder = model_config.is_encoder_decoder
    else:
        is_encoder_decoder = exp_config.is_encoder_decoder
 
    experiment_setup_info = {
        "model_name": exp_config.model_name,
        "is_encoder_decoder": is_encoder_decoder,
        "task_type": exp_config.task_type,
        "gpu_count": cores_info.gpu_count,
        "gpu_model": cores_info.gpu_model,
        "cpu_count": cores_info.cpu_count,
        "cpu_model": cores_info.cpu_model,
        "os": cores_info.os,
        "python_version": sys.version,
        "country": region_info.country_name,
        "region": region_info.region,
        "date": datetime.now().strftime("%B %d, %Y at %I:%M:%S %p"),
    }
    return experiment_setup_info

def get_exp_vars(exp_config, 
                model,
                accelerator):
    
    used_gpu = str(accelerator.device)
    effective_fp_precision = str(next(model.parameters()).dtype)
    
    sharding_config = None
    try:
        from torch.distributed.fsdp import FullyShardedDataParallel
        if isinstance(model, FullyShardedDataParallel):
            sharding_config = {
                "reshard_after_forward": str(getattr(model, "reshard_after_forward", None)),
                "cpu_offload": str(getattr(model, "cpu_offload", None)),
                "backward_prefetch": str(getattr(model, "backward_prefetch", None)),
            }
    except ImportError:
        pass
    
    exp_vars = {
         "max_input_tokens": exp_config.max_input_tokens,
         "max_output_tokens": exp_config.max_output_tokens,
         "number_input_prompts": exp_config.num_runs_inputs,
         "used_gpu": used_gpu,
         "decoder_temperature": exp_config.decoder_temperature,
         "query_rate": exp_config.query_rate,
         "fp_precision": effective_fp_precision,
         "quantisation": exp_config.quantisation,
         "batching_options": exp_config.batching_options,
         "sharding_config": sharding_config,
         "accelerate_config": {
              "distributed_type": str(accelerator.distributed_type),
              "num_processes": accelerator.num_processes,
              "local_process_index": accelerator.local_process_index
         },
         "inference_type": exp_config.inference_type,
         "backend": exp_config.backend,
    }
    return exp_vars