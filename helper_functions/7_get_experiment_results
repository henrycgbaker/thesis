import sys
from datetime import datetime
import json
import os

def combine_process_results(inference_results, 
                            inference_performance,
                            power_energy_results,
                            accelerator):
    
    process_id = getattr(accelerator, "pid", None)
    
    process_results = {
        "process_id": process_id,
        **inference_results,
        **inference_performance,
        **power_energy_results
    }
    
    return process_results


def make_json_serializable(obj):
    """Recursively convert non-JSON-serializable objects to strings."""
    if isinstance(obj, dict):
        return {k: make_json_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [make_json_serializable(item) for item in obj]
    else:
        try:
            json.dumps(obj)
            return obj
        except (TypeError, OverflowError):
            return str(obj)


def aggregate_process_results(process_results):
    aggregated = {}
    aggregated["experiment_setup"] = process_results[0]["experiment_setup"].copy()
    if "accelerate_config" in aggregated["experiment_setup"]:
        aggregated["experiment_setup"]["accelerate_config"].pop("local_process_index", None)
    aggregated["experiment_variables"] = process_results[0]["experiment_variables"]

    gpu_set = {str(proc["experiment_variables"].get("used_gpu", "N/A")) for proc in process_results}
    aggregated["experiment_variables"]["used_gpu"] = list(gpu_set)

    inf_keys = ["total_inference_time_sec", "average_latency_ms_per_batch", "throughput_queries_per_sec", "throughput_tokens_per_sec"]
    agg_inference = {key: sum(proc["experiment_results"]["inference_performance"][key] for proc in results) / len(results)
                     for key in inf_keys}

    energy_keys_avg = ["cpu_power", "gpu_power", "ram_power", "energy_efficiency_tokens_per_joule"]
    energy_keys_sum = ["cpu_energy", "gpu_energy", "ram_energy", "total_energy_consumed_kwh", "total_energy_consumed_joules", "final_emissions"]
    agg_energy = {}
    for key in energy_keys_avg:
        agg_energy[key] = sum(proc["experiment_results"]["energy_performance"][key] for proc in results) / len(results)
    for key in energy_keys_sum:
        agg_energy[key] = sum(proc["experiment_results"]["energy_performance"][key] for proc in results)

    compute_setup = process_results[0]["experiment_results"]["compute_performance"]
    agg_compute = {
        "gpu": compute_setup.get("gpu", "N/A"),
        "flops_forward_pass": compute_setup.get("flops_forward_pass", "N/A"),
    }
    numeric_compute_keys = [
        "cpu_usage_percent",
        "cpu_memory_usage_bytes",
        "gpu_utilization_percent",
        "current_memory_allocated_bytes",
        "max_memory_allocated_bytes",
        "current_memory_reserved_bytes",
        "max_memory_reserved_bytes"
    ]
    for key in numeric_compute_keys:
        values = []
        for proc in process_results:
            val = proc["experiment_results"]["compute_performance"].get(key)
            if isinstance(val, list):
                values.append(val)
            elif isinstance(val, (int, float)):
                values.append(val)
        if values:
            if isinstance(values[0], list):
                list_length = len(values[0])
                averaged = [sum(v[i] for v in values) / len(values) for i in range(list_length)]
                agg_compute[key] = averaged
            else:
                agg_compute[key] = sum(values) / len(values)
    
    for extra_key in ["cpu_vendor", "AMD_CONSTANT_POWER"]:
        if extra_key in compute_setup:
            agg_compute[extra_key] = compute_setup[extra_key]
    
    task_perf = process_results[0]["experiment_results"].get("task-specific_performance", {})

    aggregated["experiment_results"] = {
        "inference_performance": agg_inference,
        "energy_performance": agg_energy,
        "compute_performance": agg_compute,
        "task-specific_performance": task_perf
    }

    return make_json_serializable(aggregated)

def get_persistent_unique_id():
    ID_FILE = "experiment_id.txt"
    if os.path.exists(ID_FILE):
        with open(ID_FILE, "r") as f:
            last_id = int(f.read().strip())
    else:
        last_id = 0
    new_id = last_id + 1
    with open(ID_FILE, "w") as f:
        f.write(str(new_id))
    return f"{new_id:04d}"

def save_results(task_type, benchmark_results):
    output_dir = "benchmark_results"
    os.makedirs(output_dir, exist_ok=True)
    output_json_path = os.path.join(output_dir, f"{task_type}_results.json")
    
    if os.path.exists(output_json_path):
        with open(output_json_path, "r") as json_file:
            try:
                existing_data = json.load(json_file)
                if not isinstance(existing_data, list):
                    existing_data = [existing_data]
            except json.JSONDecodeError:
                existing_data = []
    else:
        existing_data = []
    
    existing_data.append(benchmark_results)
    with open(output_json_path, "w") as json_file:
        json.dump(existing_data, json_file, indent=4)
    
    return output_json_path
