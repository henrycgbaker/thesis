[
    {
        "CONFIGURATION_RUN_#0353": {
            "setup": {
                "experiment_id": "0353",
                "date_time": "April 12, 2025 at 09:17:18 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R4_High_Load_Cloud_API_Deployment",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 15368
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 179.98634032497648,
                        "average_latency_ms_per_batch": 11249.14627031103,
                        "throughput_queries_per_sec": 0.7111650793548447,
                        "throughput_tokens_per_sec": 85.38425734004105
                    }
                },
                "compute_metrics": {
                    "flops": 20248623316992,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4963382784,
                        "gpu_max_memory_allocated_bytes": 4963382784,
                        "gpu_current_memory_reserved_bytes": 6448742400,
                        "gpu_max_memory_reserved_bytes": 6448742400
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 81.5,
                        "cpu_memory_usage_bytes": 1929740288
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0353",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 37.7473035625276
                        },
                        "ram_power": {
                            "process_0": 0.6728911399841309
                        },
                        "cpu_energy": {
                            "process_0": 0.005497700122781226
                        },
                        "gpu_energy": {
                            "process_0": 0.0018915495687938932
                        },
                        "ram_energy": {
                            "process_0": 1.866476199298718e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.007407914453568107
                        },
                        "total_energy_joules": {
                            "process_0": 26668.492032845188
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 37.7473035625276,
                        "ram_power_avg": 0.6728911399841309,
                        "cpu_energy_total": 0.005497700122781226,
                        "gpu_energy_total": 0.0018915495687938932,
                        "ram_energy_total": 1.866476199298718e-05,
                        "total_energy_kwh": 0.007407914453568107,
                        "total_energy_joules": 26668.492032845188
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5762605542552842,
                        "joules_per_token": 1.7353261343600461,
                        "flops_per_joule": 759271401.3245889,
                        "joules_per_flop": 1.3170521084495577e-09
                    },
                    "per-process_emissions": [
                        0.0028220450110867704
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0353": {
            "setup": {
                "experiment_id": "0353",
                "date_time": "April 12, 2025 at 09:17:18 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R4_High_Load_Cloud_API_Deployment",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 15368
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 179.98634032497648,
                        "average_latency_ms_per_batch": 11249.14627031103,
                        "throughput_queries_per_sec": 0.7111650793548447,
                        "throughput_tokens_per_sec": 85.38425734004105
                    }
                },
                "compute_metrics": {
                    "flops": 20248623316992,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4963382784,
                        "gpu_max_memory_allocated_bytes": 4963382784,
                        "gpu_current_memory_reserved_bytes": 6448742400,
                        "gpu_max_memory_reserved_bytes": 6448742400
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            4.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 81.5,
                        "cpu_memory_usage_bytes": 1929740288
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0353",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 37.7473035625276
                        },
                        "ram_power": {
                            "process_0": 0.6728911399841309
                        },
                        "cpu_energy": {
                            "process_0": 0.005497700122781226
                        },
                        "gpu_energy": {
                            "process_0": 0.0018915495687938932
                        },
                        "ram_energy": {
                            "process_0": 1.866476199298718e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.007407914453568107
                        },
                        "total_energy_joules": {
                            "process_0": 26668.492032845188
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 37.7473035625276,
                        "ram_power_avg": 0.6728911399841309,
                        "cpu_energy_total": 0.005497700122781226,
                        "gpu_energy_total": 0.0018915495687938932,
                        "ram_energy_total": 1.866476199298718e-05,
                        "total_energy_kwh": 0.007407914453568107,
                        "total_energy_joules": 26668.492032845188
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5762605542552842,
                        "joules_per_token": 1.7353261343600461,
                        "flops_per_joule": 759271401.3245889,
                        "joules_per_flop": 1.3170521084495577e-09
                    },
                    "per-process_emissions": [
                        0.0028220450110867704
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0354": {
            "setup": {
                "experiment_id": "0354",
                "date_time": "April 12, 2025 at 09:19:31 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A3_Quantisation_Gaming",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 49.89436189098342,
                        "average_latency_ms_per_batch": 24947.18094549171,
                        "throughput_queries_per_sec": 2.565420122611716,
                        "throughput_tokens_per_sec": 328.37377569429964
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2850029568,
                        "gpu_max_memory_reserved_bytes": 2850029568
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 80.0,
                        "cpu_memory_usage_bytes": 1939718144
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0354",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 0.0
                        },
                        "ram_power": {
                            "process_0": 0.676966667175293
                        },
                        "cpu_energy": {
                            "process_0": 0.0015451625708137726
                        },
                        "gpu_energy": {
                            "process_0": 0.0006358974531621708
                        },
                        "ram_energy": {
                            "process_0": 4.6775874939662005e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.002185737611469909
                        },
                        "total_energy_joules": {
                            "process_0": 7868.655401291673
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 0.0,
                        "ram_power_avg": 0.676966667175293,
                        "cpu_energy_total": 0.0015451625708137726,
                        "gpu_energy_total": 0.0006358974531621708,
                        "ram_energy_total": 4.6775874939662005e-06,
                        "total_energy_kwh": 0.002185737611469909,
                        "total_energy_joules": 7868.655401291673
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 2.082185476988927,
                        "joules_per_token": 0.4802646118952438,
                        "flops_per_joule": 2154112758.6257734,
                        "joules_per_flop": 4.6422825174572324e-10
                    },
                    "per-process_emissions": [
                        0.0008326567430894619
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0354": {
            "setup": {
                "experiment_id": "0354",
                "date_time": "April 12, 2025 at 09:19:31 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A3_Quantisation_Gaming",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 49.89436189098342,
                        "average_latency_ms_per_batch": 24947.18094549171,
                        "throughput_queries_per_sec": 2.565420122611716,
                        "throughput_tokens_per_sec": 328.37377569429964
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2850029568,
                        "gpu_max_memory_reserved_bytes": 2850029568
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 80.0,
                        "cpu_memory_usage_bytes": 1939718144
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0354",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 0.0
                        },
                        "ram_power": {
                            "process_0": 0.676966667175293
                        },
                        "cpu_energy": {
                            "process_0": 0.0015451625708137726
                        },
                        "gpu_energy": {
                            "process_0": 0.0006358974531621708
                        },
                        "ram_energy": {
                            "process_0": 4.6775874939662005e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.002185737611469909
                        },
                        "total_energy_joules": {
                            "process_0": 7868.655401291673
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 0.0,
                        "ram_power_avg": 0.676966667175293,
                        "cpu_energy_total": 0.0015451625708137726,
                        "gpu_energy_total": 0.0006358974531621708,
                        "ram_energy_total": 4.6775874939662005e-06,
                        "total_energy_kwh": 0.002185737611469909,
                        "total_energy_joules": 7868.655401291673
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 2.082185476988927,
                        "joules_per_token": 0.4802646118952438,
                        "flops_per_joule": 2154112758.6257734,
                        "joules_per_flop": 4.6422825174572324e-10
                    },
                    "per-process_emissions": [
                        0.0008326567430894619
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0355": {
            "setup": {
                "experiment_id": "0355",
                "date_time": "April 12, 2025 at 09:22:09 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A1_Max_Throughput_Exploit",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 57.570764437987236,
                        "average_latency_ms_per_batch": 57570.764437987236,
                        "throughput_queries_per_sec": 2.2233507102007675,
                        "throughput_tokens_per_sec": 284.58889090569824
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 2039678464,
                        "gpu_max_memory_allocated_bytes": 2039678464,
                        "gpu_current_memory_reserved_bytes": 4636803072,
                        "gpu_max_memory_reserved_bytes": 4636803072
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 82.3,
                        "cpu_memory_usage_bytes": 1912037376
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0355",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 185.32420724737398,
                            "process_3": 201.41520931481045,
                            "process_2": 193.57273259108436,
                            "process_0": 226.08901614103695
                        },
                        "ram_power": {
                            "process_1": 0.7010550498962402,
                            "process_3": 0.7105865478515625,
                            "process_2": 0.6992440223693848,
                            "process_0": 0.667393684387207
                        },
                        "cpu_energy": {
                            "process_1": 0.0008001227740305692,
                            "process_3": 0.0012628235162196687,
                            "process_2": 0.000989520255156549,
                            "process_0": 0.001785023112592625
                        },
                        "gpu_energy": {
                            "process_1": 0.0013846033299047544,
                            "process_3": 0.0022090731561454913,
                            "process_2": 0.0017031416402897293,
                            "process_0": 0.003241839815692371
                        },
                        "ram_energy": {
                            "process_1": 2.6087913634645527e-06,
                            "process_3": 4.117101864297396e-06,
                            "process_2": 3.2999916503113576e-06,
                            "process_0": 4.599407548249793e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.002187334895298789,
                            "process_3": 0.003476013774229458,
                            "process_2": 0.0026959618870965895,
                            "process_0": 0.005031462335833248
                        },
                        "total_energy_joules": {
                            "process_1": 7874.4056230756405,
                            "process_3": 12513.64958722605,
                            "process_2": 9705.462793547722,
                            "process_0": 18113.264408999694
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 201.60029132357644,
                        "ram_power_avg": 0.6945698261260986,
                        "cpu_energy_total": 0.004837489657999412,
                        "gpu_energy_total": 0.008538657942032346,
                        "ram_energy_total": 1.4625292426323099e-05,
                        "total_energy_kwh": 0.013390772892458085,
                        "total_energy_joules": 48206.782412849105
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.33986918810895345,
                        "joules_per_token": 2.942308496877997,
                        "flops_per_joule": 351609672.84624517,
                        "joules_per_flop": 2.844062826557358e-09
                    },
                    "per-process_emissions": [
                        0.0008332652283640738,
                        0.001324187447292712,
                        0.0010270266808894458,
                        0.001916735576835676
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0355": {
            "setup": {
                "experiment_id": "0355",
                "date_time": "April 12, 2025 at 09:22:09 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A1_Max_Throughput_Exploit",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 57.570764437987236,
                        "average_latency_ms_per_batch": 57570.764437987236,
                        "throughput_queries_per_sec": 2.2233507102007675,
                        "throughput_tokens_per_sec": 284.58889090569824
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 2039678464,
                        "gpu_max_memory_allocated_bytes": 2039678464,
                        "gpu_current_memory_reserved_bytes": 4636803072,
                        "gpu_max_memory_reserved_bytes": 4636803072
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 82.3,
                        "cpu_memory_usage_bytes": 1912037376
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0355",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 185.32420724737398,
                            "process_3": 201.41520931481045,
                            "process_2": 193.57273259108436,
                            "process_0": 226.08901614103695
                        },
                        "ram_power": {
                            "process_1": 0.7010550498962402,
                            "process_3": 0.7105865478515625,
                            "process_2": 0.6992440223693848,
                            "process_0": 0.667393684387207
                        },
                        "cpu_energy": {
                            "process_1": 0.0008001227740305692,
                            "process_3": 0.0012628235162196687,
                            "process_2": 0.000989520255156549,
                            "process_0": 0.001785023112592625
                        },
                        "gpu_energy": {
                            "process_1": 0.0013846033299047544,
                            "process_3": 0.0022090731561454913,
                            "process_2": 0.0017031416402897293,
                            "process_0": 0.003241839815692371
                        },
                        "ram_energy": {
                            "process_1": 2.6087913634645527e-06,
                            "process_3": 4.117101864297396e-06,
                            "process_2": 3.2999916503113576e-06,
                            "process_0": 4.599407548249793e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.002187334895298789,
                            "process_3": 0.003476013774229458,
                            "process_2": 0.0026959618870965895,
                            "process_0": 0.005031462335833248
                        },
                        "total_energy_joules": {
                            "process_1": 7874.4056230756405,
                            "process_3": 12513.64958722605,
                            "process_2": 9705.462793547722,
                            "process_0": 18113.264408999694
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 201.60029132357644,
                        "ram_power_avg": 0.6945698261260986,
                        "cpu_energy_total": 0.004837489657999412,
                        "gpu_energy_total": 0.008538657942032346,
                        "ram_energy_total": 1.4625292426323099e-05,
                        "total_energy_kwh": 0.013390772892458085,
                        "total_energy_joules": 48206.782412849105
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.33986918810895345,
                        "joules_per_token": 2.942308496877997,
                        "flops_per_joule": 351609672.84624517,
                        "joules_per_flop": 2.844062826557358e-09
                    },
                    "per-process_emissions": [
                        0.0008332652283640738,
                        0.001324187447292712,
                        0.0010270266808894458,
                        0.001916735576835676
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0356": {
            "setup": {
                "experiment_id": "0356",
                "date_time": "April 12, 2025 at 09:27:49 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R2_Low_Latency_Chatbot_Deployment",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.05,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 15728
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 256.73793045594357,
                        "average_latency_ms_per_batch": 8023.060326748237,
                        "throughput_queries_per_sec": 0.4985628721579373,
                        "throughput_tokens_per_sec": 61.260912916406554
                    }
                },
                "compute_metrics": {
                    "flops": 20248623316992,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 9905837056,
                        "gpu_max_memory_allocated_bytes": 9905837056,
                        "gpu_current_memory_reserved_bytes": 11865686016,
                        "gpu_max_memory_reserved_bytes": 11865686016
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            19.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 78.4,
                        "cpu_memory_usage_bytes": 1890226176
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0356",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 43.8951493414439
                        },
                        "ram_power": {
                            "process_0": 0.6589922904968262
                        },
                        "cpu_energy": {
                            "process_0": 0.007831810926096748
                        },
                        "gpu_energy": {
                            "process_0": 0.003970084009398134
                        },
                        "ram_energy": {
                            "process_0": 2.863874528635342e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011830533680781243
                        },
                        "total_energy_joules": {
                            "process_0": 42589.92125081248
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 43.8951493414439,
                        "ram_power_avg": 0.6589922904968262,
                        "cpu_energy_total": 0.007831810926096748,
                        "gpu_energy_total": 0.003970084009398134,
                        "ram_energy_total": 2.863874528635342e-05,
                        "total_energy_kwh": 0.011830533680781243,
                        "total_energy_joules": 42589.92125081248
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.36928924820916315,
                        "joules_per_token": 2.707904453891943,
                        "flops_per_joule": 475432278.86587656,
                        "joules_per_flop": 2.103348982499584e-09
                    },
                    "per-process_emissions": [
                        0.004506841805693614
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0356": {
            "setup": {
                "experiment_id": "0356",
                "date_time": "April 12, 2025 at 09:27:49 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R2_Low_Latency_Chatbot_Deployment",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.05,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 15728
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 256.73793045594357,
                        "average_latency_ms_per_batch": 8023.060326748237,
                        "throughput_queries_per_sec": 0.4985628721579373,
                        "throughput_tokens_per_sec": 61.260912916406554
                    }
                },
                "compute_metrics": {
                    "flops": 20248623316992,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 9905837056,
                        "gpu_max_memory_allocated_bytes": 9905837056,
                        "gpu_current_memory_reserved_bytes": 11865686016,
                        "gpu_max_memory_reserved_bytes": 11865686016
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            19.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 78.4,
                        "cpu_memory_usage_bytes": 1890226176
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0356",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 43.8951493414439
                        },
                        "ram_power": {
                            "process_0": 0.6589922904968262
                        },
                        "cpu_energy": {
                            "process_0": 0.007831810926096748
                        },
                        "gpu_energy": {
                            "process_0": 0.003970084009398134
                        },
                        "ram_energy": {
                            "process_0": 2.863874528635342e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.011830533680781243
                        },
                        "total_energy_joules": {
                            "process_0": 42589.92125081248
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 43.8951493414439,
                        "ram_power_avg": 0.6589922904968262,
                        "cpu_energy_total": 0.007831810926096748,
                        "gpu_energy_total": 0.003970084009398134,
                        "ram_energy_total": 2.863874528635342e-05,
                        "total_energy_kwh": 0.011830533680781243,
                        "total_energy_joules": 42589.92125081248
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.36928924820916315,
                        "joules_per_token": 2.707904453891943,
                        "flops_per_joule": 475432278.86587656,
                        "joules_per_flop": 2.103348982499584e-09
                    },
                    "per-process_emissions": [
                        0.004506841805693614
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0357": {
            "setup": {
                "experiment_id": "0357",
                "date_time": "April 12, 2025 at 10:05:41 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R5_Real_Time_Mobile_Inference",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.6,
                    "simulate_burst": true,
                    "burst_interval": 5.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 12107
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 2200.5765247718805,
                        "average_latency_ms_per_batch": 17192.004099780315,
                        "throughput_queries_per_sec": 0.05816657524021753,
                        "throughput_tokens_per_sec": 5.5017400502602625
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 2038921216,
                        "gpu_max_memory_allocated_bytes": 2038921216,
                        "gpu_current_memory_reserved_bytes": 2764046336,
                        "gpu_max_memory_reserved_bytes": 2764046336
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 75.2,
                        "cpu_memory_usage_bytes": 1985253376
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0357",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 36.130162998747636
                        },
                        "ram_power": {
                            "process_0": 0.693152904510498
                        },
                        "cpu_energy": {
                            "process_0": 0.06689380469481962
                        },
                        "gpu_energy": {
                            "process_0": 0.020876391701100117
                        },
                        "ram_energy": {
                            "process_0": 0.00022862263482702753
                        },
                        "total_energy_kwh": {
                            "process_0": 0.08799881903074697
                        },
                        "total_energy_joules": {
                            "process_0": 316795.74851068907
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 36.130162998747636,
                        "ram_power_avg": 0.693152904510498,
                        "cpu_energy_total": 0.06689380469481962,
                        "gpu_energy_total": 0.020876391701100117,
                        "ram_energy_total": 0.00022862263482702753,
                        "total_energy_kwh": 0.08799881903074697,
                        "total_energy_joules": 316795.74851068907
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.03821705328091388,
                        "joules_per_token": 26.16632927320468,
                        "flops_per_joule": 53504414.35163417,
                        "joules_per_flop": 1.8690046646019544e-08
                    },
                    "per-process_emissions": [
                        0.033523150109763054
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0357": {
            "setup": {
                "experiment_id": "0357",
                "date_time": "April 12, 2025 at 10:05:41 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R5_Real_Time_Mobile_Inference",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.6,
                    "simulate_burst": true,
                    "burst_interval": 5.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 12107
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 2200.5765247718805,
                        "average_latency_ms_per_batch": 17192.004099780315,
                        "throughput_queries_per_sec": 0.05816657524021753,
                        "throughput_tokens_per_sec": 5.5017400502602625
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 2038921216,
                        "gpu_max_memory_allocated_bytes": 2038921216,
                        "gpu_current_memory_reserved_bytes": 2764046336,
                        "gpu_max_memory_reserved_bytes": 2764046336
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 75.2,
                        "cpu_memory_usage_bytes": 1985253376
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0357",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 36.130162998747636
                        },
                        "ram_power": {
                            "process_0": 0.693152904510498
                        },
                        "cpu_energy": {
                            "process_0": 0.06689380469481962
                        },
                        "gpu_energy": {
                            "process_0": 0.020876391701100117
                        },
                        "ram_energy": {
                            "process_0": 0.00022862263482702753
                        },
                        "total_energy_kwh": {
                            "process_0": 0.08799881903074697
                        },
                        "total_energy_joules": {
                            "process_0": 316795.74851068907
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 36.130162998747636,
                        "ram_power_avg": 0.693152904510498,
                        "cpu_energy_total": 0.06689380469481962,
                        "gpu_energy_total": 0.020876391701100117,
                        "ram_energy_total": 0.00022862263482702753,
                        "total_energy_kwh": 0.08799881903074697,
                        "total_energy_joules": 316795.74851068907
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.03821705328091388,
                        "joules_per_token": 26.16632927320468,
                        "flops_per_joule": 53504414.35163417,
                        "joules_per_flop": 1.8690046646019544e-08
                    },
                    "per-process_emissions": [
                        0.033523150109763054
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0358": {
            "setup": {
                "experiment_id": "0358",
                "date_time": "April 12, 2025 at 10:07:27 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A2_Precision_Minimalist",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 18.797311820992036,
                        "average_latency_ms_per_batch": 18797.311820992036,
                        "throughput_queries_per_sec": 6.809484314510071,
                        "throughput_tokens_per_sec": 871.613992257289
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 4001366016,
                        "gpu_max_memory_reserved_bytes": 4001366016
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 76.3,
                        "cpu_memory_usage_bytes": 1890508800
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0358",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 0.0,
                            "process_1": 188.0490561375381
                        },
                        "ram_power": {
                            "process_0": 0.6598749160766602,
                            "process_1": 0.6902003288269043
                        },
                        "cpu_energy": {
                            "process_0": 0.0006341727754665955,
                            "process_1": 0.00045273991718750044
                        },
                        "gpu_energy": {
                            "process_0": 0.0008034711983329057,
                            "process_1": 0.0006310638381843425
                        },
                        "ram_energy": {
                            "process_0": 2.0248065445619103e-06,
                            "process_1": 1.6523956700868584e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.001439668780344063,
                            "process_1": 0.0010854561510419297
                        },
                        "total_energy_joules": {
                            "process_0": 5182.807609238627,
                            "process_1": 3907.6421437509466
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 94.02452806876904,
                        "ram_power_avg": 0.6750376224517822,
                        "cpu_energy_total": 0.001086912692654096,
                        "gpu_energy_total": 0.0014345350365172482,
                        "ram_energy_total": 3.6772022146487687e-06,
                        "total_energy_kwh": 0.0025251249313859928,
                        "total_energy_joules": 9090.449752989574
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.8023310666902699,
                        "joules_per_token": 0.5548370210564926,
                        "flops_per_joule": 1864591021.756395,
                        "joules_per_flop": 5.363106377386857e-10
                    },
                    "per-process_emissions": [
                        0.0005484418218720708,
                        0.00041350452073942313
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0358": {
            "setup": {
                "experiment_id": "0358",
                "date_time": "April 12, 2025 at 10:07:27 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A2_Precision_Minimalist",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 18.797311820992036,
                        "average_latency_ms_per_batch": 18797.311820992036,
                        "throughput_queries_per_sec": 6.809484314510071,
                        "throughput_tokens_per_sec": 871.613992257289
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 4001366016,
                        "gpu_max_memory_reserved_bytes": 4001366016
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 76.3,
                        "cpu_memory_usage_bytes": 1890508800
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0358",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 0.0,
                            "process_1": 188.0490561375381
                        },
                        "ram_power": {
                            "process_0": 0.6598749160766602,
                            "process_1": 0.6902003288269043
                        },
                        "cpu_energy": {
                            "process_0": 0.0006341727754665955,
                            "process_1": 0.00045273991718750044
                        },
                        "gpu_energy": {
                            "process_0": 0.0008034711983329057,
                            "process_1": 0.0006310638381843425
                        },
                        "ram_energy": {
                            "process_0": 2.0248065445619103e-06,
                            "process_1": 1.6523956700868584e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.001439668780344063,
                            "process_1": 0.0010854561510419297
                        },
                        "total_energy_joules": {
                            "process_0": 5182.807609238627,
                            "process_1": 3907.6421437509466
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 94.02452806876904,
                        "ram_power_avg": 0.6750376224517822,
                        "cpu_energy_total": 0.001086912692654096,
                        "gpu_energy_total": 0.0014345350365172482,
                        "ram_energy_total": 3.6772022146487687e-06,
                        "total_energy_kwh": 0.0025251249313859928,
                        "total_energy_joules": 9090.449752989574
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.8023310666902699,
                        "joules_per_token": 0.5548370210564926,
                        "flops_per_joule": 1864591021.756395,
                        "joules_per_flop": 5.363106377386857e-10
                    },
                    "per-process_emissions": [
                        0.0005484418218720708,
                        0.00041350452073942313
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0359": {
            "setup": {
                "experiment_id": "0359",
                "date_time": "April 12, 2025 at 10:09:33 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R6_Medium_Scale_Language_Model_Serving",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.1,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 33.238392974002636,
                        "average_latency_ms_per_batch": 8309.598243500659,
                        "throughput_queries_per_sec": 3.850968369623496,
                        "throughput_tokens_per_sec": 492.9239513118075
                    }
                },
                "compute_metrics": {
                    "flops": 20248623316992,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4963390464,
                        "gpu_max_memory_allocated_bytes": 4963390464,
                        "gpu_current_memory_reserved_bytes": 7029653504,
                        "gpu_max_memory_reserved_bytes": 7029653504
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            2.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 77.5,
                        "cpu_memory_usage_bytes": 1934356480
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0359",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 285.5089517147027,
                            "process_3": 211.45485816746364,
                            "process_0": 210.91947710464606,
                            "process_2": 213.2903627434687
                        },
                        "ram_power": {
                            "process_1": 0.6872262954711914,
                            "process_3": 0.6795816421508789,
                            "process_0": 0.6743888854980469,
                            "process_2": 0.6878371238708496
                        },
                        "cpu_energy": {
                            "process_1": 0.0009446995142825473,
                            "process_3": 0.0011930980351553449,
                            "process_0": 0.001030741944124202,
                            "process_2": 0.0010480794316581527
                        },
                        "gpu_energy": {
                            "process_1": 0.0018042842212040444,
                            "process_3": 0.00230065600719076,
                            "process_0": 0.0019825840860672184,
                            "process_2": 0.0020140341112266924
                        },
                        "ram_energy": {
                            "process_1": 3.369796943440382e-06,
                            "process_3": 4.502565557283364e-06,
                            "process_0": 3.951859516783522e-06,
                            "process_2": 4.1383347184401105e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0027523535324300314,
                            "process_3": 0.003498256607903387,
                            "process_0": 0.0030172778897082043,
                            "process_2": 0.003066251877603285
                        },
                        "total_energy_joules": {
                            "process_1": 9908.472716748112,
                            "process_3": 12593.723788452193,
                            "process_0": 10862.200402949535,
                            "process_2": 11038.506759371827
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 230.29341243257025,
                        "ram_power_avg": 0.6822584867477417,
                        "cpu_energy_total": 0.004216618925220247,
                        "gpu_energy_total": 0.008101558425688715,
                        "ram_energy_total": 1.5962556735947376e-05,
                        "total_energy_kwh": 0.012334139907644908,
                        "total_energy_joules": 44402.90366752167
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.3689848781665153,
                        "joules_per_token": 2.710138163300883,
                        "flops_per_joule": 456020251.93237025,
                        "joules_per_flop": 2.1928850654385063e-09
                    },
                    "per-process_emissions": [
                        0.0010485090781792205,
                        0.0013326608547807952,
                        0.0011494320120843404,
                        0.0011680886527729714
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0359": {
            "setup": {
                "experiment_id": "0359",
                "date_time": "April 12, 2025 at 10:09:33 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R6_Medium_Scale_Language_Model_Serving",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.1,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 33.238392974002636,
                        "average_latency_ms_per_batch": 8309.598243500659,
                        "throughput_queries_per_sec": 3.850968369623496,
                        "throughput_tokens_per_sec": 492.9239513118075
                    }
                },
                "compute_metrics": {
                    "flops": 20248623316992,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4963390464,
                        "gpu_max_memory_allocated_bytes": 4963390464,
                        "gpu_current_memory_reserved_bytes": 7029653504,
                        "gpu_max_memory_reserved_bytes": 7029653504
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            2.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 77.5,
                        "cpu_memory_usage_bytes": 1934356480
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0359",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 285.5089517147027,
                            "process_3": 211.45485816746364,
                            "process_0": 210.91947710464606,
                            "process_2": 213.2903627434687
                        },
                        "ram_power": {
                            "process_1": 0.6872262954711914,
                            "process_3": 0.6795816421508789,
                            "process_0": 0.6743888854980469,
                            "process_2": 0.6878371238708496
                        },
                        "cpu_energy": {
                            "process_1": 0.0009446995142825473,
                            "process_3": 0.0011930980351553449,
                            "process_0": 0.001030741944124202,
                            "process_2": 0.0010480794316581527
                        },
                        "gpu_energy": {
                            "process_1": 0.0018042842212040444,
                            "process_3": 0.00230065600719076,
                            "process_0": 0.0019825840860672184,
                            "process_2": 0.0020140341112266924
                        },
                        "ram_energy": {
                            "process_1": 3.369796943440382e-06,
                            "process_3": 4.502565557283364e-06,
                            "process_0": 3.951859516783522e-06,
                            "process_2": 4.1383347184401105e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0027523535324300314,
                            "process_3": 0.003498256607903387,
                            "process_0": 0.0030172778897082043,
                            "process_2": 0.003066251877603285
                        },
                        "total_energy_joules": {
                            "process_1": 9908.472716748112,
                            "process_3": 12593.723788452193,
                            "process_0": 10862.200402949535,
                            "process_2": 11038.506759371827
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 230.29341243257025,
                        "ram_power_avg": 0.6822584867477417,
                        "cpu_energy_total": 0.004216618925220247,
                        "gpu_energy_total": 0.008101558425688715,
                        "ram_energy_total": 1.5962556735947376e-05,
                        "total_energy_kwh": 0.012334139907644908,
                        "total_energy_joules": 44402.90366752167
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.3689848781665153,
                        "joules_per_token": 2.710138163300883,
                        "flops_per_joule": 456020251.93237025,
                        "joules_per_flop": 2.1928850654385063e-09
                    },
                    "per-process_emissions": [
                        0.0010485090781792205,
                        0.0013326608547807952,
                        0.0011494320120843404,
                        0.0011680886527729714
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0360": {
            "setup": {
                "experiment_id": "0360",
                "date_time": "April 12, 2025 at 10:11:17 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A5_Parallel_Overdrive",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 15.50759629199456,
                        "average_latency_ms_per_batch": 7753.79814599728,
                        "throughput_queries_per_sec": 8.254019358633746,
                        "throughput_tokens_per_sec": 1056.5144779051195
                    }
                },
                "compute_metrics": {
                    "flops": 20248623316992,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4963390464,
                        "gpu_max_memory_allocated_bytes": 4963390464,
                        "gpu_current_memory_reserved_bytes": 7029653504,
                        "gpu_max_memory_reserved_bytes": 7029653504
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            3.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 77.5,
                        "cpu_memory_usage_bytes": 1928949760
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0360",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 398.0342521582222,
                            "process_2": 298.94050716835,
                            "process_0": 464.75391146685706,
                            "process_1": 228.27111588972545
                        },
                        "ram_power": {
                            "process_3": 0.6825485229492189,
                            "process_2": 0.6783299446105957,
                            "process_0": 0.6725034713745117,
                            "process_1": 0.6748566627502441
                        },
                        "cpu_energy": {
                            "process_3": 0.0005135501101876798,
                            "process_2": 0.0005720863712504069,
                            "process_0": 0.0004883170916882591,
                            "process_1": 0.0006780868593114064
                        },
                        "gpu_energy": {
                            "process_3": 0.001329902730587973,
                            "process_2": 0.0014885173019243414,
                            "process_0": 0.0012475268313536514,
                            "process_1": 0.001708589700204488
                        },
                        "ram_energy": {
                            "process_3": 1.954412450800924e-06,
                            "process_2": 2.075270584610214e-06,
                            "process_0": 1.7617369143337465e-06,
                            "process_1": 2.4716820566364255e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0018454072532264533,
                            "process_2": 0.0020626789437593585,
                            "process_0": 0.0017376056599562445,
                            "process_1": 0.002389148241572531
                        },
                        "total_energy_joules": {
                            "process_3": 6643.466111615232,
                            "process_2": 7425.64419753369,
                            "process_0": 6255.38037584248,
                            "process_1": 8600.933669661112
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 347.4999466707887,
                        "ram_power_avg": 0.6770596504211426,
                        "cpu_energy_total": 0.002252040432437752,
                        "gpu_energy_total": 0.005774536564070454,
                        "ram_energy_total": 8.26310200638131e-06,
                        "total_energy_kwh": 0.008034840098514587,
                        "total_energy_joules": 28925.424354652518
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5664221136090165,
                        "joules_per_token": 1.7654677950837716,
                        "flops_per_joule": 700028565.4836073,
                        "joules_per_flop": 1.4285131340449808e-09
                    },
                    "per-process_emissions": [
                        0.0007030078931166174,
                        0.0007857775436251276,
                        0.0006619408761603313,
                        0.0009101460226270556
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0360": {
            "setup": {
                "experiment_id": "0360",
                "date_time": "April 12, 2025 at 10:11:17 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A5_Parallel_Overdrive",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 15.50759629199456,
                        "average_latency_ms_per_batch": 7753.79814599728,
                        "throughput_queries_per_sec": 8.254019358633746,
                        "throughput_tokens_per_sec": 1056.5144779051195
                    }
                },
                "compute_metrics": {
                    "flops": 20248623316992,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4963390464,
                        "gpu_max_memory_allocated_bytes": 4963390464,
                        "gpu_current_memory_reserved_bytes": 7029653504,
                        "gpu_max_memory_reserved_bytes": 7029653504
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            3.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 77.5,
                        "cpu_memory_usage_bytes": 1928949760
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0360",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 398.0342521582222,
                            "process_2": 298.94050716835,
                            "process_0": 464.75391146685706,
                            "process_1": 228.27111588972545
                        },
                        "ram_power": {
                            "process_3": 0.6825485229492189,
                            "process_2": 0.6783299446105957,
                            "process_0": 0.6725034713745117,
                            "process_1": 0.6748566627502441
                        },
                        "cpu_energy": {
                            "process_3": 0.0005135501101876798,
                            "process_2": 0.0005720863712504069,
                            "process_0": 0.0004883170916882591,
                            "process_1": 0.0006780868593114064
                        },
                        "gpu_energy": {
                            "process_3": 0.001329902730587973,
                            "process_2": 0.0014885173019243414,
                            "process_0": 0.0012475268313536514,
                            "process_1": 0.001708589700204488
                        },
                        "ram_energy": {
                            "process_3": 1.954412450800924e-06,
                            "process_2": 2.075270584610214e-06,
                            "process_0": 1.7617369143337465e-06,
                            "process_1": 2.4716820566364255e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0018454072532264533,
                            "process_2": 0.0020626789437593585,
                            "process_0": 0.0017376056599562445,
                            "process_1": 0.002389148241572531
                        },
                        "total_energy_joules": {
                            "process_3": 6643.466111615232,
                            "process_2": 7425.64419753369,
                            "process_0": 6255.38037584248,
                            "process_1": 8600.933669661112
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 347.4999466707887,
                        "ram_power_avg": 0.6770596504211426,
                        "cpu_energy_total": 0.002252040432437752,
                        "gpu_energy_total": 0.005774536564070454,
                        "ram_energy_total": 8.26310200638131e-06,
                        "total_energy_kwh": 0.008034840098514587,
                        "total_energy_joules": 28925.424354652518
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5664221136090165,
                        "joules_per_token": 1.7654677950837716,
                        "flops_per_joule": 700028565.4836073,
                        "joules_per_flop": 1.4285131340449808e-09
                    },
                    "per-process_emissions": [
                        0.0007030078931166174,
                        0.0007857775436251276,
                        0.0006619408761603313,
                        0.0009101460226270556
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0361": {
            "setup": {
                "experiment_id": "0361",
                "date_time": "April 12, 2025 at 10:14:41 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "default",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 132.6861075299821,
                        "average_latency_ms_per_batch": 16585.763441247764,
                        "throughput_queries_per_sec": 0.9646827567918275,
                        "throughput_tokens_per_sec": 123.47939286935392
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 2039254528,
                        "gpu_max_memory_allocated_bytes": 2039254528,
                        "gpu_current_memory_reserved_bytes": 2751463424,
                        "gpu_max_memory_reserved_bytes": 2751463424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 73.8,
                        "cpu_memory_usage_bytes": 1963819008
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0361",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 39.80996132827895
                        },
                        "ram_power": {
                            "process_0": 0.685572624206543
                        },
                        "cpu_energy": {
                            "process_0": 0.004044605366405448
                        },
                        "gpu_energy": {
                            "process_0": 0.0014420658758740323
                        },
                        "ram_energy": {
                            "process_0": 1.5572047911509652e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005502243290190991
                        },
                        "total_energy_joules": {
                            "process_0": 19808.075844687566
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 39.80996132827895,
                        "ram_power_avg": 0.685572624206543,
                        "cpu_energy_total": 0.004044605366405448,
                        "gpu_energy_total": 0.0014420658758740323,
                        "ram_energy_total": 1.5572047911509652e-05,
                        "total_energy_kwh": 0.005502243290190991,
                        "total_energy_joules": 19808.075844687566
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.8271373821700159,
                        "joules_per_token": 1.2089890041923563,
                        "flops_per_joule": 855710121.7732818,
                        "joules_per_flop": 1.1686200438154306e-09
                    },
                    "per-process_emissions": [
                        0.002096079581398258
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0361": {
            "setup": {
                "experiment_id": "0361",
                "date_time": "April 12, 2025 at 10:14:41 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "default",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 132.6861075299821,
                        "average_latency_ms_per_batch": 16585.763441247764,
                        "throughput_queries_per_sec": 0.9646827567918275,
                        "throughput_tokens_per_sec": 123.47939286935392
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 2039254528,
                        "gpu_max_memory_allocated_bytes": 2039254528,
                        "gpu_current_memory_reserved_bytes": 2751463424,
                        "gpu_max_memory_reserved_bytes": 2751463424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 73.8,
                        "cpu_memory_usage_bytes": 1963819008
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0361",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 39.80996132827895
                        },
                        "ram_power": {
                            "process_0": 0.685572624206543
                        },
                        "cpu_energy": {
                            "process_0": 0.004044605366405448
                        },
                        "gpu_energy": {
                            "process_0": 0.0014420658758740323
                        },
                        "ram_energy": {
                            "process_0": 1.5572047911509652e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005502243290190991
                        },
                        "total_energy_joules": {
                            "process_0": 19808.075844687566
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 39.80996132827895,
                        "ram_power_avg": 0.685572624206543,
                        "cpu_energy_total": 0.004044605366405448,
                        "gpu_energy_total": 0.0014420658758740323,
                        "ram_energy_total": 1.5572047911509652e-05,
                        "total_energy_kwh": 0.005502243290190991,
                        "total_energy_joules": 19808.075844687566
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.8271373821700159,
                        "joules_per_token": 1.2089890041923563,
                        "flops_per_joule": 855710121.7732818,
                        "joules_per_flop": 1.1686200438154306e-09
                    },
                    "per-process_emissions": [
                        0.002096079581398258
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0365": {
            "setup": {
                "experiment_id": "0365",
                "date_time": "April 12, 2025 at 10:20:11 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R3_Balanced_Enterprise_Service",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 86.71755978799774,
                        "average_latency_ms_per_batch": 21679.389946999436,
                        "throughput_queries_per_sec": 1.4760562948603173,
                        "throughput_tokens_per_sec": 188.93520574212062
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 2039428096,
                        "gpu_max_memory_allocated_bytes": 2039428096,
                        "gpu_current_memory_reserved_bytes": 3024093184,
                        "gpu_max_memory_reserved_bytes": 3024093184
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 73.6,
                        "cpu_memory_usage_bytes": 2015674368
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0365",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 195.24917482733852,
                            "process_2": 208.7561242148867,
                            "process_3": 170.76927788997364,
                            "process_1": 203.44796609595573
                        },
                        "ram_power": {
                            "process_0": 0.7035884857177734,
                            "process_2": 0.738431453704834,
                            "process_3": 0.7358450889587402,
                            "process_1": 0.7396245002746582
                        },
                        "cpu_energy": {
                            "process_0": 0.0026718806218432286,
                            "process_2": 0.00322139783318744,
                            "process_3": 0.0023579812592511183,
                            "process_1": 0.003114251517405592
                        },
                        "gpu_energy": {
                            "process_0": 0.0038363386246245135,
                            "process_2": 0.004837208591984998,
                            "process_3": 0.0033497701798141932,
                            "process_1": 0.004625043700031695
                        },
                        "ram_energy": {
                            "process_0": 1.0304814595808455e-05,
                            "process_2": 1.2489489676574686e-05,
                            "process_3": 9.823072614472965e-06,
                            "process_1": 1.2647533118514433e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.006518524061063554,
                            "process_2": 0.00807109591484901,
                            "process_3": 0.005717574511679785,
                            "process_1": 0.007751942750555798
                        },
                        "total_energy_joules": {
                            "process_0": 23466.686619828793,
                            "process_2": 29055.945293456432,
                            "process_3": 20583.268242047227,
                            "process_1": 27906.99390200087
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 194.55563575703866,
                        "ram_power_avg": 0.7293723821640015,
                        "cpu_energy_total": 0.01136551123168738,
                        "gpu_energy_total": 0.0166483610964554,
                        "ram_energy_total": 4.5264910005370536e-05,
                        "total_energy_kwh": 0.028059137238148145,
                        "total_energy_joules": 101012.89405733332
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.16219711506038725,
                        "joules_per_token": 6.165337772054036,
                        "flops_per_joule": 167800072.96426398,
                        "joules_per_flop": 5.959472974800004e-09
                    },
                    "per-process_emissions": [
                        0.002483231741062161,
                        0.0030746839887617304,
                        0.0021781100102244142,
                        0.002953102590824231
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0365": {
            "setup": {
                "experiment_id": "0365",
                "date_time": "April 12, 2025 at 10:20:11 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R3_Balanced_Enterprise_Service",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 86.71755978799774,
                        "average_latency_ms_per_batch": 21679.389946999436,
                        "throughput_queries_per_sec": 1.4760562948603173,
                        "throughput_tokens_per_sec": 188.93520574212062
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 2039428096,
                        "gpu_max_memory_allocated_bytes": 2039428096,
                        "gpu_current_memory_reserved_bytes": 3024093184,
                        "gpu_max_memory_reserved_bytes": 3024093184
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 73.6,
                        "cpu_memory_usage_bytes": 2015674368
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0365",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 195.24917482733852,
                            "process_2": 208.7561242148867,
                            "process_3": 170.76927788997364,
                            "process_1": 203.44796609595573
                        },
                        "ram_power": {
                            "process_0": 0.7035884857177734,
                            "process_2": 0.738431453704834,
                            "process_3": 0.7358450889587402,
                            "process_1": 0.7396245002746582
                        },
                        "cpu_energy": {
                            "process_0": 0.0026718806218432286,
                            "process_2": 0.00322139783318744,
                            "process_3": 0.0023579812592511183,
                            "process_1": 0.003114251517405592
                        },
                        "gpu_energy": {
                            "process_0": 0.0038363386246245135,
                            "process_2": 0.004837208591984998,
                            "process_3": 0.0033497701798141932,
                            "process_1": 0.004625043700031695
                        },
                        "ram_energy": {
                            "process_0": 1.0304814595808455e-05,
                            "process_2": 1.2489489676574686e-05,
                            "process_3": 9.823072614472965e-06,
                            "process_1": 1.2647533118514433e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.006518524061063554,
                            "process_2": 0.00807109591484901,
                            "process_3": 0.005717574511679785,
                            "process_1": 0.007751942750555798
                        },
                        "total_energy_joules": {
                            "process_0": 23466.686619828793,
                            "process_2": 29055.945293456432,
                            "process_3": 20583.268242047227,
                            "process_1": 27906.99390200087
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 194.55563575703866,
                        "ram_power_avg": 0.7293723821640015,
                        "cpu_energy_total": 0.01136551123168738,
                        "gpu_energy_total": 0.0166483610964554,
                        "ram_energy_total": 4.5264910005370536e-05,
                        "total_energy_kwh": 0.028059137238148145,
                        "total_energy_joules": 101012.89405733332
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.16219711506038725,
                        "joules_per_token": 6.165337772054036,
                        "flops_per_joule": 167800072.96426398,
                        "joules_per_flop": 5.959472974800004e-09
                    },
                    "per-process_emissions": [
                        0.002483231741062161,
                        0.0030746839887617304,
                        0.0021781100102244142,
                        0.002953102590824231
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0366": {
            "setup": {
                "experiment_id": "0366",
                "date_time": "April 12, 2025 at 10:23:34 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 84.41053453102359,
                        "average_latency_ms_per_batch": 10551.316816377948,
                        "throughput_queries_per_sec": 1.5163984058524813,
                        "throughput_tokens_per_sec": 194.0989959491176
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 72.1,
                        "cpu_memory_usage_bytes": 1976598528
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0366",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 240.9088304912285,
                            "process_3": 239.61305687142178,
                            "process_1": 214.50220286839829,
                            "process_0": 211.46023102650182
                        },
                        "ram_power": {
                            "process_2": 0.7207374572753906,
                            "process_3": 0.7200908660888672,
                            "process_1": 0.7248516082763672,
                            "process_0": 0.6901302337646484
                        },
                        "cpu_energy": {
                            "process_2": 0.0034038842388149524,
                            "process_3": 0.002711070446778649,
                            "process_1": 0.0031588872162519686,
                            "process_0": 0.002595096038623524
                        },
                        "gpu_energy": {
                            "process_2": 0.005905306946464517,
                            "process_3": 0.004582235610229191,
                            "process_1": 0.005413890164442048,
                            "process_0": 0.00438113656046224
                        },
                        "ram_energy": {
                            "process_2": 1.3283133493254321e-05,
                            "process_3": 9.777017161002403e-06,
                            "process_1": 1.2154197590593534e-05,
                            "process_0": 9.321861945818998e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.009322474318772733,
                            "process_3": 0.007303083074168841,
                            "process_1": 0.00858493157828461,
                            "process_0": 0.00698555446103158
                        },
                        "total_energy_joules": {
                            "process_2": 33560.90754758184,
                            "process_3": 26291.099067007828,
                            "process_1": 30905.753681824597,
                            "process_0": 25147.99605971369
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 226.6210803143876,
                        "ram_power_avg": 0.7139525413513184,
                        "cpu_energy_total": 0.011868937940469095,
                        "gpu_energy_total": 0.020282569281597995,
                        "ram_energy_total": 4.4536210190669254e-05,
                        "total_energy_kwh": 0.032196043432257765,
                        "total_energy_joules": 115905.75635612797
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.14135622349643356,
                        "joules_per_token": 7.074325949470701,
                        "flops_per_joule": 146239250.97449097,
                        "joules_per_flop": 6.838109422308471e-09
                    },
                    "per-process_emissions": [
                        0.003551396591736473,
                        0.00278210949710462,
                        0.003270429684747522,
                        0.0026611469719299806
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0366": {
            "setup": {
                "experiment_id": "0366",
                "date_time": "April 12, 2025 at 10:23:34 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 84.41053453102359,
                        "average_latency_ms_per_batch": 10551.316816377948,
                        "throughput_queries_per_sec": 1.5163984058524813,
                        "throughput_tokens_per_sec": 194.0989959491176
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 72.1,
                        "cpu_memory_usage_bytes": 1976598528
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0366",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 240.9088304912285,
                            "process_3": 239.61305687142178,
                            "process_1": 214.50220286839829,
                            "process_0": 211.46023102650182
                        },
                        "ram_power": {
                            "process_2": 0.7207374572753906,
                            "process_3": 0.7200908660888672,
                            "process_1": 0.7248516082763672,
                            "process_0": 0.6901302337646484
                        },
                        "cpu_energy": {
                            "process_2": 0.0034038842388149524,
                            "process_3": 0.002711070446778649,
                            "process_1": 0.0031588872162519686,
                            "process_0": 0.002595096038623524
                        },
                        "gpu_energy": {
                            "process_2": 0.005905306946464517,
                            "process_3": 0.004582235610229191,
                            "process_1": 0.005413890164442048,
                            "process_0": 0.00438113656046224
                        },
                        "ram_energy": {
                            "process_2": 1.3283133493254321e-05,
                            "process_3": 9.777017161002403e-06,
                            "process_1": 1.2154197590593534e-05,
                            "process_0": 9.321861945818998e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.009322474318772733,
                            "process_3": 0.007303083074168841,
                            "process_1": 0.00858493157828461,
                            "process_0": 0.00698555446103158
                        },
                        "total_energy_joules": {
                            "process_2": 33560.90754758184,
                            "process_3": 26291.099067007828,
                            "process_1": 30905.753681824597,
                            "process_0": 25147.99605971369
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 226.6210803143876,
                        "ram_power_avg": 0.7139525413513184,
                        "cpu_energy_total": 0.011868937940469095,
                        "gpu_energy_total": 0.020282569281597995,
                        "ram_energy_total": 4.4536210190669254e-05,
                        "total_energy_kwh": 0.032196043432257765,
                        "total_energy_joules": 115905.75635612797
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.14135622349643356,
                        "joules_per_token": 7.074325949470701,
                        "flops_per_joule": 146239250.97449097,
                        "joules_per_flop": 6.838109422308471e-09
                    },
                    "per-process_emissions": [
                        0.003551396591736473,
                        0.00278210949710462,
                        0.003270429684747522,
                        0.0026611469719299806
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0367": {
            "setup": {
                "experiment_id": "0367",
                "date_time": "April 12, 2025 at 10:26:26 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 97.77569878699433,
                        "average_latency_ms_per_batch": 12221.96234837429,
                        "throughput_queries_per_sec": 1.309118744104808,
                        "throughput_tokens_per_sec": 167.56719924541542
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 72.1,
                        "cpu_memory_usage_bytes": 1933549568
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0367",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 240.76669446666347,
                            "process_1": 228.76611361293922,
                            "process_2": 216.15348071003856,
                            "process_3": 848.3309226787566
                        },
                        "ram_power": {
                            "process_0": 0.6750011444091797,
                            "process_1": 0.7036185264587402,
                            "process_2": 0.7090773582458497,
                            "process_3": 0.7059602737426758
                        },
                        "cpu_energy": {
                            "process_0": 0.0030070793988111293,
                            "process_1": 0.002773105911092443,
                            "process_2": 0.002729334560781354,
                            "process_3": 0.002281050089905421
                        },
                        "gpu_energy": {
                            "process_0": 0.0053315631541364095,
                            "process_1": 0.004856332218396542,
                            "process_2": 0.004771079094638475,
                            "process_3": 0.003937282316490265
                        },
                        "ram_energy": {
                            "process_0": 1.0871597056160025e-05,
                            "process_1": 1.0533585631364582e-05,
                            "process_2": 1.0228925341838116e-05,
                            "process_3": 8.514336637569076e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.008349514150003703,
                            "process_1": 0.007639971715120347,
                            "process_2": 0.007510642580761665,
                            "process_3": 0.00622684674303326
                        },
                        "total_energy_joules": {
                            "process_0": 30058.25094001333,
                            "process_1": 27503.89817443325,
                            "process_2": 27038.313290741993,
                            "process_3": 22416.64827491974
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 383.50430286709945,
                        "ram_power_avg": 0.6984143257141113,
                        "cpu_energy_total": 0.010790569960590347,
                        "gpu_energy_total": 0.01889625678366169,
                        "ram_energy_total": 4.0148444666931805e-05,
                        "total_energy_kwh": 0.029726975188918977,
                        "total_energy_joules": 107017.11068010831
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.15309701314002452,
                        "joules_per_token": 6.531806071783954,
                        "flops_per_joule": 158385615.9583512,
                        "joules_per_flop": 6.3137046501934755e-09
                    },
                    "per-process_emissions": [
                        0.003180747415443911,
                        0.0029104472248750964,
                        0.0028611792911411565,
                        0.0023721172667585206
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0367": {
            "setup": {
                "experiment_id": "0367",
                "date_time": "April 12, 2025 at 10:26:26 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 97.77569878699433,
                        "average_latency_ms_per_batch": 12221.96234837429,
                        "throughput_queries_per_sec": 1.309118744104808,
                        "throughput_tokens_per_sec": 167.56719924541542
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 72.1,
                        "cpu_memory_usage_bytes": 1933549568
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0367",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 240.76669446666347,
                            "process_1": 228.76611361293922,
                            "process_2": 216.15348071003856,
                            "process_3": 848.3309226787566
                        },
                        "ram_power": {
                            "process_0": 0.6750011444091797,
                            "process_1": 0.7036185264587402,
                            "process_2": 0.7090773582458497,
                            "process_3": 0.7059602737426758
                        },
                        "cpu_energy": {
                            "process_0": 0.0030070793988111293,
                            "process_1": 0.002773105911092443,
                            "process_2": 0.002729334560781354,
                            "process_3": 0.002281050089905421
                        },
                        "gpu_energy": {
                            "process_0": 0.0053315631541364095,
                            "process_1": 0.004856332218396542,
                            "process_2": 0.004771079094638475,
                            "process_3": 0.003937282316490265
                        },
                        "ram_energy": {
                            "process_0": 1.0871597056160025e-05,
                            "process_1": 1.0533585631364582e-05,
                            "process_2": 1.0228925341838116e-05,
                            "process_3": 8.514336637569076e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.008349514150003703,
                            "process_1": 0.007639971715120347,
                            "process_2": 0.007510642580761665,
                            "process_3": 0.00622684674303326
                        },
                        "total_energy_joules": {
                            "process_0": 30058.25094001333,
                            "process_1": 27503.89817443325,
                            "process_2": 27038.313290741993,
                            "process_3": 22416.64827491974
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 383.50430286709945,
                        "ram_power_avg": 0.6984143257141113,
                        "cpu_energy_total": 0.010790569960590347,
                        "gpu_energy_total": 0.01889625678366169,
                        "ram_energy_total": 4.0148444666931805e-05,
                        "total_energy_kwh": 0.029726975188918977,
                        "total_energy_joules": 107017.11068010831
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.15309701314002452,
                        "joules_per_token": 6.531806071783954,
                        "flops_per_joule": 158385615.9583512,
                        "joules_per_flop": 6.3137046501934755e-09
                    },
                    "per-process_emissions": [
                        0.003180747415443911,
                        0.0029104472248750964,
                        0.0028611792911411565,
                        0.0023721172667585206
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0368": {
            "setup": {
                "experiment_id": "0368",
                "date_time": "April 12, 2025 at 10:29:20 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_True_latency_0.05_latency_0.2_latency_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 76.88313616499363,
                        "average_latency_ms_per_batch": 9610.392020624204,
                        "throughput_queries_per_sec": 1.6648644473257175,
                        "throughput_tokens_per_sec": 213.10264925769184
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 70.6,
                        "cpu_memory_usage_bytes": 1958469632
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0368",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 339.39386826405996,
                            "process_1": 255.60982211283417,
                            "process_0": 0.0,
                            "process_2": 216.91273778758912
                        },
                        "ram_power": {
                            "process_3": 0.7102975845336914,
                            "process_1": 0.7052836418151855,
                            "process_0": 0.6837043762207031,
                            "process_2": 0.7060132026672363
                        },
                        "cpu_energy": {
                            "process_3": 0.003178313789626372,
                            "process_1": 0.00237444314778395,
                            "process_0": 0.0023956862176864892,
                            "process_2": 0.002299175305623976
                        },
                        "gpu_energy": {
                            "process_3": 0.005835801335304369,
                            "process_1": 0.004204586141443922,
                            "process_0": 0.0041874030721420485,
                            "process_2": 0.004044479902247877
                        },
                        "ram_energy": {
                            "process_3": 1.2334631336211285e-05,
                            "process_1": 9.738577800335232e-06,
                            "process_0": 9.162607166198425e-06,
                            "process_2": 9.128078807900193e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.00902644975626695,
                            "process_1": 0.006588767867028207,
                            "process_0": 0.00659225189699473,
                            "process_2": 0.0063527832866797545
                        },
                        "total_energy_joules": {
                            "process_3": 32495.219122561022,
                            "process_1": 23719.564321301543,
                            "process_0": 23732.106829181026,
                            "process_2": 22870.019832047117
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 202.9791070411208,
                        "ram_power_avg": 0.7013247013092041,
                        "cpu_energy_total": 0.010247618460720788,
                        "gpu_energy_total": 0.018272270451138217,
                        "ram_energy_total": 4.036389511064514e-05,
                        "total_energy_kwh": 0.02856025280696964,
                        "total_energy_joules": 102816.9101050907
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1593512193981872,
                        "joules_per_token": 6.275446173406416,
                        "flops_per_joule": 164855868.31803426,
                        "joules_per_flop": 6.065904782175145e-09
                    },
                    "per-process_emissions": [
                        0.003438626034649895,
                        0.0025099911189443956,
                        0.0025113183601601424,
                        0.0024200927930606527
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0368": {
            "setup": {
                "experiment_id": "0368",
                "date_time": "April 12, 2025 at 10:29:20 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_True_latency_0.05_latency_0.2_latency_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 76.88313616499363,
                        "average_latency_ms_per_batch": 9610.392020624204,
                        "throughput_queries_per_sec": 1.6648644473257175,
                        "throughput_tokens_per_sec": 213.10264925769184
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 70.6,
                        "cpu_memory_usage_bytes": 1958469632
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0368",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 339.39386826405996,
                            "process_1": 255.60982211283417,
                            "process_0": 0.0,
                            "process_2": 216.91273778758912
                        },
                        "ram_power": {
                            "process_3": 0.7102975845336914,
                            "process_1": 0.7052836418151855,
                            "process_0": 0.6837043762207031,
                            "process_2": 0.7060132026672363
                        },
                        "cpu_energy": {
                            "process_3": 0.003178313789626372,
                            "process_1": 0.00237444314778395,
                            "process_0": 0.0023956862176864892,
                            "process_2": 0.002299175305623976
                        },
                        "gpu_energy": {
                            "process_3": 0.005835801335304369,
                            "process_1": 0.004204586141443922,
                            "process_0": 0.0041874030721420485,
                            "process_2": 0.004044479902247877
                        },
                        "ram_energy": {
                            "process_3": 1.2334631336211285e-05,
                            "process_1": 9.738577800335232e-06,
                            "process_0": 9.162607166198425e-06,
                            "process_2": 9.128078807900193e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.00902644975626695,
                            "process_1": 0.006588767867028207,
                            "process_0": 0.00659225189699473,
                            "process_2": 0.0063527832866797545
                        },
                        "total_energy_joules": {
                            "process_3": 32495.219122561022,
                            "process_1": 23719.564321301543,
                            "process_0": 23732.106829181026,
                            "process_2": 22870.019832047117
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 202.9791070411208,
                        "ram_power_avg": 0.7013247013092041,
                        "cpu_energy_total": 0.010247618460720788,
                        "gpu_energy_total": 0.018272270451138217,
                        "ram_energy_total": 4.036389511064514e-05,
                        "total_energy_kwh": 0.02856025280696964,
                        "total_energy_joules": 102816.9101050907
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1593512193981872,
                        "joules_per_token": 6.275446173406416,
                        "flops_per_joule": 164855868.31803426,
                        "joules_per_flop": 6.065904782175145e-09
                    },
                    "per-process_emissions": [
                        0.003438626034649895,
                        0.0025099911189443956,
                        0.0025113183601601424,
                        0.0024200927930606527
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0369": {
            "setup": {
                "experiment_id": "0369",
                "date_time": "April 12, 2025 at 10:32:12 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 93.81337383098435,
                        "average_latency_ms_per_batch": 11726.671728873043,
                        "throughput_queries_per_sec": 1.3644110085051073,
                        "throughput_tokens_per_sec": 174.64460908865374
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 65.7,
                        "cpu_memory_usage_bytes": 1927168000
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0369",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 157.01430916922547,
                            "process_0": 167.60269292851245
                        },
                        "ram_power": {
                            "process_1": 0.7048859596252441,
                            "process_0": 0.6728668212890625
                        },
                        "cpu_energy": {
                            "process_1": 0.002631077556438867,
                            "process_0": 0.0028780873153459634
                        },
                        "gpu_energy": {
                            "process_1": 0.0038059794336702257,
                            "process_0": 0.004178067786896111
                        },
                        "ram_energy": {
                            "process_1": 9.966703278926837e-06,
                            "process_0": 1.0552034524295476e-05
                        },
                        "total_energy_kwh": {
                            "process_1": 0.006447023693388024,
                            "process_0": 0.007066707136766373
                        },
                        "total_energy_joules": {
                            "process_1": 23209.285296196886,
                            "process_0": 25440.145692358943
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 162.30850104886895,
                        "ram_power_avg": 0.6888763904571533,
                        "cpu_energy_total": 0.0055091648717848305,
                        "gpu_energy_total": 0.007984047220566337,
                        "ram_energy_total": 2.0518737803222315e-05,
                        "total_energy_kwh": 0.013513730830154396,
                        "total_energy_joules": 48649.43098855583
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.3367768063691049,
                        "joules_per_token": 2.969325621860097,
                        "flops_per_joule": 348410467.4757505,
                        "joules_per_flop": 2.870177831467134e-09
                    },
                    "per-process_emissions": [
                        0.0024559936759961675,
                        0.00269206208375115
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0369": {
            "setup": {
                "experiment_id": "0369",
                "date_time": "April 12, 2025 at 10:32:12 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 93.81337383098435,
                        "average_latency_ms_per_batch": 11726.671728873043,
                        "throughput_queries_per_sec": 1.3644110085051073,
                        "throughput_tokens_per_sec": 174.64460908865374
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 65.7,
                        "cpu_memory_usage_bytes": 1927168000
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0369",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 157.01430916922547,
                            "process_0": 167.60269292851245
                        },
                        "ram_power": {
                            "process_1": 0.7048859596252441,
                            "process_0": 0.6728668212890625
                        },
                        "cpu_energy": {
                            "process_1": 0.002631077556438867,
                            "process_0": 0.0028780873153459634
                        },
                        "gpu_energy": {
                            "process_1": 0.0038059794336702257,
                            "process_0": 0.004178067786896111
                        },
                        "ram_energy": {
                            "process_1": 9.966703278926837e-06,
                            "process_0": 1.0552034524295476e-05
                        },
                        "total_energy_kwh": {
                            "process_1": 0.006447023693388024,
                            "process_0": 0.007066707136766373
                        },
                        "total_energy_joules": {
                            "process_1": 23209.285296196886,
                            "process_0": 25440.145692358943
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 162.30850104886895,
                        "ram_power_avg": 0.6888763904571533,
                        "cpu_energy_total": 0.0055091648717848305,
                        "gpu_energy_total": 0.007984047220566337,
                        "ram_energy_total": 2.0518737803222315e-05,
                        "total_energy_kwh": 0.013513730830154396,
                        "total_energy_joules": 48649.43098855583
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.3367768063691049,
                        "joules_per_token": 2.969325621860097,
                        "flops_per_joule": 348410467.4757505,
                        "joules_per_flop": 2.870177831467134e-09
                    },
                    "per-process_emissions": [
                        0.0024559936759961675,
                        0.00269206208375115
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0370": {
            "setup": {
                "experiment_id": "0370",
                "date_time": "April 12, 2025 at 10:35:01 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 74.79167292402417,
                        "average_latency_ms_per_batch": 9348.959115503021,
                        "throughput_queries_per_sec": 1.7114204696293742,
                        "throughput_tokens_per_sec": 219.0618201125599
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 66.2,
                        "cpu_memory_usage_bytes": 1936392192
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0370",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 184.20290923757202,
                            "process_0": 206.78102984511708,
                            "process_2": 228.52068503691962,
                            "process_1": 248.66898730143282
                        },
                        "ram_power": {
                            "process_3": 0.7036442756652832,
                            "process_0": 0.6759939193725586,
                            "process_2": 0.7102274894714355,
                            "process_1": 0.703465461730957
                        },
                        "cpu_energy": {
                            "process_3": 0.0024931834879362213,
                            "process_0": 0.0023001478951891843,
                            "process_2": 0.002426014493564253,
                            "process_1": 0.002792999181689084
                        },
                        "gpu_energy": {
                            "process_3": 0.004484398031959991,
                            "process_0": 0.0041019449482200265,
                            "process_2": 0.004338721804309165,
                            "process_1": 0.005124423543979351
                        },
                        "ram_energy": {
                            "process_3": 9.970966974170687e-06,
                            "process_0": 9.076887165586985e-06,
                            "process_2": 9.620427072437513e-06,
                            "process_1": 1.1328772687830216e-05
                        },
                        "total_energy_kwh": {
                            "process_3": 0.006987552486870387,
                            "process_0": 0.006411169730574796,
                            "process_2": 0.006774356724945855,
                            "process_1": 0.007928751498356264
                        },
                        "total_energy_joules": {
                            "process_3": 25155.188952733395,
                            "process_0": 23080.211030069266,
                            "process_2": 24387.684209805077,
                            "process_1": 28543.50539408255
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 217.04340285526038,
                        "ram_power_avg": 0.6983327865600586,
                        "cpu_energy_total": 0.010012345058378742,
                        "gpu_energy_total": 0.018049488328468533,
                        "ram_energy_total": 3.99970539000254e-05,
                        "total_energy_kwh": 0.0281018304407473,
                        "total_energy_joules": 101166.58958669027
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.16195069999824843,
                        "joules_per_token": 6.174718602703264,
                        "flops_per_joule": 167545145.70867753,
                        "joules_per_flop": 5.96854057317047e-09
                    },
                    "per-process_emissions": [
                        0.0026619081198732742,
                        0.002442335108862469,
                        0.0025806911943681236,
                        0.003020457883298819
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0370": {
            "setup": {
                "experiment_id": "0370",
                "date_time": "April 12, 2025 at 10:35:01 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 74.79167292402417,
                        "average_latency_ms_per_batch": 9348.959115503021,
                        "throughput_queries_per_sec": 1.7114204696293742,
                        "throughput_tokens_per_sec": 219.0618201125599
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 66.2,
                        "cpu_memory_usage_bytes": 1936392192
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0370",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 184.20290923757202,
                            "process_0": 206.78102984511708,
                            "process_2": 228.52068503691962,
                            "process_1": 248.66898730143282
                        },
                        "ram_power": {
                            "process_3": 0.7036442756652832,
                            "process_0": 0.6759939193725586,
                            "process_2": 0.7102274894714355,
                            "process_1": 0.703465461730957
                        },
                        "cpu_energy": {
                            "process_3": 0.0024931834879362213,
                            "process_0": 0.0023001478951891843,
                            "process_2": 0.002426014493564253,
                            "process_1": 0.002792999181689084
                        },
                        "gpu_energy": {
                            "process_3": 0.004484398031959991,
                            "process_0": 0.0041019449482200265,
                            "process_2": 0.004338721804309165,
                            "process_1": 0.005124423543979351
                        },
                        "ram_energy": {
                            "process_3": 9.970966974170687e-06,
                            "process_0": 9.076887165586985e-06,
                            "process_2": 9.620427072437513e-06,
                            "process_1": 1.1328772687830216e-05
                        },
                        "total_energy_kwh": {
                            "process_3": 0.006987552486870387,
                            "process_0": 0.006411169730574796,
                            "process_2": 0.006774356724945855,
                            "process_1": 0.007928751498356264
                        },
                        "total_energy_joules": {
                            "process_3": 25155.188952733395,
                            "process_0": 23080.211030069266,
                            "process_2": 24387.684209805077,
                            "process_1": 28543.50539408255
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 217.04340285526038,
                        "ram_power_avg": 0.6983327865600586,
                        "cpu_energy_total": 0.010012345058378742,
                        "gpu_energy_total": 0.018049488328468533,
                        "ram_energy_total": 3.99970539000254e-05,
                        "total_energy_kwh": 0.0281018304407473,
                        "total_energy_joules": 101166.58958669027
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.16195069999824843,
                        "joules_per_token": 6.174718602703264,
                        "flops_per_joule": 167545145.70867753,
                        "joules_per_flop": 5.96854057317047e-09
                    },
                    "per-process_emissions": [
                        0.0026619081198732742,
                        0.002442335108862469,
                        0.0025806911943681236,
                        0.003020457883298819
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0371": {
            "setup": {
                "experiment_id": "0371",
                "date_time": "April 12, 2025 at 10:39:06 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 15368
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 155.38043772501987,
                        "average_latency_ms_per_batch": 9711.277357813742,
                        "throughput_queries_per_sec": 0.8237845244491097,
                        "throughput_tokens_per_sec": 98.90562946667123
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609963008,
                        "gpu_max_memory_allocated_bytes": 1609963008,
                        "gpu_current_memory_reserved_bytes": 2162163712,
                        "gpu_max_memory_reserved_bytes": 2162163712
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 61.5,
                        "cpu_memory_usage_bytes": 1932836864
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0371",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 456.2786124439094,
                            "process_3": 221.6035438506992,
                            "process_0": 209.91409164547687,
                            "process_1": 283.5500797379344
                        },
                        "ram_power": {
                            "process_2": 0.7069272994995117,
                            "process_3": 0.7065110206604004,
                            "process_0": 0.6747522354125977,
                            "process_1": 0.7110328674316406
                        },
                        "cpu_energy": {
                            "process_2": 0.004606606595873928,
                            "process_3": 0.004851993001658229,
                            "process_0": 0.004771999502372184,
                            "process_1": 0.0051331640136886595
                        },
                        "gpu_energy": {
                            "process_2": 0.007735243965968586,
                            "process_3": 0.008171945704218642,
                            "process_0": 0.008024101697053698,
                            "process_1": 0.008761984787360566
                        },
                        "ram_energy": {
                            "process_2": 1.7974464672099356e-05,
                            "process_3": 1.8439046183989146e-05,
                            "process_0": 1.7334691452412996e-05,
                            "process_1": 2.0558302802124817e-05
                        },
                        "total_energy_kwh": {
                            "process_2": 0.012359825026514613,
                            "process_3": 0.01304237775206086,
                            "process_0": 0.012813435890878293,
                            "process_1": 0.013915707103851342
                        },
                        "total_energy_joules": {
                            "process_2": 44495.37009545261,
                            "process_3": 46952.5599074191,
                            "process_0": 46128.36920716186,
                            "process_1": 50096.54557386483
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 292.83658191950497,
                        "ram_power_avg": 0.6998058557510376,
                        "cpu_energy_total": 0.019363763113593002,
                        "gpu_energy_total": 0.03269327615460149,
                        "ram_energy_total": 7.430650511062632e-05,
                        "total_energy_kwh": 0.05213134577330511,
                        "total_energy_joules": 187672.84478389838
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0818871798831416,
                        "joules_per_token": 12.21192378864513,
                        "flops_per_joule": 90316587.95746161,
                        "joules_per_flop": 1.1072163183035567e-08
                    },
                    "per-process_emissions": [
                        0.004708475343850742,
                        0.004968493804647585,
                        0.004881278402630086,
                        0.005301188621212169
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0371": {
            "setup": {
                "experiment_id": "0371",
                "date_time": "April 12, 2025 at 10:39:06 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 15368
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 155.38043772501987,
                        "average_latency_ms_per_batch": 9711.277357813742,
                        "throughput_queries_per_sec": 0.8237845244491097,
                        "throughput_tokens_per_sec": 98.90562946667123
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609963008,
                        "gpu_max_memory_allocated_bytes": 1609963008,
                        "gpu_current_memory_reserved_bytes": 2162163712,
                        "gpu_max_memory_reserved_bytes": 2162163712
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 61.5,
                        "cpu_memory_usage_bytes": 1932836864
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0371",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 456.2786124439094,
                            "process_3": 221.6035438506992,
                            "process_0": 209.91409164547687,
                            "process_1": 283.5500797379344
                        },
                        "ram_power": {
                            "process_2": 0.7069272994995117,
                            "process_3": 0.7065110206604004,
                            "process_0": 0.6747522354125977,
                            "process_1": 0.7110328674316406
                        },
                        "cpu_energy": {
                            "process_2": 0.004606606595873928,
                            "process_3": 0.004851993001658229,
                            "process_0": 0.004771999502372184,
                            "process_1": 0.0051331640136886595
                        },
                        "gpu_energy": {
                            "process_2": 0.007735243965968586,
                            "process_3": 0.008171945704218642,
                            "process_0": 0.008024101697053698,
                            "process_1": 0.008761984787360566
                        },
                        "ram_energy": {
                            "process_2": 1.7974464672099356e-05,
                            "process_3": 1.8439046183989146e-05,
                            "process_0": 1.7334691452412996e-05,
                            "process_1": 2.0558302802124817e-05
                        },
                        "total_energy_kwh": {
                            "process_2": 0.012359825026514613,
                            "process_3": 0.01304237775206086,
                            "process_0": 0.012813435890878293,
                            "process_1": 0.013915707103851342
                        },
                        "total_energy_joules": {
                            "process_2": 44495.37009545261,
                            "process_3": 46952.5599074191,
                            "process_0": 46128.36920716186,
                            "process_1": 50096.54557386483
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 292.83658191950497,
                        "ram_power_avg": 0.6998058557510376,
                        "cpu_energy_total": 0.019363763113593002,
                        "gpu_energy_total": 0.03269327615460149,
                        "ram_energy_total": 7.430650511062632e-05,
                        "total_energy_kwh": 0.05213134577330511,
                        "total_energy_joules": 187672.84478389838
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.0818871798831416,
                        "joules_per_token": 12.21192378864513,
                        "flops_per_joule": 90316587.95746161,
                        "joules_per_flop": 1.1072163183035567e-08
                    },
                    "per-process_emissions": [
                        0.004708475343850742,
                        0.004968493804647585,
                        0.004881278402630086,
                        0.005301188621212169
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0372": {
            "setup": {
                "experiment_id": "0372",
                "date_time": "April 12, 2025 at 10:41:30 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_3",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 3
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 71.19548901300004,
                        "average_latency_ms_per_batch": 8899.436126625005,
                        "throughput_queries_per_sec": 1.7978667156373862,
                        "throughput_tokens_per_sec": 230.12693960158543
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            0.0
                        ],
                        "cpu_usage_percent": 57.7,
                        "cpu_memory_usage_bytes": 1927921664
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0372",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 173.92476428313964,
                            "process_1": 229.06720485142023,
                            "process_2": 194.27193762427052
                        },
                        "ram_power": {
                            "process_0": 0.6731300354003906,
                            "process_1": 0.7089257240295411,
                            "process_2": 0.7104120254516602
                        },
                        "cpu_energy": {
                            "process_0": 0.0021935728217490585,
                            "process_1": 0.001920820514311345,
                            "process_2": 0.001709183523593311
                        },
                        "gpu_energy": {
                            "process_0": 0.00413024691530639,
                            "process_1": 0.0036450626382695805,
                            "process_2": 0.003256504549646877
                        },
                        "ram_energy": {
                            "process_0": 8.602473819067454e-06,
                            "process_1": 8.371039266471178e-06,
                            "process_2": 7.069619804793643e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.006332422210874515,
                            "process_1": 0.005574254191847396,
                            "process_2": 0.00497275769304498
                        },
                        "total_energy_joules": {
                            "process_0": 22796.719959148253,
                            "process_1": 20067.315090650627,
                            "process_2": 17901.92769496193
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 199.08796891961015,
                        "ram_power_avg": 0.6974892616271973,
                        "cpu_energy_total": 0.005823576859653715,
                        "gpu_energy_total": 0.011031814103222848,
                        "ram_energy_total": 2.4043132890332273e-05,
                        "total_energy_kwh": 0.016879434095766893,
                        "total_energy_joules": 60765.962744760815
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2696246263524001,
                        "joules_per_token": 3.7088600308081556,
                        "flops_per_joule": 278938573.9570696,
                        "joules_per_flop": 3.5850186864220016e-09
                    },
                    "per-process_emissions": [
                        0.0024123362412326464,
                        0.0021235121343842656,
                        0.0018943720431654854
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0372": {
            "setup": {
                "experiment_id": "0372",
                "date_time": "April 12, 2025 at 10:41:30 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_3",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 3
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 71.19548901300004,
                        "average_latency_ms_per_batch": 8899.436126625005,
                        "throughput_queries_per_sec": 1.7978667156373862,
                        "throughput_tokens_per_sec": 230.12693960158543
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            0.0
                        ],
                        "cpu_usage_percent": 57.7,
                        "cpu_memory_usage_bytes": 1927921664
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0372",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 173.92476428313964,
                            "process_1": 229.06720485142023,
                            "process_2": 194.27193762427052
                        },
                        "ram_power": {
                            "process_0": 0.6731300354003906,
                            "process_1": 0.7089257240295411,
                            "process_2": 0.7104120254516602
                        },
                        "cpu_energy": {
                            "process_0": 0.0021935728217490585,
                            "process_1": 0.001920820514311345,
                            "process_2": 0.001709183523593311
                        },
                        "gpu_energy": {
                            "process_0": 0.00413024691530639,
                            "process_1": 0.0036450626382695805,
                            "process_2": 0.003256504549646877
                        },
                        "ram_energy": {
                            "process_0": 8.602473819067454e-06,
                            "process_1": 8.371039266471178e-06,
                            "process_2": 7.069619804793643e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.006332422210874515,
                            "process_1": 0.005574254191847396,
                            "process_2": 0.00497275769304498
                        },
                        "total_energy_joules": {
                            "process_0": 22796.719959148253,
                            "process_1": 20067.315090650627,
                            "process_2": 17901.92769496193
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 199.08796891961015,
                        "ram_power_avg": 0.6974892616271973,
                        "cpu_energy_total": 0.005823576859653715,
                        "gpu_energy_total": 0.011031814103222848,
                        "ram_energy_total": 2.4043132890332273e-05,
                        "total_energy_kwh": 0.016879434095766893,
                        "total_energy_joules": 60765.962744760815
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2696246263524001,
                        "joules_per_token": 3.7088600308081556,
                        "flops_per_joule": 278938573.9570696,
                        "joules_per_flop": 3.5850186864220016e-09
                    },
                    "per-process_emissions": [
                        0.0024123362412326464,
                        0.0021235121343842656,
                        0.0018943720431654854
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0373": {
            "setup": {
                "experiment_id": "0373",
                "date_time": "April 12, 2025 at 10:44:09 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 54.54915047400573,
                        "average_latency_ms_per_batch": 6818.643809250716,
                        "throughput_queries_per_sec": 2.3465076703806003,
                        "throughput_tokens_per_sec": 300.35298180871683
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 58.8,
                        "cpu_memory_usage_bytes": 1952161792
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0373",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 219.69164937314716,
                            "process_2": 221.68403577596666,
                            "process_1": 242.27589447426985,
                            "process_3": 218.97595820244248
                        },
                        "ram_power": {
                            "process_0": 0.6815958023071289,
                            "process_2": 0.7048230171203613,
                            "process_1": 0.705681324005127,
                            "process_3": 0.7042121887207031
                        },
                        "cpu_energy": {
                            "process_0": 0.0016825566485927086,
                            "process_2": 0.0024820939764072136,
                            "process_1": 0.0026131710782815384,
                            "process_3": 0.0018776430215643818
                        },
                        "gpu_energy": {
                            "process_0": 0.0034631511038523133,
                            "process_2": 0.004963871193315983,
                            "process_1": 0.005230780851288586,
                            "process_3": 0.0038235680588523113
                        },
                        "ram_energy": {
                            "process_0": 6.541298383091977e-06,
                            "process_2": 1.0023896096041505e-05,
                            "process_1": 1.0462898477068963e-05,
                            "process_3": 7.82278656589239e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0051522490508281145,
                            "process_2": 0.007455989065819237,
                            "process_1": 0.007854414828047194,
                            "process_3": 0.005709033866982586
                        },
                        "total_energy_joules": {
                            "process_0": 18548.096582981212,
                            "process_2": 26841.560636949253,
                            "process_1": 28275.893380969897,
                            "process_3": 20552.52192113731
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 225.65688445645654,
                        "ram_power_avg": 0.6990780830383301,
                        "cpu_energy_total": 0.008655464724845843,
                        "gpu_energy_total": 0.017481371207309193,
                        "ram_energy_total": 3.485087952209484e-05,
                        "total_energy_kwh": 0.02617168681167713,
                        "total_energy_joules": 94218.07252203766
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.17389445104778356,
                        "joules_per_token": 5.7506147779564,
                        "flops_per_joule": 179901483.22326794,
                        "joules_per_flop": 5.55859786191392e-09
                    },
                    "per-process_emissions": [
                        0.00196274927591297,
                        0.0028403590346238383,
                        0.0029921393287445784,
                        0.0021748564516270164
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0373": {
            "setup": {
                "experiment_id": "0373",
                "date_time": "April 12, 2025 at 10:44:09 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 54.54915047400573,
                        "average_latency_ms_per_batch": 6818.643809250716,
                        "throughput_queries_per_sec": 2.3465076703806003,
                        "throughput_tokens_per_sec": 300.35298180871683
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 58.8,
                        "cpu_memory_usage_bytes": 1952161792
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0373",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 219.69164937314716,
                            "process_2": 221.68403577596666,
                            "process_1": 242.27589447426985,
                            "process_3": 218.97595820244248
                        },
                        "ram_power": {
                            "process_0": 0.6815958023071289,
                            "process_2": 0.7048230171203613,
                            "process_1": 0.705681324005127,
                            "process_3": 0.7042121887207031
                        },
                        "cpu_energy": {
                            "process_0": 0.0016825566485927086,
                            "process_2": 0.0024820939764072136,
                            "process_1": 0.0026131710782815384,
                            "process_3": 0.0018776430215643818
                        },
                        "gpu_energy": {
                            "process_0": 0.0034631511038523133,
                            "process_2": 0.004963871193315983,
                            "process_1": 0.005230780851288586,
                            "process_3": 0.0038235680588523113
                        },
                        "ram_energy": {
                            "process_0": 6.541298383091977e-06,
                            "process_2": 1.0023896096041505e-05,
                            "process_1": 1.0462898477068963e-05,
                            "process_3": 7.82278656589239e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0051522490508281145,
                            "process_2": 0.007455989065819237,
                            "process_1": 0.007854414828047194,
                            "process_3": 0.005709033866982586
                        },
                        "total_energy_joules": {
                            "process_0": 18548.096582981212,
                            "process_2": 26841.560636949253,
                            "process_1": 28275.893380969897,
                            "process_3": 20552.52192113731
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 225.65688445645654,
                        "ram_power_avg": 0.6990780830383301,
                        "cpu_energy_total": 0.008655464724845843,
                        "gpu_energy_total": 0.017481371207309193,
                        "ram_energy_total": 3.485087952209484e-05,
                        "total_energy_kwh": 0.02617168681167713,
                        "total_energy_joules": 94218.07252203766
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.17389445104778356,
                        "joules_per_token": 5.7506147779564,
                        "flops_per_joule": 179901483.22326794,
                        "joules_per_flop": 5.55859786191392e-09
                    },
                    "per-process_emissions": [
                        0.00196274927591297,
                        0.0028403590346238383,
                        0.0029921393287445784,
                        0.0021748564516270164
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0374": {
            "setup": {
                "experiment_id": "0374",
                "date_time": "April 12, 2025 at 10:45:50 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_1",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 51.46769522198883,
                        "average_latency_ms_per_batch": 6433.461902748604,
                        "throughput_queries_per_sec": 2.486996929781185,
                        "throughput_tokens_per_sec": 318.33560701199167
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 1906311168,
                        "gpu_max_memory_reserved_bytes": 1906311168
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 53.4,
                        "cpu_memory_usage_bytes": 1949032448
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0374",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 170.80214223087614
                        },
                        "ram_power": {
                            "process_0": 0.6804084777832031
                        },
                        "cpu_energy": {
                            "process_0": 0.001575507727562126
                        },
                        "gpu_energy": {
                            "process_0": 0.002325691582773848
                        },
                        "ram_energy": {
                            "process_0": 7.111859972663061e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.003908311170308637
                        },
                        "total_energy_joules": {
                            "process_0": 14069.920213111094
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 170.80214223087614,
                        "ram_power_avg": 0.6804084777832031,
                        "cpu_energy_total": 0.001575507727562126,
                        "gpu_energy_total": 0.002325691582773848,
                        "ram_energy_total": 7.111859972663061e-06,
                        "total_energy_kwh": 0.003908311170308637,
                        "total_energy_joules": 14069.920213111094
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.16447000067083,
                        "joules_per_token": 0.8587597786322689,
                        "flops_per_joule": 1204695601.4261632,
                        "joules_per_flop": 8.300852089242818e-10
                    },
                    "per-process_emissions": [
                        0.0014888711403290753
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0374": {
            "setup": {
                "experiment_id": "0374",
                "date_time": "April 12, 2025 at 10:45:50 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_1",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 51.46769522198883,
                        "average_latency_ms_per_batch": 6433.461902748604,
                        "throughput_queries_per_sec": 2.486996929781185,
                        "throughput_tokens_per_sec": 318.33560701199167
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 1906311168,
                        "gpu_max_memory_reserved_bytes": 1906311168
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 53.4,
                        "cpu_memory_usage_bytes": 1949032448
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0374",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 170.80214223087614
                        },
                        "ram_power": {
                            "process_0": 0.6804084777832031
                        },
                        "cpu_energy": {
                            "process_0": 0.001575507727562126
                        },
                        "gpu_energy": {
                            "process_0": 0.002325691582773848
                        },
                        "ram_energy": {
                            "process_0": 7.111859972663061e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.003908311170308637
                        },
                        "total_energy_joules": {
                            "process_0": 14069.920213111094
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 170.80214223087614,
                        "ram_power_avg": 0.6804084777832031,
                        "cpu_energy_total": 0.001575507727562126,
                        "gpu_energy_total": 0.002325691582773848,
                        "ram_energy_total": 7.111859972663061e-06,
                        "total_energy_kwh": 0.003908311170308637,
                        "total_energy_joules": 14069.920213111094
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.16447000067083,
                        "joules_per_token": 0.8587597786322689,
                        "flops_per_joule": 1204695601.4261632,
                        "joules_per_flop": 8.300852089242818e-10
                    },
                    "per-process_emissions": [
                        0.0014888711403290753
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0375": {
            "setup": {
                "experiment_id": "0375",
                "date_time": "April 12, 2025 at 10:53:52 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 2,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 13316
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 346.01829330195324,
                        "average_latency_ms_per_batch": 5406.535832843019,
                        "throughput_queries_per_sec": 0.369922638420451,
                        "throughput_tokens_per_sec": 38.48351447817754
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609951744,
                        "gpu_max_memory_allocated_bytes": 1609951744,
                        "gpu_current_memory_reserved_bytes": 2153775104,
                        "gpu_max_memory_reserved_bytes": 2153775104
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 47.0,
                        "cpu_memory_usage_bytes": 1930141696
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0375",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 230.2732420862348,
                            "process_3": 231.85320951069932,
                            "process_1": 338.0126586193921,
                            "process_0": 165.7792290407671
                        },
                        "ram_power": {
                            "process_2": 0.7064065933227539,
                            "process_3": 0.7046141624450684,
                            "process_1": 0.7044296264648438,
                            "process_0": 0.6738109588623047
                        },
                        "cpu_energy": {
                            "process_2": 0.010569813363501001,
                            "process_3": 0.010667391607871647,
                            "process_1": 0.012835925866404512,
                            "process_0": 0.010614340196593273
                        },
                        "gpu_energy": {
                            "process_2": 0.0193837266180783,
                            "process_3": 0.01962022347394332,
                            "process_1": 0.024063443972961673,
                            "process_0": 0.019516357557517683
                        },
                        "ram_energy": {
                            "process_2": 4.507348210062548e-05,
                            "process_3": 4.6766954416608224e-05,
                            "process_1": 5.3534360231740934e-05,
                            "process_0": 4.3815117324245974e-05
                        },
                        "total_energy_kwh": {
                            "process_2": 0.029998613463679928,
                            "process_3": 0.030334382036231584,
                            "process_1": 0.036952904199597965,
                            "process_0": 0.03017451287143517
                        },
                        "total_energy_joules": {
                            "process_2": 107995.00846924774,
                            "process_3": 109203.7753304337,
                            "process_1": 133030.45511855267,
                            "process_0": 108628.24633716661
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 241.4795848142733,
                        "ram_power_avg": 0.6973153352737427,
                        "cpu_energy_total": 0.04468747103437043,
                        "gpu_energy_total": 0.08258375162250098,
                        "ram_energy_total": 0.0001891899140732206,
                        "total_energy_kwh": 0.12746041257094465,
                        "total_energy_joules": 458857.48525540077
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.02901990362560675,
                        "joules_per_token": 34.45910823486037,
                        "flops_per_joule": 36939510.7147868,
                        "joules_per_flop": 2.7071284395754124e-08
                    },
                    "per-process_emissions": [
                        0.011427971798988868,
                        0.011555882836702423,
                        0.014077208854836844,
                        0.01149498067837323
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0375": {
            "setup": {
                "experiment_id": "0375",
                "date_time": "April 12, 2025 at 10:53:52 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 2,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 13316
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 346.01829330195324,
                        "average_latency_ms_per_batch": 5406.535832843019,
                        "throughput_queries_per_sec": 0.369922638420451,
                        "throughput_tokens_per_sec": 38.48351447817754
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609951744,
                        "gpu_max_memory_allocated_bytes": 1609951744,
                        "gpu_current_memory_reserved_bytes": 2153775104,
                        "gpu_max_memory_reserved_bytes": 2153775104
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 47.0,
                        "cpu_memory_usage_bytes": 1930141696
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0375",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 230.2732420862348,
                            "process_3": 231.85320951069932,
                            "process_1": 338.0126586193921,
                            "process_0": 165.7792290407671
                        },
                        "ram_power": {
                            "process_2": 0.7064065933227539,
                            "process_3": 0.7046141624450684,
                            "process_1": 0.7044296264648438,
                            "process_0": 0.6738109588623047
                        },
                        "cpu_energy": {
                            "process_2": 0.010569813363501001,
                            "process_3": 0.010667391607871647,
                            "process_1": 0.012835925866404512,
                            "process_0": 0.010614340196593273
                        },
                        "gpu_energy": {
                            "process_2": 0.0193837266180783,
                            "process_3": 0.01962022347394332,
                            "process_1": 0.024063443972961673,
                            "process_0": 0.019516357557517683
                        },
                        "ram_energy": {
                            "process_2": 4.507348210062548e-05,
                            "process_3": 4.6766954416608224e-05,
                            "process_1": 5.3534360231740934e-05,
                            "process_0": 4.3815117324245974e-05
                        },
                        "total_energy_kwh": {
                            "process_2": 0.029998613463679928,
                            "process_3": 0.030334382036231584,
                            "process_1": 0.036952904199597965,
                            "process_0": 0.03017451287143517
                        },
                        "total_energy_joules": {
                            "process_2": 107995.00846924774,
                            "process_3": 109203.7753304337,
                            "process_1": 133030.45511855267,
                            "process_0": 108628.24633716661
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 241.4795848142733,
                        "ram_power_avg": 0.6973153352737427,
                        "cpu_energy_total": 0.04468747103437043,
                        "gpu_energy_total": 0.08258375162250098,
                        "ram_energy_total": 0.0001891899140732206,
                        "total_energy_kwh": 0.12746041257094465,
                        "total_energy_joules": 458857.48525540077
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.02901990362560675,
                        "joules_per_token": 34.45910823486037,
                        "flops_per_joule": 36939510.7147868,
                        "joules_per_flop": 2.7071284395754124e-08
                    },
                    "per-process_emissions": [
                        0.011427971798988868,
                        0.011555882836702423,
                        0.014077208854836844,
                        0.01149498067837323
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0376": {
            "setup": {
                "experiment_id": "0376",
                "date_time": "April 12, 2025 at 10:55:41 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 44.596077145048184,
                        "average_latency_ms_per_batch": 5574.509643131023,
                        "throughput_queries_per_sec": 2.8702076100478884,
                        "throughput_tokens_per_sec": 367.3865740861297
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 43.1,
                        "cpu_memory_usage_bytes": 1929682944
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0376",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 267.12961395502697,
                            "process_3": 256.93830346073463,
                            "process_2": 388.3205661857658,
                            "process_0": 295.7234698701622
                        },
                        "ram_power": {
                            "process_1": 0.7097940444946289,
                            "process_3": 0.7092890739440918,
                            "process_2": 0.7043924331665039,
                            "process_0": 0.6737451553344728
                        },
                        "cpu_energy": {
                            "process_1": 0.0014938066915347015,
                            "process_3": 0.0014653339852784485,
                            "process_2": 0.0015699562715344652,
                            "process_0": 0.0013779912688742118
                        },
                        "gpu_energy": {
                            "process_1": 0.0037894321982090418,
                            "process_3": 0.0037256710360891176,
                            "process_2": 0.003979969572861952,
                            "process_0": 0.0035302033797151022
                        },
                        "ram_energy": {
                            "process_1": 6.312039657878627e-06,
                            "process_3": 6.3947600245494084e-06,
                            "process_2": 6.728224344272005e-06,
                            "process_0": 5.55332350689436e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.00528955092940162,
                            "process_3": 0.005197399781392116,
                            "process_2": 0.005556654068740689,
                            "process_0": 0.004913747972096209
                        },
                        "total_energy_joules": {
                            "process_1": 19042.383345845832,
                            "process_3": 18710.63921301162,
                            "process_2": 20003.95464746648,
                            "process_0": 17689.49269954635
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 302.02798836792243,
                        "ram_power_avg": 0.6993051767349243,
                        "cpu_energy_total": 0.005907088217221827,
                        "gpu_energy_total": 0.015025276186875214,
                        "ram_energy_total": 2.4988347533594403e-05,
                        "total_energy_kwh": 0.020957352751630635,
                        "total_energy_joules": 75446.46990587028
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21716059108453006,
                        "joules_per_token": 4.6048870792157155,
                        "flops_per_joule": 224662214.33950973,
                        "joules_per_flop": 4.451126785783385e-09
                    },
                    "per-process_emissions": [
                        0.0020150544265555475,
                        0.0019799494467213265,
                        0.0021168073674867655,
                        0.0018718922899700508
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0376": {
            "setup": {
                "experiment_id": "0376",
                "date_time": "April 12, 2025 at 10:55:41 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 44.596077145048184,
                        "average_latency_ms_per_batch": 5574.509643131023,
                        "throughput_queries_per_sec": 2.8702076100478884,
                        "throughput_tokens_per_sec": 367.3865740861297
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 43.1,
                        "cpu_memory_usage_bytes": 1929682944
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0376",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 267.12961395502697,
                            "process_3": 256.93830346073463,
                            "process_2": 388.3205661857658,
                            "process_0": 295.7234698701622
                        },
                        "ram_power": {
                            "process_1": 0.7097940444946289,
                            "process_3": 0.7092890739440918,
                            "process_2": 0.7043924331665039,
                            "process_0": 0.6737451553344728
                        },
                        "cpu_energy": {
                            "process_1": 0.0014938066915347015,
                            "process_3": 0.0014653339852784485,
                            "process_2": 0.0015699562715344652,
                            "process_0": 0.0013779912688742118
                        },
                        "gpu_energy": {
                            "process_1": 0.0037894321982090418,
                            "process_3": 0.0037256710360891176,
                            "process_2": 0.003979969572861952,
                            "process_0": 0.0035302033797151022
                        },
                        "ram_energy": {
                            "process_1": 6.312039657878627e-06,
                            "process_3": 6.3947600245494084e-06,
                            "process_2": 6.728224344272005e-06,
                            "process_0": 5.55332350689436e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.00528955092940162,
                            "process_3": 0.005197399781392116,
                            "process_2": 0.005556654068740689,
                            "process_0": 0.004913747972096209
                        },
                        "total_energy_joules": {
                            "process_1": 19042.383345845832,
                            "process_3": 18710.63921301162,
                            "process_2": 20003.95464746648,
                            "process_0": 17689.49269954635
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 302.02798836792243,
                        "ram_power_avg": 0.6993051767349243,
                        "cpu_energy_total": 0.005907088217221827,
                        "gpu_energy_total": 0.015025276186875214,
                        "ram_energy_total": 2.4988347533594403e-05,
                        "total_energy_kwh": 0.020957352751630635,
                        "total_energy_joules": 75446.46990587028
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21716059108453006,
                        "joules_per_token": 4.6048870792157155,
                        "flops_per_joule": 224662214.33950973,
                        "joules_per_flop": 4.451126785783385e-09
                    },
                    "per-process_emissions": [
                        0.0020150544265555475,
                        0.0019799494467213265,
                        0.0021168073674867655,
                        0.0018718922899700508
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0377": {
            "setup": {
                "experiment_id": "0377",
                "date_time": "April 12, 2025 at 10:58:06 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precis_float16_quant_True_quant8_True_quant4_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 91.11832809992484,
                        "average_latency_ms_per_batch": 11389.791012490605,
                        "throughput_queries_per_sec": 1.404766776006128,
                        "throughput_tokens_per_sec": 179.81014732878438
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 2039254528,
                        "gpu_max_memory_allocated_bytes": 2039254528,
                        "gpu_current_memory_reserved_bytes": 3021996032,
                        "gpu_max_memory_reserved_bytes": 3021996032
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.6,
                        "cpu_memory_usage_bytes": 1969041408
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0377",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 194.4067763433872,
                            "process_3": 181.92185756493663,
                            "process_1": 269.38560894180097,
                            "process_0": 350.516955912512
                        },
                        "ram_power": {
                            "process_2": 0.7198162078857422,
                            "process_3": 0.7204070091247559,
                            "process_1": 0.7222509384155273,
                            "process_0": 0.6873965263366699
                        },
                        "cpu_energy": {
                            "process_2": 0.002742715633156876,
                            "process_3": 0.0027589259838759973,
                            "process_1": 0.0027818678900621307,
                            "process_0": 0.0028034300860663285
                        },
                        "gpu_energy": {
                            "process_2": 0.004367300993838752,
                            "process_3": 0.004376210723188612,
                            "process_1": 0.004414148253538253,
                            "process_0": 0.004449608559683904
                        },
                        "ram_energy": {
                            "process_2": 1.2036975015633689e-05,
                            "process_3": 1.2373639027008377e-05,
                            "process_1": 1.2296645910491351e-05,
                            "process_0": 1.1901716187391793e-05
                        },
                        "total_energy_kwh": {
                            "process_2": 0.007122053602011263,
                            "process_3": 0.00714751034609162,
                            "process_1": 0.007208312789510876,
                            "process_0": 0.007264940361937628
                        },
                        "total_energy_joules": {
                            "process_2": 25639.392967240547,
                            "process_3": 25731.03724592983,
                            "process_1": 25949.926042239153,
                            "process_0": 26153.785302975462
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 249.05779969065918,
                        "ram_power_avg": 0.7124676704406738,
                        "cpu_energy_total": 0.011086939593161333,
                        "gpu_energy_total": 0.01760726853024952,
                        "ram_energy_total": 4.860897614052521e-05,
                        "total_energy_kwh": 0.028742817099551385,
                        "total_energy_joules": 103474.14155838499
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.15833907634551744,
                        "joules_per_token": 6.315560397850646,
                        "flops_per_joule": 163808761.66619876,
                        "joules_per_flop": 6.10467956553966e-09
                    },
                    "per-process_emissions": [
                        0.0027131463196861908,
                        0.002722844066343603,
                        0.0027460067571641682,
                        0.0027675790308801396
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0377": {
            "setup": {
                "experiment_id": "0377",
                "date_time": "April 12, 2025 at 10:58:06 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precis_float16_quant_True_quant8_True_quant4_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 91.11832809992484,
                        "average_latency_ms_per_batch": 11389.791012490605,
                        "throughput_queries_per_sec": 1.404766776006128,
                        "throughput_tokens_per_sec": 179.81014732878438
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 2039254528,
                        "gpu_max_memory_allocated_bytes": 2039254528,
                        "gpu_current_memory_reserved_bytes": 3021996032,
                        "gpu_max_memory_reserved_bytes": 3021996032
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.6,
                        "cpu_memory_usage_bytes": 1969041408
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0377",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 194.4067763433872,
                            "process_3": 181.92185756493663,
                            "process_1": 269.38560894180097,
                            "process_0": 350.516955912512
                        },
                        "ram_power": {
                            "process_2": 0.7198162078857422,
                            "process_3": 0.7204070091247559,
                            "process_1": 0.7222509384155273,
                            "process_0": 0.6873965263366699
                        },
                        "cpu_energy": {
                            "process_2": 0.002742715633156876,
                            "process_3": 0.0027589259838759973,
                            "process_1": 0.0027818678900621307,
                            "process_0": 0.0028034300860663285
                        },
                        "gpu_energy": {
                            "process_2": 0.004367300993838752,
                            "process_3": 0.004376210723188612,
                            "process_1": 0.004414148253538253,
                            "process_0": 0.004449608559683904
                        },
                        "ram_energy": {
                            "process_2": 1.2036975015633689e-05,
                            "process_3": 1.2373639027008377e-05,
                            "process_1": 1.2296645910491351e-05,
                            "process_0": 1.1901716187391793e-05
                        },
                        "total_energy_kwh": {
                            "process_2": 0.007122053602011263,
                            "process_3": 0.00714751034609162,
                            "process_1": 0.007208312789510876,
                            "process_0": 0.007264940361937628
                        },
                        "total_energy_joules": {
                            "process_2": 25639.392967240547,
                            "process_3": 25731.03724592983,
                            "process_1": 25949.926042239153,
                            "process_0": 26153.785302975462
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 249.05779969065918,
                        "ram_power_avg": 0.7124676704406738,
                        "cpu_energy_total": 0.011086939593161333,
                        "gpu_energy_total": 0.01760726853024952,
                        "ram_energy_total": 4.860897614052521e-05,
                        "total_energy_kwh": 0.028742817099551385,
                        "total_energy_joules": 103474.14155838499
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.15833907634551744,
                        "joules_per_token": 6.315560397850646,
                        "flops_per_joule": 163808761.66619876,
                        "joules_per_flop": 6.10467956553966e-09
                    },
                    "per-process_emissions": [
                        0.0027131463196861908,
                        0.002722844066343603,
                        0.0027460067571641682,
                        0.0027675790308801396
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0378": {
            "setup": {
                "experiment_id": "0378",
                "date_time": "April 12, 2025 at 10:59:57 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_True_latency_0.05_latency_0.2_latency_True_latency_4.0_latency_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 52.28149952401873,
                        "average_latency_ms_per_batch": 6535.187440502341,
                        "throughput_queries_per_sec": 2.4482847884115357,
                        "throughput_tokens_per_sec": 313.3804529166766
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 43.5,
                        "cpu_memory_usage_bytes": 1928679424
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0378",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 271.36413222102203,
                            "process_3": 289.0097449957946,
                            "process_1": 300.47513066864377,
                            "process_2": 259.53357251376696
                        },
                        "ram_power": {
                            "process_0": 0.6733002662658691,
                            "process_3": 0.7104849815368653,
                            "process_1": 0.7055511474609376,
                            "process_2": 0.7054738998413086
                        },
                        "cpu_energy": {
                            "process_0": 0.0016151302295338613,
                            "process_3": 0.0015708297864002816,
                            "process_1": 0.0015382322316218058,
                            "process_2": 0.0016803636978138456
                        },
                        "gpu_energy": {
                            "process_0": 0.0038763400455124053,
                            "process_3": 0.003778197189221677,
                            "process_1": 0.003697219624439141,
                            "process_2": 0.0040235721077430675
                        },
                        "ram_energy": {
                            "process_0": 6.859629648548667e-06,
                            "process_3": 7.288752588589955e-06,
                            "process_1": 7.071778554138997e-06,
                            "process_2": 7.510613177426709e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005498329904694812,
                            "process_3": 0.005356315728210548,
                            "process_1": 0.005242523634615086,
                            "process_2": 0.005711446418734342
                        },
                        "total_energy_joules": {
                            "process_0": 19793.98765690132,
                            "process_3": 19282.73662155797,
                            "process_1": 18873.08508461431,
                            "process_2": 20561.20710744363
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 280.09564509980686,
                        "ram_power_avg": 0.6987025737762451,
                        "cpu_energy_total": 0.006404555945369794,
                        "gpu_energy_total": 0.01537532896691629,
                        "ram_energy_total": 2.873077396870433e-05,
                        "total_energy_kwh": 0.021808615686254786,
                        "total_energy_joules": 78511.01647051724
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20868408965450835,
                        "joules_per_token": 4.791932157624343,
                        "flops_per_joule": 215892899.55909714,
                        "joules_per_flop": 4.63192630254274e-09
                    },
                    "per-process_emissions": [
                        0.0020945887771934885,
                        0.002040488476661808,
                        0.001997139378606617,
                        0.0021757755132168475
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0378": {
            "setup": {
                "experiment_id": "0378",
                "date_time": "April 12, 2025 at 10:59:57 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_True_latency_0.05_latency_0.2_latency_True_latency_4.0_latency_5",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 52.28149952401873,
                        "average_latency_ms_per_batch": 6535.187440502341,
                        "throughput_queries_per_sec": 2.4482847884115357,
                        "throughput_tokens_per_sec": 313.3804529166766
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 43.5,
                        "cpu_memory_usage_bytes": 1928679424
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0378",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 271.36413222102203,
                            "process_3": 289.0097449957946,
                            "process_1": 300.47513066864377,
                            "process_2": 259.53357251376696
                        },
                        "ram_power": {
                            "process_0": 0.6733002662658691,
                            "process_3": 0.7104849815368653,
                            "process_1": 0.7055511474609376,
                            "process_2": 0.7054738998413086
                        },
                        "cpu_energy": {
                            "process_0": 0.0016151302295338613,
                            "process_3": 0.0015708297864002816,
                            "process_1": 0.0015382322316218058,
                            "process_2": 0.0016803636978138456
                        },
                        "gpu_energy": {
                            "process_0": 0.0038763400455124053,
                            "process_3": 0.003778197189221677,
                            "process_1": 0.003697219624439141,
                            "process_2": 0.0040235721077430675
                        },
                        "ram_energy": {
                            "process_0": 6.859629648548667e-06,
                            "process_3": 7.288752588589955e-06,
                            "process_1": 7.071778554138997e-06,
                            "process_2": 7.510613177426709e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005498329904694812,
                            "process_3": 0.005356315728210548,
                            "process_1": 0.005242523634615086,
                            "process_2": 0.005711446418734342
                        },
                        "total_energy_joules": {
                            "process_0": 19793.98765690132,
                            "process_3": 19282.73662155797,
                            "process_1": 18873.08508461431,
                            "process_2": 20561.20710744363
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 280.09564509980686,
                        "ram_power_avg": 0.6987025737762451,
                        "cpu_energy_total": 0.006404555945369794,
                        "gpu_energy_total": 0.01537532896691629,
                        "ram_energy_total": 2.873077396870433e-05,
                        "total_energy_kwh": 0.021808615686254786,
                        "total_energy_joules": 78511.01647051724
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.20868408965450835,
                        "joules_per_token": 4.791932157624343,
                        "flops_per_joule": 215892899.55909714,
                        "joules_per_flop": 4.63192630254274e-09
                    },
                    "per-process_emissions": [
                        0.0020945887771934885,
                        0.002040488476661808,
                        0.001997139378606617,
                        0.0021757755132168475
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0379": {
            "setup": {
                "experiment_id": "0379",
                "date_time": "April 12, 2025 at 11:01:47 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.69924069897388,
                        "average_latency_ms_per_batch": 5212.405087371735,
                        "throughput_queries_per_sec": 3.0696002578087658,
                        "throughput_tokens_per_sec": 392.908832999522
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.8,
                        "cpu_memory_usage_bytes": 1975156736
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0379",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 244.35109022032125,
                            "process_2": 233.5722634961714,
                            "process_0": 321.0375975403645,
                            "process_1": 263.81003427824504
                        },
                        "ram_power": {
                            "process_3": 0.7215371131896973,
                            "process_2": 0.7241621017456055,
                            "process_0": 0.6896266937255859,
                            "process_1": 0.725529670715332
                        },
                        "cpu_energy": {
                            "process_3": 0.0013849655138774325,
                            "process_2": 0.0016894882771812266,
                            "process_0": 0.001290782928000226,
                            "process_1": 0.0015683675262216643
                        },
                        "gpu_energy": {
                            "process_3": 0.003507111139019914,
                            "process_2": 0.004158246382148567,
                            "process_0": 0.0032838626270865756,
                            "process_1": 0.003906253402777837
                        },
                        "ram_energy": {
                            "process_3": 6.594133792472898e-06,
                            "process_2": 7.546745136131543e-06,
                            "process_0": 5.95458544533532e-06,
                            "process_1": 7.196096486767268e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.004898670786689819,
                            "process_2": 0.0058552814044659255,
                            "process_0": 0.004580600140532137,
                            "process_1": 0.005481817025486266
                        },
                        "total_energy_joules": {
                            "process_3": 17635.214832083348,
                            "process_2": 21079.013056077332,
                            "process_0": 16490.160505915694,
                            "process_1": 19734.541291750556
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 265.69274638377556,
                        "ram_power_avg": 0.7152138948440552,
                        "cpu_energy_total": 0.00593360424528055,
                        "gpu_energy_total": 0.014855473551032894,
                        "ram_energy_total": 2.7291560860707032e-05,
                        "total_energy_kwh": 0.020816369357174146,
                        "total_energy_joules": 74938.92968582692
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2186313584766701,
                        "joules_per_token": 4.57390928258221,
                        "flops_per_joule": 226183788.10870206,
                        "joules_per_flop": 4.421183358726879e-09
                    },
                    "per-process_emissions": [
                        0.0018661486361894867,
                        0.0022305694510312943,
                        0.0017449796235357177,
                        0.0020882981958589932
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0379": {
            "setup": {
                "experiment_id": "0379",
                "date_time": "April 12, 2025 at 11:01:47 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.69924069897388,
                        "average_latency_ms_per_batch": 5212.405087371735,
                        "throughput_queries_per_sec": 3.0696002578087658,
                        "throughput_tokens_per_sec": 392.908832999522
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.8,
                        "cpu_memory_usage_bytes": 1975156736
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0379",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 244.35109022032125,
                            "process_2": 233.5722634961714,
                            "process_0": 321.0375975403645,
                            "process_1": 263.81003427824504
                        },
                        "ram_power": {
                            "process_3": 0.7215371131896973,
                            "process_2": 0.7241621017456055,
                            "process_0": 0.6896266937255859,
                            "process_1": 0.725529670715332
                        },
                        "cpu_energy": {
                            "process_3": 0.0013849655138774325,
                            "process_2": 0.0016894882771812266,
                            "process_0": 0.001290782928000226,
                            "process_1": 0.0015683675262216643
                        },
                        "gpu_energy": {
                            "process_3": 0.003507111139019914,
                            "process_2": 0.004158246382148567,
                            "process_0": 0.0032838626270865756,
                            "process_1": 0.003906253402777837
                        },
                        "ram_energy": {
                            "process_3": 6.594133792472898e-06,
                            "process_2": 7.546745136131543e-06,
                            "process_0": 5.95458544533532e-06,
                            "process_1": 7.196096486767268e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.004898670786689819,
                            "process_2": 0.0058552814044659255,
                            "process_0": 0.004580600140532137,
                            "process_1": 0.005481817025486266
                        },
                        "total_energy_joules": {
                            "process_3": 17635.214832083348,
                            "process_2": 21079.013056077332,
                            "process_0": 16490.160505915694,
                            "process_1": 19734.541291750556
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 265.69274638377556,
                        "ram_power_avg": 0.7152138948440552,
                        "cpu_energy_total": 0.00593360424528055,
                        "gpu_energy_total": 0.014855473551032894,
                        "ram_energy_total": 2.7291560860707032e-05,
                        "total_energy_kwh": 0.020816369357174146,
                        "total_energy_joules": 74938.92968582692
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2186313584766701,
                        "joules_per_token": 4.57390928258221,
                        "flops_per_joule": 226183788.10870206,
                        "joules_per_flop": 4.421183358726879e-09
                    },
                    "per-process_emissions": [
                        0.0018661486361894867,
                        0.0022305694510312943,
                        0.0017449796235357177,
                        0.0020882981958589932
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0380": {
            "setup": {
                "experiment_id": "0380",
                "date_time": "April 12, 2025 at 11:03:32 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 47.48766216900549,
                        "average_latency_ms_per_batch": 5935.957771125686,
                        "throughput_queries_per_sec": 2.6954369651733194,
                        "throughput_tokens_per_sec": 345.0159315421849
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 42.4,
                        "cpu_memory_usage_bytes": 1954836480
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0380",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 254.84657895183,
                            "process_2": 252.04504086558956,
                            "process_0": 275.6523111985208,
                            "process_3": 249.18023399017963
                        },
                        "ram_power": {
                            "process_1": 0.7071418762207031,
                            "process_2": 0.7046499252319336,
                            "process_0": 0.6824355125427246,
                            "process_3": 0.7147822380065918
                        },
                        "cpu_energy": {
                            "process_1": 0.0015592543086295342,
                            "process_2": 0.0015253338289367094,
                            "process_0": 0.0014668728793740234,
                            "process_3": 0.0015408725408151442
                        },
                        "gpu_energy": {
                            "process_1": 0.0037218699219383478,
                            "process_2": 0.0035772361951207365,
                            "process_0": 0.003529684212634665,
                            "process_3": 0.0036844704475742507
                        },
                        "ram_energy": {
                            "process_1": 6.911445956287618e-06,
                            "process_2": 6.719070264073556e-06,
                            "process_0": 6.266690474105679e-06,
                            "process_3": 7.0997597061525095e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.00528803567652417,
                            "process_2": 0.00510928909432152,
                            "process_0": 0.005002823782482793,
                            "process_3": 0.005232442748095547
                        },
                        "total_energy_joules": {
                            "process_1": 19036.92843548701,
                            "process_2": 18393.44073955747,
                            "process_0": 18010.165616938055,
                            "process_3": 18836.793893143968
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 257.93104125153,
                        "ram_power_avg": 0.7022523880004883,
                        "cpu_energy_total": 0.0060923335577554106,
                        "gpu_energy_total": 0.014513260777268,
                        "ram_energy_total": 2.6996966400619363e-05,
                        "total_energy_kwh": 0.02063259130142403,
                        "total_energy_joules": 74277.32868512651
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.220578745763117,
                        "joules_per_token": 4.533528362129304,
                        "flops_per_joule": 228198446.1908376,
                        "joules_per_flop": 4.38215078451376e-09
                    },
                    "per-process_emissions": [
                        0.0020144771909718826,
                        0.001946383680481783,
                        0.0019058257199368201,
                        0.0019932990648869986
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0380": {
            "setup": {
                "experiment_id": "0380",
                "date_time": "April 12, 2025 at 11:03:32 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 47.48766216900549,
                        "average_latency_ms_per_batch": 5935.957771125686,
                        "throughput_queries_per_sec": 2.6954369651733194,
                        "throughput_tokens_per_sec": 345.0159315421849
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 42.4,
                        "cpu_memory_usage_bytes": 1954836480
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0380",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 254.84657895183,
                            "process_2": 252.04504086558956,
                            "process_0": 275.6523111985208,
                            "process_3": 249.18023399017963
                        },
                        "ram_power": {
                            "process_1": 0.7071418762207031,
                            "process_2": 0.7046499252319336,
                            "process_0": 0.6824355125427246,
                            "process_3": 0.7147822380065918
                        },
                        "cpu_energy": {
                            "process_1": 0.0015592543086295342,
                            "process_2": 0.0015253338289367094,
                            "process_0": 0.0014668728793740234,
                            "process_3": 0.0015408725408151442
                        },
                        "gpu_energy": {
                            "process_1": 0.0037218699219383478,
                            "process_2": 0.0035772361951207365,
                            "process_0": 0.003529684212634665,
                            "process_3": 0.0036844704475742507
                        },
                        "ram_energy": {
                            "process_1": 6.911445956287618e-06,
                            "process_2": 6.719070264073556e-06,
                            "process_0": 6.266690474105679e-06,
                            "process_3": 7.0997597061525095e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.00528803567652417,
                            "process_2": 0.00510928909432152,
                            "process_0": 0.005002823782482793,
                            "process_3": 0.005232442748095547
                        },
                        "total_energy_joules": {
                            "process_1": 19036.92843548701,
                            "process_2": 18393.44073955747,
                            "process_0": 18010.165616938055,
                            "process_3": 18836.793893143968
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 257.93104125153,
                        "ram_power_avg": 0.7022523880004883,
                        "cpu_energy_total": 0.0060923335577554106,
                        "gpu_energy_total": 0.014513260777268,
                        "ram_energy_total": 2.6996966400619363e-05,
                        "total_energy_kwh": 0.02063259130142403,
                        "total_energy_joules": 74277.32868512651
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.220578745763117,
                        "joules_per_token": 4.533528362129304,
                        "flops_per_joule": 228198446.1908376,
                        "joules_per_flop": 4.38215078451376e-09
                    },
                    "per-process_emissions": [
                        0.0020144771909718826,
                        0.001946383680481783,
                        0.0019058257199368201,
                        0.0019932990648869986
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0381": {
            "setup": {
                "experiment_id": "0381",
                "date_time": "April 12, 2025 at 11:05:20 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 48.86340590205509,
                        "average_latency_ms_per_batch": 6107.9257377568865,
                        "throughput_queries_per_sec": 2.6195472386139294,
                        "throughput_tokens_per_sec": 335.30204654258296
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 42.3,
                        "cpu_memory_usage_bytes": 1985560576
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0381",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 0.0,
                            "process_3": 301.0511768206997,
                            "process_1": 290.44176257626077,
                            "process_0": 281.32909525513946
                        },
                        "ram_power": {
                            "process_2": 0.7220821380615234,
                            "process_3": 0.7231163978576661,
                            "process_1": 0.7205958366394043,
                            "process_0": 0.6932601928710938
                        },
                        "cpu_energy": {
                            "process_2": 0.0014446845268139441,
                            "process_3": 0.001550886678154711,
                            "process_1": 0.0014649672409677808,
                            "process_0": 0.0015090534964410836
                        },
                        "gpu_energy": {
                            "process_2": 0.0035117778094198027,
                            "process_3": 0.0038098316589740833,
                            "process_1": 0.003618026227750981,
                            "process_0": 0.0037214101993474635
                        },
                        "ram_energy": {
                            "process_2": 6.610504672777376e-06,
                            "process_3": 7.0666059160621215e-06,
                            "process_1": 6.398673054956385e-06,
                            "process_0": 6.515659759385321e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.004963072840906523,
                            "process_3": 0.005367784943044856,
                            "process_1": 0.0050893921417737185,
                            "process_0": 0.0052369793555479315
                        },
                        "total_energy_joules": {
                            "process_2": 17867.062227263483,
                            "process_3": 19324.025794961482,
                            "process_1": 18321.811710385387,
                            "process_0": 18853.125679972552
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 218.205508663025,
                        "ram_power_avg": 0.7147636413574219,
                        "cpu_energy_total": 0.0059695919423775196,
                        "gpu_energy_total": 0.01466104589549233,
                        "ram_energy_total": 2.6591443403181207e-05,
                        "total_energy_kwh": 0.02065722928127303,
                        "total_energy_joules": 74366.02541258291
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2203156603987039,
                        "joules_per_token": 4.5389419807484686,
                        "flops_per_joule": 227926272.77192125,
                        "joules_per_flop": 4.387383638746504e-09
                    },
                    "per-process_emissions": [
                        0.00189068259874334,
                        0.002044857674052938,
                        0.0019388039364086982,
                        0.0019950272854959844
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0381": {
            "setup": {
                "experiment_id": "0381",
                "date_time": "April 12, 2025 at 11:05:20 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 48.86340590205509,
                        "average_latency_ms_per_batch": 6107.9257377568865,
                        "throughput_queries_per_sec": 2.6195472386139294,
                        "throughput_tokens_per_sec": 335.30204654258296
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 42.3,
                        "cpu_memory_usage_bytes": 1985560576
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0381",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 0.0,
                            "process_3": 301.0511768206997,
                            "process_1": 290.44176257626077,
                            "process_0": 281.32909525513946
                        },
                        "ram_power": {
                            "process_2": 0.7220821380615234,
                            "process_3": 0.7231163978576661,
                            "process_1": 0.7205958366394043,
                            "process_0": 0.6932601928710938
                        },
                        "cpu_energy": {
                            "process_2": 0.0014446845268139441,
                            "process_3": 0.001550886678154711,
                            "process_1": 0.0014649672409677808,
                            "process_0": 0.0015090534964410836
                        },
                        "gpu_energy": {
                            "process_2": 0.0035117778094198027,
                            "process_3": 0.0038098316589740833,
                            "process_1": 0.003618026227750981,
                            "process_0": 0.0037214101993474635
                        },
                        "ram_energy": {
                            "process_2": 6.610504672777376e-06,
                            "process_3": 7.0666059160621215e-06,
                            "process_1": 6.398673054956385e-06,
                            "process_0": 6.515659759385321e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.004963072840906523,
                            "process_3": 0.005367784943044856,
                            "process_1": 0.0050893921417737185,
                            "process_0": 0.0052369793555479315
                        },
                        "total_energy_joules": {
                            "process_2": 17867.062227263483,
                            "process_3": 19324.025794961482,
                            "process_1": 18321.811710385387,
                            "process_0": 18853.125679972552
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 218.205508663025,
                        "ram_power_avg": 0.7147636413574219,
                        "cpu_energy_total": 0.0059695919423775196,
                        "gpu_energy_total": 0.01466104589549233,
                        "ram_energy_total": 2.6591443403181207e-05,
                        "total_energy_kwh": 0.02065722928127303,
                        "total_energy_joules": 74366.02541258291
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2203156603987039,
                        "joules_per_token": 4.5389419807484686,
                        "flops_per_joule": 227926272.77192125,
                        "joules_per_flop": 4.387383638746504e-09
                    },
                    "per-process_emissions": [
                        0.00189068259874334,
                        0.002044857674052938,
                        0.0019388039364086982,
                        0.0019950272854959844
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0382": {
            "setup": {
                "experiment_id": "0382",
                "date_time": "April 12, 2025 at 11:06:33 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_64",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 13.841182310978184,
                        "average_latency_ms_per_batch": 6920.591155489092,
                        "throughput_queries_per_sec": 9.247764903614941,
                        "throughput_tokens_per_sec": 1183.7139076627125
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2904555520,
                        "gpu_max_memory_reserved_bytes": 2904555520
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 42.5,
                        "cpu_memory_usage_bytes": 1929519104
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0382",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 338.63040612823426,
                            "process_1": 540.8995953715374,
                            "process_2": 371.0792096865589,
                            "process_3": 455.73240459263354
                        },
                        "ram_power": {
                            "process_0": 0.6734991073608398,
                            "process_1": 0.7043581008911133,
                            "process_2": 0.7051820755004883,
                            "process_3": 0.7040863037109375
                        },
                        "cpu_energy": {
                            "process_0": 0.00043263662228218885,
                            "process_1": 0.00036704041478151334,
                            "process_2": 0.0004234159124671351,
                            "process_3": 0.0003931769639375489
                        },
                        "gpu_energy": {
                            "process_0": 0.0018209484012021715,
                            "process_1": 0.0016031126713780264,
                            "process_2": 0.0017945142133877212,
                            "process_3": 0.0016921080203511352
                        },
                        "ram_energy": {
                            "process_0": 1.8113243194190602e-06,
                            "process_1": 1.6788292417984598e-06,
                            "process_2": 1.8950327631736593e-06,
                            "process_3": 1.7460473184874607e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0022553963478037795,
                            "process_1": 0.0019718319154013383,
                            "process_2": 0.00221982515861803,
                            "process_3": 0.002087031031607172
                        },
                        "total_energy_joules": {
                            "process_0": 8119.426852093607,
                            "process_1": 7098.594895444818,
                            "process_2": 7991.370571024909,
                            "process_3": 7513.311713785818
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 426.5854039447411,
                        "ram_power_avg": 0.6967813968658447,
                        "cpu_energy_total": 0.001616269913468386,
                        "gpu_energy_total": 0.006910683306319054,
                        "ram_energy_total": 7.13123364287864e-06,
                        "total_energy_kwh": 0.00853408445343032,
                        "total_energy_joules": 30722.70403234915
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.533286392459096,
                        "joules_per_token": 1.8751650410369354,
                        "flops_per_joule": 551708305.8608613,
                        "joules_per_flop": 1.812552012316806e-09
                    },
                    "per-process_emissions": [
                        0.0008591932386958498,
                        0.0007511693681721399,
                        0.0008456423941755386,
                        0.0007950544714907522
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0382": {
            "setup": {
                "experiment_id": "0382",
                "date_time": "April 12, 2025 at 11:06:33 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_64",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 13.841182310978184,
                        "average_latency_ms_per_batch": 6920.591155489092,
                        "throughput_queries_per_sec": 9.247764903614941,
                        "throughput_tokens_per_sec": 1183.7139076627125
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2904555520,
                        "gpu_max_memory_reserved_bytes": 2904555520
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 42.5,
                        "cpu_memory_usage_bytes": 1929519104
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0382",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 338.63040612823426,
                            "process_1": 540.8995953715374,
                            "process_2": 371.0792096865589,
                            "process_3": 455.73240459263354
                        },
                        "ram_power": {
                            "process_0": 0.6734991073608398,
                            "process_1": 0.7043581008911133,
                            "process_2": 0.7051820755004883,
                            "process_3": 0.7040863037109375
                        },
                        "cpu_energy": {
                            "process_0": 0.00043263662228218885,
                            "process_1": 0.00036704041478151334,
                            "process_2": 0.0004234159124671351,
                            "process_3": 0.0003931769639375489
                        },
                        "gpu_energy": {
                            "process_0": 0.0018209484012021715,
                            "process_1": 0.0016031126713780264,
                            "process_2": 0.0017945142133877212,
                            "process_3": 0.0016921080203511352
                        },
                        "ram_energy": {
                            "process_0": 1.8113243194190602e-06,
                            "process_1": 1.6788292417984598e-06,
                            "process_2": 1.8950327631736593e-06,
                            "process_3": 1.7460473184874607e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0022553963478037795,
                            "process_1": 0.0019718319154013383,
                            "process_2": 0.00221982515861803,
                            "process_3": 0.002087031031607172
                        },
                        "total_energy_joules": {
                            "process_0": 8119.426852093607,
                            "process_1": 7098.594895444818,
                            "process_2": 7991.370571024909,
                            "process_3": 7513.311713785818
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 426.5854039447411,
                        "ram_power_avg": 0.6967813968658447,
                        "cpu_energy_total": 0.001616269913468386,
                        "gpu_energy_total": 0.006910683306319054,
                        "ram_energy_total": 7.13123364287864e-06,
                        "total_energy_kwh": 0.00853408445343032,
                        "total_energy_joules": 30722.70403234915
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.533286392459096,
                        "joules_per_token": 1.8751650410369354,
                        "flops_per_joule": 551708305.8608613,
                        "joules_per_flop": 1.812552012316806e-09
                    },
                    "per-process_emissions": [
                        0.0008591932386958498,
                        0.0007511693681721399,
                        0.0008456423941755386,
                        0.0007950544714907522
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0383": {
            "setup": {
                "experiment_id": "0383",
                "date_time": "April 12, 2025 at 11:08:19 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 50.82135290798033,
                        "average_latency_ms_per_batch": 6352.669113497541,
                        "throughput_queries_per_sec": 2.5186263780061733,
                        "throughput_tokens_per_sec": 322.3841763847902
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 42.4,
                        "cpu_memory_usage_bytes": 1926586368
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0383",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 242.44813171685513,
                            "process_3": 314.4083808924082,
                            "process_1": 0.0,
                            "process_0": 251.65453847562637
                        },
                        "ram_power": {
                            "process_2": 0.708892822265625,
                            "process_3": 0.7081489562988281,
                            "process_1": 0.7063136100769043,
                            "process_0": 0.672663688659668
                        },
                        "cpu_energy": {
                            "process_2": 0.001530712923965439,
                            "process_3": 0.0013346015318466016,
                            "process_1": 0.0013320381703433672,
                            "process_0": 0.0015698782007884798
                        },
                        "gpu_energy": {
                            "process_2": 0.0040282462781515704,
                            "process_3": 0.003584570923210073,
                            "process_1": 0.0035203775385219682,
                            "process_0": 0.00411557995912859
                        },
                        "ram_energy": {
                            "process_2": 6.613951595069654e-06,
                            "process_3": 5.9271686050787894e-06,
                            "process_1": 5.961315848076698e-06,
                            "process_0": 6.50737751263327e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005565573153712079,
                            "process_3": 0.004925099623661752,
                            "process_1": 0.0048583770247134115,
                            "process_0": 0.005691965537429707
                        },
                        "total_energy_joules": {
                            "process_2": 20036.063353363486,
                            "process_3": 17730.358645182307,
                            "process_1": 17490.157288968283,
                            "process_0": 20491.075934746943
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 202.12776277122242,
                        "ram_power_avg": 0.6990047693252563,
                        "cpu_energy_total": 0.005767230826943888,
                        "gpu_energy_total": 0.015248774699012202,
                        "ram_energy_total": 2.5009813560858412e-05,
                        "total_energy_kwh": 0.021041015339516952,
                        "total_energy_joules": 75747.65522226102
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21629712433903836,
                        "joules_per_token": 4.62326997206183,
                        "flops_per_joule": 223768919.888238,
                        "joules_per_flop": 4.468895861406726e-09
                    },
                    "per-process_emissions": [
                        0.002120205092906617,
                        0.0018762167016339445,
                        0.0018507987275645742,
                        0.002168354271483847
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0383": {
            "setup": {
                "experiment_id": "0383",
                "date_time": "April 12, 2025 at 11:08:19 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 50.82135290798033,
                        "average_latency_ms_per_batch": 6352.669113497541,
                        "throughput_queries_per_sec": 2.5186263780061733,
                        "throughput_tokens_per_sec": 322.3841763847902
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 42.4,
                        "cpu_memory_usage_bytes": 1926586368
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0383",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 242.44813171685513,
                            "process_3": 314.4083808924082,
                            "process_1": 0.0,
                            "process_0": 251.65453847562637
                        },
                        "ram_power": {
                            "process_2": 0.708892822265625,
                            "process_3": 0.7081489562988281,
                            "process_1": 0.7063136100769043,
                            "process_0": 0.672663688659668
                        },
                        "cpu_energy": {
                            "process_2": 0.001530712923965439,
                            "process_3": 0.0013346015318466016,
                            "process_1": 0.0013320381703433672,
                            "process_0": 0.0015698782007884798
                        },
                        "gpu_energy": {
                            "process_2": 0.0040282462781515704,
                            "process_3": 0.003584570923210073,
                            "process_1": 0.0035203775385219682,
                            "process_0": 0.00411557995912859
                        },
                        "ram_energy": {
                            "process_2": 6.613951595069654e-06,
                            "process_3": 5.9271686050787894e-06,
                            "process_1": 5.961315848076698e-06,
                            "process_0": 6.50737751263327e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005565573153712079,
                            "process_3": 0.004925099623661752,
                            "process_1": 0.0048583770247134115,
                            "process_0": 0.005691965537429707
                        },
                        "total_energy_joules": {
                            "process_2": 20036.063353363486,
                            "process_3": 17730.358645182307,
                            "process_1": 17490.157288968283,
                            "process_0": 20491.075934746943
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 202.12776277122242,
                        "ram_power_avg": 0.6990047693252563,
                        "cpu_energy_total": 0.005767230826943888,
                        "gpu_energy_total": 0.015248774699012202,
                        "ram_energy_total": 2.5009813560858412e-05,
                        "total_energy_kwh": 0.021041015339516952,
                        "total_energy_joules": 75747.65522226102
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21629712433903836,
                        "joules_per_token": 4.62326997206183,
                        "flops_per_joule": 223768919.888238,
                        "joules_per_flop": 4.468895861406726e-09
                    },
                    "per-process_emissions": [
                        0.002120205092906617,
                        0.0018762167016339445,
                        0.0018507987275645742,
                        0.002168354271483847
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0384": {
            "setup": {
                "experiment_id": "0384",
                "date_time": "April 12, 2025 at 11:10:08 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 44.6313599240093,
                        "average_latency_ms_per_batch": 5578.919990501163,
                        "throughput_queries_per_sec": 2.8679386023176674,
                        "throughput_tokens_per_sec": 367.0961410966614
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 42.4,
                        "cpu_memory_usage_bytes": 1973243904
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0384",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 281.87065439364125,
                            "process_2": 231.06892596434776,
                            "process_1": 276.85469546692934,
                            "process_3": 5.166118797503078
                        },
                        "ram_power": {
                            "process_0": 0.6888642311096191,
                            "process_2": 0.7210636138916016,
                            "process_1": 0.7204828262329102,
                            "process_3": 0.7245669364929199
                        },
                        "cpu_energy": {
                            "process_0": 0.0013795107244668537,
                            "process_2": 0.0016557581988081438,
                            "process_1": 0.0016139061905005294,
                            "process_3": 0.0015243102428457859
                        },
                        "gpu_energy": {
                            "process_0": 0.0033262007165149043,
                            "process_2": 0.003906372847317652,
                            "process_1": 0.003820389167420002,
                            "process_3": 0.003563565906405275
                        },
                        "ram_energy": {
                            "process_0": 6.312136449005664e-06,
                            "process_2": 7.418592894295553e-06,
                            "process_1": 7.3745107545220214e-06,
                            "process_3": 6.923114865418909e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.004712023577430766,
                            "process_2": 0.00556954963902009,
                            "process_1": 0.005441669868675054,
                            "process_3": 0.005094799264116482
                        },
                        "total_energy_joules": {
                            "process_0": 16963.284878750757,
                            "process_2": 20050.378700472324,
                            "process_1": 19590.011527230195,
                            "process_3": 18341.277350819335
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 198.74009865560535,
                        "ram_power_avg": 0.7137444019317627,
                        "cpu_energy_total": 0.0061734853566213135,
                        "gpu_energy_total": 0.014616528637657833,
                        "ram_energy_total": 2.8028354963242146e-05,
                        "total_energy_kwh": 0.020818042349242394,
                        "total_energy_joules": 74944.95245727261
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21861378869165068,
                        "joules_per_token": 4.574276883378455,
                        "flops_per_joule": 226165611.39078,
                        "joules_per_flop": 4.421538685083963e-09
                    },
                    "per-process_emissions": [
                        0.0017950453818222501,
                        0.0021217199349847036,
                        0.002073004136471762,
                        0.0019408637796651737
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0384": {
            "setup": {
                "experiment_id": "0384",
                "date_time": "April 12, 2025 at 11:10:08 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 44.6313599240093,
                        "average_latency_ms_per_batch": 5578.919990501163,
                        "throughput_queries_per_sec": 2.8679386023176674,
                        "throughput_tokens_per_sec": 367.0961410966614
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 42.4,
                        "cpu_memory_usage_bytes": 1973243904
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0384",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 281.87065439364125,
                            "process_2": 231.06892596434776,
                            "process_1": 276.85469546692934,
                            "process_3": 5.166118797503078
                        },
                        "ram_power": {
                            "process_0": 0.6888642311096191,
                            "process_2": 0.7210636138916016,
                            "process_1": 0.7204828262329102,
                            "process_3": 0.7245669364929199
                        },
                        "cpu_energy": {
                            "process_0": 0.0013795107244668537,
                            "process_2": 0.0016557581988081438,
                            "process_1": 0.0016139061905005294,
                            "process_3": 0.0015243102428457859
                        },
                        "gpu_energy": {
                            "process_0": 0.0033262007165149043,
                            "process_2": 0.003906372847317652,
                            "process_1": 0.003820389167420002,
                            "process_3": 0.003563565906405275
                        },
                        "ram_energy": {
                            "process_0": 6.312136449005664e-06,
                            "process_2": 7.418592894295553e-06,
                            "process_1": 7.3745107545220214e-06,
                            "process_3": 6.923114865418909e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.004712023577430766,
                            "process_2": 0.00556954963902009,
                            "process_1": 0.005441669868675054,
                            "process_3": 0.005094799264116482
                        },
                        "total_energy_joules": {
                            "process_0": 16963.284878750757,
                            "process_2": 20050.378700472324,
                            "process_1": 19590.011527230195,
                            "process_3": 18341.277350819335
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 198.74009865560535,
                        "ram_power_avg": 0.7137444019317627,
                        "cpu_energy_total": 0.0061734853566213135,
                        "gpu_energy_total": 0.014616528637657833,
                        "ram_energy_total": 2.8028354963242146e-05,
                        "total_energy_kwh": 0.020818042349242394,
                        "total_energy_joules": 74944.95245727261
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21861378869165068,
                        "joules_per_token": 4.574276883378455,
                        "flops_per_joule": 226165611.39078,
                        "joules_per_flop": 4.421538685083963e-09
                    },
                    "per-process_emissions": [
                        0.0017950453818222501,
                        0.0021217199349847036,
                        0.002073004136471762,
                        0.0019408637796651737
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0385": {
            "setup": {
                "experiment_id": "0385",
                "date_time": "April 12, 2025 at 11:11:57 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.40216620199499,
                        "average_latency_ms_per_batch": 4925.270775249373,
                        "throughput_queries_per_sec": 3.2485523598831776,
                        "throughput_tokens_per_sec": 415.81470206504673
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.6,
                        "cpu_memory_usage_bytes": 1943584768
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0385",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 420.0658483883378,
                            "process_1": 261.40907004132276,
                            "process_2": 274.0922391027453,
                            "process_0": 302.1860588481177
                        },
                        "ram_power": {
                            "process_3": 0.7060160636901855,
                            "process_1": 0.7067627906799316,
                            "process_2": 0.7054324150085449,
                            "process_0": 0.6786003112792969
                        },
                        "cpu_energy": {
                            "process_3": 0.0011777550451542993,
                            "process_1": 0.0013254746782822623,
                            "process_2": 0.00157148506475005,
                            "process_0": 0.001218492736193184
                        },
                        "gpu_energy": {
                            "process_3": 0.0035740036925342444,
                            "process_1": 0.003940977597223894,
                            "process_2": 0.004510170830356763,
                            "process_0": 0.003683277668842244
                        },
                        "ram_energy": {
                            "process_3": 5.675983410945812e-06,
                            "process_1": 5.944423398612439e-06,
                            "process_2": 7.017927581600151e-06,
                            "process_0": 5.202368582425634e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0047574347210994905,
                            "process_1": 0.005272396698904766,
                            "process_2": 0.006088673822688413,
                            "process_0": 0.0049069727736178554
                        },
                        "total_energy_joules": {
                            "process_3": 17126.764995958165,
                            "process_1": 18980.628116057156,
                            "process_2": 21919.225761678288,
                            "process_0": 17665.10198502428
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 314.4383040951309,
                        "ram_power_avg": 0.6992028951644897,
                        "cpu_energy_total": 0.005293207524379796,
                        "gpu_energy_total": 0.015708429788957146,
                        "ram_energy_total": 2.3840702973584036e-05,
                        "total_energy_kwh": 0.021025478016310526,
                        "total_energy_joules": 75691.72085871789
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21645696271830703,
                        "joules_per_token": 4.619856009443231,
                        "flops_per_joule": 223934279.74493945,
                        "joules_per_flop": 4.465595893308507e-09
                    },
                    "per-process_emissions": [
                        0.001812344757002851,
                        0.0020085195224477707,
                        0.002319480292753151,
                        0.0018693112781097221
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0385": {
            "setup": {
                "experiment_id": "0385",
                "date_time": "April 12, 2025 at 11:11:57 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.40216620199499,
                        "average_latency_ms_per_batch": 4925.270775249373,
                        "throughput_queries_per_sec": 3.2485523598831776,
                        "throughput_tokens_per_sec": 415.81470206504673
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.6,
                        "cpu_memory_usage_bytes": 1943584768
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0385",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 420.0658483883378,
                            "process_1": 261.40907004132276,
                            "process_2": 274.0922391027453,
                            "process_0": 302.1860588481177
                        },
                        "ram_power": {
                            "process_3": 0.7060160636901855,
                            "process_1": 0.7067627906799316,
                            "process_2": 0.7054324150085449,
                            "process_0": 0.6786003112792969
                        },
                        "cpu_energy": {
                            "process_3": 0.0011777550451542993,
                            "process_1": 0.0013254746782822623,
                            "process_2": 0.00157148506475005,
                            "process_0": 0.001218492736193184
                        },
                        "gpu_energy": {
                            "process_3": 0.0035740036925342444,
                            "process_1": 0.003940977597223894,
                            "process_2": 0.004510170830356763,
                            "process_0": 0.003683277668842244
                        },
                        "ram_energy": {
                            "process_3": 5.675983410945812e-06,
                            "process_1": 5.944423398612439e-06,
                            "process_2": 7.017927581600151e-06,
                            "process_0": 5.202368582425634e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0047574347210994905,
                            "process_1": 0.005272396698904766,
                            "process_2": 0.006088673822688413,
                            "process_0": 0.0049069727736178554
                        },
                        "total_energy_joules": {
                            "process_3": 17126.764995958165,
                            "process_1": 18980.628116057156,
                            "process_2": 21919.225761678288,
                            "process_0": 17665.10198502428
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 314.4383040951309,
                        "ram_power_avg": 0.6992028951644897,
                        "cpu_energy_total": 0.005293207524379796,
                        "gpu_energy_total": 0.015708429788957146,
                        "ram_energy_total": 2.3840702973584036e-05,
                        "total_energy_kwh": 0.021025478016310526,
                        "total_energy_joules": 75691.72085871789
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21645696271830703,
                        "joules_per_token": 4.619856009443231,
                        "flops_per_joule": 223934279.74493945,
                        "joules_per_flop": 4.465595893308507e-09
                    },
                    "per-process_emissions": [
                        0.001812344757002851,
                        0.0020085195224477707,
                        0.002319480292753151,
                        0.0018693112781097221
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0386": {
            "setup": {
                "experiment_id": "0386",
                "date_time": "April 12, 2025 at 11:13:44 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 51.367374780005775,
                        "average_latency_ms_per_batch": 6420.921847500722,
                        "throughput_queries_per_sec": 2.491854032801822,
                        "throughput_tokens_per_sec": 318.95731619863324
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.6,
                        "cpu_memory_usage_bytes": 1983455232
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0386",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 265.09536874136876,
                            "process_2": 331.838746610514,
                            "process_3": 236.05229434699837,
                            "process_0": 272.11646790429785
                        },
                        "ram_power": {
                            "process_1": 0.721285343170166,
                            "process_2": 0.719670295715332,
                            "process_3": 0.7239847183227539,
                            "process_0": 0.6925249099731445
                        },
                        "cpu_energy": {
                            "process_1": 0.0015837407877788796,
                            "process_2": 0.001540005081785239,
                            "process_3": 0.001621657267966839,
                            "process_0": 0.001584638717935377
                        },
                        "gpu_energy": {
                            "process_1": 0.0035999981577743423,
                            "process_2": 0.0034824488970672807,
                            "process_3": 0.003680601277812112,
                            "process_0": 0.0036041862166791283
                        },
                        "ram_energy": {
                            "process_1": 7.061454629422286e-06,
                            "process_2": 6.892541665349702e-06,
                            "process_3": 7.164203514240098e-06,
                            "process_0": 6.7507720522893935e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005190800400182643,
                            "process_2": 0.005029346520517868,
                            "process_3": 0.005309422749293192,
                            "process_0": 0.0051955757066667945
                        },
                        "total_energy_joules": {
                            "process_1": 18686.881440657515,
                            "process_2": 18105.647473864326,
                            "process_3": 19113.92189745549,
                            "process_0": 18704.07254400046
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 276.27571940079474,
                        "ram_power_avg": 0.7143663167953491,
                        "cpu_energy_total": 0.006330041855466335,
                        "gpu_energy_total": 0.014367234549332863,
                        "ram_energy_total": 2.786897186130148e-05,
                        "total_energy_kwh": 0.0207251453766605,
                        "total_energy_joules": 74610.52335597778
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21959368816955652,
                        "joules_per_token": 4.553864950926378,
                        "flops_per_joule": 227179360.64167777,
                        "joules_per_flop": 4.4018083208591545e-09
                    },
                    "per-process_emissions": [
                        0.001977435412449578,
                        0.001915929556991282,
                        0.0020226245963432415,
                        0.0019792545654547153
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0386": {
            "setup": {
                "experiment_id": "0386",
                "date_time": "April 12, 2025 at 11:13:44 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 51.367374780005775,
                        "average_latency_ms_per_batch": 6420.921847500722,
                        "throughput_queries_per_sec": 2.491854032801822,
                        "throughput_tokens_per_sec": 318.95731619863324
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.6,
                        "cpu_memory_usage_bytes": 1983455232
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0386",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 265.09536874136876,
                            "process_2": 331.838746610514,
                            "process_3": 236.05229434699837,
                            "process_0": 272.11646790429785
                        },
                        "ram_power": {
                            "process_1": 0.721285343170166,
                            "process_2": 0.719670295715332,
                            "process_3": 0.7239847183227539,
                            "process_0": 0.6925249099731445
                        },
                        "cpu_energy": {
                            "process_1": 0.0015837407877788796,
                            "process_2": 0.001540005081785239,
                            "process_3": 0.001621657267966839,
                            "process_0": 0.001584638717935377
                        },
                        "gpu_energy": {
                            "process_1": 0.0035999981577743423,
                            "process_2": 0.0034824488970672807,
                            "process_3": 0.003680601277812112,
                            "process_0": 0.0036041862166791283
                        },
                        "ram_energy": {
                            "process_1": 7.061454629422286e-06,
                            "process_2": 6.892541665349702e-06,
                            "process_3": 7.164203514240098e-06,
                            "process_0": 6.7507720522893935e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005190800400182643,
                            "process_2": 0.005029346520517868,
                            "process_3": 0.005309422749293192,
                            "process_0": 0.0051955757066667945
                        },
                        "total_energy_joules": {
                            "process_1": 18686.881440657515,
                            "process_2": 18105.647473864326,
                            "process_3": 19113.92189745549,
                            "process_0": 18704.07254400046
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 276.27571940079474,
                        "ram_power_avg": 0.7143663167953491,
                        "cpu_energy_total": 0.006330041855466335,
                        "gpu_energy_total": 0.014367234549332863,
                        "ram_energy_total": 2.786897186130148e-05,
                        "total_energy_kwh": 0.0207251453766605,
                        "total_energy_joules": 74610.52335597778
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21959368816955652,
                        "joules_per_token": 4.553864950926378,
                        "flops_per_joule": 227179360.64167777,
                        "joules_per_flop": 4.4018083208591545e-09
                    },
                    "per-process_emissions": [
                        0.001977435412449578,
                        0.001915929556991282,
                        0.0020226245963432415,
                        0.0019792545654547153
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0387": {
            "setup": {
                "experiment_id": "0387",
                "date_time": "April 12, 2025 at 11:15:33 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 46.80632990493905,
                        "average_latency_ms_per_batch": 5850.791238117381,
                        "throughput_queries_per_sec": 2.734672858563374,
                        "throughput_tokens_per_sec": 350.03812589611186
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            99.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.5,
                        "cpu_memory_usage_bytes": 1973346304
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0387",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 352.7559690096097,
                            "process_2": 290.956034181896,
                            "process_1": 318.5880881462545,
                            "process_0": 328.28623225840767
                        },
                        "ram_power": {
                            "process_3": 0.7204656600952148,
                            "process_2": 0.7208347320556641,
                            "process_1": 0.7190666198730469,
                            "process_0": 0.6888999938964844
                        },
                        "cpu_energy": {
                            "process_3": 0.0013717310991241904,
                            "process_2": 0.0014332460093728515,
                            "process_1": 0.0014492413666257562,
                            "process_0": 0.001446461162999185
                        },
                        "gpu_energy": {
                            "process_3": 0.0036957226787988073,
                            "process_2": 0.0038561166960020543,
                            "process_1": 0.003903690622950151,
                            "process_0": 0.0038959464500887275
                        },
                        "ram_energy": {
                            "process_3": 6.158470975631641e-06,
                            "process_2": 6.231608825107565e-06,
                            "process_1": 6.451952990744314e-06,
                            "process_0": 6.267109238168117e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005073612248898628,
                            "process_2": 0.0052955943142000104,
                            "process_1": 0.005359383942566649,
                            "process_0": 0.005348674722326081
                        },
                        "total_energy_joules": {
                            "process_3": 18265.00409603506,
                            "process_2": 19064.139531120036,
                            "process_1": 19293.78219323994,
                            "process_0": 19255.229000373893
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 322.64658089904196,
                        "ram_power_avg": 0.7123167514801025,
                        "cpu_energy_total": 0.0057006796381219825,
                        "gpu_energy_total": 0.01535147644783974,
                        "ram_energy_total": 2.5109142029651637e-05,
                        "total_energy_kwh": 0.02107726522799137,
                        "total_energy_joules": 75878.15482076892
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21592512415069257,
                        "joules_per_token": 4.631235035447322,
                        "flops_per_joule": 223384069.27776998,
                        "joules_per_flop": 4.476594965939743e-09
                    },
                    "per-process_emissions": [
                        0.0019327925862179325,
                        0.002017356653994494,
                        0.0020416573129207654,
                        0.0020375776354701206
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0387": {
            "setup": {
                "experiment_id": "0387",
                "date_time": "April 12, 2025 at 11:15:33 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 46.80632990493905,
                        "average_latency_ms_per_batch": 5850.791238117381,
                        "throughput_queries_per_sec": 2.734672858563374,
                        "throughput_tokens_per_sec": 350.03812589611186
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            99.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.5,
                        "cpu_memory_usage_bytes": 1973346304
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0387",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 352.7559690096097,
                            "process_2": 290.956034181896,
                            "process_1": 318.5880881462545,
                            "process_0": 328.28623225840767
                        },
                        "ram_power": {
                            "process_3": 0.7204656600952148,
                            "process_2": 0.7208347320556641,
                            "process_1": 0.7190666198730469,
                            "process_0": 0.6888999938964844
                        },
                        "cpu_energy": {
                            "process_3": 0.0013717310991241904,
                            "process_2": 0.0014332460093728515,
                            "process_1": 0.0014492413666257562,
                            "process_0": 0.001446461162999185
                        },
                        "gpu_energy": {
                            "process_3": 0.0036957226787988073,
                            "process_2": 0.0038561166960020543,
                            "process_1": 0.003903690622950151,
                            "process_0": 0.0038959464500887275
                        },
                        "ram_energy": {
                            "process_3": 6.158470975631641e-06,
                            "process_2": 6.231608825107565e-06,
                            "process_1": 6.451952990744314e-06,
                            "process_0": 6.267109238168117e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005073612248898628,
                            "process_2": 0.0052955943142000104,
                            "process_1": 0.005359383942566649,
                            "process_0": 0.005348674722326081
                        },
                        "total_energy_joules": {
                            "process_3": 18265.00409603506,
                            "process_2": 19064.139531120036,
                            "process_1": 19293.78219323994,
                            "process_0": 19255.229000373893
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 322.64658089904196,
                        "ram_power_avg": 0.7123167514801025,
                        "cpu_energy_total": 0.0057006796381219825,
                        "gpu_energy_total": 0.01535147644783974,
                        "ram_energy_total": 2.5109142029651637e-05,
                        "total_energy_kwh": 0.02107726522799137,
                        "total_energy_joules": 75878.15482076892
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21592512415069257,
                        "joules_per_token": 4.631235035447322,
                        "flops_per_joule": 223384069.27776998,
                        "joules_per_flop": 4.476594965939743e-09
                    },
                    "per-process_emissions": [
                        0.0019327925862179325,
                        0.002017356653994494,
                        0.0020416573129207654,
                        0.0020375776354701206
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0388": {
            "setup": {
                "experiment_id": "0388",
                "date_time": "April 12, 2025 at 11:17:21 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 47.56580112397205,
                        "average_latency_ms_per_batch": 5945.725140496506,
                        "throughput_queries_per_sec": 2.6910090227722665,
                        "throughput_tokens_per_sec": 344.4491549148501
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.6,
                        "cpu_memory_usage_bytes": 1982672896
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0388",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 286.42307056283187,
                            "process_2": 250.99499837588812,
                            "process_1": 274.4399369594962,
                            "process_3": 358.48460150098214
                        },
                        "ram_power": {
                            "process_0": 0.6921572685241699,
                            "process_2": 0.7252779006958008,
                            "process_1": 0.7284035682678223,
                            "process_3": 0.7248215675354004
                        },
                        "cpu_energy": {
                            "process_0": 0.0014675844061894168,
                            "process_2": 0.0016303654507746621,
                            "process_1": 0.0014701046964682974,
                            "process_3": 0.001445986764031659
                        },
                        "gpu_energy": {
                            "process_0": 0.003661328762395888,
                            "process_2": 0.004020638772064711,
                            "process_1": 0.003668186267881346,
                            "process_3": 0.0036039920498591727
                        },
                        "ram_energy": {
                            "process_0": 6.313550351759185e-06,
                            "process_2": 7.261577313970985e-06,
                            "process_1": 6.635658359850407e-06,
                            "process_3": 6.447143203786921e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005135226718937065,
                            "process_2": 0.005658265800153343,
                            "process_1": 0.005144926622709494,
                            "process_3": 0.0050564259570946195
                        },
                        "total_energy_joules": {
                            "process_0": 18486.816188173434,
                            "process_2": 20369.756880552035,
                            "process_1": 18521.73584175418,
                            "process_3": 18203.13344554063
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 292.5856518497996,
                        "ram_power_avg": 0.7176650762557983,
                        "cpu_energy_total": 0.0060140413174640354,
                        "gpu_energy_total": 0.014954145852201117,
                        "ram_energy_total": 2.6657929229367496e-05,
                        "total_energy_kwh": 0.02099484509889452,
                        "total_energy_joules": 75581.44235602028
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21677278825699686,
                        "joules_per_token": 4.613125143800066,
                        "flops_per_joule": 224261015.20146346,
                        "joules_per_flop": 4.459089775820627e-09
                    },
                    "per-process_emissions": [
                        0.0019562646185790748,
                        0.002155516356568416,
                        0.001959959796921182,
                        0.0019262454683551954
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0388": {
            "setup": {
                "experiment_id": "0388",
                "date_time": "April 12, 2025 at 11:17:21 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 47.56580112397205,
                        "average_latency_ms_per_batch": 5945.725140496506,
                        "throughput_queries_per_sec": 2.6910090227722665,
                        "throughput_tokens_per_sec": 344.4491549148501
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.6,
                        "cpu_memory_usage_bytes": 1982672896
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0388",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 286.42307056283187,
                            "process_2": 250.99499837588812,
                            "process_1": 274.4399369594962,
                            "process_3": 358.48460150098214
                        },
                        "ram_power": {
                            "process_0": 0.6921572685241699,
                            "process_2": 0.7252779006958008,
                            "process_1": 0.7284035682678223,
                            "process_3": 0.7248215675354004
                        },
                        "cpu_energy": {
                            "process_0": 0.0014675844061894168,
                            "process_2": 0.0016303654507746621,
                            "process_1": 0.0014701046964682974,
                            "process_3": 0.001445986764031659
                        },
                        "gpu_energy": {
                            "process_0": 0.003661328762395888,
                            "process_2": 0.004020638772064711,
                            "process_1": 0.003668186267881346,
                            "process_3": 0.0036039920498591727
                        },
                        "ram_energy": {
                            "process_0": 6.313550351759185e-06,
                            "process_2": 7.261577313970985e-06,
                            "process_1": 6.635658359850407e-06,
                            "process_3": 6.447143203786921e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005135226718937065,
                            "process_2": 0.005658265800153343,
                            "process_1": 0.005144926622709494,
                            "process_3": 0.0050564259570946195
                        },
                        "total_energy_joules": {
                            "process_0": 18486.816188173434,
                            "process_2": 20369.756880552035,
                            "process_1": 18521.73584175418,
                            "process_3": 18203.13344554063
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 292.5856518497996,
                        "ram_power_avg": 0.7176650762557983,
                        "cpu_energy_total": 0.0060140413174640354,
                        "gpu_energy_total": 0.014954145852201117,
                        "ram_energy_total": 2.6657929229367496e-05,
                        "total_energy_kwh": 0.02099484509889452,
                        "total_energy_joules": 75581.44235602028
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21677278825699686,
                        "joules_per_token": 4.613125143800066,
                        "flops_per_joule": 224261015.20146346,
                        "joules_per_flop": 4.459089775820627e-09
                    },
                    "per-process_emissions": [
                        0.0019562646185790748,
                        0.002155516356568416,
                        0.001959959796921182,
                        0.0019262454683551954
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0389": {
            "setup": {
                "experiment_id": "0389",
                "date_time": "April 12, 2025 at 11:19:08 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 48.11525033399812,
                        "average_latency_ms_per_batch": 6014.406291749765,
                        "throughput_queries_per_sec": 2.660279206934844,
                        "throughput_tokens_per_sec": 340.51573848766003
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.6,
                        "cpu_memory_usage_bytes": 1925582848
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0389",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 236.82020369150294,
                            "process_1": 0.0,
                            "process_0": 352.1756998884932,
                            "process_2": 266.4210037016145
                        },
                        "ram_power": {
                            "process_3": 0.7109098434448242,
                            "process_1": 0.7063236236572266,
                            "process_0": 0.6722187995910646,
                            "process_2": 0.7034282684326173
                        },
                        "cpu_energy": {
                            "process_3": 0.0016130517360334125,
                            "process_1": 0.0014727098343773832,
                            "process_0": 0.0014877229200928929,
                            "process_2": 0.0015690728412191676
                        },
                        "gpu_energy": {
                            "process_3": 0.003799635539705193,
                            "process_1": 0.003443957477385684,
                            "process_0": 0.003526431710031641,
                            "process_2": 0.0037049262972712604
                        },
                        "ram_energy": {
                            "process_3": 7.025460787348777e-06,
                            "process_1": 6.314767956783892e-06,
                            "process_0": 6.283779671441783e-06,
                            "process_2": 6.7373956284997425e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005419712736525955,
                            "process_1": 0.0049229820797198525,
                            "process_0": 0.005020438409795974,
                            "process_2": 0.00528073653411893
                        },
                        "total_energy_joules": {
                            "process_3": 19510.965851493438,
                            "process_1": 17722.73548699147,
                            "process_0": 18073.578275265507,
                            "process_2": 19010.651522828146
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 213.85422682040266,
                        "ram_power_avg": 0.6982201337814331,
                        "cpu_energy_total": 0.0061425573317228565,
                        "gpu_energy_total": 0.014474951024393778,
                        "ram_energy_total": 2.6361404044074193e-05,
                        "total_energy_kwh": 0.02064386976016071,
                        "total_energy_joules": 74317.93113657857
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2204582359792838,
                        "joules_per_token": 4.536006539097813,
                        "flops_per_joule": 228073773.50160637,
                        "joules_per_flop": 4.38454621347753e-09
                    },
                    "per-process_emissions": [
                        0.0020646395669795627,
                        0.0018754100232692779,
                        0.0019125360122117766,
                        0.0020116965826726062
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0389": {
            "setup": {
                "experiment_id": "0389",
                "date_time": "April 12, 2025 at 11:19:08 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 48.11525033399812,
                        "average_latency_ms_per_batch": 6014.406291749765,
                        "throughput_queries_per_sec": 2.660279206934844,
                        "throughput_tokens_per_sec": 340.51573848766003
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.6,
                        "cpu_memory_usage_bytes": 1925582848
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0389",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 236.82020369150294,
                            "process_1": 0.0,
                            "process_0": 352.1756998884932,
                            "process_2": 266.4210037016145
                        },
                        "ram_power": {
                            "process_3": 0.7109098434448242,
                            "process_1": 0.7063236236572266,
                            "process_0": 0.6722187995910646,
                            "process_2": 0.7034282684326173
                        },
                        "cpu_energy": {
                            "process_3": 0.0016130517360334125,
                            "process_1": 0.0014727098343773832,
                            "process_0": 0.0014877229200928929,
                            "process_2": 0.0015690728412191676
                        },
                        "gpu_energy": {
                            "process_3": 0.003799635539705193,
                            "process_1": 0.003443957477385684,
                            "process_0": 0.003526431710031641,
                            "process_2": 0.0037049262972712604
                        },
                        "ram_energy": {
                            "process_3": 7.025460787348777e-06,
                            "process_1": 6.314767956783892e-06,
                            "process_0": 6.283779671441783e-06,
                            "process_2": 6.7373956284997425e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005419712736525955,
                            "process_1": 0.0049229820797198525,
                            "process_0": 0.005020438409795974,
                            "process_2": 0.00528073653411893
                        },
                        "total_energy_joules": {
                            "process_3": 19510.965851493438,
                            "process_1": 17722.73548699147,
                            "process_0": 18073.578275265507,
                            "process_2": 19010.651522828146
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 213.85422682040266,
                        "ram_power_avg": 0.6982201337814331,
                        "cpu_energy_total": 0.0061425573317228565,
                        "gpu_energy_total": 0.014474951024393778,
                        "ram_energy_total": 2.6361404044074193e-05,
                        "total_energy_kwh": 0.02064386976016071,
                        "total_energy_joules": 74317.93113657857
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2204582359792838,
                        "joules_per_token": 4.536006539097813,
                        "flops_per_joule": 228073773.50160637,
                        "joules_per_flop": 4.38454621347753e-09
                    },
                    "per-process_emissions": [
                        0.0020646395669795627,
                        0.0018754100232692779,
                        0.0019125360122117766,
                        0.0020116965826726062
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0390": {
            "setup": {
                "experiment_id": "0390",
                "date_time": "April 12, 2025 at 11:20:55 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 47.45599760199548,
                        "average_latency_ms_per_batch": 5931.999700249435,
                        "throughput_queries_per_sec": 2.6972354700771843,
                        "throughput_tokens_per_sec": 345.2461401698796
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            99.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.5,
                        "cpu_memory_usage_bytes": 1939943424
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0390",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 315.1693778663399,
                            "process_1": 247.38237251717038,
                            "process_3": 0.0,
                            "process_2": 253.37467184756585
                        },
                        "ram_power": {
                            "process_0": 0.6772341728210449,
                            "process_1": 0.7058086395263672,
                            "process_3": 0.7058901786804199,
                            "process_2": 0.7100472450256348
                        },
                        "cpu_energy": {
                            "process_0": 0.0014631420720643293,
                            "process_1": 0.0015266484590001708,
                            "process_3": 0.00154912462543507,
                            "process_2": 0.0015612953688441849
                        },
                        "gpu_energy": {
                            "process_0": 0.0035251755979170962,
                            "process_1": 0.0036665876554897725,
                            "process_3": 0.0036429654143705914,
                            "process_2": 0.00373860937977355
                        },
                        "ram_energy": {
                            "process_0": 6.2783064828901945e-06,
                            "process_1": 6.689659655620541e-06,
                            "process_3": 6.683587934144257e-06,
                            "process_2": 6.8403250929594295e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.004994595976464317,
                            "process_1": 0.005199925774145565,
                            "process_3": 0.005198773627739807,
                            "process_2": 0.005306745073710693
                        },
                        "total_energy_joules": {
                            "process_0": 17980.54551527154,
                            "process_1": 18719.732786924033,
                            "process_3": 18715.585059863304,
                            "process_2": 19104.282265358495
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 203.98160555776906,
                        "ram_power_avg": 0.6997450590133667,
                        "cpu_energy_total": 0.0061002105253437555,
                        "gpu_energy_total": 0.01457333804755101,
                        "ram_energy_total": 2.6491879165614422e-05,
                        "total_energy_kwh": 0.02070004045206038,
                        "total_energy_joules": 74520.14562741737
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2198600104985841,
                        "joules_per_token": 4.548348732142173,
                        "flops_per_joule": 227454882.84332857,
                        "joules_per_flop": 4.396476292350261e-09
                    },
                    "per-process_emissions": [
                        0.0019026913372340816,
                        0.001980911723660753,
                        0.0019804728134874793,
                        0.0020216045358300884
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0390": {
            "setup": {
                "experiment_id": "0390",
                "date_time": "April 12, 2025 at 11:20:55 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 47.45599760199548,
                        "average_latency_ms_per_batch": 5931.999700249435,
                        "throughput_queries_per_sec": 2.6972354700771843,
                        "throughput_tokens_per_sec": 345.2461401698796
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            99.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.5,
                        "cpu_memory_usage_bytes": 1939943424
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0390",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 315.1693778663399,
                            "process_1": 247.38237251717038,
                            "process_3": 0.0,
                            "process_2": 253.37467184756585
                        },
                        "ram_power": {
                            "process_0": 0.6772341728210449,
                            "process_1": 0.7058086395263672,
                            "process_3": 0.7058901786804199,
                            "process_2": 0.7100472450256348
                        },
                        "cpu_energy": {
                            "process_0": 0.0014631420720643293,
                            "process_1": 0.0015266484590001708,
                            "process_3": 0.00154912462543507,
                            "process_2": 0.0015612953688441849
                        },
                        "gpu_energy": {
                            "process_0": 0.0035251755979170962,
                            "process_1": 0.0036665876554897725,
                            "process_3": 0.0036429654143705914,
                            "process_2": 0.00373860937977355
                        },
                        "ram_energy": {
                            "process_0": 6.2783064828901945e-06,
                            "process_1": 6.689659655620541e-06,
                            "process_3": 6.683587934144257e-06,
                            "process_2": 6.8403250929594295e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.004994595976464317,
                            "process_1": 0.005199925774145565,
                            "process_3": 0.005198773627739807,
                            "process_2": 0.005306745073710693
                        },
                        "total_energy_joules": {
                            "process_0": 17980.54551527154,
                            "process_1": 18719.732786924033,
                            "process_3": 18715.585059863304,
                            "process_2": 19104.282265358495
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 203.98160555776906,
                        "ram_power_avg": 0.6997450590133667,
                        "cpu_energy_total": 0.0061002105253437555,
                        "gpu_energy_total": 0.01457333804755101,
                        "ram_energy_total": 2.6491879165614422e-05,
                        "total_energy_kwh": 0.02070004045206038,
                        "total_energy_joules": 74520.14562741737
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2198600104985841,
                        "joules_per_token": 4.548348732142173,
                        "flops_per_joule": 227454882.84332857,
                        "joules_per_flop": 4.396476292350261e-09
                    },
                    "per-process_emissions": [
                        0.0019026913372340816,
                        0.001980911723660753,
                        0.0019804728134874793,
                        0.0020216045358300884
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0391": {
            "setup": {
                "experiment_id": "0391",
                "date_time": "April 12, 2025 at 11:22:42 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 50.57010413397802,
                        "average_latency_ms_per_batch": 6321.263016747253,
                        "throughput_queries_per_sec": 2.5311397354627334,
                        "throughput_tokens_per_sec": 323.9858861392299
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.5,
                        "cpu_memory_usage_bytes": 1987235840
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0391",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 0.0,
                            "process_0": 273.8589102419407,
                            "process_1": 275.40908392116205,
                            "process_2": 250.5565184845816
                        },
                        "ram_power": {
                            "process_3": 0.7200508117675781,
                            "process_0": 0.693845272064209,
                            "process_1": 0.7208619117736816,
                            "process_2": 0.7261347770690918
                        },
                        "cpu_energy": {
                            "process_3": 0.0015498161987507046,
                            "process_0": 0.0015604886749324579,
                            "process_1": 0.0014597947111287797,
                            "process_2": 0.0015849717460014287
                        },
                        "gpu_energy": {
                            "process_3": 0.003623122342939711,
                            "process_0": 0.0036509034762763903,
                            "process_1": 0.003419223013154893,
                            "process_2": 0.003699520181835947
                        },
                        "ram_energy": {
                            "process_3": 7.082325237347744e-06,
                            "process_0": 6.811778308338837e-06,
                            "process_1": 6.719651392436748e-06,
                            "process_2": 7.074286450225777e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005180020866927764,
                            "process_0": 0.005218203929517187,
                            "process_1": 0.004885737375676109,
                            "process_2": 0.005291566214287602
                        },
                        "total_energy_joules": {
                            "process_3": 18648.075120939953,
                            "process_0": 18785.534146261874,
                            "process_1": 17588.654552433993,
                            "process_2": 19049.638371435365
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 199.95612816192107,
                        "ram_power_avg": 0.7152231931686401,
                        "cpu_energy_total": 0.006155071330813371,
                        "gpu_energy_total": 0.014392769014206941,
                        "ram_energy_total": 2.7688041388349104e-05,
                        "total_energy_kwh": 0.020575528386408665,
                        "total_energy_joules": 74071.90219107119
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22119048539805108,
                        "joules_per_token": 4.520990123966747,
                        "flops_per_joule": 228831317.83802348,
                        "joules_per_flop": 4.370031206601897e-09
                    },
                    "per-process_emissions": [
                        0.001973328949256132,
                        0.0019878747869495725,
                        0.0018612216532638138,
                        0.002015822149332862
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0391": {
            "setup": {
                "experiment_id": "0391",
                "date_time": "April 12, 2025 at 11:22:42 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 50.57010413397802,
                        "average_latency_ms_per_batch": 6321.263016747253,
                        "throughput_queries_per_sec": 2.5311397354627334,
                        "throughput_tokens_per_sec": 323.9858861392299
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.5,
                        "cpu_memory_usage_bytes": 1987235840
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0391",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 0.0,
                            "process_0": 273.8589102419407,
                            "process_1": 275.40908392116205,
                            "process_2": 250.5565184845816
                        },
                        "ram_power": {
                            "process_3": 0.7200508117675781,
                            "process_0": 0.693845272064209,
                            "process_1": 0.7208619117736816,
                            "process_2": 0.7261347770690918
                        },
                        "cpu_energy": {
                            "process_3": 0.0015498161987507046,
                            "process_0": 0.0015604886749324579,
                            "process_1": 0.0014597947111287797,
                            "process_2": 0.0015849717460014287
                        },
                        "gpu_energy": {
                            "process_3": 0.003623122342939711,
                            "process_0": 0.0036509034762763903,
                            "process_1": 0.003419223013154893,
                            "process_2": 0.003699520181835947
                        },
                        "ram_energy": {
                            "process_3": 7.082325237347744e-06,
                            "process_0": 6.811778308338837e-06,
                            "process_1": 6.719651392436748e-06,
                            "process_2": 7.074286450225777e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005180020866927764,
                            "process_0": 0.005218203929517187,
                            "process_1": 0.004885737375676109,
                            "process_2": 0.005291566214287602
                        },
                        "total_energy_joules": {
                            "process_3": 18648.075120939953,
                            "process_0": 18785.534146261874,
                            "process_1": 17588.654552433993,
                            "process_2": 19049.638371435365
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 199.95612816192107,
                        "ram_power_avg": 0.7152231931686401,
                        "cpu_energy_total": 0.006155071330813371,
                        "gpu_energy_total": 0.014392769014206941,
                        "ram_energy_total": 2.7688041388349104e-05,
                        "total_energy_kwh": 0.020575528386408665,
                        "total_energy_joules": 74071.90219107119
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22119048539805108,
                        "joules_per_token": 4.520990123966747,
                        "flops_per_joule": 228831317.83802348,
                        "joules_per_flop": 4.370031206601897e-09
                    },
                    "per-process_emissions": [
                        0.001973328949256132,
                        0.0019878747869495725,
                        0.0018612216532638138,
                        0.002015822149332862
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0392": {
            "setup": {
                "experiment_id": "0392",
                "date_time": "April 12, 2025 at 11:24:27 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.865979424037505,
                        "average_latency_ms_per_batch": 5483.247428004688,
                        "throughput_queries_per_sec": 2.917978845580251,
                        "throughput_tokens_per_sec": 373.5012922342721
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.6,
                        "cpu_memory_usage_bytes": 2000171008
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0392",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 283.9560595191798,
                            "process_3": 264.24280418737044,
                            "process_0": 296.1920509270065,
                            "process_2": 260.1574621080767
                        },
                        "ram_power": {
                            "process_1": 0.7210850715637208,
                            "process_3": 0.7212953567504883,
                            "process_0": 0.6982684135437012,
                            "process_2": 0.7214713096618652
                        },
                        "cpu_energy": {
                            "process_1": 0.0013548507044652065,
                            "process_3": 0.001539000300342196,
                            "process_0": 0.0013540239380936331,
                            "process_2": 0.0015364171915935002
                        },
                        "gpu_energy": {
                            "process_1": 0.003547257004470339,
                            "process_3": 0.003964816227406631,
                            "process_0": 0.003553251175932637,
                            "process_2": 0.003959723167775486
                        },
                        "ram_energy": {
                            "process_1": 6.274572245124532e-06,
                            "process_3": 6.849078142595535e-06,
                            "process_0": 6.05814618411249e-06,
                            "process_2": 6.978276615690665e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.004908382281180671,
                            "process_3": 0.005510665605891422,
                            "process_0": 0.004913333260210381,
                            "process_2": 0.005503118635984676
                        },
                        "total_energy_joules": {
                            "process_1": 17670.176212250415,
                            "process_3": 19838.39618120912,
                            "process_0": 17687.99973675737,
                            "process_2": 19811.227089544835
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 276.1370941854084,
                        "ram_power_avg": 0.7155300378799438,
                        "cpu_energy_total": 0.005784292134494535,
                        "gpu_energy_total": 0.015025047575585093,
                        "ram_energy_total": 2.616007318752322e-05,
                        "total_energy_kwh": 0.02083549978326715,
                        "total_energy_joules": 75007.79921976173
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2184306188213483,
                        "joules_per_token": 4.578112745346785,
                        "flops_per_joule": 225976114.07703215,
                        "joules_per_flop": 4.425246465027333e-09
                    },
                    "per-process_emissions": [
                        0.0018698482300157767,
                        0.0020992880625643373,
                        0.0018717343054771446,
                        0.002096413044378362
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0392": {
            "setup": {
                "experiment_id": "0392",
                "date_time": "April 12, 2025 at 11:24:27 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.865979424037505,
                        "average_latency_ms_per_batch": 5483.247428004688,
                        "throughput_queries_per_sec": 2.917978845580251,
                        "throughput_tokens_per_sec": 373.5012922342721
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.6,
                        "cpu_memory_usage_bytes": 2000171008
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0392",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 283.9560595191798,
                            "process_3": 264.24280418737044,
                            "process_0": 296.1920509270065,
                            "process_2": 260.1574621080767
                        },
                        "ram_power": {
                            "process_1": 0.7210850715637208,
                            "process_3": 0.7212953567504883,
                            "process_0": 0.6982684135437012,
                            "process_2": 0.7214713096618652
                        },
                        "cpu_energy": {
                            "process_1": 0.0013548507044652065,
                            "process_3": 0.001539000300342196,
                            "process_0": 0.0013540239380936331,
                            "process_2": 0.0015364171915935002
                        },
                        "gpu_energy": {
                            "process_1": 0.003547257004470339,
                            "process_3": 0.003964816227406631,
                            "process_0": 0.003553251175932637,
                            "process_2": 0.003959723167775486
                        },
                        "ram_energy": {
                            "process_1": 6.274572245124532e-06,
                            "process_3": 6.849078142595535e-06,
                            "process_0": 6.05814618411249e-06,
                            "process_2": 6.978276615690665e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.004908382281180671,
                            "process_3": 0.005510665605891422,
                            "process_0": 0.004913333260210381,
                            "process_2": 0.005503118635984676
                        },
                        "total_energy_joules": {
                            "process_1": 17670.176212250415,
                            "process_3": 19838.39618120912,
                            "process_0": 17687.99973675737,
                            "process_2": 19811.227089544835
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 276.1370941854084,
                        "ram_power_avg": 0.7155300378799438,
                        "cpu_energy_total": 0.005784292134494535,
                        "gpu_energy_total": 0.015025047575585093,
                        "ram_energy_total": 2.616007318752322e-05,
                        "total_energy_kwh": 0.02083549978326715,
                        "total_energy_joules": 75007.79921976173
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2184306188213483,
                        "joules_per_token": 4.578112745346785,
                        "flops_per_joule": 225976114.07703215,
                        "joules_per_flop": 4.425246465027333e-09
                    },
                    "per-process_emissions": [
                        0.0018698482300157767,
                        0.0020992880625643373,
                        0.0018717343054771446,
                        0.002096413044378362
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0395": {
            "setup": {
                "experiment_id": "0395",
                "date_time": "April 12, 2025 at 11:27:58 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 47.71047981901211,
                        "average_latency_ms_per_batch": 5963.809977376513,
                        "throughput_queries_per_sec": 2.682848726014979,
                        "throughput_tokens_per_sec": 343.40463692991733
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            99.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.6,
                        "cpu_memory_usage_bytes": 1835167744
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0395",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 262.8249776710837,
                            "process_1": 322.4839825954428,
                            "process_2": 271.9605426416404,
                            "process_3": 259.0531600651615
                        },
                        "ram_power": {
                            "process_0": 0.6406416893005372,
                            "process_1": 0.6645956039428711,
                            "process_2": 0.6714620590209962,
                            "process_3": 0.6697154045104982
                        },
                        "cpu_energy": {
                            "process_0": 0.0014743596289972629,
                            "process_1": 0.0012907855749353984,
                            "process_2": 0.0014337446886856927,
                            "process_3": 0.001373781271156077
                        },
                        "gpu_energy": {
                            "process_0": 0.003612169834179335,
                            "process_1": 0.003200646449405431,
                            "process_2": 0.0035205017052883925,
                            "process_3": 0.003392406602811926
                        },
                        "ram_energy": {
                            "process_0": 5.894859452043731e-06,
                            "process_1": 5.553724296861599e-06,
                            "process_2": 6.1311064174456684e-06,
                            "process_3": 6.029156770980324e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005092424322628642,
                            "process_1": 0.004496985748637691,
                            "process_2": 0.004960377500391531,
                            "process_3": 0.004772217030738983
                        },
                        "total_energy_joules": {
                            "process_0": 18332.72756146311,
                            "process_1": 16189.148695095688,
                            "process_2": 17857.359001409513,
                            "process_3": 17179.98131066034
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 279.08066574333213,
                        "ram_power_avg": 0.6616036891937256,
                        "cpu_energy_total": 0.005572671163774431,
                        "gpu_energy_total": 0.013725724591685085,
                        "ram_energy_total": 2.3608846937331324e-05,
                        "total_energy_kwh": 0.019322004602396848,
                        "total_energy_joules": 69559.21656862865
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.23554031813793053,
                        "joules_per_token": 4.245557651893838,
                        "flops_per_joule": 243676853.03684792,
                        "joules_per_flop": 4.103795611020895e-09
                    },
                    "per-process_emissions": [
                        0.001939959045705381,
                        0.0017131267209435285,
                        0.0018896558087741538,
                        0.0018179760778600155
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0395": {
            "setup": {
                "experiment_id": "0395",
                "date_time": "April 12, 2025 at 11:27:58 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 47.71047981901211,
                        "average_latency_ms_per_batch": 5963.809977376513,
                        "throughput_queries_per_sec": 2.682848726014979,
                        "throughput_tokens_per_sec": 343.40463692991733
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            99.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.6,
                        "cpu_memory_usage_bytes": 1835167744
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0395",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 262.8249776710837,
                            "process_1": 322.4839825954428,
                            "process_2": 271.9605426416404,
                            "process_3": 259.0531600651615
                        },
                        "ram_power": {
                            "process_0": 0.6406416893005372,
                            "process_1": 0.6645956039428711,
                            "process_2": 0.6714620590209962,
                            "process_3": 0.6697154045104982
                        },
                        "cpu_energy": {
                            "process_0": 0.0014743596289972629,
                            "process_1": 0.0012907855749353984,
                            "process_2": 0.0014337446886856927,
                            "process_3": 0.001373781271156077
                        },
                        "gpu_energy": {
                            "process_0": 0.003612169834179335,
                            "process_1": 0.003200646449405431,
                            "process_2": 0.0035205017052883925,
                            "process_3": 0.003392406602811926
                        },
                        "ram_energy": {
                            "process_0": 5.894859452043731e-06,
                            "process_1": 5.553724296861599e-06,
                            "process_2": 6.1311064174456684e-06,
                            "process_3": 6.029156770980324e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005092424322628642,
                            "process_1": 0.004496985748637691,
                            "process_2": 0.004960377500391531,
                            "process_3": 0.004772217030738983
                        },
                        "total_energy_joules": {
                            "process_0": 18332.72756146311,
                            "process_1": 16189.148695095688,
                            "process_2": 17857.359001409513,
                            "process_3": 17179.98131066034
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 279.08066574333213,
                        "ram_power_avg": 0.6616036891937256,
                        "cpu_energy_total": 0.005572671163774431,
                        "gpu_energy_total": 0.013725724591685085,
                        "ram_energy_total": 2.3608846937331324e-05,
                        "total_energy_kwh": 0.019322004602396848,
                        "total_energy_joules": 69559.21656862865
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.23554031813793053,
                        "joules_per_token": 4.245557651893838,
                        "flops_per_joule": 243676853.03684792,
                        "joules_per_flop": 4.103795611020895e-09
                    },
                    "per-process_emissions": [
                        0.001939959045705381,
                        0.0017131267209435285,
                        0.0018896558087741538,
                        0.0018179760778600155
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0396": {
            "setup": {
                "experiment_id": "0396",
                "date_time": "April 12, 2025 at 11:29:40 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 44.61095956101781,
                        "average_latency_ms_per_batch": 5576.369945127226,
                        "throughput_queries_per_sec": 2.869250095930455,
                        "throughput_tokens_per_sec": 367.26401227909827
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.8,
                        "cpu_memory_usage_bytes": 1839005696
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0396",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 315.60638324035267,
                            "process_2": 235.85956048273903,
                            "process_1": 242.78654271179346,
                            "process_3": 267.12113661310246
                        },
                        "ram_power": {
                            "process_0": 0.6419820785522461,
                            "process_2": 0.6727023124694824,
                            "process_1": 0.6652650833129883,
                            "process_3": 0.6717967987060548
                        },
                        "cpu_energy": {
                            "process_0": 0.0013778631837440118,
                            "process_2": 0.0014122875318471415,
                            "process_1": 0.0014015752297491418,
                            "process_3": 0.0015031331499058072
                        },
                        "gpu_energy": {
                            "process_0": 0.003335945724310063,
                            "process_2": 0.003409253838511317,
                            "process_1": 0.0033903277122608877,
                            "process_3": 0.0036035992717664556
                        },
                        "ram_energy": {
                            "process_0": 5.590464286394763e-06,
                            "process_2": 6.045532998779379e-06,
                            "process_1": 6.026674577486497e-06,
                            "process_3": 6.218259350582113e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.00471939937234047,
                            "process_2": 0.00482758690335724,
                            "process_1": 0.004797929616587517,
                            "process_3": 0.005112950681022845
                        },
                        "total_energy_joules": {
                            "process_0": 16989.837740425693,
                            "process_2": 17379.312852086063,
                            "process_1": 17272.546619715064,
                            "process_3": 18406.62245168224
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 265.3434057619969,
                        "ram_power_avg": 0.6629365682601929,
                        "cpu_energy_total": 0.005694859095246103,
                        "gpu_energy_total": 0.013739126546848723,
                        "ram_energy_total": 2.388093121324275e-05,
                        "total_energy_kwh": 0.019457866573308072,
                        "total_energy_joules": 70048.31966390906
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.23389568912730843,
                        "joules_per_token": 4.275410135736637,
                        "flops_per_joule": 241975411.7511704,
                        "joules_per_flop": 4.1326513002417206e-09
                    },
                    "per-process_emissions": [
                        0.0017978551908931022,
                        0.0018390692308339405,
                        0.0018277712874390148,
                        0.0019477785619356528
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0396": {
            "setup": {
                "experiment_id": "0396",
                "date_time": "April 12, 2025 at 11:29:40 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 44.61095956101781,
                        "average_latency_ms_per_batch": 5576.369945127226,
                        "throughput_queries_per_sec": 2.869250095930455,
                        "throughput_tokens_per_sec": 367.26401227909827
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.8,
                        "cpu_memory_usage_bytes": 1839005696
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0396",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 315.60638324035267,
                            "process_2": 235.85956048273903,
                            "process_1": 242.78654271179346,
                            "process_3": 267.12113661310246
                        },
                        "ram_power": {
                            "process_0": 0.6419820785522461,
                            "process_2": 0.6727023124694824,
                            "process_1": 0.6652650833129883,
                            "process_3": 0.6717967987060548
                        },
                        "cpu_energy": {
                            "process_0": 0.0013778631837440118,
                            "process_2": 0.0014122875318471415,
                            "process_1": 0.0014015752297491418,
                            "process_3": 0.0015031331499058072
                        },
                        "gpu_energy": {
                            "process_0": 0.003335945724310063,
                            "process_2": 0.003409253838511317,
                            "process_1": 0.0033903277122608877,
                            "process_3": 0.0036035992717664556
                        },
                        "ram_energy": {
                            "process_0": 5.590464286394763e-06,
                            "process_2": 6.045532998779379e-06,
                            "process_1": 6.026674577486497e-06,
                            "process_3": 6.218259350582113e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.00471939937234047,
                            "process_2": 0.00482758690335724,
                            "process_1": 0.004797929616587517,
                            "process_3": 0.005112950681022845
                        },
                        "total_energy_joules": {
                            "process_0": 16989.837740425693,
                            "process_2": 17379.312852086063,
                            "process_1": 17272.546619715064,
                            "process_3": 18406.62245168224
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 265.3434057619969,
                        "ram_power_avg": 0.6629365682601929,
                        "cpu_energy_total": 0.005694859095246103,
                        "gpu_energy_total": 0.013739126546848723,
                        "ram_energy_total": 2.388093121324275e-05,
                        "total_energy_kwh": 0.019457866573308072,
                        "total_energy_joules": 70048.31966390906
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.23389568912730843,
                        "joules_per_token": 4.275410135736637,
                        "flops_per_joule": 241975411.7511704,
                        "joules_per_flop": 4.1326513002417206e-09
                    },
                    "per-process_emissions": [
                        0.0017978551908931022,
                        0.0018390692308339405,
                        0.0018277712874390148,
                        0.0019477785619356528
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0397": {
            "setup": {
                "experiment_id": "0397",
                "date_time": "April 12, 2025 at 11:31:21 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.34060064703226,
                        "average_latency_ms_per_batch": 4792.575080879033,
                        "throughput_queries_per_sec": 3.3384975154244954,
                        "throughput_tokens_per_sec": 427.3276819743354
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.6,
                        "cpu_memory_usage_bytes": 1979043840
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0397",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 286.7093721801209,
                            "process_3": 313.3670507862093,
                            "process_1": 388.2927299168356,
                            "process_0": 371.64737745794037
                        },
                        "ram_power": {
                            "process_2": 0.719520092010498,
                            "process_3": 0.7265768051147461,
                            "process_1": 0.7213196754455566,
                            "process_0": 0.6909842491149902
                        },
                        "cpu_energy": {
                            "process_2": 0.001412614730158566,
                            "process_3": 0.0014598174097809535,
                            "process_1": 0.001243409356312441,
                            "process_0": 0.0011854112167502535
                        },
                        "gpu_energy": {
                            "process_2": 0.004153018322411661,
                            "process_3": 0.004253882569769107,
                            "process_1": 0.003742322438300194,
                            "process_0": 0.003580796475746384
                        },
                        "ram_energy": {
                            "process_2": 6.50578240996521e-06,
                            "process_3": 6.813800395918415e-06,
                            "process_1": 5.690122741602603e-06,
                            "process_0": 5.533888010087767e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005572138834980191,
                            "process_3": 0.005720513779945979,
                            "process_1": 0.004991421917354238,
                            "process_0": 0.004771741580506726
                        },
                        "total_energy_joules": {
                            "process_2": 20059.699805928685,
                            "process_3": 20593.849607805525,
                            "process_1": 17969.118902475257,
                            "process_0": 17178.269689824214
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 340.0041325852766,
                        "ram_power_avg": 0.7146002054214478,
                        "cpu_energy_total": 0.005301252713002214,
                        "gpu_energy_total": 0.015730019806227347,
                        "ram_energy_total": 2.4543593557573994e-05,
                        "total_energy_kwh": 0.021055816112787132,
                        "total_energy_joules": 75800.93800603368
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21614508251462336,
                        "joules_per_token": 4.626522095094829,
                        "flops_per_joule": 223611625.91157907,
                        "joules_per_flop": 4.472039393852545e-09
                    },
                    "per-process_emissions": [
                        0.002122706289185704,
                        0.0021792297244704207,
                        0.001901482179416097,
                        0.0018177949550940375
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0397": {
            "setup": {
                "experiment_id": "0397",
                "date_time": "April 12, 2025 at 11:31:21 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.34060064703226,
                        "average_latency_ms_per_batch": 4792.575080879033,
                        "throughput_queries_per_sec": 3.3384975154244954,
                        "throughput_tokens_per_sec": 427.3276819743354
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.6,
                        "cpu_memory_usage_bytes": 1979043840
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0397",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 286.7093721801209,
                            "process_3": 313.3670507862093,
                            "process_1": 388.2927299168356,
                            "process_0": 371.64737745794037
                        },
                        "ram_power": {
                            "process_2": 0.719520092010498,
                            "process_3": 0.7265768051147461,
                            "process_1": 0.7213196754455566,
                            "process_0": 0.6909842491149902
                        },
                        "cpu_energy": {
                            "process_2": 0.001412614730158566,
                            "process_3": 0.0014598174097809535,
                            "process_1": 0.001243409356312441,
                            "process_0": 0.0011854112167502535
                        },
                        "gpu_energy": {
                            "process_2": 0.004153018322411661,
                            "process_3": 0.004253882569769107,
                            "process_1": 0.003742322438300194,
                            "process_0": 0.003580796475746384
                        },
                        "ram_energy": {
                            "process_2": 6.50578240996521e-06,
                            "process_3": 6.813800395918415e-06,
                            "process_1": 5.690122741602603e-06,
                            "process_0": 5.533888010087767e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005572138834980191,
                            "process_3": 0.005720513779945979,
                            "process_1": 0.004991421917354238,
                            "process_0": 0.004771741580506726
                        },
                        "total_energy_joules": {
                            "process_2": 20059.699805928685,
                            "process_3": 20593.849607805525,
                            "process_1": 17969.118902475257,
                            "process_0": 17178.269689824214
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 340.0041325852766,
                        "ram_power_avg": 0.7146002054214478,
                        "cpu_energy_total": 0.005301252713002214,
                        "gpu_energy_total": 0.015730019806227347,
                        "ram_energy_total": 2.4543593557573994e-05,
                        "total_energy_kwh": 0.021055816112787132,
                        "total_energy_joules": 75800.93800603368
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21614508251462336,
                        "joules_per_token": 4.626522095094829,
                        "flops_per_joule": 223611625.91157907,
                        "joules_per_flop": 4.472039393852545e-09
                    },
                    "per-process_emissions": [
                        0.002122706289185704,
                        0.0021792297244704207,
                        0.001901482179416097,
                        0.0018177949550940375
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0398": {
            "setup": {
                "experiment_id": "0398",
                "date_time": "April 12, 2025 at 11:33:04 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 47.271403127961094,
                        "average_latency_ms_per_batch": 5908.925390995137,
                        "throughput_queries_per_sec": 2.7077681543217795,
                        "throughput_tokens_per_sec": 346.5943237531878
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.5,
                        "cpu_memory_usage_bytes": 1925578752
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0398",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 206.39051649308848,
                            "process_1": 273.96556854837496,
                            "process_2": 0.0,
                            "process_3": 380.6076581810228
                        },
                        "ram_power": {
                            "process_0": 0.6723117828369141,
                            "process_1": 0.7080059051513672,
                            "process_2": 0.7055268287658691,
                            "process_3": 0.7052206993103027
                        },
                        "cpu_energy": {
                            "process_0": 0.0014600938060602856,
                            "process_1": 0.0015031756418402438,
                            "process_2": 0.0014568179090265407,
                            "process_3": 0.0014576548447166712
                        },
                        "gpu_energy": {
                            "process_0": 0.0036091895540164565,
                            "process_1": 0.003706418520687871,
                            "process_2": 0.003607591497182261,
                            "process_3": 0.003606642051977582
                        },
                        "ram_energy": {
                            "process_0": 6.24647511541839e-06,
                            "process_1": 6.521249882188541e-06,
                            "process_2": 6.733308992385221e-06,
                            "process_3": 6.472202159802746e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.00507552983519216,
                            "process_1": 0.0052161154124103016,
                            "process_2": 0.005071142715201186,
                            "process_3": 0.005070769098854055
                        },
                        "total_energy_joules": {
                            "process_0": 18271.907406691775,
                            "process_1": 18778.015484677086,
                            "process_2": 18256.113774724272,
                            "process_3": 18254.7687558746
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 215.24093580562158,
                        "ram_power_avg": 0.6977663040161133,
                        "cpu_energy_total": 0.005877742201643742,
                        "gpu_energy_total": 0.01452984162386417,
                        "ram_energy_total": 2.59732361497949e-05,
                        "total_energy_kwh": 0.020433557061657705,
                        "total_energy_joules": 73560.80542196773
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22272730574408836,
                        "joules_per_token": 4.4897952528056475,
                        "flops_per_joule": 230421226.3028073,
                        "joules_per_flop": 4.3398779532830595e-09
                    },
                    "per-process_emissions": [
                        0.0019335230907164534,
                        0.0019870791663577046,
                        0.001931851817355892,
                        0.0019317094882084524
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0398": {
            "setup": {
                "experiment_id": "0398",
                "date_time": "April 12, 2025 at 11:33:04 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 47.271403127961094,
                        "average_latency_ms_per_batch": 5908.925390995137,
                        "throughput_queries_per_sec": 2.7077681543217795,
                        "throughput_tokens_per_sec": 346.5943237531878
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.5,
                        "cpu_memory_usage_bytes": 1925578752
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0398",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 206.39051649308848,
                            "process_1": 273.96556854837496,
                            "process_2": 0.0,
                            "process_3": 380.6076581810228
                        },
                        "ram_power": {
                            "process_0": 0.6723117828369141,
                            "process_1": 0.7080059051513672,
                            "process_2": 0.7055268287658691,
                            "process_3": 0.7052206993103027
                        },
                        "cpu_energy": {
                            "process_0": 0.0014600938060602856,
                            "process_1": 0.0015031756418402438,
                            "process_2": 0.0014568179090265407,
                            "process_3": 0.0014576548447166712
                        },
                        "gpu_energy": {
                            "process_0": 0.0036091895540164565,
                            "process_1": 0.003706418520687871,
                            "process_2": 0.003607591497182261,
                            "process_3": 0.003606642051977582
                        },
                        "ram_energy": {
                            "process_0": 6.24647511541839e-06,
                            "process_1": 6.521249882188541e-06,
                            "process_2": 6.733308992385221e-06,
                            "process_3": 6.472202159802746e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.00507552983519216,
                            "process_1": 0.0052161154124103016,
                            "process_2": 0.005071142715201186,
                            "process_3": 0.005070769098854055
                        },
                        "total_energy_joules": {
                            "process_0": 18271.907406691775,
                            "process_1": 18778.015484677086,
                            "process_2": 18256.113774724272,
                            "process_3": 18254.7687558746
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 215.24093580562158,
                        "ram_power_avg": 0.6977663040161133,
                        "cpu_energy_total": 0.005877742201643742,
                        "gpu_energy_total": 0.01452984162386417,
                        "ram_energy_total": 2.59732361497949e-05,
                        "total_energy_kwh": 0.020433557061657705,
                        "total_energy_joules": 73560.80542196773
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22272730574408836,
                        "joules_per_token": 4.4897952528056475,
                        "flops_per_joule": 230421226.3028073,
                        "joules_per_flop": 4.3398779532830595e-09
                    },
                    "per-process_emissions": [
                        0.0019335230907164534,
                        0.0019870791663577046,
                        0.001931851817355892,
                        0.0019317094882084524
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0399": {
            "setup": {
                "experiment_id": "0399",
                "date_time": "April 12, 2025 at 11:34:54 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_True_latency_0.2_latency_0.6_latency_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.6,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 50.250264270056505,
                        "average_latency_ms_per_batch": 6281.283033757063,
                        "throughput_queries_per_sec": 2.5472502853337944,
                        "throughput_tokens_per_sec": 326.0480365227257
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.5,
                        "cpu_memory_usage_bytes": 1932464128
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0399",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 246.00500444532736,
                            "process_0": 284.03253922627863,
                            "process_1": 245.27837559815748,
                            "process_3": 241.47066203214794
                        },
                        "ram_power": {
                            "process_2": 0.7035326957702637,
                            "process_0": 0.6746220588684082,
                            "process_1": 0.7064337730407715,
                            "process_3": 0.7058615684509277
                        },
                        "cpu_energy": {
                            "process_2": 0.001585731931125338,
                            "process_0": 0.0015514360595607283,
                            "process_1": 0.0016232439892201003,
                            "process_3": 0.0017481078949076617
                        },
                        "gpu_energy": {
                            "process_2": 0.0036870329496250243,
                            "process_0": 0.003616749837842992,
                            "process_1": 0.0037691113486200134,
                            "process_3": 0.004038698230957483
                        },
                        "ram_energy": {
                            "process_2": 7.097524929099228e-06,
                            "process_0": 6.570611542838847e-06,
                            "process_1": 7.221126712089794e-06,
                            "process_3": 7.948083578521868e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005279862405679459,
                            "process_0": 0.005174756508946557,
                            "process_1": 0.005399576464552203,
                            "process_3": 0.005794754209443667
                        },
                        "total_energy_joules": {
                            "process_2": 19007.504660446055,
                            "process_0": 18629.123432207605,
                            "process_1": 19438.475272387932,
                            "process_3": 20861.1151539972
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 254.19664532547787,
                        "ram_power_avg": 0.6976125240325928,
                        "cpu_energy_total": 0.006508519874813828,
                        "gpu_energy_total": 0.015111592367045512,
                        "ram_energy_total": 2.8837346762549734e-05,
                        "total_energy_kwh": 0.021648949588621885,
                        "total_energy_joules": 77936.2185190388
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21022318392311534,
                        "joules_per_token": 4.756849274843677,
                        "flops_per_joule": 217485160.49712297,
                        "joules_per_flop": 4.598014860941414e-09
                    },
                    "per-process_emissions": [
                        0.0020113635834435903,
                        0.0019713234920831907,
                        0.002056968654171162,
                        0.002207511616087565
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0399": {
            "setup": {
                "experiment_id": "0399",
                "date_time": "April 12, 2025 at 11:34:54 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_True_latency_0.2_latency_0.6_latency_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.6,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 50.250264270056505,
                        "average_latency_ms_per_batch": 6281.283033757063,
                        "throughput_queries_per_sec": 2.5472502853337944,
                        "throughput_tokens_per_sec": 326.0480365227257
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.5,
                        "cpu_memory_usage_bytes": 1932464128
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0399",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 246.00500444532736,
                            "process_0": 284.03253922627863,
                            "process_1": 245.27837559815748,
                            "process_3": 241.47066203214794
                        },
                        "ram_power": {
                            "process_2": 0.7035326957702637,
                            "process_0": 0.6746220588684082,
                            "process_1": 0.7064337730407715,
                            "process_3": 0.7058615684509277
                        },
                        "cpu_energy": {
                            "process_2": 0.001585731931125338,
                            "process_0": 0.0015514360595607283,
                            "process_1": 0.0016232439892201003,
                            "process_3": 0.0017481078949076617
                        },
                        "gpu_energy": {
                            "process_2": 0.0036870329496250243,
                            "process_0": 0.003616749837842992,
                            "process_1": 0.0037691113486200134,
                            "process_3": 0.004038698230957483
                        },
                        "ram_energy": {
                            "process_2": 7.097524929099228e-06,
                            "process_0": 6.570611542838847e-06,
                            "process_1": 7.221126712089794e-06,
                            "process_3": 7.948083578521868e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005279862405679459,
                            "process_0": 0.005174756508946557,
                            "process_1": 0.005399576464552203,
                            "process_3": 0.005794754209443667
                        },
                        "total_energy_joules": {
                            "process_2": 19007.504660446055,
                            "process_0": 18629.123432207605,
                            "process_1": 19438.475272387932,
                            "process_3": 20861.1151539972
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 254.19664532547787,
                        "ram_power_avg": 0.6976125240325928,
                        "cpu_energy_total": 0.006508519874813828,
                        "gpu_energy_total": 0.015111592367045512,
                        "ram_energy_total": 2.8837346762549734e-05,
                        "total_energy_kwh": 0.021648949588621885,
                        "total_energy_joules": 77936.2185190388
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21022318392311534,
                        "joules_per_token": 4.756849274843677,
                        "flops_per_joule": 217485160.49712297,
                        "joules_per_flop": 4.598014860941414e-09
                    },
                    "per-process_emissions": [
                        0.0020113635834435903,
                        0.0019713234920831907,
                        0.002056968654171162,
                        0.002207511616087565
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0400": {
            "setup": {
                "experiment_id": "0400",
                "date_time": "April 12, 2025 at 11:36:42 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 44.2267587769893,
                        "average_latency_ms_per_batch": 5528.3448471236625,
                        "throughput_queries_per_sec": 2.8941754616347106,
                        "throughput_tokens_per_sec": 370.45445908924296
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.5,
                        "cpu_memory_usage_bytes": 1817665536
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0400",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 222.74769903357543,
                            "process_3": 231.1283092995596,
                            "process_1": 220.55281983821345,
                            "process_2": 264.66213448921485
                        },
                        "ram_power": {
                            "process_0": 0.6346235275268555,
                            "process_3": 0.6700844764709473,
                            "process_1": 0.6666040420532227,
                            "process_2": 0.6663951873779297
                        },
                        "cpu_energy": {
                            "process_0": 0.0013625363724040654,
                            "process_3": 0.0014254411591255124,
                            "process_1": 0.001584693451342901,
                            "process_2": 0.0015426175575285021
                        },
                        "gpu_energy": {
                            "process_0": 0.003094780809155928,
                            "process_3": 0.00322724591512924,
                            "process_1": 0.0035693956332922827,
                            "process_2": 0.0034786769496055747
                        },
                        "ram_energy": {
                            "process_0": 5.659348759131857e-06,
                            "process_3": 6.15450298802312e-06,
                            "process_1": 6.574144239019027e-06,
                            "process_2": 6.407333518418667e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.004462976530319127,
                            "process_3": 0.004658841577242774,
                            "process_1": 0.005160663228874198,
                            "process_2": 0.0050277018406524945
                        },
                        "total_energy_joules": {
                            "process_0": 16066.715509148857,
                            "process_3": 16771.829678073987,
                            "process_1": 18578.38762394711,
                            "process_2": 18099.72662634898
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 234.77274066514082,
                        "ram_power_avg": 0.6594268083572388,
                        "cpu_energy_total": 0.005915288540400981,
                        "gpu_energy_total": 0.013370099307183025,
                        "ram_energy_total": 2.4795329504592674e-05,
                        "total_energy_kwh": 0.019310183177088592,
                        "total_energy_joules": 69516.65943751895
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.23568451264154627,
                        "joules_per_token": 4.242960170747006,
                        "flops_per_joule": 243826028.61385345,
                        "joules_per_flop": 4.10128486152599e-09
                    },
                    "per-process_emissions": [
                        0.0017001709092250713,
                        0.0017747856988506348,
                        0.0019659546570396256,
                        0.0019153030161965679
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0400": {
            "setup": {
                "experiment_id": "0400",
                "date_time": "April 12, 2025 at 11:36:42 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 44.2267587769893,
                        "average_latency_ms_per_batch": 5528.3448471236625,
                        "throughput_queries_per_sec": 2.8941754616347106,
                        "throughput_tokens_per_sec": 370.45445908924296
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.5,
                        "cpu_memory_usage_bytes": 1817665536
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0400",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 222.74769903357543,
                            "process_3": 231.1283092995596,
                            "process_1": 220.55281983821345,
                            "process_2": 264.66213448921485
                        },
                        "ram_power": {
                            "process_0": 0.6346235275268555,
                            "process_3": 0.6700844764709473,
                            "process_1": 0.6666040420532227,
                            "process_2": 0.6663951873779297
                        },
                        "cpu_energy": {
                            "process_0": 0.0013625363724040654,
                            "process_3": 0.0014254411591255124,
                            "process_1": 0.001584693451342901,
                            "process_2": 0.0015426175575285021
                        },
                        "gpu_energy": {
                            "process_0": 0.003094780809155928,
                            "process_3": 0.00322724591512924,
                            "process_1": 0.0035693956332922827,
                            "process_2": 0.0034786769496055747
                        },
                        "ram_energy": {
                            "process_0": 5.659348759131857e-06,
                            "process_3": 6.15450298802312e-06,
                            "process_1": 6.574144239019027e-06,
                            "process_2": 6.407333518418667e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.004462976530319127,
                            "process_3": 0.004658841577242774,
                            "process_1": 0.005160663228874198,
                            "process_2": 0.0050277018406524945
                        },
                        "total_energy_joules": {
                            "process_0": 16066.715509148857,
                            "process_3": 16771.829678073987,
                            "process_1": 18578.38762394711,
                            "process_2": 18099.72662634898
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 234.77274066514082,
                        "ram_power_avg": 0.6594268083572388,
                        "cpu_energy_total": 0.005915288540400981,
                        "gpu_energy_total": 0.013370099307183025,
                        "ram_energy_total": 2.4795329504592674e-05,
                        "total_energy_kwh": 0.019310183177088592,
                        "total_energy_joules": 69516.65943751895
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.23568451264154627,
                        "joules_per_token": 4.242960170747006,
                        "flops_per_joule": 243826028.61385345,
                        "joules_per_flop": 4.10128486152599e-09
                    },
                    "per-process_emissions": [
                        0.0017001709092250713,
                        0.0017747856988506348,
                        0.0019659546570396256,
                        0.0019153030161965679
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0401": {
            "setup": {
                "experiment_id": "0401",
                "date_time": "April 12, 2025 at 11:38:28 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 46.90067927495693,
                        "average_latency_ms_per_batch": 5862.584909369616,
                        "throughput_queries_per_sec": 2.7291715595331865,
                        "throughput_tokens_per_sec": 349.33395962024787
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.6,
                        "cpu_memory_usage_bytes": 1930051584
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0401",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 276.36728675805796,
                            "process_0": 264.3151978624959,
                            "process_1": 893.5701589204424,
                            "process_2": 243.06183666706988
                        },
                        "ram_power": {
                            "process_3": 0.7063493728637695,
                            "process_0": 0.6737794876098633,
                            "process_1": 0.7183585166931152,
                            "process_2": 0.709043025970459
                        },
                        "cpu_energy": {
                            "process_3": 0.0013826502565916598,
                            "process_0": 0.0014486427812198596,
                            "process_1": 0.0014207591660633625,
                            "process_2": 0.0015184167487514058
                        },
                        "gpu_energy": {
                            "process_3": 0.00361428511364692,
                            "process_0": 0.003770246071747918,
                            "process_1": 0.0037074318548313556,
                            "process_2": 0.0039194986911508245
                        },
                        "ram_energy": {
                            "process_3": 6.269147666498022e-06,
                            "process_0": 6.285522030513556e-06,
                            "process_1": 6.631961490390543e-06,
                            "process_2": 7.018155168064236e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005003204517905077,
                            "process_0": 0.005225174374998291,
                            "process_1": 0.005134822982385108,
                            "process_2": 0.005444933595070295
                        },
                        "total_energy_joules": {
                            "process_3": 18011.53626445828,
                            "process_0": 18810.627749993848,
                            "process_1": 18485.36273658639,
                            "process_2": 19601.760942253062
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 419.32862005201656,
                        "ram_power_avg": 0.7018826007843018,
                        "cpu_energy_total": 0.005770468952626288,
                        "gpu_energy_total": 0.015011461731377018,
                        "ram_energy_total": 2.6204786355466357e-05,
                        "total_energy_kwh": 0.02080813547035877,
                        "total_energy_joules": 74909.28769329158
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21871787203587642,
                        "joules_per_token": 4.572100078936254,
                        "flops_per_joule": 226273290.20337135,
                        "joules_per_flop": 4.419434565614057e-09
                    },
                    "per-process_emissions": [
                        0.0019059707610959392,
                        0.001990530178155599,
                        0.001956110815139607,
                        0.002074247453042029
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0401": {
            "setup": {
                "experiment_id": "0401",
                "date_time": "April 12, 2025 at 11:38:28 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 46.90067927495693,
                        "average_latency_ms_per_batch": 5862.584909369616,
                        "throughput_queries_per_sec": 2.7291715595331865,
                        "throughput_tokens_per_sec": 349.33395962024787
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.6,
                        "cpu_memory_usage_bytes": 1930051584
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0401",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 276.36728675805796,
                            "process_0": 264.3151978624959,
                            "process_1": 893.5701589204424,
                            "process_2": 243.06183666706988
                        },
                        "ram_power": {
                            "process_3": 0.7063493728637695,
                            "process_0": 0.6737794876098633,
                            "process_1": 0.7183585166931152,
                            "process_2": 0.709043025970459
                        },
                        "cpu_energy": {
                            "process_3": 0.0013826502565916598,
                            "process_0": 0.0014486427812198596,
                            "process_1": 0.0014207591660633625,
                            "process_2": 0.0015184167487514058
                        },
                        "gpu_energy": {
                            "process_3": 0.00361428511364692,
                            "process_0": 0.003770246071747918,
                            "process_1": 0.0037074318548313556,
                            "process_2": 0.0039194986911508245
                        },
                        "ram_energy": {
                            "process_3": 6.269147666498022e-06,
                            "process_0": 6.285522030513556e-06,
                            "process_1": 6.631961490390543e-06,
                            "process_2": 7.018155168064236e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.005003204517905077,
                            "process_0": 0.005225174374998291,
                            "process_1": 0.005134822982385108,
                            "process_2": 0.005444933595070295
                        },
                        "total_energy_joules": {
                            "process_3": 18011.53626445828,
                            "process_0": 18810.627749993848,
                            "process_1": 18485.36273658639,
                            "process_2": 19601.760942253062
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 419.32862005201656,
                        "ram_power_avg": 0.7018826007843018,
                        "cpu_energy_total": 0.005770468952626288,
                        "gpu_energy_total": 0.015011461731377018,
                        "ram_energy_total": 2.6204786355466357e-05,
                        "total_energy_kwh": 0.02080813547035877,
                        "total_energy_joules": 74909.28769329158
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21871787203587642,
                        "joules_per_token": 4.572100078936254,
                        "flops_per_joule": 226273290.20337135,
                        "joules_per_flop": 4.419434565614057e-09
                    },
                    "per-process_emissions": [
                        0.0019059707610959392,
                        0.001990530178155599,
                        0.001956110815139607,
                        0.002074247453042029
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0402": {
            "setup": {
                "experiment_id": "0402",
                "date_time": "April 12, 2025 at 11:40:08 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 45.40517473896034,
                        "average_latency_ms_per_batch": 5675.646842370043,
                        "throughput_queries_per_sec": 2.8190619403159873,
                        "throughput_tokens_per_sec": 360.8399283604464
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            99.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.7,
                        "cpu_memory_usage_bytes": 1813827584
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0402",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 288.40988955242,
                            "process_1": 230.58018618694354,
                            "process_2": 270.4675759039401,
                            "process_3": 362.9203269316764
                        },
                        "ram_power": {
                            "process_0": 0.6331887245178223,
                            "process_1": 0.6802024841308594,
                            "process_2": 0.6641879081726074,
                            "process_3": 0.6679744720458984
                        },
                        "cpu_energy": {
                            "process_0": 0.0014037699878463171,
                            "process_1": 0.0014274823274354278,
                            "process_2": 0.0014288029606841517,
                            "process_3": 0.0013613095050650375
                        },
                        "gpu_energy": {
                            "process_0": 0.003385362986066287,
                            "process_1": 0.003443824699501885,
                            "process_2": 0.0034504466492437658,
                            "process_3": 0.0032785515117279562
                        },
                        "ram_energy": {
                            "process_0": 5.5218232026652795e-06,
                            "process_1": 6.075808589405569e-06,
                            "process_2": 6.043640886608966e-06,
                            "process_3": 5.876034235847315e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.004794654797115271,
                            "process_1": 0.004877382835526717,
                            "process_2": 0.004885293250814529,
                            "process_3": 0.004645737051028841
                        },
                        "total_energy_joules": {
                            "process_0": 17260.757269614976,
                            "process_1": 17558.578207896182,
                            "process_2": 17587.055702932303,
                            "process_3": 16724.65338370383
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 288.094494643745,
                        "ram_power_avg": 0.6613883972167969,
                        "cpu_energy_total": 0.005621364781030934,
                        "gpu_energy_total": 0.013558185846539894,
                        "ram_energy_total": 2.351730691452713e-05,
                        "total_energy_kwh": 0.019203067934485358,
                        "total_energy_joules": 69131.04456414729
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2369991673537805,
                        "joules_per_token": 4.219424106698443,
                        "flops_per_joule": 245186096.92674288,
                        "joules_per_flop": 4.078534682571262e-09
                    },
                    "per-process_emissions": [
                        0.0018265237449610625,
                        0.0018580389911939028,
                        0.0018610524638977947,
                        0.0017697935295894372
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0402": {
            "setup": {
                "experiment_id": "0402",
                "date_time": "April 12, 2025 at 11:40:08 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 45.40517473896034,
                        "average_latency_ms_per_batch": 5675.646842370043,
                        "throughput_queries_per_sec": 2.8190619403159873,
                        "throughput_tokens_per_sec": 360.8399283604464
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            99.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.7,
                        "cpu_memory_usage_bytes": 1813827584
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0402",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 288.40988955242,
                            "process_1": 230.58018618694354,
                            "process_2": 270.4675759039401,
                            "process_3": 362.9203269316764
                        },
                        "ram_power": {
                            "process_0": 0.6331887245178223,
                            "process_1": 0.6802024841308594,
                            "process_2": 0.6641879081726074,
                            "process_3": 0.6679744720458984
                        },
                        "cpu_energy": {
                            "process_0": 0.0014037699878463171,
                            "process_1": 0.0014274823274354278,
                            "process_2": 0.0014288029606841517,
                            "process_3": 0.0013613095050650375
                        },
                        "gpu_energy": {
                            "process_0": 0.003385362986066287,
                            "process_1": 0.003443824699501885,
                            "process_2": 0.0034504466492437658,
                            "process_3": 0.0032785515117279562
                        },
                        "ram_energy": {
                            "process_0": 5.5218232026652795e-06,
                            "process_1": 6.075808589405569e-06,
                            "process_2": 6.043640886608966e-06,
                            "process_3": 5.876034235847315e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.004794654797115271,
                            "process_1": 0.004877382835526717,
                            "process_2": 0.004885293250814529,
                            "process_3": 0.004645737051028841
                        },
                        "total_energy_joules": {
                            "process_0": 17260.757269614976,
                            "process_1": 17558.578207896182,
                            "process_2": 17587.055702932303,
                            "process_3": 16724.65338370383
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 288.094494643745,
                        "ram_power_avg": 0.6613883972167969,
                        "cpu_energy_total": 0.005621364781030934,
                        "gpu_energy_total": 0.013558185846539894,
                        "ram_energy_total": 2.351730691452713e-05,
                        "total_energy_kwh": 0.019203067934485358,
                        "total_energy_joules": 69131.04456414729
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2369991673537805,
                        "joules_per_token": 4.219424106698443,
                        "flops_per_joule": 245186096.92674288,
                        "joules_per_flop": 4.078534682571262e-09
                    },
                    "per-process_emissions": [
                        0.0018265237449610625,
                        0.0018580389911939028,
                        0.0018610524638977947,
                        0.0017697935295894372
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0403": {
            "setup": {
                "experiment_id": "0403",
                "date_time": "April 12, 2025 at 11:41:51 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 47.89150324004004,
                        "average_latency_ms_per_batch": 5986.437905005005,
                        "throughput_queries_per_sec": 2.672707919783664,
                        "throughput_tokens_per_sec": 342.106613732309
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.4,
                        "cpu_memory_usage_bytes": 1931264000
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0403",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 266.95266645278735,
                            "process_2": 309.9441740466533,
                            "process_3": 259.96148896501245,
                            "process_1": 270.82165353624424
                        },
                        "ram_power": {
                            "process_0": 0.6742029190063477,
                            "process_2": 0.6963815689086915,
                            "process_3": 0.7122631072998047,
                            "process_1": 0.7062935829162598
                        },
                        "cpu_energy": {
                            "process_0": 0.0014794694897427689,
                            "process_2": 0.001346362376656543,
                            "process_3": 0.001480581425219498,
                            "process_1": 0.001439035118969514
                        },
                        "gpu_energy": {
                            "process_0": 0.0038207027787826586,
                            "process_2": 0.003507544472700186,
                            "process_3": 0.0038207027787826586,
                            "process_1": 0.003727151037274723
                        },
                        "ram_energy": {
                            "process_0": 6.190332391764584e-06,
                            "process_2": 5.967379474320154e-06,
                            "process_3": 6.5993483781222984e-06,
                            "process_1": 6.420771519615621e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005306362600917191,
                            "process_2": 0.004859874228831051,
                            "process_3": 0.005307883552380278,
                            "process_1": 0.005172606927763852
                        },
                        "total_energy_joules": {
                            "process_0": 19102.90536330189,
                            "process_2": 17495.547223791782,
                            "process_3": 19108.380788569,
                            "process_1": 18621.384939949865
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 276.91999575017434,
                        "ram_power_avg": 0.6972852945327759,
                        "cpu_energy_total": 0.005745448410588323,
                        "gpu_energy_total": 0.014876101067540226,
                        "ram_energy_total": 2.517783176382266e-05,
                        "total_energy_kwh": 0.020646727309892372,
                        "total_energy_joules": 74328.21831561252
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22042772410378855,
                        "joules_per_token": 4.536634418677522,
                        "flops_per_joule": 228042207.6199785,
                        "joules_per_flop": 4.385153127733496e-09
                    },
                    "per-process_emissions": [
                        0.002021458832819404,
                        0.0018513690874731887,
                        0.002022038239279267,
                        0.0019705046091316393
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0403": {
            "setup": {
                "experiment_id": "0403",
                "date_time": "April 12, 2025 at 11:41:51 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 47.89150324004004,
                        "average_latency_ms_per_batch": 5986.437905005005,
                        "throughput_queries_per_sec": 2.672707919783664,
                        "throughput_tokens_per_sec": 342.106613732309
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.4,
                        "cpu_memory_usage_bytes": 1931264000
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0403",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 266.95266645278735,
                            "process_2": 309.9441740466533,
                            "process_3": 259.96148896501245,
                            "process_1": 270.82165353624424
                        },
                        "ram_power": {
                            "process_0": 0.6742029190063477,
                            "process_2": 0.6963815689086915,
                            "process_3": 0.7122631072998047,
                            "process_1": 0.7062935829162598
                        },
                        "cpu_energy": {
                            "process_0": 0.0014794694897427689,
                            "process_2": 0.001346362376656543,
                            "process_3": 0.001480581425219498,
                            "process_1": 0.001439035118969514
                        },
                        "gpu_energy": {
                            "process_0": 0.0038207027787826586,
                            "process_2": 0.003507544472700186,
                            "process_3": 0.0038207027787826586,
                            "process_1": 0.003727151037274723
                        },
                        "ram_energy": {
                            "process_0": 6.190332391764584e-06,
                            "process_2": 5.967379474320154e-06,
                            "process_3": 6.5993483781222984e-06,
                            "process_1": 6.420771519615621e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005306362600917191,
                            "process_2": 0.004859874228831051,
                            "process_3": 0.005307883552380278,
                            "process_1": 0.005172606927763852
                        },
                        "total_energy_joules": {
                            "process_0": 19102.90536330189,
                            "process_2": 17495.547223791782,
                            "process_3": 19108.380788569,
                            "process_1": 18621.384939949865
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 276.91999575017434,
                        "ram_power_avg": 0.6972852945327759,
                        "cpu_energy_total": 0.005745448410588323,
                        "gpu_energy_total": 0.014876101067540226,
                        "ram_energy_total": 2.517783176382266e-05,
                        "total_energy_kwh": 0.020646727309892372,
                        "total_energy_joules": 74328.21831561252
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22042772410378855,
                        "joules_per_token": 4.536634418677522,
                        "flops_per_joule": 228042207.6199785,
                        "joules_per_flop": 4.385153127733496e-09
                    },
                    "per-process_emissions": [
                        0.002021458832819404,
                        0.0018513690874731887,
                        0.002022038239279267,
                        0.0019705046091316393
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0404": {
            "setup": {
                "experiment_id": "0404",
                "date_time": "April 12, 2025 at 11:43:36 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 48.65203271294013,
                        "average_latency_ms_per_batch": 6081.504089117516,
                        "throughput_queries_per_sec": 2.6309281002755607,
                        "throughput_tokens_per_sec": 336.75879683527177
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.6,
                        "cpu_memory_usage_bytes": 1969479680
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0404",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 239.5843672789322,
                            "process_0": 258.2240767636241,
                            "process_3": 279.87866397731904,
                            "process_1": 261.64344854925537
                        },
                        "ram_power": {
                            "process_2": 0.7201652526855469,
                            "process_0": 0.6875495910644531,
                            "process_3": 0.7245182991027832,
                            "process_1": 0.7196531295776367
                        },
                        "cpu_energy": {
                            "process_2": 0.0015737179815623675,
                            "process_0": 0.0015009003554978338,
                            "process_3": 0.0014946204247771673,
                            "process_1": 0.0014989762550958402
                        },
                        "gpu_energy": {
                            "process_2": 0.003795153591676703,
                            "process_0": 0.0036480390295392517,
                            "process_3": 0.0036344481853340938,
                            "process_1": 0.003642409858370499
                        },
                        "ram_energy": {
                            "process_2": 7.156981637660553e-06,
                            "process_0": 6.633159971391655e-06,
                            "process_3": 6.840812147542347e-06,
                            "process_1": 6.837125786194636e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.00537602855487673,
                            "process_0": 0.005155572545008477,
                            "process_3": 0.005135909422258805,
                            "process_1": 0.005148223239252533
                        },
                        "total_energy_joules": {
                            "process_2": 19353.70279755623,
                            "process_0": 18560.061162030517,
                            "process_3": 18489.273920131698,
                            "process_1": 18533.60366130912
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 259.8326391422827,
                        "ram_power_avg": 0.712971568107605,
                        "cpu_energy_total": 0.006068215016933209,
                        "gpu_energy_total": 0.014720050664920548,
                        "ram_energy_total": 2.7468079542789187e-05,
                        "total_energy_kwh": 0.020815733761396546,
                        "total_energy_joules": 74936.64154102758
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21863803425230646,
                        "joules_per_token": 4.573769625306859,
                        "flops_per_joule": 226190694.49318653,
                        "joules_per_flop": 4.421048364702389e-09
                    },
                    "per-process_emissions": [
                        0.0020479980779802903,
                        0.0019640153610209795,
                        0.001956524694409492,
                        0.0019612156429932525
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0404": {
            "setup": {
                "experiment_id": "0404",
                "date_time": "April 12, 2025 at 11:43:36 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 48.65203271294013,
                        "average_latency_ms_per_batch": 6081.504089117516,
                        "throughput_queries_per_sec": 2.6309281002755607,
                        "throughput_tokens_per_sec": 336.75879683527177
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.6,
                        "cpu_memory_usage_bytes": 1969479680
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0404",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 239.5843672789322,
                            "process_0": 258.2240767636241,
                            "process_3": 279.87866397731904,
                            "process_1": 261.64344854925537
                        },
                        "ram_power": {
                            "process_2": 0.7201652526855469,
                            "process_0": 0.6875495910644531,
                            "process_3": 0.7245182991027832,
                            "process_1": 0.7196531295776367
                        },
                        "cpu_energy": {
                            "process_2": 0.0015737179815623675,
                            "process_0": 0.0015009003554978338,
                            "process_3": 0.0014946204247771673,
                            "process_1": 0.0014989762550958402
                        },
                        "gpu_energy": {
                            "process_2": 0.003795153591676703,
                            "process_0": 0.0036480390295392517,
                            "process_3": 0.0036344481853340938,
                            "process_1": 0.003642409858370499
                        },
                        "ram_energy": {
                            "process_2": 7.156981637660553e-06,
                            "process_0": 6.633159971391655e-06,
                            "process_3": 6.840812147542347e-06,
                            "process_1": 6.837125786194636e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.00537602855487673,
                            "process_0": 0.005155572545008477,
                            "process_3": 0.005135909422258805,
                            "process_1": 0.005148223239252533
                        },
                        "total_energy_joules": {
                            "process_2": 19353.70279755623,
                            "process_0": 18560.061162030517,
                            "process_3": 18489.273920131698,
                            "process_1": 18533.60366130912
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 259.8326391422827,
                        "ram_power_avg": 0.712971568107605,
                        "cpu_energy_total": 0.006068215016933209,
                        "gpu_energy_total": 0.014720050664920548,
                        "ram_energy_total": 2.7468079542789187e-05,
                        "total_energy_kwh": 0.020815733761396546,
                        "total_energy_joules": 74936.64154102758
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21863803425230646,
                        "joules_per_token": 4.573769625306859,
                        "flops_per_joule": 226190694.49318653,
                        "joules_per_flop": 4.421048364702389e-09
                    },
                    "per-process_emissions": [
                        0.0020479980779802903,
                        0.0019640153610209795,
                        0.001956524694409492,
                        0.0019612156429932525
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0406": {
            "setup": {
                "experiment_id": "0406",
                "date_time": "April 12, 2025 at 11:46:12 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 44.558094358973904,
                        "average_latency_ms_per_batch": 5569.761794871738,
                        "throughput_queries_per_sec": 2.872654269475532,
                        "throughput_tokens_per_sec": 367.6997464928681
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            98.0
                        ],
                        "cpu_usage_percent": 41.6,
                        "cpu_memory_usage_bytes": 1990201344
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0406",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 310.7729457188909,
                            "process_0": 264.9200447854536,
                            "process_2": 296.1519252006642,
                            "process_1": 372.12869972315406
                        },
                        "ram_power": {
                            "process_3": 0.7208347320556641,
                            "process_0": 0.694786548614502,
                            "process_2": 0.7220578193664551,
                            "process_1": 0.7115492820739746
                        },
                        "cpu_energy": {
                            "process_3": 0.001276723576312179,
                            "process_0": 0.0013790599107205705,
                            "process_2": 0.001351347187814099,
                            "process_1": 0.001256347985126013
                        },
                        "gpu_energy": {
                            "process_3": 0.0037146785272959004,
                            "process_0": 0.00395963094548013,
                            "process_2": 0.0039001003423007052,
                            "process_1": 0.003665126820988185
                        },
                        "ram_energy": {
                            "process_3": 6.086666033423142e-06,
                            "process_0": 6.344370254469146e-06,
                            "process_2": 6.277382978991789e-06,
                            "process_1": 6.110746027685136e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.004997488769641504,
                            "process_0": 0.00534503522645517,
                            "process_2": 0.005257724913093793,
                            "process_1": 0.0049275855521418825
                        },
                        "total_energy_joules": {
                            "process_3": 17990.959570709412,
                            "process_0": 19242.126815238615,
                            "process_2": 18927.809687137655,
                            "process_1": 17739.307987710778
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 310.99340385704073,
                        "ram_power_avg": 0.7123070955276489,
                        "cpu_energy_total": 0.005263478659972861,
                        "gpu_energy_total": 0.015239536636064921,
                        "ram_energy_total": 2.481916529456921e-05,
                        "total_energy_kwh": 0.02052783446133235,
                        "total_energy_joules": 73900.20406079646
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22170439457137572,
                        "joules_per_token": 4.510510501757596,
                        "flops_per_joule": 229362979.55561182,
                        "joules_per_flop": 4.3599015060647045e-09
                    },
                    "per-process_emissions": [
                        0.0019037933467949309,
                        0.002036191169518097,
                        0.0020029303056430807,
                        0.00187716371608845
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0406": {
            "setup": {
                "experiment_id": "0406",
                "date_time": "April 12, 2025 at 11:46:12 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 44.558094358973904,
                        "average_latency_ms_per_batch": 5569.761794871738,
                        "throughput_queries_per_sec": 2.872654269475532,
                        "throughput_tokens_per_sec": 367.6997464928681
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            98.0
                        ],
                        "cpu_usage_percent": 41.6,
                        "cpu_memory_usage_bytes": 1990201344
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0406",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 310.7729457188909,
                            "process_0": 264.9200447854536,
                            "process_2": 296.1519252006642,
                            "process_1": 372.12869972315406
                        },
                        "ram_power": {
                            "process_3": 0.7208347320556641,
                            "process_0": 0.694786548614502,
                            "process_2": 0.7220578193664551,
                            "process_1": 0.7115492820739746
                        },
                        "cpu_energy": {
                            "process_3": 0.001276723576312179,
                            "process_0": 0.0013790599107205705,
                            "process_2": 0.001351347187814099,
                            "process_1": 0.001256347985126013
                        },
                        "gpu_energy": {
                            "process_3": 0.0037146785272959004,
                            "process_0": 0.00395963094548013,
                            "process_2": 0.0039001003423007052,
                            "process_1": 0.003665126820988185
                        },
                        "ram_energy": {
                            "process_3": 6.086666033423142e-06,
                            "process_0": 6.344370254469146e-06,
                            "process_2": 6.277382978991789e-06,
                            "process_1": 6.110746027685136e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.004997488769641504,
                            "process_0": 0.00534503522645517,
                            "process_2": 0.005257724913093793,
                            "process_1": 0.0049275855521418825
                        },
                        "total_energy_joules": {
                            "process_3": 17990.959570709412,
                            "process_0": 19242.126815238615,
                            "process_2": 18927.809687137655,
                            "process_1": 17739.307987710778
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 310.99340385704073,
                        "ram_power_avg": 0.7123070955276489,
                        "cpu_energy_total": 0.005263478659972861,
                        "gpu_energy_total": 0.015239536636064921,
                        "ram_energy_total": 2.481916529456921e-05,
                        "total_energy_kwh": 0.02052783446133235,
                        "total_energy_joules": 73900.20406079646
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22170439457137572,
                        "joules_per_token": 4.510510501757596,
                        "flops_per_joule": 229362979.55561182,
                        "joules_per_flop": 4.3599015060647045e-09
                    },
                    "per-process_emissions": [
                        0.0019037933467949309,
                        0.002036191169518097,
                        0.0020029303056430807,
                        0.00187716371608845
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0407": {
            "setup": {
                "experiment_id": "0407",
                "date_time": "April 12, 2025 at 11:47:58 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 49.21706634495058,
                        "average_latency_ms_per_batch": 6152.133293118823,
                        "throughput_queries_per_sec": 2.6007238851433927,
                        "throughput_tokens_per_sec": 332.89265729835427
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.6,
                        "cpu_memory_usage_bytes": 1929113600
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0407",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 280.6098968754774,
                            "process_2": 268.6408895099496,
                            "process_1": 290.11078933375,
                            "process_3": 242.20276891194646
                        },
                        "ram_power": {
                            "process_0": 0.6734519004821778,
                            "process_2": 0.704413890838623,
                            "process_1": 0.7089385986328125,
                            "process_3": 0.7035255432128906
                        },
                        "cpu_energy": {
                            "process_0": 0.0015180277346889851,
                            "process_2": 0.0015581393710026533,
                            "process_1": 0.0014776497713746726,
                            "process_3": 0.0016330749088119781
                        },
                        "gpu_energy": {
                            "process_0": 0.0035803439753836486,
                            "process_2": 0.00367209432656046,
                            "process_1": 0.003484999454664184,
                            "process_3": 0.0038274891730991634
                        },
                        "ram_energy": {
                            "process_0": 6.468304604713638e-06,
                            "process_2": 6.868761400335207e-06,
                            "process_1": 6.684375139347721e-06,
                            "process_3": 7.112282095688605e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005104840014677348,
                            "process_2": 0.005237102458963448,
                            "process_1": 0.004969333601178204,
                            "process_3": 0.0054676763640068285
                        },
                        "total_energy_joules": {
                            "process_0": 18377.424052838454,
                            "process_2": 18853.568852268414,
                            "process_1": 17889.600964241534,
                            "process_3": 19683.634910424582
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 270.3910861577808,
                        "ram_power_avg": 0.697582483291626,
                        "cpu_energy_total": 0.006186891785878289,
                        "gpu_energy_total": 0.014564926929707456,
                        "ram_energy_total": 2.713372324008517e-05,
                        "total_energy_kwh": 0.02077895243882583,
                        "total_energy_joules": 74804.228779773
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2190250506857738,
                        "joules_per_token": 4.565687791734192,
                        "flops_per_joule": 226591080.07186967,
                        "joules_per_flop": 4.413236389017706e-09
                    },
                    "per-process_emissions": [
                        0.0019446888035913358,
                        0.0019950741817421256,
                        0.0018930676353688368,
                        0.0020829113108684016
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0407": {
            "setup": {
                "experiment_id": "0407",
                "date_time": "April 12, 2025 at 11:47:58 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 49.21706634495058,
                        "average_latency_ms_per_batch": 6152.133293118823,
                        "throughput_queries_per_sec": 2.6007238851433927,
                        "throughput_tokens_per_sec": 332.89265729835427
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.6,
                        "cpu_memory_usage_bytes": 1929113600
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0407",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 280.6098968754774,
                            "process_2": 268.6408895099496,
                            "process_1": 290.11078933375,
                            "process_3": 242.20276891194646
                        },
                        "ram_power": {
                            "process_0": 0.6734519004821778,
                            "process_2": 0.704413890838623,
                            "process_1": 0.7089385986328125,
                            "process_3": 0.7035255432128906
                        },
                        "cpu_energy": {
                            "process_0": 0.0015180277346889851,
                            "process_2": 0.0015581393710026533,
                            "process_1": 0.0014776497713746726,
                            "process_3": 0.0016330749088119781
                        },
                        "gpu_energy": {
                            "process_0": 0.0035803439753836486,
                            "process_2": 0.00367209432656046,
                            "process_1": 0.003484999454664184,
                            "process_3": 0.0038274891730991634
                        },
                        "ram_energy": {
                            "process_0": 6.468304604713638e-06,
                            "process_2": 6.868761400335207e-06,
                            "process_1": 6.684375139347721e-06,
                            "process_3": 7.112282095688605e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005104840014677348,
                            "process_2": 0.005237102458963448,
                            "process_1": 0.004969333601178204,
                            "process_3": 0.0054676763640068285
                        },
                        "total_energy_joules": {
                            "process_0": 18377.424052838454,
                            "process_2": 18853.568852268414,
                            "process_1": 17889.600964241534,
                            "process_3": 19683.634910424582
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 270.3910861577808,
                        "ram_power_avg": 0.697582483291626,
                        "cpu_energy_total": 0.006186891785878289,
                        "gpu_energy_total": 0.014564926929707456,
                        "ram_energy_total": 2.713372324008517e-05,
                        "total_energy_kwh": 0.02077895243882583,
                        "total_energy_joules": 74804.228779773
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2190250506857738,
                        "joules_per_token": 4.565687791734192,
                        "flops_per_joule": 226591080.07186967,
                        "joules_per_flop": 4.413236389017706e-09
                    },
                    "per-process_emissions": [
                        0.0019446888035913358,
                        0.0019950741817421256,
                        0.0018930676353688368,
                        0.0020829113108684016
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0408": {
            "setup": {
                "experiment_id": "0408",
                "date_time": "April 12, 2025 at 11:49:22 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precis_float32_quant_False_quant8_False_quant4_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 25.901165007962845,
                        "average_latency_ms_per_batch": 3237.6456259953557,
                        "throughput_queries_per_sec": 4.941862652148995,
                        "throughput_tokens_per_sec": 632.5584194750713
                    }
                },
                "compute_metrics": {
                    "flops": 20248623316992,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 9905841664,
                        "gpu_max_memory_allocated_bytes": 9905841664,
                        "gpu_current_memory_reserved_bytes": 14004781056,
                        "gpu_max_memory_reserved_bytes": 14004781056
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.5,
                        "cpu_memory_usage_bytes": 1894211584
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0408",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 570.9761225400529,
                            "process_3": 351.34276920730275,
                            "process_0": 662.8708872305718,
                            "process_1": 0.0
                        },
                        "ram_power": {
                            "process_2": 0.6934790611267091,
                            "process_3": 0.6941571235656738,
                            "process_0": 0.6598191261291504,
                            "process_1": 0.6199464797973633
                        },
                        "cpu_energy": {
                            "process_2": 0.000817230513002869,
                            "process_3": 0.0008894043810623771,
                            "process_0": 0.000803917208373605,
                            "process_1": 0.0009056790871563863
                        },
                        "gpu_energy": {
                            "process_2": 0.004734430176429383,
                            "process_3": 0.004995203718381136,
                            "process_0": 0.004669072624142956,
                            "process_1": 0.004937236727563743
                        },
                        "ram_energy": {
                            "process_2": 3.874900679723348e-06,
                            "process_3": 4.195554869060256e-06,
                            "process_0": 3.6320228708308007e-06,
                            "process_1": 3.8213676469162365e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005555535590111973,
                            "process_3": 0.005888803654312572,
                            "process_0": 0.005476621855387392,
                            "process_1": 0.005846737182367045
                        },
                        "total_energy_joules": {
                            "process_2": 19999.9281244031,
                            "process_3": 21199.69315552526,
                            "process_0": 19715.838679394612,
                            "process_1": 21048.25385652136
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 396.2974447444818,
                        "ram_power_avg": 0.6668504476547241,
                        "cpu_energy_total": 0.003416231189595237,
                        "gpu_energy_total": 0.019335943246517218,
                        "ram_energy_total": 1.552384606653064e-05,
                        "total_energy_kwh": 0.02276769828217898,
                        "total_energy_joules": 81963.71381584433
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1998933337356027,
                        "joules_per_token": 5.002668079580342,
                        "flops_per_joule": 247043751.1224358,
                        "joules_per_flop": 4.047865997243525e-09
                    },
                    "per-process_emissions": [
                        0.002116381283053156,
                        0.0022433397521103744,
                        0.002086319095809827,
                        0.002227314529622726
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0408": {
            "setup": {
                "experiment_id": "0408",
                "date_time": "April 12, 2025 at 11:49:22 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "precis_float32_quant_False_quant8_False_quant4_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 25.901165007962845,
                        "average_latency_ms_per_batch": 3237.6456259953557,
                        "throughput_queries_per_sec": 4.941862652148995,
                        "throughput_tokens_per_sec": 632.5584194750713
                    }
                },
                "compute_metrics": {
                    "flops": 20248623316992,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 9905841664,
                        "gpu_max_memory_allocated_bytes": 9905841664,
                        "gpu_current_memory_reserved_bytes": 14004781056,
                        "gpu_max_memory_reserved_bytes": 14004781056
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.5,
                        "cpu_memory_usage_bytes": 1894211584
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0408",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 570.9761225400529,
                            "process_3": 351.34276920730275,
                            "process_0": 662.8708872305718,
                            "process_1": 0.0
                        },
                        "ram_power": {
                            "process_2": 0.6934790611267091,
                            "process_3": 0.6941571235656738,
                            "process_0": 0.6598191261291504,
                            "process_1": 0.6199464797973633
                        },
                        "cpu_energy": {
                            "process_2": 0.000817230513002869,
                            "process_3": 0.0008894043810623771,
                            "process_0": 0.000803917208373605,
                            "process_1": 0.0009056790871563863
                        },
                        "gpu_energy": {
                            "process_2": 0.004734430176429383,
                            "process_3": 0.004995203718381136,
                            "process_0": 0.004669072624142956,
                            "process_1": 0.004937236727563743
                        },
                        "ram_energy": {
                            "process_2": 3.874900679723348e-06,
                            "process_3": 4.195554869060256e-06,
                            "process_0": 3.6320228708308007e-06,
                            "process_1": 3.8213676469162365e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005555535590111973,
                            "process_3": 0.005888803654312572,
                            "process_0": 0.005476621855387392,
                            "process_1": 0.005846737182367045
                        },
                        "total_energy_joules": {
                            "process_2": 19999.9281244031,
                            "process_3": 21199.69315552526,
                            "process_0": 19715.838679394612,
                            "process_1": 21048.25385652136
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 396.2974447444818,
                        "ram_power_avg": 0.6668504476547241,
                        "cpu_energy_total": 0.003416231189595237,
                        "gpu_energy_total": 0.019335943246517218,
                        "ram_energy_total": 1.552384606653064e-05,
                        "total_energy_kwh": 0.02276769828217898,
                        "total_energy_joules": 81963.71381584433
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1998933337356027,
                        "joules_per_token": 5.002668079580342,
                        "flops_per_joule": 247043751.1224358,
                        "joules_per_flop": 4.047865997243525e-09
                    },
                    "per-process_emissions": [
                        0.002116381283053156,
                        0.0022433397521103744,
                        0.002086319095809827,
                        0.002227314529622726
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0409": {
            "setup": {
                "experiment_id": "0409",
                "date_time": "April 12, 2025 at 11:51:08 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 51.40292954200413,
                        "average_latency_ms_per_batch": 6425.366192750516,
                        "throughput_queries_per_sec": 2.4901304486041838,
                        "throughput_tokens_per_sec": 318.7366974213355
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.4,
                        "cpu_memory_usage_bytes": 1975504896
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0409",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 219.43342190343768,
                            "process_3": 263.9496854050716,
                            "process_0": 275.27531827606686,
                            "process_2": 327.8405998107189
                        },
                        "ram_power": {
                            "process_1": 0.7208218574523926,
                            "process_3": 0.722501277923584,
                            "process_0": 0.6896538734436035,
                            "process_2": 0.7257428169250488
                        },
                        "cpu_energy": {
                            "process_1": 0.0015186304569042475,
                            "process_3": 0.0015982989755293606,
                            "process_0": 0.001583323487966481,
                            "process_2": 0.0015497239599044403
                        },
                        "gpu_energy": {
                            "process_1": 0.0034639002711183142,
                            "process_3": 0.003658949038267467,
                            "process_0": 0.003626947345999554,
                            "process_2": 0.0035395667205397885
                        },
                        "ram_energy": {
                            "process_1": 6.688081425201263e-06,
                            "process_3": 7.151493438849062e-06,
                            "process_0": 6.6962789185751605e-06,
                            "process_2": 6.879397748070049e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.004989218809447765,
                            "process_3": 0.005264399507235675,
                            "process_0": 0.005216967112884611,
                            "process_2": 0.005096170078192299
                        },
                        "total_energy_joules": {
                            "process_1": 17961.187714011954,
                            "process_3": 18951.83822604843,
                            "process_0": 18781.0816063846,
                            "process_2": 18346.212281492277
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 271.6247563488238,
                        "ram_power_avg": 0.7146799564361572,
                        "cpu_energy_total": 0.006249976880304529,
                        "gpu_energy_total": 0.014289363375925124,
                        "ram_energy_total": 2.7415251530695534e-05,
                        "total_energy_kwh": 0.020566755507760347,
                        "total_energy_joules": 74040.31982793726
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2212848355878915,
                        "joules_per_token": 4.519062489498124,
                        "flops_per_joule": 228928927.27289858,
                        "joules_per_flop": 4.368167937151661e-09
                    },
                    "per-process_emissions": [
                        0.0019006429054591262,
                        0.0020054729922814305,
                        0.0019874036216533924,
                        0.0019413859912873561
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0409": {
            "setup": {
                "experiment_id": "0409",
                "date_time": "April 12, 2025 at 11:51:08 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 51.40292954200413,
                        "average_latency_ms_per_batch": 6425.366192750516,
                        "throughput_queries_per_sec": 2.4901304486041838,
                        "throughput_tokens_per_sec": 318.7366974213355
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.4,
                        "cpu_memory_usage_bytes": 1975504896
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0409",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 219.43342190343768,
                            "process_3": 263.9496854050716,
                            "process_0": 275.27531827606686,
                            "process_2": 327.8405998107189
                        },
                        "ram_power": {
                            "process_1": 0.7208218574523926,
                            "process_3": 0.722501277923584,
                            "process_0": 0.6896538734436035,
                            "process_2": 0.7257428169250488
                        },
                        "cpu_energy": {
                            "process_1": 0.0015186304569042475,
                            "process_3": 0.0015982989755293606,
                            "process_0": 0.001583323487966481,
                            "process_2": 0.0015497239599044403
                        },
                        "gpu_energy": {
                            "process_1": 0.0034639002711183142,
                            "process_3": 0.003658949038267467,
                            "process_0": 0.003626947345999554,
                            "process_2": 0.0035395667205397885
                        },
                        "ram_energy": {
                            "process_1": 6.688081425201263e-06,
                            "process_3": 7.151493438849062e-06,
                            "process_0": 6.6962789185751605e-06,
                            "process_2": 6.879397748070049e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.004989218809447765,
                            "process_3": 0.005264399507235675,
                            "process_0": 0.005216967112884611,
                            "process_2": 0.005096170078192299
                        },
                        "total_energy_joules": {
                            "process_1": 17961.187714011954,
                            "process_3": 18951.83822604843,
                            "process_0": 18781.0816063846,
                            "process_2": 18346.212281492277
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 271.6247563488238,
                        "ram_power_avg": 0.7146799564361572,
                        "cpu_energy_total": 0.006249976880304529,
                        "gpu_energy_total": 0.014289363375925124,
                        "ram_energy_total": 2.7415251530695534e-05,
                        "total_energy_kwh": 0.020566755507760347,
                        "total_energy_joules": 74040.31982793726
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2212848355878915,
                        "joules_per_token": 4.519062489498124,
                        "flops_per_joule": 228928927.27289858,
                        "joules_per_flop": 4.368167937151661e-09
                    },
                    "per-process_emissions": [
                        0.0019006429054591262,
                        0.0020054729922814305,
                        0.0019874036216533924,
                        0.0019413859912873561
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0410": {
            "setup": {
                "experiment_id": "0410",
                "date_time": "April 12, 2025 at 11:52:48 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.90576710997266,
                        "average_latency_ms_per_batch": 4988.220888746582,
                        "throughput_queries_per_sec": 3.2075564328147483,
                        "throughput_tokens_per_sec": 410.5672234002878
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.5,
                        "cpu_memory_usage_bytes": 1972760576
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0410",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 340.2727918064699,
                            "process_1": 299.6320689832447,
                            "process_3": 350.22715852801565,
                            "process_0": 408.10804395544903
                        },
                        "ram_power": {
                            "process_2": 0.7201709747314453,
                            "process_1": 0.7240548133850098,
                            "process_3": 0.7240762710571289,
                            "process_0": 0.6886954307556152
                        },
                        "cpu_energy": {
                            "process_2": 0.0013398052891261617,
                            "process_1": 0.0013814127791565625,
                            "process_3": 0.00125954541734518,
                            "process_0": 0.0012338105634380559
                        },
                        "gpu_energy": {
                            "process_2": 0.004080447986578584,
                            "process_1": 0.004190523074639074,
                            "process_3": 0.00383795918147678,
                            "process_0": 0.003760490508390779
                        },
                        "ram_energy": {
                            "process_2": 6.0485179446149845e-06,
                            "process_1": 6.480717374182017e-06,
                            "process_3": 5.8231950147412634e-06,
                            "process_0": 5.707617149201967e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005426301793649359,
                            "process_1": 0.005578416571169819,
                            "process_3": 0.0051033277938367025,
                            "process_0": 0.0050000086889780354
                        },
                        "total_energy_joules": {
                            "process_2": 19534.68645713769,
                            "process_1": 20082.299656211348,
                            "process_3": 18371.98005781213,
                            "process_0": 18000.031280320927
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 349.5600158182948,
                        "ram_power_avg": 0.7142493724822998,
                        "cpu_energy_total": 0.00521457404906596,
                        "gpu_energy_total": 0.015869420751085217,
                        "ram_energy_total": 2.406004748274023e-05,
                        "total_energy_kwh": 0.021108054847633916,
                        "total_energy_joules": 75988.9974514821
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21561016133238176,
                        "joules_per_token": 4.638000332732061,
                        "flops_per_joule": 223058226.3435482,
                        "joules_per_flop": 4.483134365373404e-09
                    },
                    "per-process_emissions": [
                        0.002067149668290723,
                        0.0021250977927871426,
                        0.0019441127230620918,
                        0.0019047533100661827
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0410": {
            "setup": {
                "experiment_id": "0410",
                "date_time": "April 12, 2025 at 11:52:48 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 39.90576710997266,
                        "average_latency_ms_per_batch": 4988.220888746582,
                        "throughput_queries_per_sec": 3.2075564328147483,
                        "throughput_tokens_per_sec": 410.5672234002878
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 41.5,
                        "cpu_memory_usage_bytes": 1972760576
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0410",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 340.2727918064699,
                            "process_1": 299.6320689832447,
                            "process_3": 350.22715852801565,
                            "process_0": 408.10804395544903
                        },
                        "ram_power": {
                            "process_2": 0.7201709747314453,
                            "process_1": 0.7240548133850098,
                            "process_3": 0.7240762710571289,
                            "process_0": 0.6886954307556152
                        },
                        "cpu_energy": {
                            "process_2": 0.0013398052891261617,
                            "process_1": 0.0013814127791565625,
                            "process_3": 0.00125954541734518,
                            "process_0": 0.0012338105634380559
                        },
                        "gpu_energy": {
                            "process_2": 0.004080447986578584,
                            "process_1": 0.004190523074639074,
                            "process_3": 0.00383795918147678,
                            "process_0": 0.003760490508390779
                        },
                        "ram_energy": {
                            "process_2": 6.0485179446149845e-06,
                            "process_1": 6.480717374182017e-06,
                            "process_3": 5.8231950147412634e-06,
                            "process_0": 5.707617149201967e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.005426301793649359,
                            "process_1": 0.005578416571169819,
                            "process_3": 0.0051033277938367025,
                            "process_0": 0.0050000086889780354
                        },
                        "total_energy_joules": {
                            "process_2": 19534.68645713769,
                            "process_1": 20082.299656211348,
                            "process_3": 18371.98005781213,
                            "process_0": 18000.031280320927
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 349.5600158182948,
                        "ram_power_avg": 0.7142493724822998,
                        "cpu_energy_total": 0.00521457404906596,
                        "gpu_energy_total": 0.015869420751085217,
                        "ram_energy_total": 2.406004748274023e-05,
                        "total_energy_kwh": 0.021108054847633916,
                        "total_energy_joules": 75988.9974514821
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21561016133238176,
                        "joules_per_token": 4.638000332732061,
                        "flops_per_joule": 223058226.3435482,
                        "joules_per_flop": 4.483134365373404e-09
                    },
                    "per-process_emissions": [
                        0.002067149668290723,
                        0.0021250977927871426,
                        0.0019441127230620918,
                        0.0019047533100661827
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0411": {
            "setup": {
                "experiment_id": "0411",
                "date_time": "April 12, 2025 at 11:54:31 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 47.65826638205908,
                        "average_latency_ms_per_batch": 5957.283297757385,
                        "throughput_queries_per_sec": 2.6857880010546396,
                        "throughput_tokens_per_sec": 343.78086413499386
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 40.7,
                        "cpu_memory_usage_bytes": 1976963072
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0411",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 296.97063564686977,
                            "process_2": 254.5445777400689,
                            "process_1": 331.40360073887035,
                            "process_3": 286.197000634918
                        },
                        "ram_power": {
                            "process_0": 0.6901631355285645,
                            "process_2": 0.7189407348632812,
                            "process_1": 0.7221722602844238,
                            "process_3": 0.7222881317138672
                        },
                        "cpu_energy": {
                            "process_0": 0.0014702853506578324,
                            "process_2": 0.0015525229358736396,
                            "process_1": 0.0015370633072825512,
                            "process_3": 0.001475521362187465
                        },
                        "gpu_energy": {
                            "process_0": 0.0036045751058806363,
                            "process_2": 0.00380914749176009,
                            "process_1": 0.0037736180188907564,
                            "process_3": 0.00361301066818287
                        },
                        "ram_energy": {
                            "process_0": 6.608693438731605e-06,
                            "process_2": 7.167295473815836e-06,
                            "process_1": 7.040307830998865e-06,
                            "process_3": 6.81937539673904e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005081469149977199,
                            "process_2": 0.005368837723107545,
                            "process_1": 0.005317721634004305,
                            "process_3": 0.005095351405767072
                        },
                        "total_energy_joules": {
                            "process_0": 18293.288939917915,
                            "process_2": 19327.81580318716,
                            "process_1": 19143.797882415496,
                            "process_3": 18343.26506076146
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 292.27895369018177,
                        "ram_power_avg": 0.7133910655975342,
                        "cpu_energy_total": 0.006035392956001489,
                        "gpu_energy_total": 0.014800351284714353,
                        "ram_energy_total": 2.7635672140285346e-05,
                        "total_energy_kwh": 0.02086337991285612,
                        "total_energy_joules": 75108.16768628203
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21813872584981753,
                        "joules_per_token": 4.584238750383425,
                        "flops_per_joule": 225674137.91733053,
                        "joules_per_flop": 4.431167918613351e-09
                    },
                    "per-process_emissions": [
                        0.001935785672683814,
                        0.002045258730617819,
                        0.00202578605647394,
                        0.0019410741180269663
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0411": {
            "setup": {
                "experiment_id": "0411",
                "date_time": "April 12, 2025 at 11:54:31 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 47.65826638205908,
                        "average_latency_ms_per_batch": 5957.283297757385,
                        "throughput_queries_per_sec": 2.6857880010546396,
                        "throughput_tokens_per_sec": 343.78086413499386
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 40.7,
                        "cpu_memory_usage_bytes": 1976963072
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0411",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 296.97063564686977,
                            "process_2": 254.5445777400689,
                            "process_1": 331.40360073887035,
                            "process_3": 286.197000634918
                        },
                        "ram_power": {
                            "process_0": 0.6901631355285645,
                            "process_2": 0.7189407348632812,
                            "process_1": 0.7221722602844238,
                            "process_3": 0.7222881317138672
                        },
                        "cpu_energy": {
                            "process_0": 0.0014702853506578324,
                            "process_2": 0.0015525229358736396,
                            "process_1": 0.0015370633072825512,
                            "process_3": 0.001475521362187465
                        },
                        "gpu_energy": {
                            "process_0": 0.0036045751058806363,
                            "process_2": 0.00380914749176009,
                            "process_1": 0.0037736180188907564,
                            "process_3": 0.00361301066818287
                        },
                        "ram_energy": {
                            "process_0": 6.608693438731605e-06,
                            "process_2": 7.167295473815836e-06,
                            "process_1": 7.040307830998865e-06,
                            "process_3": 6.81937539673904e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005081469149977199,
                            "process_2": 0.005368837723107545,
                            "process_1": 0.005317721634004305,
                            "process_3": 0.005095351405767072
                        },
                        "total_energy_joules": {
                            "process_0": 18293.288939917915,
                            "process_2": 19327.81580318716,
                            "process_1": 19143.797882415496,
                            "process_3": 18343.26506076146
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 292.27895369018177,
                        "ram_power_avg": 0.7133910655975342,
                        "cpu_energy_total": 0.006035392956001489,
                        "gpu_energy_total": 0.014800351284714353,
                        "ram_energy_total": 2.7635672140285346e-05,
                        "total_energy_kwh": 0.02086337991285612,
                        "total_energy_joules": 75108.16768628203
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21813872584981753,
                        "joules_per_token": 4.584238750383425,
                        "flops_per_joule": 225674137.91733053,
                        "joules_per_flop": 4.431167918613351e-09
                    },
                    "per-process_emissions": [
                        0.001935785672683814,
                        0.002045258730617819,
                        0.00202578605647394,
                        0.0019410741180269663
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0412": {
            "setup": {
                "experiment_id": "0412",
                "date_time": "April 12, 2025 at 11:56:19 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 51.88484842397156,
                        "average_latency_ms_per_batch": 6485.606052996445,
                        "throughput_queries_per_sec": 2.4670015214087457,
                        "throughput_tokens_per_sec": 315.77619474031945
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 40.7,
                        "cpu_memory_usage_bytes": 1938518016
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0412",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 251.80867462506328,
                            "process_1": 342.21244993662947,
                            "process_2": 267.9322717241126,
                            "process_0": 263.5286408784059
                        },
                        "ram_power": {
                            "process_3": 0.7050933837890625,
                            "process_1": 0.7146005630493164,
                            "process_2": 0.7064638137817384,
                            "process_0": 0.6767363548278809
                        },
                        "cpu_energy": {
                            "process_3": 0.0013973218744058613,
                            "process_1": 0.0012777349984398826,
                            "process_2": 0.001633601110247583,
                            "process_0": 0.0016054774939348137
                        },
                        "gpu_energy": {
                            "process_3": 0.003503929747585399,
                            "process_1": 0.0032008145050945735,
                            "process_2": 0.004043984068518025,
                            "process_0": 0.003975621236050886
                        },
                        "ram_energy": {
                            "process_3": 6.188621892262254e-06,
                            "process_1": 6.181970218807053e-06,
                            "process_2": 7.230717122868017e-06,
                            "process_0": 6.808770150172153e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.004907440243883522,
                            "process_1": 0.004484731473753262,
                            "process_2": 0.005684815895888477,
                            "process_0": 0.0055879075001358715
                        },
                        "total_energy_joules": {
                            "process_3": 17666.784877980677,
                            "process_1": 16145.033305511743,
                            "process_2": 20465.337225198517,
                            "process_0": 20116.467000489138
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 281.3705092910528,
                        "ram_power_avg": 0.7007235288619995,
                        "cpu_energy_total": 0.00591413547702814,
                        "gpu_energy_total": 0.014724349557248884,
                        "ram_energy_total": 2.6410079384109474e-05,
                        "total_energy_kwh": 0.02066489511366113,
                        "total_energy_joules": 74393.62240918007
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22023393228366625,
                        "joules_per_token": 4.540626367747807,
                        "flops_per_joule": 227841721.43041655,
                        "joules_per_flop": 4.38901178292494e-09
                    },
                    "per-process_emissions": [
                        0.0018694893609074276,
                        0.0017084584549263051,
                        0.0021656306155387155,
                        0.0021287133621767603
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0412": {
            "setup": {
                "experiment_id": "0412",
                "date_time": "April 12, 2025 at 11:56:19 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 51.88484842397156,
                        "average_latency_ms_per_batch": 6485.606052996445,
                        "throughput_queries_per_sec": 2.4670015214087457,
                        "throughput_tokens_per_sec": 315.77619474031945
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 40.7,
                        "cpu_memory_usage_bytes": 1938518016
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0412",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 251.80867462506328,
                            "process_1": 342.21244993662947,
                            "process_2": 267.9322717241126,
                            "process_0": 263.5286408784059
                        },
                        "ram_power": {
                            "process_3": 0.7050933837890625,
                            "process_1": 0.7146005630493164,
                            "process_2": 0.7064638137817384,
                            "process_0": 0.6767363548278809
                        },
                        "cpu_energy": {
                            "process_3": 0.0013973218744058613,
                            "process_1": 0.0012777349984398826,
                            "process_2": 0.001633601110247583,
                            "process_0": 0.0016054774939348137
                        },
                        "gpu_energy": {
                            "process_3": 0.003503929747585399,
                            "process_1": 0.0032008145050945735,
                            "process_2": 0.004043984068518025,
                            "process_0": 0.003975621236050886
                        },
                        "ram_energy": {
                            "process_3": 6.188621892262254e-06,
                            "process_1": 6.181970218807053e-06,
                            "process_2": 7.230717122868017e-06,
                            "process_0": 6.808770150172153e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.004907440243883522,
                            "process_1": 0.004484731473753262,
                            "process_2": 0.005684815895888477,
                            "process_0": 0.0055879075001358715
                        },
                        "total_energy_joules": {
                            "process_3": 17666.784877980677,
                            "process_1": 16145.033305511743,
                            "process_2": 20465.337225198517,
                            "process_0": 20116.467000489138
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 281.3705092910528,
                        "ram_power_avg": 0.7007235288619995,
                        "cpu_energy_total": 0.00591413547702814,
                        "gpu_energy_total": 0.014724349557248884,
                        "ram_energy_total": 2.6410079384109474e-05,
                        "total_energy_kwh": 0.02066489511366113,
                        "total_energy_joules": 74393.62240918007
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22023393228366625,
                        "joules_per_token": 4.540626367747807,
                        "flops_per_joule": 227841721.43041655,
                        "joules_per_flop": 4.38901178292494e-09
                    },
                    "per-process_emissions": [
                        0.0018694893609074276,
                        0.0017084584549263051,
                        0.0021656306155387155,
                        0.0021287133621767603
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0413": {
            "setup": {
                "experiment_id": "0413",
                "date_time": "April 12, 2025 at 11:58:05 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_16",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 51.18128817901015,
                        "average_latency_ms_per_batch": 6397.661022376269,
                        "throughput_queries_per_sec": 2.500913997168477,
                        "throughput_tokens_per_sec": 320.11699163756504
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 40.0,
                        "cpu_memory_usage_bytes": 1932324864
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0413",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 226.9098664570262,
                            "process_3": 256.5672690556606,
                            "process_2": 261.5074916305575,
                            "process_1": 289.8319260637459
                        },
                        "ram_power": {
                            "process_0": 0.6745734214782715,
                            "process_3": 0.7066226005554199,
                            "process_2": 0.7044539451599121,
                            "process_1": 0.7057957649230957
                        },
                        "cpu_energy": {
                            "process_0": 0.001580947968689543,
                            "process_3": 0.001445787987596304,
                            "process_2": 0.0014380086663404656,
                            "process_1": 0.0013978302529030781
                        },
                        "gpu_energy": {
                            "process_0": 0.003895968116773174,
                            "process_3": 0.0036053323287084105,
                            "process_2": 0.0035866050915034364,
                            "process_1": 0.003508381695592977
                        },
                        "ram_energy": {
                            "process_0": 6.800400201082243e-06,
                            "process_3": 6.402335872593611e-06,
                            "process_2": 6.350319854798585e-06,
                            "process_1": 6.628474701094395e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0054837164856637996,
                            "process_3": 0.005057522652177308,
                            "process_2": 0.0050309640776987005,
                            "process_1": 0.004912840423197146
                        },
                        "total_energy_joules": {
                            "process_0": 19741.379348389677,
                            "process_3": 18207.08154783831,
                            "process_2": 18111.470679715323,
                            "process_1": 17686.225523509725
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 258.70413830174755,
                        "ram_power_avg": 0.6978614330291748,
                        "cpu_energy_total": 0.005862574875529391,
                        "gpu_energy_total": 0.014596287232577998,
                        "ram_energy_total": 2.6181530629568834e-05,
                        "total_energy_kwh": 0.020485043638736954,
                        "total_energy_joules": 73746.15709945303
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22216750871377294,
                        "joules_per_token": 4.501108221402163,
                        "flops_per_joule": 229842091.57222262,
                        "joules_per_flop": 4.350813174208228e-09
                    },
                    "per-process_emissions": [
                        0.0020890217952136245,
                        0.0019266632543469455,
                        0.00191654576539932,
                        0.0018715465592169527
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0413": {
            "setup": {
                "experiment_id": "0413",
                "date_time": "April 12, 2025 at 11:58:05 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_16",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 51.18128817901015,
                        "average_latency_ms_per_batch": 6397.661022376269,
                        "throughput_queries_per_sec": 2.500913997168477,
                        "throughput_tokens_per_sec": 320.11699163756504
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 40.0,
                        "cpu_memory_usage_bytes": 1932324864
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0413",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 226.9098664570262,
                            "process_3": 256.5672690556606,
                            "process_2": 261.5074916305575,
                            "process_1": 289.8319260637459
                        },
                        "ram_power": {
                            "process_0": 0.6745734214782715,
                            "process_3": 0.7066226005554199,
                            "process_2": 0.7044539451599121,
                            "process_1": 0.7057957649230957
                        },
                        "cpu_energy": {
                            "process_0": 0.001580947968689543,
                            "process_3": 0.001445787987596304,
                            "process_2": 0.0014380086663404656,
                            "process_1": 0.0013978302529030781
                        },
                        "gpu_energy": {
                            "process_0": 0.003895968116773174,
                            "process_3": 0.0036053323287084105,
                            "process_2": 0.0035866050915034364,
                            "process_1": 0.003508381695592977
                        },
                        "ram_energy": {
                            "process_0": 6.800400201082243e-06,
                            "process_3": 6.402335872593611e-06,
                            "process_2": 6.350319854798585e-06,
                            "process_1": 6.628474701094395e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0054837164856637996,
                            "process_3": 0.005057522652177308,
                            "process_2": 0.0050309640776987005,
                            "process_1": 0.004912840423197146
                        },
                        "total_energy_joules": {
                            "process_0": 19741.379348389677,
                            "process_3": 18207.08154783831,
                            "process_2": 18111.470679715323,
                            "process_1": 17686.225523509725
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 258.70413830174755,
                        "ram_power_avg": 0.6978614330291748,
                        "cpu_energy_total": 0.005862574875529391,
                        "gpu_energy_total": 0.014596287232577998,
                        "ram_energy_total": 2.6181530629568834e-05,
                        "total_energy_kwh": 0.020485043638736954,
                        "total_energy_joules": 73746.15709945303
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22216750871377294,
                        "joules_per_token": 4.501108221402163,
                        "flops_per_joule": 229842091.57222262,
                        "joules_per_flop": 4.350813174208228e-09
                    },
                    "per-process_emissions": [
                        0.0020890217952136245,
                        0.0019266632543469455,
                        0.00191654576539932,
                        0.0018715465592169527
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0414": {
            "setup": {
                "experiment_id": "0414",
                "date_time": "April 12, 2025 at 11:59:52 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.65783732806449,
                        "average_latency_ms_per_batch": 4707.229666008061,
                        "throughput_queries_per_sec": 3.3990268449273917,
                        "throughput_tokens_per_sec": 435.07543615070614
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 39.9,
                        "cpu_memory_usage_bytes": 1943494656
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0414",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 286.9221859264501,
                            "process_1": 329.6178368999609,
                            "process_2": 290.6035927260081,
                            "process_3": 274.70301518137666
                        },
                        "ram_power": {
                            "process_0": 0.6785688400268556,
                            "process_1": 0.7054252624511719,
                            "process_2": 0.7045841217041017,
                            "process_3": 0.7044968605041505
                        },
                        "cpu_energy": {
                            "process_0": 0.0011649756401548074,
                            "process_1": 0.0011358307028103809,
                            "process_2": 0.0015701286109688225,
                            "process_3": 0.0016901428314122315
                        },
                        "gpu_energy": {
                            "process_0": 0.003346985177585582,
                            "process_1": 0.0032827087372753194,
                            "process_2": 0.004233800609259264,
                            "process_3": 0.004513717499858405
                        },
                        "ram_energy": {
                            "process_0": 5.174168274412236e-06,
                            "process_1": 5.5691658786533445e-06,
                            "process_2": 7.027394289863527e-06,
                            "process_3": 7.598346830068568e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.004517134986014802,
                            "process_1": 0.004424108605964355,
                            "process_2": 0.005810956614517949,
                            "process_3": 0.0062114586781007045
                        },
                        "total_energy_joules": {
                            "process_0": 16261.685949653287,
                            "process_1": 15926.790981471677,
                            "process_2": 20919.443812264613,
                            "process_3": 22361.251241162536
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 295.46165768344895,
                        "ram_power_avg": 0.6982687711715698,
                        "cpu_energy_total": 0.005561077785346242,
                        "gpu_energy_total": 0.01537721202397857,
                        "ram_energy_total": 2.5369075272997678e-05,
                        "total_energy_kwh": 0.020963658884597808,
                        "total_energy_joules": 75469.17198455211
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21709526644009905,
                        "joules_per_token": 4.606272704135261,
                        "flops_per_joule": 224594633.11219993,
                        "joules_per_flop": 4.452466143749898e-09
                    },
                    "per-process_emissions": [
                        0.0017208025729223387,
                        0.0016853641734421209,
                        0.0022136839223006128,
                        0.0023662551834224632
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0414": {
            "setup": {
                "experiment_id": "0414",
                "date_time": "April 12, 2025 at 11:59:52 PM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "latency_False",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 37.65783732806449,
                        "average_latency_ms_per_batch": 4707.229666008061,
                        "throughput_queries_per_sec": 3.3990268449273917,
                        "throughput_tokens_per_sec": 435.07543615070614
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 39.9,
                        "cpu_memory_usage_bytes": 1943494656
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0414",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 286.9221859264501,
                            "process_1": 329.6178368999609,
                            "process_2": 290.6035927260081,
                            "process_3": 274.70301518137666
                        },
                        "ram_power": {
                            "process_0": 0.6785688400268556,
                            "process_1": 0.7054252624511719,
                            "process_2": 0.7045841217041017,
                            "process_3": 0.7044968605041505
                        },
                        "cpu_energy": {
                            "process_0": 0.0011649756401548074,
                            "process_1": 0.0011358307028103809,
                            "process_2": 0.0015701286109688225,
                            "process_3": 0.0016901428314122315
                        },
                        "gpu_energy": {
                            "process_0": 0.003346985177585582,
                            "process_1": 0.0032827087372753194,
                            "process_2": 0.004233800609259264,
                            "process_3": 0.004513717499858405
                        },
                        "ram_energy": {
                            "process_0": 5.174168274412236e-06,
                            "process_1": 5.5691658786533445e-06,
                            "process_2": 7.027394289863527e-06,
                            "process_3": 7.598346830068568e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.004517134986014802,
                            "process_1": 0.004424108605964355,
                            "process_2": 0.005810956614517949,
                            "process_3": 0.0062114586781007045
                        },
                        "total_energy_joules": {
                            "process_0": 16261.685949653287,
                            "process_1": 15926.790981471677,
                            "process_2": 20919.443812264613,
                            "process_3": 22361.251241162536
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 295.46165768344895,
                        "ram_power_avg": 0.6982687711715698,
                        "cpu_energy_total": 0.005561077785346242,
                        "gpu_energy_total": 0.01537721202397857,
                        "ram_energy_total": 2.5369075272997678e-05,
                        "total_energy_kwh": 0.020963658884597808,
                        "total_energy_joules": 75469.17198455211
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21709526644009905,
                        "joules_per_token": 4.606272704135261,
                        "flops_per_joule": 224594633.11219993,
                        "joules_per_flop": 4.452466143749898e-09
                    },
                    "per-process_emissions": [
                        0.0017208025729223387,
                        0.0016853641734421209,
                        0.0022136839223006128,
                        0.0023662551834224632
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0415": {
            "setup": {
                "experiment_id": "0415",
                "date_time": "April 13, 2025 at 12:01:10 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_32",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.63824447596562,
                        "average_latency_ms_per_batch": 5909.561118991405,
                        "throughput_queries_per_sec": 5.41495372594801,
                        "throughput_tokens_per_sec": 693.1140769213453
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2287992832,
                        "gpu_max_memory_reserved_bytes": 2287992832
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 40.0,
                        "cpu_memory_usage_bytes": 1947963392
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0415",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 362.9951944880982,
                            "process_2": 427.6232011005906,
                            "process_3": 300.45743023838514,
                            "process_1": 440.0231523744358
                        },
                        "ram_power": {
                            "process_0": 0.6800351142883301,
                            "process_2": 0.7102818489074707,
                            "process_3": 0.7144317626953125,
                            "process_1": 0.7115707397460938
                        },
                        "cpu_energy": {
                            "process_0": 0.0007334311520335177,
                            "process_2": 0.0007016002928730813,
                            "process_3": 0.0007825080200318554,
                            "process_1": 0.0007234451695012469
                        },
                        "gpu_energy": {
                            "process_0": 0.0024732383674783698,
                            "process_2": 0.0023758388451140533,
                            "process_3": 0.0026010095808055578,
                            "process_1": 0.0024444172333102365
                        },
                        "ram_energy": {
                            "process_0": 3.190637232399929e-06,
                            "process_2": 3.188023068280025e-06,
                            "process_3": 3.7008832502855327e-06,
                            "process_1": 3.3672087875246737e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0032098601567442865,
                            "process_2": 0.003080627161055414,
                            "process_3": 0.0033872184840876983,
                            "process_1": 0.0031712296115990074
                        },
                        "total_energy_joules": {
                            "process_0": 11555.496564279432,
                            "process_2": 11090.25777979949,
                            "process_3": 12193.986542715715,
                            "process_1": 11416.426601756426
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 382.7747445503774,
                        "ram_power_avg": 0.7040798664093018,
                        "cpu_energy_total": 0.002940984634439701,
                        "gpu_energy_total": 0.009894504026708217,
                        "ram_energy_total": 1.3446752338490161e-05,
                        "total_energy_kwh": 0.012848935413486405,
                        "total_energy_joules": 46256.16748855106
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.354201415498922,
                        "joules_per_token": 2.823252410189884,
                        "flops_per_joule": 366436994.5336979,
                        "joules_per_flop": 2.7289821031103315e-09
                    },
                    "per-process_emissions": [
                        0.001222796226711736,
                        0.0011735649170040601,
                        0.0012903608815132087,
                        0.001208079920538642
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0415": {
            "setup": {
                "experiment_id": "0415",
                "date_time": "April 13, 2025 at 12:01:10 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_32",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.63824447596562,
                        "average_latency_ms_per_batch": 5909.561118991405,
                        "throughput_queries_per_sec": 5.41495372594801,
                        "throughput_tokens_per_sec": 693.1140769213453
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2287992832,
                        "gpu_max_memory_reserved_bytes": 2287992832
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 40.0,
                        "cpu_memory_usage_bytes": 1947963392
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0415",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 362.9951944880982,
                            "process_2": 427.6232011005906,
                            "process_3": 300.45743023838514,
                            "process_1": 440.0231523744358
                        },
                        "ram_power": {
                            "process_0": 0.6800351142883301,
                            "process_2": 0.7102818489074707,
                            "process_3": 0.7144317626953125,
                            "process_1": 0.7115707397460938
                        },
                        "cpu_energy": {
                            "process_0": 0.0007334311520335177,
                            "process_2": 0.0007016002928730813,
                            "process_3": 0.0007825080200318554,
                            "process_1": 0.0007234451695012469
                        },
                        "gpu_energy": {
                            "process_0": 0.0024732383674783698,
                            "process_2": 0.0023758388451140533,
                            "process_3": 0.0026010095808055578,
                            "process_1": 0.0024444172333102365
                        },
                        "ram_energy": {
                            "process_0": 3.190637232399929e-06,
                            "process_2": 3.188023068280025e-06,
                            "process_3": 3.7008832502855327e-06,
                            "process_1": 3.3672087875246737e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0032098601567442865,
                            "process_2": 0.003080627161055414,
                            "process_3": 0.0033872184840876983,
                            "process_1": 0.0031712296115990074
                        },
                        "total_energy_joules": {
                            "process_0": 11555.496564279432,
                            "process_2": 11090.25777979949,
                            "process_3": 12193.986542715715,
                            "process_1": 11416.426601756426
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 382.7747445503774,
                        "ram_power_avg": 0.7040798664093018,
                        "cpu_energy_total": 0.002940984634439701,
                        "gpu_energy_total": 0.009894504026708217,
                        "ram_energy_total": 1.3446752338490161e-05,
                        "total_energy_kwh": 0.012848935413486405,
                        "total_energy_joules": 46256.16748855106
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.354201415498922,
                        "joules_per_token": 2.823252410189884,
                        "flops_per_joule": 366436994.5336979,
                        "joules_per_flop": 2.7289821031103315e-09
                    },
                    "per-process_emissions": [
                        0.001222796226711736,
                        0.0011735649170040601,
                        0.0012903608815132087,
                        0.001208079920538642
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0416": {
            "setup": {
                "experiment_id": "0416",
                "date_time": "April 13, 2025 at 12:02:52 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.90371294107172,
                        "average_latency_ms_per_batch": 5487.964117633965,
                        "throughput_queries_per_sec": 2.9154709573607978,
                        "throughput_tokens_per_sec": 373.1802825421821
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 39.2,
                        "cpu_memory_usage_bytes": 1927159808
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0416",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 283.4643294040321,
                            "process_0": 343.2714791314625,
                            "process_2": 264.2935700842934,
                            "process_3": 334.43662633287505
                        },
                        "ram_power": {
                            "process_1": 0.7059230804443359,
                            "process_0": 0.6727695465087891,
                            "process_2": 0.708651065826416,
                            "process_3": 0.709146022796631
                        },
                        "cpu_energy": {
                            "process_1": 0.0015587252590612485,
                            "process_0": 0.0013560244030304607,
                            "process_2": 0.001510545844719673,
                            "process_3": 0.0014355725278755926
                        },
                        "gpu_energy": {
                            "process_1": 0.0038908872793737004,
                            "process_0": 0.0033930518811060573,
                            "process_2": 0.003781093024872062,
                            "process_3": 0.0036181212278276753
                        },
                        "ram_energy": {
                            "process_1": 6.933644343574671e-06,
                            "process_0": 6.059039232413427e-06,
                            "process_2": 6.735710161899771e-06,
                            "process_3": 6.6462698703080985e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005456546182778524,
                            "process_0": 0.004755135323368932,
                            "process_2": 0.005298374579753633,
                            "process_3": 0.005060340025573575
                        },
                        "total_energy_joules": {
                            "process_1": 19643.566258002684,
                            "process_0": 17118.487164128157,
                            "process_2": 19074.148487113078,
                            "process_3": 18217.224092064873
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 306.36650123816577,
                        "ram_power_avg": 0.699122428894043,
                        "cpu_energy_total": 0.005860868034686975,
                        "gpu_energy_total": 0.014683153413179495,
                        "ram_energy_total": 2.6374663608195967e-05,
                        "total_energy_kwh": 0.020570396111474664,
                        "total_energy_joules": 74053.42600130879
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22124567200591685,
                        "joules_per_token": 4.519862426837695,
                        "flops_per_joule": 228888410.81913528,
                        "joules_per_flop": 4.36894116404254e-09
                    },
                    "per-process_emissions": [
                        0.0020786712683294787,
                        0.0018114688014373946,
                        0.0020184157961571467,
                        0.0019277365327422535
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0416": {
            "setup": {
                "experiment_id": "0416",
                "date_time": "April 13, 2025 at 12:02:52 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.90371294107172,
                        "average_latency_ms_per_batch": 5487.964117633965,
                        "throughput_queries_per_sec": 2.9154709573607978,
                        "throughput_tokens_per_sec": 373.1802825421821
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 39.2,
                        "cpu_memory_usage_bytes": 1927159808
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0416",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 283.4643294040321,
                            "process_0": 343.2714791314625,
                            "process_2": 264.2935700842934,
                            "process_3": 334.43662633287505
                        },
                        "ram_power": {
                            "process_1": 0.7059230804443359,
                            "process_0": 0.6727695465087891,
                            "process_2": 0.708651065826416,
                            "process_3": 0.709146022796631
                        },
                        "cpu_energy": {
                            "process_1": 0.0015587252590612485,
                            "process_0": 0.0013560244030304607,
                            "process_2": 0.001510545844719673,
                            "process_3": 0.0014355725278755926
                        },
                        "gpu_energy": {
                            "process_1": 0.0038908872793737004,
                            "process_0": 0.0033930518811060573,
                            "process_2": 0.003781093024872062,
                            "process_3": 0.0036181212278276753
                        },
                        "ram_energy": {
                            "process_1": 6.933644343574671e-06,
                            "process_0": 6.059039232413427e-06,
                            "process_2": 6.735710161899771e-06,
                            "process_3": 6.6462698703080985e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.005456546182778524,
                            "process_0": 0.004755135323368932,
                            "process_2": 0.005298374579753633,
                            "process_3": 0.005060340025573575
                        },
                        "total_energy_joules": {
                            "process_1": 19643.566258002684,
                            "process_0": 17118.487164128157,
                            "process_2": 19074.148487113078,
                            "process_3": 18217.224092064873
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 306.36650123816577,
                        "ram_power_avg": 0.699122428894043,
                        "cpu_energy_total": 0.005860868034686975,
                        "gpu_energy_total": 0.014683153413179495,
                        "ram_energy_total": 2.6374663608195967e-05,
                        "total_energy_kwh": 0.020570396111474664,
                        "total_energy_joules": 74053.42600130879
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.22124567200591685,
                        "joules_per_token": 4.519862426837695,
                        "flops_per_joule": 228888410.81913528,
                        "joules_per_flop": 4.36894116404254e-09
                    },
                    "per-process_emissions": [
                        0.0020786712683294787,
                        0.0018114688014373946,
                        0.0020184157961571467,
                        0.0019277365327422535
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0417": {
            "setup": {
                "experiment_id": "0417",
                "date_time": "April 13, 2025 at 12:04:37 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 51.61812365896185,
                        "average_latency_ms_per_batch": 6452.265457370231,
                        "throughput_queries_per_sec": 2.479749183555936,
                        "throughput_tokens_per_sec": 317.40789549515983
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 38.4,
                        "cpu_memory_usage_bytes": 1935450112
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0417",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 361.1755833084371,
                            "process_1": 316.1832564827257,
                            "process_2": 244.37892373332537,
                            "process_0": 238.7474632866973
                        },
                        "ram_power": {
                            "process_3": 0.7052063941955566,
                            "process_1": 0.7112689018249513,
                            "process_2": 0.7061605453491211,
                            "process_0": 0.6756649017333984
                        },
                        "cpu_energy": {
                            "process_3": 0.0011585506229066594,
                            "process_1": 0.0011954264951236839,
                            "process_2": 0.001419366927030751,
                            "process_0": 0.0015955228840030037
                        },
                        "gpu_energy": {
                            "process_3": 0.0035680481322135638,
                            "process_1": 0.0036674573784085673,
                            "process_2": 0.004148707485630565,
                            "process_0": 0.004527079454993643
                        },
                        "ram_energy": {
                            "process_3": 5.527279551797935e-06,
                            "process_1": 5.560906502933405e-06,
                            "process_2": 6.521674294887058e-06,
                            "process_0": 6.796465628767817e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.004732126034672021,
                            "process_1": 0.004868444780035187,
                            "process_2": 0.005574596086956204,
                            "process_0": 0.006129398804625418
                        },
                        "total_energy_joules": {
                            "process_3": 17035.653724819276,
                            "process_1": 17526.401208126674,
                            "process_2": 20068.545913042333,
                            "process_0": 22065.835696651506
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 290.12130670279635,
                        "ram_power_avg": 0.6995751857757568,
                        "cpu_energy_total": 0.005368866929064098,
                        "gpu_energy_total": 0.01591129245124634,
                        "ram_energy_total": 2.4406325978386214e-05,
                        "total_energy_kwh": 0.02130456570628883,
                        "total_energy_joules": 76696.43654263979
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2136213980540182,
                        "joules_per_token": 4.681178988198229,
                        "flops_per_joule": 221000762.97193515,
                        "joules_per_flop": 4.524871256335843e-09
                    },
                    "per-process_emissions": [
                        0.0018027034129083064,
                        0.0018546340389544046,
                        0.002123642379325966,
                        0.002334994474622053
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0417": {
            "setup": {
                "experiment_id": "0417",
                "date_time": "April 13, 2025 at 12:04:37 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.7_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.7,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 51.61812365896185,
                        "average_latency_ms_per_batch": 6452.265457370231,
                        "throughput_queries_per_sec": 2.479749183555936,
                        "throughput_tokens_per_sec": 317.40789549515983
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 38.4,
                        "cpu_memory_usage_bytes": 1935450112
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0417",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 361.1755833084371,
                            "process_1": 316.1832564827257,
                            "process_2": 244.37892373332537,
                            "process_0": 238.7474632866973
                        },
                        "ram_power": {
                            "process_3": 0.7052063941955566,
                            "process_1": 0.7112689018249513,
                            "process_2": 0.7061605453491211,
                            "process_0": 0.6756649017333984
                        },
                        "cpu_energy": {
                            "process_3": 0.0011585506229066594,
                            "process_1": 0.0011954264951236839,
                            "process_2": 0.001419366927030751,
                            "process_0": 0.0015955228840030037
                        },
                        "gpu_energy": {
                            "process_3": 0.0035680481322135638,
                            "process_1": 0.0036674573784085673,
                            "process_2": 0.004148707485630565,
                            "process_0": 0.004527079454993643
                        },
                        "ram_energy": {
                            "process_3": 5.527279551797935e-06,
                            "process_1": 5.560906502933405e-06,
                            "process_2": 6.521674294887058e-06,
                            "process_0": 6.796465628767817e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.004732126034672021,
                            "process_1": 0.004868444780035187,
                            "process_2": 0.005574596086956204,
                            "process_0": 0.006129398804625418
                        },
                        "total_energy_joules": {
                            "process_3": 17035.653724819276,
                            "process_1": 17526.401208126674,
                            "process_2": 20068.545913042333,
                            "process_0": 22065.835696651506
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 290.12130670279635,
                        "ram_power_avg": 0.6995751857757568,
                        "cpu_energy_total": 0.005368866929064098,
                        "gpu_energy_total": 0.01591129245124634,
                        "ram_energy_total": 2.4406325978386214e-05,
                        "total_energy_kwh": 0.02130456570628883,
                        "total_energy_joules": 76696.43654263979
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2136213980540182,
                        "joules_per_token": 4.681178988198229,
                        "flops_per_joule": 221000762.97193515,
                        "joules_per_flop": 4.524871256335843e-09
                    },
                    "per-process_emissions": [
                        0.0018027034129083064,
                        0.0018546340389544046,
                        0.002123642379325966,
                        0.002334994474622053
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0418": {
            "setup": {
                "experiment_id": "0418",
                "date_time": "April 13, 2025 at 12:11:07 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_1",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 11147
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 333.4333662870631,
                        "average_latency_ms_per_batch": 2604.9481741176805,
                        "throughput_queries_per_sec": 0.38388479660970953,
                        "throughput_tokens_per_sec": 33.430967404753375
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609948672,
                        "gpu_max_memory_allocated_bytes": 1609948672,
                        "gpu_current_memory_reserved_bytes": 2143289344,
                        "gpu_max_memory_reserved_bytes": 2143289344
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 37.8,
                        "cpu_memory_usage_bytes": 1905819648
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0418",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 210.99845146698914,
                            "process_3": 212.11599950499595,
                            "process_1": 239.20344688909069,
                            "process_0": 187.53073864649073
                        },
                        "ram_power": {
                            "process_2": 0.6930255889892578,
                            "process_3": 0.7053437232971191,
                            "process_1": 0.6949152946472168,
                            "process_0": 0.6654109954833984
                        },
                        "cpu_energy": {
                            "process_2": 0.01031849799581596,
                            "process_3": 0.01035805044282188,
                            "process_1": 0.010391990683240697,
                            "process_0": 0.010221101307806749
                        },
                        "gpu_energy": {
                            "process_2": 0.015797384860120722,
                            "process_3": 0.015869252139836476,
                            "process_1": 0.015954391930170342,
                            "process_0": 0.01564220362486468
                        },
                        "ram_energy": {
                            "process_2": 4.541604450340593e-05,
                            "process_3": 4.627210568515521e-05,
                            "process_1": 4.6344712900764614e-05,
                            "process_0": 4.365763896804678e-05
                        },
                        "total_energy_kwh": {
                            "process_2": 0.026161298900440076,
                            "process_3": 0.026273574688343532,
                            "process_1": 0.026392727326311795,
                            "process_0": 0.025906962571639443
                        },
                        "total_energy_joules": {
                            "process_2": 94180.67604158427,
                            "process_3": 94584.86887803672,
                            "process_1": 95013.81837472247,
                            "process_0": 93265.065257902
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 212.46215912689163,
                        "ram_power_avg": 0.689673900604248,
                        "cpu_energy_total": 0.04128964042968529,
                        "gpu_energy_total": 0.06326323255499222,
                        "ram_energy_total": 0.00018169050205737254,
                        "total_energy_kwh": 0.10473456348673485,
                        "total_energy_joules": 377044.4285522455
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.02956415519200652,
                        "joules_per_token": 33.824744644500356,
                        "flops_per_joule": 44954837.43980933,
                        "joules_per_flop": 2.2244547126633794e-08
                    },
                    "per-process_emissions": [
                        0.009966146816122647,
                        0.010008918277524469,
                        0.010054309474958478,
                        0.009869257391666046
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0418": {
            "setup": {
                "experiment_id": "0418",
                "date_time": "April 13, 2025 at 12:11:07 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_1",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 11147
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 333.4333662870631,
                        "average_latency_ms_per_batch": 2604.9481741176805,
                        "throughput_queries_per_sec": 0.38388479660970953,
                        "throughput_tokens_per_sec": 33.430967404753375
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609948672,
                        "gpu_max_memory_allocated_bytes": 1609948672,
                        "gpu_current_memory_reserved_bytes": 2143289344,
                        "gpu_max_memory_reserved_bytes": 2143289344
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 37.8,
                        "cpu_memory_usage_bytes": 1905819648
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0418",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 210.99845146698914,
                            "process_3": 212.11599950499595,
                            "process_1": 239.20344688909069,
                            "process_0": 187.53073864649073
                        },
                        "ram_power": {
                            "process_2": 0.6930255889892578,
                            "process_3": 0.7053437232971191,
                            "process_1": 0.6949152946472168,
                            "process_0": 0.6654109954833984
                        },
                        "cpu_energy": {
                            "process_2": 0.01031849799581596,
                            "process_3": 0.01035805044282188,
                            "process_1": 0.010391990683240697,
                            "process_0": 0.010221101307806749
                        },
                        "gpu_energy": {
                            "process_2": 0.015797384860120722,
                            "process_3": 0.015869252139836476,
                            "process_1": 0.015954391930170342,
                            "process_0": 0.01564220362486468
                        },
                        "ram_energy": {
                            "process_2": 4.541604450340593e-05,
                            "process_3": 4.627210568515521e-05,
                            "process_1": 4.6344712900764614e-05,
                            "process_0": 4.365763896804678e-05
                        },
                        "total_energy_kwh": {
                            "process_2": 0.026161298900440076,
                            "process_3": 0.026273574688343532,
                            "process_1": 0.026392727326311795,
                            "process_0": 0.025906962571639443
                        },
                        "total_energy_joules": {
                            "process_2": 94180.67604158427,
                            "process_3": 94584.86887803672,
                            "process_1": 95013.81837472247,
                            "process_0": 93265.065257902
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 212.46215912689163,
                        "ram_power_avg": 0.689673900604248,
                        "cpu_energy_total": 0.04128964042968529,
                        "gpu_energy_total": 0.06326323255499222,
                        "ram_energy_total": 0.00018169050205737254,
                        "total_energy_kwh": 0.10473456348673485,
                        "total_energy_joules": 377044.4285522455
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.02956415519200652,
                        "joules_per_token": 33.824744644500356,
                        "flops_per_joule": 44954837.43980933,
                        "joules_per_flop": 2.2244547126633794e-08
                    },
                    "per-process_emissions": [
                        0.009966146816122647,
                        0.010008918277524469,
                        0.010054309474958478,
                        0.009869257391666046
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0419": {
            "setup": {
                "experiment_id": "0419",
                "date_time": "April 13, 2025 at 12:12:42 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 42.84638323698891,
                        "average_latency_ms_per_batch": 5355.797904623614,
                        "throughput_queries_per_sec": 2.987416680936997,
                        "throughput_tokens_per_sec": 382.3893351599356
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 37.6,
                        "cpu_memory_usage_bytes": 1834799104
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0419",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 305.91567609114406,
                            "process_0": 261.99400465699887,
                            "process_2": 262.43442759739804,
                            "process_1": 283.47797370459074
                        },
                        "ram_power": {
                            "process_3": 0.6672449111938477,
                            "process_0": 0.6405129432678223,
                            "process_2": 0.6662092208862305,
                            "process_1": 0.6665067672729492
                        },
                        "cpu_energy": {
                            "process_3": 0.0011905684565272168,
                            "process_0": 0.0013234496625009339,
                            "process_2": 0.0013246828757837645,
                            "process_1": 0.0012592634992188326
                        },
                        "gpu_energy": {
                            "process_3": 0.0033627082457194923,
                            "process_0": 0.003675504051510403,
                            "process_2": 0.003675334329153568,
                            "process_1": 0.003528095878028603
                        },
                        "ram_energy": {
                            "process_3": 5.226950347868599e-06,
                            "process_0": 5.493575261048759e-06,
                            "process_2": 5.714204159546373e-06,
                            "process_1": 5.392464953405147e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0045585036525945775,
                            "process_0": 0.005004447289272388,
                            "process_2": 0.005005731409096878,
                            "process_1": 0.00479275184220084
                        },
                        "total_energy_joules": {
                            "process_3": 16410.61314934048,
                            "process_0": 18016.010241380594,
                            "process_2": 18020.633072748762,
                            "process_1": 17253.906631923022
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 278.4555205125329,
                        "ram_power_avg": 0.6601184606552124,
                        "cpu_energy_total": 0.005097964494030749,
                        "gpu_energy_total": 0.014241642504412066,
                        "ram_energy_total": 2.1827194721868878e-05,
                        "total_energy_kwh": 0.019361434193164682,
                        "total_energy_joules": 69701.16309539285
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.23506063991467252,
                        "joules_per_token": 4.2542213803340365,
                        "flops_per_joule": 243180604.74764687,
                        "joules_per_flop": 4.112170051709999e-09
                    },
                    "per-process_emissions": [
                        0.0017365619664559044,
                        0.001906444194848316,
                        0.0019069333802954557,
                        0.00182579881428641
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0419": {
            "setup": {
                "experiment_id": "0419",
                "date_time": "April 13, 2025 at 12:12:42 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 42.84638323698891,
                        "average_latency_ms_per_batch": 5355.797904623614,
                        "throughput_queries_per_sec": 2.987416680936997,
                        "throughput_tokens_per_sec": 382.3893351599356
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 37.6,
                        "cpu_memory_usage_bytes": 1834799104
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0419",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 305.91567609114406,
                            "process_0": 261.99400465699887,
                            "process_2": 262.43442759739804,
                            "process_1": 283.47797370459074
                        },
                        "ram_power": {
                            "process_3": 0.6672449111938477,
                            "process_0": 0.6405129432678223,
                            "process_2": 0.6662092208862305,
                            "process_1": 0.6665067672729492
                        },
                        "cpu_energy": {
                            "process_3": 0.0011905684565272168,
                            "process_0": 0.0013234496625009339,
                            "process_2": 0.0013246828757837645,
                            "process_1": 0.0012592634992188326
                        },
                        "gpu_energy": {
                            "process_3": 0.0033627082457194923,
                            "process_0": 0.003675504051510403,
                            "process_2": 0.003675334329153568,
                            "process_1": 0.003528095878028603
                        },
                        "ram_energy": {
                            "process_3": 5.226950347868599e-06,
                            "process_0": 5.493575261048759e-06,
                            "process_2": 5.714204159546373e-06,
                            "process_1": 5.392464953405147e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.0045585036525945775,
                            "process_0": 0.005004447289272388,
                            "process_2": 0.005005731409096878,
                            "process_1": 0.00479275184220084
                        },
                        "total_energy_joules": {
                            "process_3": 16410.61314934048,
                            "process_0": 18016.010241380594,
                            "process_2": 18020.633072748762,
                            "process_1": 17253.906631923022
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 278.4555205125329,
                        "ram_power_avg": 0.6601184606552124,
                        "cpu_energy_total": 0.005097964494030749,
                        "gpu_energy_total": 0.014241642504412066,
                        "ram_energy_total": 2.1827194721868878e-05,
                        "total_energy_kwh": 0.019361434193164682,
                        "total_energy_joules": 69701.16309539285
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.23506063991467252,
                        "joules_per_token": 4.2542213803340365,
                        "flops_per_joule": 243180604.74764687,
                        "joules_per_flop": 4.112170051709999e-09
                    },
                    "per-process_emissions": [
                        0.0017365619664559044,
                        0.001906444194848316,
                        0.0019069333802954557,
                        0.00182579881428641
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0420": {
            "setup": {
                "experiment_id": "0420",
                "date_time": "April 13, 2025 at 12:14:14 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.621403962984914,
                        "average_latency_ms_per_batch": 5202.675495373114,
                        "throughput_queries_per_sec": 3.075340757698467,
                        "throughput_tokens_per_sec": 393.64361698540375
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 36.2,
                        "cpu_memory_usage_bytes": 1935269888
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0420",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 382.1798159418378,
                            "process_3": 0.0,
                            "process_2": 363.88642829245487,
                            "process_1": 505.1609714124011
                        },
                        "ram_power": {
                            "process_0": 0.6756019592285156,
                            "process_3": 0.7063107490539551,
                            "process_2": 0.7057757377624513,
                            "process_1": 0.7053065299987793
                        },
                        "cpu_energy": {
                            "process_0": 0.0012861918962844355,
                            "process_3": 0.0013641562259708731,
                            "process_2": 0.001286630350314226,
                            "process_1": 0.0013632094535032592
                        },
                        "gpu_energy": {
                            "process_0": 0.0038657997593032345,
                            "process_3": 0.003975386235861134,
                            "process_2": 0.0038674655939692215,
                            "process_1": 0.004046249903663535
                        },
                        "ram_energy": {
                            "process_0": 5.752570466294922e-06,
                            "process_3": 6.35896320327507e-06,
                            "process_2": 6.000851040009658e-06,
                            "process_1": 6.490155406420496e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0051577442260539634,
                            "process_3": 0.005345901425035282,
                            "process_2": 0.0051600967953234584,
                            "process_1": 0.005415949512573215
                        },
                        "total_energy_joules": {
                            "process_0": 18567.87921379427,
                            "process_3": 19245.245130127016,
                            "process_2": 18576.34846316445,
                            "process_1": 19497.418245263576
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 312.80680391167346,
                        "ram_power_avg": 0.6982487440109253,
                        "cpu_energy_total": 0.005300187926072794,
                        "gpu_energy_total": 0.015754901492797124,
                        "ram_energy_total": 2.4602540116000145e-05,
                        "total_energy_kwh": 0.02107969195898592,
                        "total_energy_joules": 75886.89105234931
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21590026647287172,
                        "joules_per_token": 4.631768252706867,
                        "flops_per_joule": 223358352.9131447,
                        "joules_per_flop": 4.4771103787144274e-09
                    },
                    "per-process_emissions": [
                        0.0019648426629152575,
                        0.002036521147867191,
                        0.0019657388741784717,
                        0.0020632059668147665
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0420": {
            "setup": {
                "experiment_id": "0420",
                "date_time": "April 13, 2025 at 12:14:14 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.621403962984914,
                        "average_latency_ms_per_batch": 5202.675495373114,
                        "throughput_queries_per_sec": 3.075340757698467,
                        "throughput_tokens_per_sec": 393.64361698540375
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 36.2,
                        "cpu_memory_usage_bytes": 1935269888
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0420",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 382.1798159418378,
                            "process_3": 0.0,
                            "process_2": 363.88642829245487,
                            "process_1": 505.1609714124011
                        },
                        "ram_power": {
                            "process_0": 0.6756019592285156,
                            "process_3": 0.7063107490539551,
                            "process_2": 0.7057757377624513,
                            "process_1": 0.7053065299987793
                        },
                        "cpu_energy": {
                            "process_0": 0.0012861918962844355,
                            "process_3": 0.0013641562259708731,
                            "process_2": 0.001286630350314226,
                            "process_1": 0.0013632094535032592
                        },
                        "gpu_energy": {
                            "process_0": 0.0038657997593032345,
                            "process_3": 0.003975386235861134,
                            "process_2": 0.0038674655939692215,
                            "process_1": 0.004046249903663535
                        },
                        "ram_energy": {
                            "process_0": 5.752570466294922e-06,
                            "process_3": 6.35896320327507e-06,
                            "process_2": 6.000851040009658e-06,
                            "process_1": 6.490155406420496e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0051577442260539634,
                            "process_3": 0.005345901425035282,
                            "process_2": 0.0051600967953234584,
                            "process_1": 0.005415949512573215
                        },
                        "total_energy_joules": {
                            "process_0": 18567.87921379427,
                            "process_3": 19245.245130127016,
                            "process_2": 18576.34846316445,
                            "process_1": 19497.418245263576
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 312.80680391167346,
                        "ram_power_avg": 0.6982487440109253,
                        "cpu_energy_total": 0.005300187926072794,
                        "gpu_energy_total": 0.015754901492797124,
                        "ram_energy_total": 2.4602540116000145e-05,
                        "total_energy_kwh": 0.02107969195898592,
                        "total_energy_joules": 75886.89105234931
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.21590026647287172,
                        "joules_per_token": 4.631768252706867,
                        "flops_per_joule": 223358352.9131447,
                        "joules_per_flop": 4.4771103787144274e-09
                    },
                    "per-process_emissions": [
                        0.0019648426629152575,
                        0.002036521147867191,
                        0.0019657388741784717,
                        0.0020632059668147665
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0423": {
            "setup": {
                "experiment_id": "0423",
                "date_time": "April 13, 2025 at 12:17:21 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 42.27221711794846,
                        "average_latency_ms_per_batch": 5284.027139743557,
                        "throughput_queries_per_sec": 3.027993531610912,
                        "throughput_tokens_per_sec": 387.58317204619675
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 36.9,
                        "cpu_memory_usage_bytes": 1985560576
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0423",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 538.6579118146559,
                            "process_3": 379.8043206819077,
                            "process_1": 530.2632706411144,
                            "process_0": 450.53137971692536
                        },
                        "ram_power": {
                            "process_2": 0.7244296073913574,
                            "process_3": 0.7243123054504395,
                            "process_1": 0.7210493087768555,
                            "process_0": 0.6931657791137695
                        },
                        "cpu_energy": {
                            "process_2": 0.0012452994110672081,
                            "process_3": 0.0014128098684413999,
                            "process_1": 0.0012749637430933944,
                            "process_0": 0.0013073219279076515
                        },
                        "gpu_energy": {
                            "process_2": 0.004884323629676279,
                            "process_3": 0.005486283277911452,
                            "process_1": 0.005006285949469502,
                            "process_0": 0.005115317981140288
                        },
                        "ram_energy": {
                            "process_2": 5.98354479818862e-06,
                            "process_3": 6.907538611320186e-06,
                            "process_1": 6.154395060552318e-06,
                            "process_0": 5.955077847797961e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.006135606585541674,
                            "process_3": 0.006906000684964173,
                            "process_1": 0.006287404087623449,
                            "process_0": 0.006428594986895737
                        },
                        "total_energy_joules": {
                            "process_2": 22088.183707950026,
                            "process_3": 24861.602465871023,
                            "process_1": 22634.654715444416,
                            "process_0": 23142.941952824654
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 474.81422071365085,
                        "ram_power_avg": 0.7157392501831055,
                        "cpu_energy_total": 0.005240394950509654,
                        "gpu_energy_total": 0.020492210838197522,
                        "ram_energy_total": 2.5000556317859083e-05,
                        "total_energy_kwh": 0.025757606345025035,
                        "total_energy_joules": 92727.38284209011
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.17668998625682228,
                        "joules_per_token": 5.659630300420539,
                        "flops_per_joule": 182793587.75839618,
                        "joules_per_flop": 5.47065141760733e-09
                    },
                    "per-process_emissions": [
                        0.0023373593287621007,
                        0.0026308409609371018,
                        0.002395186587180153,
                        0.002448973260257931
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0423": {
            "setup": {
                "experiment_id": "0423",
                "date_time": "April 13, 2025 at 12:17:21 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 42.27221711794846,
                        "average_latency_ms_per_batch": 5284.027139743557,
                        "throughput_queries_per_sec": 3.027993531610912,
                        "throughput_tokens_per_sec": 387.58317204619675
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 36.9,
                        "cpu_memory_usage_bytes": 1985560576
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0423",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 538.6579118146559,
                            "process_3": 379.8043206819077,
                            "process_1": 530.2632706411144,
                            "process_0": 450.53137971692536
                        },
                        "ram_power": {
                            "process_2": 0.7244296073913574,
                            "process_3": 0.7243123054504395,
                            "process_1": 0.7210493087768555,
                            "process_0": 0.6931657791137695
                        },
                        "cpu_energy": {
                            "process_2": 0.0012452994110672081,
                            "process_3": 0.0014128098684413999,
                            "process_1": 0.0012749637430933944,
                            "process_0": 0.0013073219279076515
                        },
                        "gpu_energy": {
                            "process_2": 0.004884323629676279,
                            "process_3": 0.005486283277911452,
                            "process_1": 0.005006285949469502,
                            "process_0": 0.005115317981140288
                        },
                        "ram_energy": {
                            "process_2": 5.98354479818862e-06,
                            "process_3": 6.907538611320186e-06,
                            "process_1": 6.154395060552318e-06,
                            "process_0": 5.955077847797961e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.006135606585541674,
                            "process_3": 0.006906000684964173,
                            "process_1": 0.006287404087623449,
                            "process_0": 0.006428594986895737
                        },
                        "total_energy_joules": {
                            "process_2": 22088.183707950026,
                            "process_3": 24861.602465871023,
                            "process_1": 22634.654715444416,
                            "process_0": 23142.941952824654
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 474.81422071365085,
                        "ram_power_avg": 0.7157392501831055,
                        "cpu_energy_total": 0.005240394950509654,
                        "gpu_energy_total": 0.020492210838197522,
                        "ram_energy_total": 2.5000556317859083e-05,
                        "total_energy_kwh": 0.025757606345025035,
                        "total_energy_joules": 92727.38284209011
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.17668998625682228,
                        "joules_per_token": 5.659630300420539,
                        "flops_per_joule": 182793587.75839618,
                        "joules_per_flop": 5.47065141760733e-09
                    },
                    "per-process_emissions": [
                        0.0023373593287621007,
                        0.0026308409609371018,
                        0.002395186587180153,
                        0.002448973260257931
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0424": {
            "setup": {
                "experiment_id": "0424",
                "date_time": "April 13, 2025 at 12:18:58 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.61714293103432,
                        "average_latency_ms_per_batch": 4827.14286637929,
                        "throughput_queries_per_sec": 3.3145901090765038,
                        "throughput_tokens_per_sec": 424.2675339617925
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            6.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 35.3,
                        "cpu_memory_usage_bytes": 1926049792
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0424",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 396.692773672862,
                            "process_0": 548.2471859550178,
                            "process_3": 639.4300741471608,
                            "process_2": 370.75623600350053
                        },
                        "ram_power": {
                            "process_1": 0.7051548957824707,
                            "process_0": 0.6723818778991699,
                            "process_3": 0.7085995674133301,
                            "process_2": 0.7130484580993652
                        },
                        "cpu_energy": {
                            "process_1": 0.0012802943250007956,
                            "process_0": 0.001190907736309782,
                            "process_3": 0.0014869967196264042,
                            "process_2": 0.0013092530709691347
                        },
                        "gpu_energy": {
                            "process_1": 0.0050561179337804685,
                            "process_0": 0.004709157933989294,
                            "process_3": 0.005782732681738123,
                            "process_2": 0.005152523288682209
                        },
                        "ram_energy": {
                            "process_1": 5.9899162385782125e-06,
                            "process_0": 5.379629159307155e-06,
                            "process_3": 7.39947919396546e-06,
                            "process_2": 6.294140067321394e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.00634240217501984,
                            "process_0": 0.005905445299458383,
                            "process_3": 0.007277128880558495,
                            "process_2": 0.006468070499718664
                        },
                        "total_energy_joules": {
                            "process_1": 22832.647830071426,
                            "process_0": 21259.60307805018,
                            "process_3": 26197.66397001058,
                            "process_2": 23285.053798987192
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 488.7815674446353,
                        "ram_power_avg": 0.699796199798584,
                        "cpu_energy_total": 0.005267451851906117,
                        "gpu_energy_total": 0.020700531838190095,
                        "ram_energy_total": 2.5063164659172223e-05,
                        "total_energy_kwh": 0.025993046854755383,
                        "total_energy_joules": 93574.96867711938
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.17508955900945075,
                        "joules_per_token": 5.711362834296837,
                        "flops_per_joule": 181137875.14733678,
                        "joules_per_flop": 5.520656567195592e-09
                    },
                    "per-process_emissions": [
                        0.0024161381085738083,
                        0.002249679386828671,
                        0.0027722222470487586,
                        0.002464011456867825
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0424": {
            "setup": {
                "experiment_id": "0424",
                "date_time": "April 13, 2025 at 12:18:58 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.61714293103432,
                        "average_latency_ms_per_batch": 4827.14286637929,
                        "throughput_queries_per_sec": 3.3145901090765038,
                        "throughput_tokens_per_sec": 424.2675339617925
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            6.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 35.3,
                        "cpu_memory_usage_bytes": 1926049792
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0424",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 396.692773672862,
                            "process_0": 548.2471859550178,
                            "process_3": 639.4300741471608,
                            "process_2": 370.75623600350053
                        },
                        "ram_power": {
                            "process_1": 0.7051548957824707,
                            "process_0": 0.6723818778991699,
                            "process_3": 0.7085995674133301,
                            "process_2": 0.7130484580993652
                        },
                        "cpu_energy": {
                            "process_1": 0.0012802943250007956,
                            "process_0": 0.001190907736309782,
                            "process_3": 0.0014869967196264042,
                            "process_2": 0.0013092530709691347
                        },
                        "gpu_energy": {
                            "process_1": 0.0050561179337804685,
                            "process_0": 0.004709157933989294,
                            "process_3": 0.005782732681738123,
                            "process_2": 0.005152523288682209
                        },
                        "ram_energy": {
                            "process_1": 5.9899162385782125e-06,
                            "process_0": 5.379629159307155e-06,
                            "process_3": 7.39947919396546e-06,
                            "process_2": 6.294140067321394e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.00634240217501984,
                            "process_0": 0.005905445299458383,
                            "process_3": 0.007277128880558495,
                            "process_2": 0.006468070499718664
                        },
                        "total_energy_joules": {
                            "process_1": 22832.647830071426,
                            "process_0": 21259.60307805018,
                            "process_3": 26197.66397001058,
                            "process_2": 23285.053798987192
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 488.7815674446353,
                        "ram_power_avg": 0.699796199798584,
                        "cpu_energy_total": 0.005267451851906117,
                        "gpu_energy_total": 0.020700531838190095,
                        "ram_energy_total": 2.5063164659172223e-05,
                        "total_energy_kwh": 0.025993046854755383,
                        "total_energy_joules": 93574.96867711938
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.17508955900945075,
                        "joules_per_token": 5.711362834296837,
                        "flops_per_joule": 181137875.14733678,
                        "joules_per_flop": 5.520656567195592e-09
                    },
                    "per-process_emissions": [
                        0.0024161381085738083,
                        0.002249679386828671,
                        0.0027722222470487586,
                        0.002464011456867825
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0425": {
            "setup": {
                "experiment_id": "0425",
                "date_time": "April 13, 2025 at 12:20:32 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.98379503597971,
                        "average_latency_ms_per_batch": 5247.974379497464,
                        "throughput_queries_per_sec": 3.048795371888254,
                        "throughput_tokens_per_sec": 390.2458076016965
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 35.3,
                        "cpu_memory_usage_bytes": 1978052608
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0425",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 0.0,
                            "process_1": 308.09406710772424,
                            "process_2": 524.6948875710311,
                            "process_3": 421.5486178709368
                        },
                        "ram_power": {
                            "process_0": 0.6905436515808105,
                            "process_1": 0.7231521606445312,
                            "process_2": 0.725797176361084,
                            "process_3": 0.7212224006652832
                        },
                        "cpu_energy": {
                            "process_0": 0.0012973982642824921,
                            "process_1": 0.001336948128973745,
                            "process_2": 0.0012544926709069841,
                            "process_3": 0.0014436518085612989
                        },
                        "gpu_energy": {
                            "process_0": 0.004992795938677075,
                            "process_1": 0.005135244663747374,
                            "process_2": 0.0048088135692705425,
                            "process_3": 0.005445381022967322
                        },
                        "ram_energy": {
                            "process_0": 6.121741150556042e-06,
                            "process_1": 6.4037898695988955e-06,
                            "process_2": 6.0955336881055375e-06,
                            "process_3": 7.022052559357467e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.006296315944110123,
                            "process_1": 0.0064785965825907175,
                            "process_2": 0.00606940177386563,
                            "process_3": 0.006896054884087976
                        },
                        "total_energy_joules": {
                            "process_0": 22666.737398796442,
                            "process_1": 23322.947697326585,
                            "process_2": 21849.84638591627,
                            "process_3": 24825.797582716714
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 313.58439313742304,
                        "ram_power_avg": 0.7151788473129272,
                        "cpu_energy_total": 0.00533249087272452,
                        "gpu_energy_total": 0.020382235194662313,
                        "ram_energy_total": 2.5643117267617944e-05,
                        "total_energy_kwh": 0.025740369184654444,
                        "total_energy_joules": 92665.329064756
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.17680830754456825,
                        "joules_per_token": 5.655842838425049,
                        "flops_per_joule": 182915996.3518512,
                        "joules_per_flop": 5.466990421528978e-09
                    },
                    "per-process_emissions": [
                        0.002398581558908751,
                        0.0024680213681379337,
                        0.002312138605754112,
                        0.0026270521080933146
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0425": {
            "setup": {
                "experiment_id": "0425",
                "date_time": "April 13, 2025 at 12:20:32 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.98379503597971,
                        "average_latency_ms_per_batch": 5247.974379497464,
                        "throughput_queries_per_sec": 3.048795371888254,
                        "throughput_tokens_per_sec": 390.2458076016965
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 35.3,
                        "cpu_memory_usage_bytes": 1978052608
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0425",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 0.0,
                            "process_1": 308.09406710772424,
                            "process_2": 524.6948875710311,
                            "process_3": 421.5486178709368
                        },
                        "ram_power": {
                            "process_0": 0.6905436515808105,
                            "process_1": 0.7231521606445312,
                            "process_2": 0.725797176361084,
                            "process_3": 0.7212224006652832
                        },
                        "cpu_energy": {
                            "process_0": 0.0012973982642824921,
                            "process_1": 0.001336948128973745,
                            "process_2": 0.0012544926709069841,
                            "process_3": 0.0014436518085612989
                        },
                        "gpu_energy": {
                            "process_0": 0.004992795938677075,
                            "process_1": 0.005135244663747374,
                            "process_2": 0.0048088135692705425,
                            "process_3": 0.005445381022967322
                        },
                        "ram_energy": {
                            "process_0": 6.121741150556042e-06,
                            "process_1": 6.4037898695988955e-06,
                            "process_2": 6.0955336881055375e-06,
                            "process_3": 7.022052559357467e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.006296315944110123,
                            "process_1": 0.0064785965825907175,
                            "process_2": 0.00606940177386563,
                            "process_3": 0.006896054884087976
                        },
                        "total_energy_joules": {
                            "process_0": 22666.737398796442,
                            "process_1": 23322.947697326585,
                            "process_2": 21849.84638591627,
                            "process_3": 24825.797582716714
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 313.58439313742304,
                        "ram_power_avg": 0.7151788473129272,
                        "cpu_energy_total": 0.00533249087272452,
                        "gpu_energy_total": 0.020382235194662313,
                        "ram_energy_total": 2.5643117267617944e-05,
                        "total_energy_kwh": 0.025740369184654444,
                        "total_energy_joules": 92665.329064756
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.17680830754456825,
                        "joules_per_token": 5.655842838425049,
                        "flops_per_joule": 182915996.3518512,
                        "joules_per_flop": 5.466990421528978e-09
                    },
                    "per-process_emissions": [
                        0.002398581558908751,
                        0.0024680213681379337,
                        0.002312138605754112,
                        0.0026270521080933146
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0426": {
            "setup": {
                "experiment_id": "0426",
                "date_time": "April 13, 2025 at 12:22:07 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.32380582697806,
                        "average_latency_ms_per_batch": 5415.475728372257,
                        "throughput_queries_per_sec": 2.9544957456229164,
                        "throughput_tokens_per_sec": 378.1754554397333
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 35.3,
                        "cpu_memory_usage_bytes": 1977712640
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0426",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 415.51532638430433,
                            "process_1": 391.23392832152587,
                            "process_2": 411.54464319109786,
                            "process_0": 458.2793917369088
                        },
                        "ram_power": {
                            "process_3": 0.7234525680541992,
                            "process_1": 0.7189650535583496,
                            "process_2": 0.7255125045776367,
                            "process_0": 0.690424919128418
                        },
                        "cpu_energy": {
                            "process_3": 0.0014948716444350796,
                            "process_1": 0.0013198907132764364,
                            "process_2": 0.0012887255817531694,
                            "process_0": 0.0013379164595007749
                        },
                        "gpu_energy": {
                            "process_3": 0.00566374203099107,
                            "process_1": 0.005025033742246832,
                            "process_2": 0.004921172548046915,
                            "process_0": 0.005089089349046949
                        },
                        "ram_energy": {
                            "process_3": 7.22931527395556e-06,
                            "process_1": 6.286230683231623e-06,
                            "process_2": 6.196019199664413e-06,
                            "process_0": 6.100396069019648e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.007165842990700108,
                            "process_1": 0.006351210686206497,
                            "process_2": 0.006216094148999748,
                            "process_0": 0.0064331062046167455
                        },
                        "total_energy_joules": {
                            "process_3": 25797.034766520388,
                            "process_1": 22864.35847034339,
                            "process_2": 22377.938936399092,
                            "process_0": 23159.182336620284
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 419.14332240845926,
                        "ram_power_avg": 0.7145887613296509,
                        "cpu_energy_total": 0.00544140439896546,
                        "gpu_energy_total": 0.020699037670331766,
                        "ram_energy_total": 2.5811961225871245e-05,
                        "total_energy_kwh": 0.026166254030523098,
                        "total_energy_joules": 94198.51450988316
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.17393055596732385,
                        "joules_per_token": 5.749421051628611,
                        "flops_per_joule": 179938835.35577026,
                        "joules_per_flop": 5.557443994915421e-09
                    },
                    "per-process_emissions": [
                        0.0027298278873072062,
                        0.0024194937109103653,
                        0.0023680210660614537,
                        0.002450691808648749
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0426": {
            "setup": {
                "experiment_id": "0426",
                "date_time": "April 13, 2025 at 12:22:07 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.32380582697806,
                        "average_latency_ms_per_batch": 5415.475728372257,
                        "throughput_queries_per_sec": 2.9544957456229164,
                        "throughput_tokens_per_sec": 378.1754554397333
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 35.3,
                        "cpu_memory_usage_bytes": 1977712640
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0426",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 415.51532638430433,
                            "process_1": 391.23392832152587,
                            "process_2": 411.54464319109786,
                            "process_0": 458.2793917369088
                        },
                        "ram_power": {
                            "process_3": 0.7234525680541992,
                            "process_1": 0.7189650535583496,
                            "process_2": 0.7255125045776367,
                            "process_0": 0.690424919128418
                        },
                        "cpu_energy": {
                            "process_3": 0.0014948716444350796,
                            "process_1": 0.0013198907132764364,
                            "process_2": 0.0012887255817531694,
                            "process_0": 0.0013379164595007749
                        },
                        "gpu_energy": {
                            "process_3": 0.00566374203099107,
                            "process_1": 0.005025033742246832,
                            "process_2": 0.004921172548046915,
                            "process_0": 0.005089089349046949
                        },
                        "ram_energy": {
                            "process_3": 7.22931527395556e-06,
                            "process_1": 6.286230683231623e-06,
                            "process_2": 6.196019199664413e-06,
                            "process_0": 6.100396069019648e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.007165842990700108,
                            "process_1": 0.006351210686206497,
                            "process_2": 0.006216094148999748,
                            "process_0": 0.0064331062046167455
                        },
                        "total_energy_joules": {
                            "process_3": 25797.034766520388,
                            "process_1": 22864.35847034339,
                            "process_2": 22377.938936399092,
                            "process_0": 23159.182336620284
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 419.14332240845926,
                        "ram_power_avg": 0.7145887613296509,
                        "cpu_energy_total": 0.00544140439896546,
                        "gpu_energy_total": 0.020699037670331766,
                        "ram_energy_total": 2.5811961225871245e-05,
                        "total_energy_kwh": 0.026166254030523098,
                        "total_energy_joules": 94198.51450988316
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.17393055596732385,
                        "joules_per_token": 5.749421051628611,
                        "flops_per_joule": 179938835.35577026,
                        "joules_per_flop": 5.557443994915421e-09
                    },
                    "per-process_emissions": [
                        0.0027298278873072062,
                        0.0024194937109103653,
                        0.0023680210660614537,
                        0.002450691808648749
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0429": {
            "setup": {
                "experiment_id": "0429",
                "date_time": "April 13, 2025 at 12:25:17 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 44.11961353200604,
                        "average_latency_ms_per_batch": 5514.951691500755,
                        "throughput_queries_per_sec": 2.901204016829022,
                        "throughput_tokens_per_sec": 371.3541141541148
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 35.4,
                        "cpu_memory_usage_bytes": 1929535488
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0429",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 382.1936557606932,
                            "process_0": 411.91010701269727,
                            "process_1": 433.18917370373134,
                            "process_2": 567.9117745103373
                        },
                        "ram_power": {
                            "process_3": 0.7076411247253418,
                            "process_0": 0.6735992431640625,
                            "process_1": 0.7044267654418945,
                            "process_2": 0.7048001289367676
                        },
                        "cpu_energy": {
                            "process_3": 0.001528186400936648,
                            "process_0": 0.001362592478781153,
                            "process_1": 0.0011644357372815645,
                            "process_2": 0.001272177784841915
                        },
                        "gpu_energy": {
                            "process_3": 0.0059556205978270604,
                            "process_0": 0.005421759615182431,
                            "process_1": 0.004674517628498354,
                            "process_2": 0.0051031101935963274
                        },
                        "ram_energy": {
                            "process_3": 7.2357468409957394e-06,
                            "process_0": 6.2254825819087446e-06,
                            "process_1": 5.388480929953553e-06,
                            "process_2": 6.0850466032338225e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.007491042745604702,
                            "process_0": 0.006790577576545493,
                            "process_1": 0.005844341846709871,
                            "process_2": 0.006381373025041475
                        },
                        "total_energy_joules": {
                            "process_3": 26967.753884176927,
                            "process_0": 24446.079275563774,
                            "process_1": 21039.630648155537,
                            "process_2": 22972.94289014931
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 448.8011777468648,
                        "ram_power_avg": 0.6976168155670166,
                        "cpu_energy_total": 0.005327392401841281,
                        "gpu_energy_total": 0.021155008035104172,
                        "ram_energy_total": 2.4934756956091858e-05,
                        "total_energy_kwh": 0.02650733519390154,
                        "total_energy_joules": 95426.40669804555
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.17169251747939457,
                        "joules_per_token": 5.824365643191257,
                        "flops_per_joule": 177623485.779845,
                        "joules_per_flop": 5.629886135887726e-09
                    },
                    "per-process_emissions": [
                        0.0028537127339381112,
                        0.0025868705277850056,
                        0.0022264020265041254,
                        0.00243098405388955
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0429": {
            "setup": {
                "experiment_id": "0429",
                "date_time": "April 13, 2025 at 12:25:17 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 44.11961353200604,
                        "average_latency_ms_per_batch": 5514.951691500755,
                        "throughput_queries_per_sec": 2.901204016829022,
                        "throughput_tokens_per_sec": 371.3541141541148
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 35.4,
                        "cpu_memory_usage_bytes": 1929535488
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0429",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 382.1936557606932,
                            "process_0": 411.91010701269727,
                            "process_1": 433.18917370373134,
                            "process_2": 567.9117745103373
                        },
                        "ram_power": {
                            "process_3": 0.7076411247253418,
                            "process_0": 0.6735992431640625,
                            "process_1": 0.7044267654418945,
                            "process_2": 0.7048001289367676
                        },
                        "cpu_energy": {
                            "process_3": 0.001528186400936648,
                            "process_0": 0.001362592478781153,
                            "process_1": 0.0011644357372815645,
                            "process_2": 0.001272177784841915
                        },
                        "gpu_energy": {
                            "process_3": 0.0059556205978270604,
                            "process_0": 0.005421759615182431,
                            "process_1": 0.004674517628498354,
                            "process_2": 0.0051031101935963274
                        },
                        "ram_energy": {
                            "process_3": 7.2357468409957394e-06,
                            "process_0": 6.2254825819087446e-06,
                            "process_1": 5.388480929953553e-06,
                            "process_2": 6.0850466032338225e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.007491042745604702,
                            "process_0": 0.006790577576545493,
                            "process_1": 0.005844341846709871,
                            "process_2": 0.006381373025041475
                        },
                        "total_energy_joules": {
                            "process_3": 26967.753884176927,
                            "process_0": 24446.079275563774,
                            "process_1": 21039.630648155537,
                            "process_2": 22972.94289014931
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 448.8011777468648,
                        "ram_power_avg": 0.6976168155670166,
                        "cpu_energy_total": 0.005327392401841281,
                        "gpu_energy_total": 0.021155008035104172,
                        "ram_energy_total": 2.4934756956091858e-05,
                        "total_energy_kwh": 0.02650733519390154,
                        "total_energy_joules": 95426.40669804555
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.17169251747939457,
                        "joules_per_token": 5.824365643191257,
                        "flops_per_joule": 177623485.779845,
                        "joules_per_flop": 5.629886135887726e-09
                    },
                    "per-process_emissions": [
                        0.0028537127339381112,
                        0.0025868705277850056,
                        0.0022264020265041254,
                        0.00243098405388955
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0431": {
            "setup": {
                "experiment_id": "0431",
                "date_time": "April 13, 2025 at 12:27:34 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.21530380702461,
                        "average_latency_ms_per_batch": 5401.912975878076,
                        "throughput_queries_per_sec": 2.961913690843791,
                        "throughput_tokens_per_sec": 379.12495242800526
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 34.6,
                        "cpu_memory_usage_bytes": 1973121024
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0431",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 314.7415137734159,
                            "process_3": 305.09479084929615,
                            "process_1": 317.59688469345673,
                            "process_0": 442.7079148298535
                        },
                        "ram_power": {
                            "process_2": 0.7200965881347656,
                            "process_3": 0.7269430160522461,
                            "process_1": 0.7213253974914551,
                            "process_0": 0.6888213157653809
                        },
                        "cpu_energy": {
                            "process_2": 0.0013483234752793575,
                            "process_3": 0.0014140680095642891,
                            "process_1": 0.0012730171687207986,
                            "process_0": 0.0013327849298721051
                        },
                        "gpu_energy": {
                            "process_2": 0.00510234213742633,
                            "process_3": 0.0052820606145336235,
                            "process_1": 0.004806009400359468,
                            "process_0": 0.005066874886830108
                        },
                        "ram_energy": {
                            "process_2": 6.461699842695247e-06,
                            "process_3": 6.975154441356671e-06,
                            "process_1": 6.052149674021779e-06,
                            "process_0": 6.059740962227628e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.006457127312548384,
                            "process_3": 0.0067031037785392706,
                            "process_1": 0.0060850787187542894,
                            "process_0": 0.0064057195576644405
                        },
                        "total_energy_joules": {
                            "process_2": 23245.658325174183,
                            "process_3": 24131.173602741375,
                            "process_1": 21906.283387515443,
                            "process_0": 23060.590407591986
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 345.0352760365056,
                        "ram_power_avg": 0.7142965793609619,
                        "cpu_energy_total": 0.005368193583436549,
                        "gpu_energy_total": 0.02025728703914953,
                        "ram_energy_total": 2.5548744920301323e-05,
                        "total_energy_kwh": 0.025651029367506385,
                        "total_energy_joules": 92343.70572302298
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1774241121440632,
                        "joules_per_token": 5.636212507508727,
                        "flops_per_joule": 183553073.38425407,
                        "joules_per_flop": 5.448015560636121e-09
                    },
                    "per-process_emissions": [
                        0.002459842649715307,
                        0.002553547384434535,
                        0.0023181107379094466,
                        0.002440258865492269
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0431": {
            "setup": {
                "experiment_id": "0431",
                "date_time": "April 13, 2025 at 12:27:34 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 43.21530380702461,
                        "average_latency_ms_per_batch": 5401.912975878076,
                        "throughput_queries_per_sec": 2.961913690843791,
                        "throughput_tokens_per_sec": 379.12495242800526
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 34.6,
                        "cpu_memory_usage_bytes": 1973121024
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0431",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 314.7415137734159,
                            "process_3": 305.09479084929615,
                            "process_1": 317.59688469345673,
                            "process_0": 442.7079148298535
                        },
                        "ram_power": {
                            "process_2": 0.7200965881347656,
                            "process_3": 0.7269430160522461,
                            "process_1": 0.7213253974914551,
                            "process_0": 0.6888213157653809
                        },
                        "cpu_energy": {
                            "process_2": 0.0013483234752793575,
                            "process_3": 0.0014140680095642891,
                            "process_1": 0.0012730171687207986,
                            "process_0": 0.0013327849298721051
                        },
                        "gpu_energy": {
                            "process_2": 0.00510234213742633,
                            "process_3": 0.0052820606145336235,
                            "process_1": 0.004806009400359468,
                            "process_0": 0.005066874886830108
                        },
                        "ram_energy": {
                            "process_2": 6.461699842695247e-06,
                            "process_3": 6.975154441356671e-06,
                            "process_1": 6.052149674021779e-06,
                            "process_0": 6.059740962227628e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.006457127312548384,
                            "process_3": 0.0067031037785392706,
                            "process_1": 0.0060850787187542894,
                            "process_0": 0.0064057195576644405
                        },
                        "total_energy_joules": {
                            "process_2": 23245.658325174183,
                            "process_3": 24131.173602741375,
                            "process_1": 21906.283387515443,
                            "process_0": 23060.590407591986
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 345.0352760365056,
                        "ram_power_avg": 0.7142965793609619,
                        "cpu_energy_total": 0.005368193583436549,
                        "gpu_energy_total": 0.02025728703914953,
                        "ram_energy_total": 2.5548744920301323e-05,
                        "total_energy_kwh": 0.025651029367506385,
                        "total_energy_joules": 92343.70572302298
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1774241121440632,
                        "joules_per_token": 5.636212507508727,
                        "flops_per_joule": 183553073.38425407,
                        "joules_per_flop": 5.448015560636121e-09
                    },
                    "per-process_emissions": [
                        0.002459842649715307,
                        0.002553547384434535,
                        0.0023181107379094466,
                        0.002440258865492269
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0432": {
            "setup": {
                "experiment_id": "0432",
                "date_time": "April 13, 2025 at 12:29:07 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 40.87745979006286,
                        "average_latency_ms_per_batch": 5109.682473757857,
                        "throughput_queries_per_sec": 3.131310033876329,
                        "throughput_tokens_per_sec": 400.8076843361701
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 34.6,
                        "cpu_memory_usage_bytes": 1975341056
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0432",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 480.0625976003331,
                            "process_2": 410.37926270292513,
                            "process_1": 366.51462475488506,
                            "process_3": 297.2027124307129
                        },
                        "ram_power": {
                            "process_0": 0.6895966529846191,
                            "process_2": 0.7203011512756349,
                            "process_1": 0.7210307121276855,
                            "process_3": 0.7278599739074707
                        },
                        "cpu_energy": {
                            "process_0": 0.0012638616327167256,
                            "process_2": 0.0013469107535938748,
                            "process_1": 0.001364884477529813,
                            "process_3": 0.0014041714237473567
                        },
                        "gpu_energy": {
                            "process_0": 0.004841170817376783,
                            "process_2": 0.005161286073469329,
                            "process_1": 0.005215485283496024,
                            "process_3": 0.005331077042634913
                        },
                        "ram_energy": {
                            "process_0": 5.778008934683954e-06,
                            "process_2": 6.460910951172967e-06,
                            "process_1": 6.689411190075405e-06,
                            "process_3": 7.028738139140117e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.006110810459028193,
                            "process_2": 0.0065146577380143745,
                            "process_1": 0.0065870591722159135,
                            "process_3": 0.006742277204521407
                        },
                        "total_energy_joules": {
                            "process_0": 21998.917652501492,
                            "process_2": 23452.767856851748,
                            "process_1": 23713.413019977288,
                            "process_3": 24272.197936277065
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 388.539799372214,
                        "ram_power_avg": 0.7146971225738525,
                        "cpu_energy_total": 0.00537982828758777,
                        "gpu_energy_total": 0.02054901921697705,
                        "ram_energy_total": 2.5957069215072444e-05,
                        "total_energy_kwh": 0.02595480457377989,
                        "total_energy_joules": 93437.2964656076
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1753475391492157,
                        "joules_per_token": 5.702959989355932,
                        "flops_per_joule": 181404766.9860712,
                        "joules_per_flop": 5.512534298929327e-09
                    },
                    "per-process_emissions": [
                        0.00232791324436679,
                        0.002481758865296576,
                        0.0025093401916556524,
                        0.00256847050106243
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0432": {
            "setup": {
                "experiment_id": "0432",
                "date_time": "April 13, 2025 at 12:29:07 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_20_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 20,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 40.87745979006286,
                        "average_latency_ms_per_batch": 5109.682473757857,
                        "throughput_queries_per_sec": 3.131310033876329,
                        "throughput_tokens_per_sec": 400.8076843361701
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 34.6,
                        "cpu_memory_usage_bytes": 1975341056
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0432",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 480.0625976003331,
                            "process_2": 410.37926270292513,
                            "process_1": 366.51462475488506,
                            "process_3": 297.2027124307129
                        },
                        "ram_power": {
                            "process_0": 0.6895966529846191,
                            "process_2": 0.7203011512756349,
                            "process_1": 0.7210307121276855,
                            "process_3": 0.7278599739074707
                        },
                        "cpu_energy": {
                            "process_0": 0.0012638616327167256,
                            "process_2": 0.0013469107535938748,
                            "process_1": 0.001364884477529813,
                            "process_3": 0.0014041714237473567
                        },
                        "gpu_energy": {
                            "process_0": 0.004841170817376783,
                            "process_2": 0.005161286073469329,
                            "process_1": 0.005215485283496024,
                            "process_3": 0.005331077042634913
                        },
                        "ram_energy": {
                            "process_0": 5.778008934683954e-06,
                            "process_2": 6.460910951172967e-06,
                            "process_1": 6.689411190075405e-06,
                            "process_3": 7.028738139140117e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.006110810459028193,
                            "process_2": 0.0065146577380143745,
                            "process_1": 0.0065870591722159135,
                            "process_3": 0.006742277204521407
                        },
                        "total_energy_joules": {
                            "process_0": 21998.917652501492,
                            "process_2": 23452.767856851748,
                            "process_1": 23713.413019977288,
                            "process_3": 24272.197936277065
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 388.539799372214,
                        "ram_power_avg": 0.7146971225738525,
                        "cpu_energy_total": 0.00537982828758777,
                        "gpu_energy_total": 0.02054901921697705,
                        "ram_energy_total": 2.5957069215072444e-05,
                        "total_energy_kwh": 0.02595480457377989,
                        "total_energy_joules": 93437.2964656076
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1753475391492157,
                        "joules_per_token": 5.702959989355932,
                        "flops_per_joule": 181404766.9860712,
                        "joules_per_flop": 5.512534298929327e-09
                    },
                    "per-process_emissions": [
                        0.00232791324436679,
                        0.002481758865296576,
                        0.0025093401916556524,
                        0.00256847050106243
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0433": {
            "setup": {
                "experiment_id": "0433",
                "date_time": "April 13, 2025 at 12:30:39 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.59849355099141,
                        "average_latency_ms_per_batch": 4824.811693873926,
                        "throughput_queries_per_sec": 3.3161915977602265,
                        "throughput_tokens_per_sec": 424.472524513309
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 34.7,
                        "cpu_memory_usage_bytes": 1931759616
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0433",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 447.8414435837722,
                            "process_1": 488.8589741101385,
                            "process_0": 501.95757061636857,
                            "process_3": 474.8937172341794
                        },
                        "ram_power": {
                            "process_2": 0.708526611328125,
                            "process_1": 0.705021858215332,
                            "process_0": 0.6744704246520996,
                            "process_3": 0.7074995040893555
                        },
                        "cpu_energy": {
                            "process_2": 0.0012511596696886042,
                            "process_1": 0.0013315865207487146,
                            "process_0": 0.0011929352889064834,
                            "process_3": 0.0013995643337793811
                        },
                        "gpu_energy": {
                            "process_2": 0.005046646815091149,
                            "process_1": 0.005280067835162328,
                            "process_0": 0.0048103730149620105,
                            "process_3": 0.005522231640003206
                        },
                        "ram_energy": {
                            "process_2": 5.905695010962513e-06,
                            "process_1": 6.330910215764585e-06,
                            "process_0": 5.401618919379863e-06,
                            "process_3": 6.767469741085586e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.006303712179790716,
                            "process_1": 0.006617985266126808,
                            "process_0": 0.006008709922787873,
                            "process_3": 0.006928563443523673
                        },
                        "total_energy_joules": {
                            "process_2": 22693.363847246575,
                            "process_1": 23824.74695805651,
                            "process_0": 21631.355722036344,
                            "process_3": 24942.82839668522
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 478.38792638611466,
                        "ram_power_avg": 0.698879599571228,
                        "cpu_energy_total": 0.0051752458131231836,
                        "gpu_energy_total": 0.020659319305218693,
                        "ram_energy_total": 2.4405693887192544e-05,
                        "total_energy_kwh": 0.02585897081222907,
                        "total_energy_joules": 93092.29492402464
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1759973799482703,
                        "joules_per_token": 5.681902766358926,
                        "flops_per_joule": 182077055.968868,
                        "joules_per_flop": 5.492180190847234e-09
                    },
                    "per-process_emissions": [
                        0.0024013991548912732,
                        0.0025211214871310077,
                        0.0022890180450860402,
                        0.0026394362438103432
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0433": {
            "setup": {
                "experiment_id": "0433",
                "date_time": "April 13, 2025 at 12:30:39 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 38.59849355099141,
                        "average_latency_ms_per_batch": 4824.811693873926,
                        "throughput_queries_per_sec": 3.3161915977602265,
                        "throughput_tokens_per_sec": 424.472524513309
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 34.7,
                        "cpu_memory_usage_bytes": 1931759616
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0433",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 447.8414435837722,
                            "process_1": 488.8589741101385,
                            "process_0": 501.95757061636857,
                            "process_3": 474.8937172341794
                        },
                        "ram_power": {
                            "process_2": 0.708526611328125,
                            "process_1": 0.705021858215332,
                            "process_0": 0.6744704246520996,
                            "process_3": 0.7074995040893555
                        },
                        "cpu_energy": {
                            "process_2": 0.0012511596696886042,
                            "process_1": 0.0013315865207487146,
                            "process_0": 0.0011929352889064834,
                            "process_3": 0.0013995643337793811
                        },
                        "gpu_energy": {
                            "process_2": 0.005046646815091149,
                            "process_1": 0.005280067835162328,
                            "process_0": 0.0048103730149620105,
                            "process_3": 0.005522231640003206
                        },
                        "ram_energy": {
                            "process_2": 5.905695010962513e-06,
                            "process_1": 6.330910215764585e-06,
                            "process_0": 5.401618919379863e-06,
                            "process_3": 6.767469741085586e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.006303712179790716,
                            "process_1": 0.006617985266126808,
                            "process_0": 0.006008709922787873,
                            "process_3": 0.006928563443523673
                        },
                        "total_energy_joules": {
                            "process_2": 22693.363847246575,
                            "process_1": 23824.74695805651,
                            "process_0": 21631.355722036344,
                            "process_3": 24942.82839668522
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 478.38792638611466,
                        "ram_power_avg": 0.698879599571228,
                        "cpu_energy_total": 0.0051752458131231836,
                        "gpu_energy_total": 0.020659319305218693,
                        "ram_energy_total": 2.4405693887192544e-05,
                        "total_energy_kwh": 0.02585897081222907,
                        "total_energy_joules": 93092.29492402464
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1759973799482703,
                        "joules_per_token": 5.681902766358926,
                        "flops_per_joule": 182077055.968868,
                        "joules_per_flop": 5.492180190847234e-09
                    },
                    "per-process_emissions": [
                        0.0024013991548912732,
                        0.0025211214871310077,
                        0.0022890180450860402,
                        0.0026394362438103432
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0434": {
            "setup": {
                "experiment_id": "0434",
                "date_time": "April 13, 2025 at 12:33:59 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 14352
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 142.94063776300754,
                        "average_latency_ms_per_batch": 4466.894930093986,
                        "throughput_queries_per_sec": 0.8954766258439479,
                        "throughput_tokens_per_sec": 100.40531667275266
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609956352,
                        "gpu_max_memory_allocated_bytes": 1609956352,
                        "gpu_current_memory_reserved_bytes": 2166358016,
                        "gpu_max_memory_reserved_bytes": 2166358016
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 34.6,
                        "cpu_memory_usage_bytes": 1929310208
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0434",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 393.1422226318234,
                            "process_1": 334.3873909658116,
                            "process_0": 0.0,
                            "process_2": 460.2072169792021
                        },
                        "ram_power": {
                            "process_3": 0.7071175575256348,
                            "process_1": 0.7040891647338867,
                            "process_0": 0.6735205650329591,
                            "process_2": 0.7047600746154785
                        },
                        "cpu_energy": {
                            "process_3": 0.004699496051160168,
                            "process_1": 0.004258521989968359,
                            "process_0": 0.004430123276313681,
                            "process_2": 0.004232104677011197
                        },
                        "gpu_energy": {
                            "process_3": 0.015953368318239347,
                            "process_1": 0.014486168811148392,
                            "process_0": 0.01493580028196484,
                            "process_2": 0.014384653729936359
                        },
                        "ram_energy": {
                            "process_3": 2.279441298213508e-05,
                            "process_1": 2.005918528892067e-05,
                            "process_0": 1.981790556937953e-05,
                            "process_2": 1.9858288647497006e-05
                        },
                        "total_energy_kwh": {
                            "process_3": 0.02067565878238167,
                            "process_1": 0.018764749986405652,
                            "process_0": 0.019385741463847875,
                            "process_2": 0.018636616695595038
                        },
                        "total_energy_joules": {
                            "process_3": 74432.371616574,
                            "process_1": 67553.09995106034,
                            "process_0": 69788.66926985235,
                            "process_2": 67091.82010414214
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 296.93420764420927,
                        "ram_power_avg": 0.6973718404769897,
                        "cpu_energy_total": 0.017620245994453405,
                        "gpu_energy_total": 0.05975999114128894,
                        "ram_energy_total": 8.252979248793229e-05,
                        "total_energy_kwh": 0.07746276692823023,
                        "total_energy_joules": 278865.96094162884
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.05146558565820841,
                        "joules_per_token": 19.430459931830324,
                        "flops_per_joule": 60781785.39940163,
                        "joules_per_flop": 1.6452297237222068e-08
                    },
                    "per-process_emissions": [
                        0.007876392213148297,
                        0.007148431507321234,
                        0.007384998210652848,
                        0.00709961913018693
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0434": {
            "setup": {
                "experiment_id": "0434",
                "date_time": "April 13, 2025 at 12:33:59 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 14352
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 142.94063776300754,
                        "average_latency_ms_per_batch": 4466.894930093986,
                        "throughput_queries_per_sec": 0.8954766258439479,
                        "throughput_tokens_per_sec": 100.40531667275266
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609956352,
                        "gpu_max_memory_allocated_bytes": 1609956352,
                        "gpu_current_memory_reserved_bytes": 2166358016,
                        "gpu_max_memory_reserved_bytes": 2166358016
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 34.6,
                        "cpu_memory_usage_bytes": 1929310208
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0434",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 393.1422226318234,
                            "process_1": 334.3873909658116,
                            "process_0": 0.0,
                            "process_2": 460.2072169792021
                        },
                        "ram_power": {
                            "process_3": 0.7071175575256348,
                            "process_1": 0.7040891647338867,
                            "process_0": 0.6735205650329591,
                            "process_2": 0.7047600746154785
                        },
                        "cpu_energy": {
                            "process_3": 0.004699496051160168,
                            "process_1": 0.004258521989968359,
                            "process_0": 0.004430123276313681,
                            "process_2": 0.004232104677011197
                        },
                        "gpu_energy": {
                            "process_3": 0.015953368318239347,
                            "process_1": 0.014486168811148392,
                            "process_0": 0.01493580028196484,
                            "process_2": 0.014384653729936359
                        },
                        "ram_energy": {
                            "process_3": 2.279441298213508e-05,
                            "process_1": 2.005918528892067e-05,
                            "process_0": 1.981790556937953e-05,
                            "process_2": 1.9858288647497006e-05
                        },
                        "total_energy_kwh": {
                            "process_3": 0.02067565878238167,
                            "process_1": 0.018764749986405652,
                            "process_0": 0.019385741463847875,
                            "process_2": 0.018636616695595038
                        },
                        "total_energy_joules": {
                            "process_3": 74432.371616574,
                            "process_1": 67553.09995106034,
                            "process_0": 69788.66926985235,
                            "process_2": 67091.82010414214
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 296.93420764420927,
                        "ram_power_avg": 0.6973718404769897,
                        "cpu_energy_total": 0.017620245994453405,
                        "gpu_energy_total": 0.05975999114128894,
                        "ram_energy_total": 8.252979248793229e-05,
                        "total_energy_kwh": 0.07746276692823023,
                        "total_energy_joules": 278865.96094162884
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.05146558565820841,
                        "joules_per_token": 19.430459931830324,
                        "flops_per_joule": 60781785.39940163,
                        "joules_per_flop": 1.6452297237222068e-08
                    },
                    "per-process_emissions": [
                        0.007876392213148297,
                        0.007148431507321234,
                        0.007384998210652848,
                        0.00709961913018693
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0435": {
            "setup": {
                "experiment_id": "0435",
                "date_time": "April 13, 2025 at 12:35:33 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 40.36579676292604,
                        "average_latency_ms_per_batch": 5045.724595365755,
                        "throughput_queries_per_sec": 3.171001448373777,
                        "throughput_tokens_per_sec": 405.8881853918435
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 34.5,
                        "cpu_memory_usage_bytes": 1975861248
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0435",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 363.4198067202734,
                            "process_2": 394.7247829136976,
                            "process_3": 0.0,
                            "process_0": 567.9091529666117
                        },
                        "ram_power": {
                            "process_1": 0.7206587791442872,
                            "process_2": 0.7212553024291992,
                            "process_3": 0.7244067192077637,
                            "process_0": 0.6897783279418945
                        },
                        "cpu_energy": {
                            "process_1": 0.0013734422586567235,
                            "process_2": 0.0013113342243468655,
                            "process_3": 0.0015157210562829276,
                            "process_0": 0.0012474400130668072
                        },
                        "gpu_energy": {
                            "process_1": 0.005277781722222041,
                            "process_2": 0.005046317092604724,
                            "process_3": 0.005616971438017204,
                            "process_0": 0.0047903896656418254
                        },
                        "ram_energy": {
                            "process_1": 6.576351880387492e-06,
                            "process_2": 6.259487345240684e-06,
                            "process_3": 7.495866158862856e-06,
                            "process_0": 5.796935058150534e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0066578003327591485,
                            "process_2": 0.006363910804296831,
                            "process_3": 0.007140188360458995,
                            "process_0": 0.006043626613766784
                        },
                        "total_energy_joules": {
                            "process_1": 23968.081197932934,
                            "process_2": 22910.078895468592,
                            "process_3": 25704.67809765238,
                            "process_0": 21757.05580956042
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 331.51343565014565,
                        "ram_power_avg": 0.7140247821807861,
                        "cpu_energy_total": 0.005447937552353324,
                        "gpu_energy_total": 0.020731459918485795,
                        "ram_energy_total": 2.6128640442641565e-05,
                        "total_energy_kwh": 0.026205526111281758,
                        "total_energy_joules": 94339.89400061434
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1736699004547674,
                        "joules_per_token": 5.758050170935934,
                        "flops_per_joule": 179669175.72582415,
                        "joules_per_flop": 5.565784982094001e-09
                    },
                    "per-process_emissions": [
                        0.0025362890367645976,
                        0.002424331820896878,
                        0.0027200547559168542,
                        0.002302319558514456
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0435": {
            "setup": {
                "experiment_id": "0435",
                "date_time": "April 13, 2025 at 12:35:33 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 40.36579676292604,
                        "average_latency_ms_per_batch": 5045.724595365755,
                        "throughput_queries_per_sec": 3.171001448373777,
                        "throughput_tokens_per_sec": 405.8881853918435
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 34.5,
                        "cpu_memory_usage_bytes": 1975861248
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0435",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 363.4198067202734,
                            "process_2": 394.7247829136976,
                            "process_3": 0.0,
                            "process_0": 567.9091529666117
                        },
                        "ram_power": {
                            "process_1": 0.7206587791442872,
                            "process_2": 0.7212553024291992,
                            "process_3": 0.7244067192077637,
                            "process_0": 0.6897783279418945
                        },
                        "cpu_energy": {
                            "process_1": 0.0013734422586567235,
                            "process_2": 0.0013113342243468655,
                            "process_3": 0.0015157210562829276,
                            "process_0": 0.0012474400130668072
                        },
                        "gpu_energy": {
                            "process_1": 0.005277781722222041,
                            "process_2": 0.005046317092604724,
                            "process_3": 0.005616971438017204,
                            "process_0": 0.0047903896656418254
                        },
                        "ram_energy": {
                            "process_1": 6.576351880387492e-06,
                            "process_2": 6.259487345240684e-06,
                            "process_3": 7.495866158862856e-06,
                            "process_0": 5.796935058150534e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0066578003327591485,
                            "process_2": 0.006363910804296831,
                            "process_3": 0.007140188360458995,
                            "process_0": 0.006043626613766784
                        },
                        "total_energy_joules": {
                            "process_1": 23968.081197932934,
                            "process_2": 22910.078895468592,
                            "process_3": 25704.67809765238,
                            "process_0": 21757.05580956042
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 331.51343565014565,
                        "ram_power_avg": 0.7140247821807861,
                        "cpu_energy_total": 0.005447937552353324,
                        "gpu_energy_total": 0.020731459918485795,
                        "ram_energy_total": 2.6128640442641565e-05,
                        "total_energy_kwh": 0.026205526111281758,
                        "total_energy_joules": 94339.89400061434
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1736699004547674,
                        "joules_per_token": 5.758050170935934,
                        "flops_per_joule": 179669175.72582415,
                        "joules_per_flop": 5.565784982094001e-09
                    },
                    "per-process_emissions": [
                        0.0025362890367645976,
                        0.002424331820896878,
                        0.0027200547559168542,
                        0.002302319558514456
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0436": {
            "setup": {
                "experiment_id": "0436",
                "date_time": "April 13, 2025 at 12:37:03 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.29073256702395,
                        "average_latency_ms_per_batch": 5161.341570877994,
                        "throughput_queries_per_sec": 3.0999692192970376,
                        "throughput_tokens_per_sec": 396.7960600700208
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 33.9,
                        "cpu_memory_usage_bytes": 1933484032
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0436",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 444.8292475897042,
                            "process_1": 439.08813239043457,
                            "process_0": 456.79123343452596,
                            "process_3": 304.6330816779753
                        },
                        "ram_power": {
                            "process_2": 0.7063779830932617,
                            "process_1": 0.7055168151855469,
                            "process_0": 0.6749782562255859,
                            "process_3": 0.7101302146911621
                        },
                        "cpu_energy": {
                            "process_2": 0.0012167165577229753,
                            "process_1": 0.001345554472374715,
                            "process_0": 0.0012740189444684802,
                            "process_3": 0.0014016072146887388
                        },
                        "gpu_energy": {
                            "process_2": 0.0047780046557339695,
                            "process_1": 0.005272506718001946,
                            "process_0": 0.005006849283253789,
                            "process_3": 0.005426584063485285
                        },
                        "ram_energy": {
                            "process_2": 5.775254592845442e-06,
                            "process_1": 6.338625016601088e-06,
                            "process_0": 5.74073287859971e-06,
                            "process_3": 6.866843947430188e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.006000496468049793,
                            "process_1": 0.006624399815393261,
                            "process_0": 0.006286608960600866,
                            "process_3": 0.006835058122121449
                        },
                        "total_energy_joules": {
                            "process_2": 21601.787284979255,
                            "process_1": 23847.83933541574,
                            "process_0": 22631.792258163117,
                            "process_3": 24606.209239637217
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 411.33542377316,
                        "ram_power_avg": 0.6992508172988892,
                        "cpu_energy_total": 0.0052378971892549095,
                        "gpu_energy_total": 0.02048394472047499,
                        "ram_energy_total": 2.4721456435476426e-05,
                        "total_energy_kwh": 0.02574656336616537,
                        "total_energy_joules": 92687.62811819532
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.17676577049859463,
                        "joules_per_token": 5.657203864635945,
                        "flops_per_joule": 182871989.9007167,
                        "joules_per_flop": 5.468306002154357e-09
                    },
                    "per-process_emissions": [
                        0.0022858891295035686,
                        0.002523565109674063,
                        0.0023948836835409,
                        0.002603815391622166
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0436": {
            "setup": {
                "experiment_id": "0436",
                "date_time": "April 13, 2025 at 12:37:03 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.29073256702395,
                        "average_latency_ms_per_batch": 5161.341570877994,
                        "throughput_queries_per_sec": 3.0999692192970376,
                        "throughput_tokens_per_sec": 396.7960600700208
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 33.9,
                        "cpu_memory_usage_bytes": 1933484032
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0436",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 444.8292475897042,
                            "process_1": 439.08813239043457,
                            "process_0": 456.79123343452596,
                            "process_3": 304.6330816779753
                        },
                        "ram_power": {
                            "process_2": 0.7063779830932617,
                            "process_1": 0.7055168151855469,
                            "process_0": 0.6749782562255859,
                            "process_3": 0.7101302146911621
                        },
                        "cpu_energy": {
                            "process_2": 0.0012167165577229753,
                            "process_1": 0.001345554472374715,
                            "process_0": 0.0012740189444684802,
                            "process_3": 0.0014016072146887388
                        },
                        "gpu_energy": {
                            "process_2": 0.0047780046557339695,
                            "process_1": 0.005272506718001946,
                            "process_0": 0.005006849283253789,
                            "process_3": 0.005426584063485285
                        },
                        "ram_energy": {
                            "process_2": 5.775254592845442e-06,
                            "process_1": 6.338625016601088e-06,
                            "process_0": 5.74073287859971e-06,
                            "process_3": 6.866843947430188e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.006000496468049793,
                            "process_1": 0.006624399815393261,
                            "process_0": 0.006286608960600866,
                            "process_3": 0.006835058122121449
                        },
                        "total_energy_joules": {
                            "process_2": 21601.787284979255,
                            "process_1": 23847.83933541574,
                            "process_0": 22631.792258163117,
                            "process_3": 24606.209239637217
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 411.33542377316,
                        "ram_power_avg": 0.6992508172988892,
                        "cpu_energy_total": 0.0052378971892549095,
                        "gpu_energy_total": 0.02048394472047499,
                        "ram_energy_total": 2.4721456435476426e-05,
                        "total_energy_kwh": 0.02574656336616537,
                        "total_energy_joules": 92687.62811819532
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.17676577049859463,
                        "joules_per_token": 5.657203864635945,
                        "flops_per_joule": 182871989.9007167,
                        "joules_per_flop": 5.468306002154357e-09
                    },
                    "per-process_emissions": [
                        0.0022858891295035686,
                        0.002523565109674063,
                        0.0023948836835409,
                        0.002603815391622166
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0437": {
            "setup": {
                "experiment_id": "0437",
                "date_time": "April 13, 2025 at 12:38:35 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.525775562011404,
                        "average_latency_ms_per_batch": 5190.7219452514255,
                        "throughput_queries_per_sec": 3.0824228630926984,
                        "throughput_tokens_per_sec": 394.5501264758654
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 33.8,
                        "cpu_memory_usage_bytes": 1975414784
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0437",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 422.3977651817785,
                            "process_3": 291.40922777140884,
                            "process_2": 415.4131566460245,
                            "process_1": 448.88970074703695
                        },
                        "ram_power": {
                            "process_0": 0.6897168159484863,
                            "process_3": 0.7193055152893066,
                            "process_2": 0.7278428077697754,
                            "process_1": 0.7191910743713379
                        },
                        "cpu_energy": {
                            "process_0": 0.0012792739178121338,
                            "process_3": 0.0014153888727169034,
                            "process_2": 0.0012914847868760262,
                            "process_1": 0.0012769215429334507
                        },
                        "gpu_energy": {
                            "process_0": 0.005006103449323174,
                            "process_3": 0.005431501289641316,
                            "process_2": 0.005043456812540592,
                            "process_1": 0.004984117876178473
                        },
                        "ram_energy": {
                            "process_0": 5.989109864227199e-06,
                            "process_3": 7.044586725028223e-06,
                            "process_2": 6.345292764936296e-06,
                            "process_1": 6.230053740597546e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.006291366476999532,
                            "process_3": 0.006853934749083249,
                            "process_2": 0.006341286892181552,
                            "process_1": 0.006267269472852523
                        },
                        "total_energy_joules": {
                            "process_0": 22648.919317198317,
                            "process_3": 24674.165096699697,
                            "process_2": 22828.63281185359,
                            "process_1": 22562.170102269083
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 394.52746258656214,
                        "ram_power_avg": 0.7140140533447266,
                        "cpu_energy_total": 0.005263069120338514,
                        "gpu_energy_total": 0.020465179427683555,
                        "ram_energy_total": 2.5609043094789264e-05,
                        "total_energy_kwh": 0.02575385759111686,
                        "total_energy_joules": 92713.88732802069
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1767157054048828,
                        "joules_per_token": 5.658806599610638,
                        "flops_per_joule": 182820195.35199934,
                        "joules_per_flop": 5.469855220724464e-09
                    },
                    "per-process_emissions": [
                        0.0023966960594129717,
                        0.002611006442663264,
                        0.0024157132415765625,
                        0.002387516305683169
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0437": {
            "setup": {
                "experiment_id": "0437",
                "date_time": "April 13, 2025 at 12:38:35 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 41.525775562011404,
                        "average_latency_ms_per_batch": 5190.7219452514255,
                        "throughput_queries_per_sec": 3.0824228630926984,
                        "throughput_tokens_per_sec": 394.5501264758654
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 33.8,
                        "cpu_memory_usage_bytes": 1975414784
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0437",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 422.3977651817785,
                            "process_3": 291.40922777140884,
                            "process_2": 415.4131566460245,
                            "process_1": 448.88970074703695
                        },
                        "ram_power": {
                            "process_0": 0.6897168159484863,
                            "process_3": 0.7193055152893066,
                            "process_2": 0.7278428077697754,
                            "process_1": 0.7191910743713379
                        },
                        "cpu_energy": {
                            "process_0": 0.0012792739178121338,
                            "process_3": 0.0014153888727169034,
                            "process_2": 0.0012914847868760262,
                            "process_1": 0.0012769215429334507
                        },
                        "gpu_energy": {
                            "process_0": 0.005006103449323174,
                            "process_3": 0.005431501289641316,
                            "process_2": 0.005043456812540592,
                            "process_1": 0.004984117876178473
                        },
                        "ram_energy": {
                            "process_0": 5.989109864227199e-06,
                            "process_3": 7.044586725028223e-06,
                            "process_2": 6.345292764936296e-06,
                            "process_1": 6.230053740597546e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.006291366476999532,
                            "process_3": 0.006853934749083249,
                            "process_2": 0.006341286892181552,
                            "process_1": 0.006267269472852523
                        },
                        "total_energy_joules": {
                            "process_0": 22648.919317198317,
                            "process_3": 24674.165096699697,
                            "process_2": 22828.63281185359,
                            "process_1": 22562.170102269083
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 394.52746258656214,
                        "ram_power_avg": 0.7140140533447266,
                        "cpu_energy_total": 0.005263069120338514,
                        "gpu_energy_total": 0.020465179427683555,
                        "ram_energy_total": 2.5609043094789264e-05,
                        "total_energy_kwh": 0.02575385759111686,
                        "total_energy_joules": 92713.88732802069
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.1767157054048828,
                        "joules_per_token": 5.658806599610638,
                        "flops_per_joule": 182820195.35199934,
                        "joules_per_flop": 5.469855220724464e-09
                    },
                    "per-process_emissions": [
                        0.0023966960594129717,
                        0.002611006442663264,
                        0.0024157132415765625,
                        0.002387516305683169
                    ]
                },
                "local_energy_results": {}
            }
        }
    }
]