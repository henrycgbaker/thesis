[
    {
        "CONFIGURATION_RUN_#0049": {
            "setup": {
                "experiment_id": "0049",
                "date_time": "April 11, 2025 at 12:57:13 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R6_Medium_Scale_Language_Model_Serving",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.1,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 13.31368777800003,
                        "average_latency_ms_per_batch": 3328.4219445000076,
                        "throughput_queries_per_sec": 9.614165671776632,
                        "throughput_tokens_per_sec": 1230.613205987409
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419853312,
                        "gpu_max_memory_allocated_bytes": 4419853312,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            16.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.9,
                        "cpu_memory_usage_bytes": 3101548544
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0049",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 562.1259309393143,
                            "process_1": 882.1011576819886,
                            "process_0": 621.7032351200693,
                            "process_3": 555.0770556727754
                        },
                        "ram_power": {
                            "process_2": 0.8817844390869141,
                            "process_1": 0.8730368614196777,
                            "process_0": 1.0818772315979004,
                            "process_3": 0.8617730140686035
                        },
                        "cpu_energy": {
                            "process_2": 0.0004090357763437424,
                            "process_1": 0.000410175754843749,
                            "process_0": 0.00041266911162500015,
                            "process_3": 0.00041331551903124364
                        },
                        "gpu_energy": {
                            "process_2": 0.0021869356384360064,
                            "process_1": 0.002199253981623999,
                            "process_0": 0.0022183292746620065,
                            "process_3": 0.0022123423254280065
                        },
                        "ram_energy": {
                            "process_2": 2.7632262027687574e-06,
                            "process_1": 2.754277704089616e-06,
                            "process_0": 3.4501062010190474e-06,
                            "process_3": 2.7355864920094968e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0025987346409825176,
                            "process_1": 0.0026121840141718378,
                            "process_0": 0.0026344484924880254,
                            "process_3": 0.0026283934309512596
                        },
                        "total_energy_joules": {
                            "process_2": 9355.444707537063,
                            "process_1": 9403.862451018616,
                            "process_0": 9484.014572956892,
                            "process_3": 9462.216351424535
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 655.2518448535369,
                        "ram_power_avg": 0.9246178865432739,
                        "cpu_energy_total": 0.001645196161843735,
                        "gpu_energy_total": 0.008816861220150018,
                        "ram_energy_total": 1.1703196599886918e-05,
                        "total_energy_kwh": 0.01047376057859364,
                        "total_energy_joules": 37705.538082937106
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.43452502823223876,
                        "joules_per_token": 2.3013634083823917,
                        "flops_per_joule": 449535316.42669684,
                        "joules_per_flop": 2.2245193279782376e-09
                    },
                    "per-process_emissions": [
                        0.0009899879614822902,
                        0.0009951115001987617,
                        0.0010035931532133132,
                        0.0010012864775208823
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0049": {
            "setup": {
                "experiment_id": "0049",
                "date_time": "April 11, 2025 at 12:57:13 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R6_Medium_Scale_Language_Model_Serving",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.1,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 13.31368777800003,
                        "average_latency_ms_per_batch": 3328.4219445000076,
                        "throughput_queries_per_sec": 9.614165671776632,
                        "throughput_tokens_per_sec": 1230.613205987409
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419853312,
                        "gpu_max_memory_allocated_bytes": 4419853312,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            16.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.9,
                        "cpu_memory_usage_bytes": 3101548544
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0049",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 562.1259309393143,
                            "process_1": 882.1011576819886,
                            "process_0": 621.7032351200693,
                            "process_3": 555.0770556727754
                        },
                        "ram_power": {
                            "process_2": 0.8817844390869141,
                            "process_1": 0.8730368614196777,
                            "process_0": 1.0818772315979004,
                            "process_3": 0.8617730140686035
                        },
                        "cpu_energy": {
                            "process_2": 0.0004090357763437424,
                            "process_1": 0.000410175754843749,
                            "process_0": 0.00041266911162500015,
                            "process_3": 0.00041331551903124364
                        },
                        "gpu_energy": {
                            "process_2": 0.0021869356384360064,
                            "process_1": 0.002199253981623999,
                            "process_0": 0.0022183292746620065,
                            "process_3": 0.0022123423254280065
                        },
                        "ram_energy": {
                            "process_2": 2.7632262027687574e-06,
                            "process_1": 2.754277704089616e-06,
                            "process_0": 3.4501062010190474e-06,
                            "process_3": 2.7355864920094968e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0025987346409825176,
                            "process_1": 0.0026121840141718378,
                            "process_0": 0.0026344484924880254,
                            "process_3": 0.0026283934309512596
                        },
                        "total_energy_joules": {
                            "process_2": 9355.444707537063,
                            "process_1": 9403.862451018616,
                            "process_0": 9484.014572956892,
                            "process_3": 9462.216351424535
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 655.2518448535369,
                        "ram_power_avg": 0.9246178865432739,
                        "cpu_energy_total": 0.001645196161843735,
                        "gpu_energy_total": 0.008816861220150018,
                        "ram_energy_total": 1.1703196599886918e-05,
                        "total_energy_kwh": 0.01047376057859364,
                        "total_energy_joules": 37705.538082937106
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.43452502823223876,
                        "joules_per_token": 2.3013634083823917,
                        "flops_per_joule": 449535316.42669684,
                        "joules_per_flop": 2.2245193279782376e-09
                    },
                    "per-process_emissions": [
                        0.0009899879614822902,
                        0.0009951115001987617,
                        0.0010035931532133132,
                        0.0010012864775208823
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0050": {
            "setup": {
                "experiment_id": "0050",
                "date_time": "April 11, 2025 at 12:57:50 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A5_Parallel_Overdrive",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 7.086054954000019,
                        "average_latency_ms_per_batch": 3543.0274770000096,
                        "throughput_queries_per_sec": 18.063647661629417,
                        "throughput_tokens_per_sec": 2312.1469006885654
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419853312,
                        "gpu_max_memory_allocated_bytes": 4419853312,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            18.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 3099123712
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0050",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 305.97959510892525,
                            "process_3": 0.0,
                            "process_2": 806.0764331650836,
                            "process_1": 806.553364267072
                        },
                        "ram_power": {
                            "process_0": 1.0806126594543457,
                            "process_3": 0.8699040412902833,
                            "process_2": 0.8503775596618652,
                            "process_1": 0.8721199035644531
                        },
                        "cpu_energy": {
                            "process_0": 0.00022243310946874572,
                            "process_3": 0.00025411739096875866,
                            "process_2": 0.00021831649506249564,
                            "process_1": 0.000215408917281259
                        },
                        "gpu_energy": {
                            "process_0": 0.001436343649073999,
                            "process_3": 0.001438450039648001,
                            "process_2": 0.001420120302762,
                            "process_1": 0.0014025727887240005
                        },
                        "ram_energy": {
                            "process_0": 1.8616714426552095e-06,
                            "process_3": 1.6799436705000766e-06,
                            "process_2": 1.4446461710463208e-06,
                            "process_1": 1.4605765755373294e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0016606384299854,
                            "process_3": 0.00169424737428726,
                            "process_2": 0.0016398814439955417,
                            "process_1": 0.0016194422825807967
                        },
                        "total_energy_joules": {
                            "process_0": 5978.29834794744,
                            "process_3": 6099.290547434136,
                            "process_2": 5903.57319838395,
                            "process_1": 5829.992217290868
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 479.65234813527024,
                        "ram_power_avg": 0.9182535409927368,
                        "cpu_energy_total": 0.0009102759127812591,
                        "gpu_energy_total": 0.0056974867802080005,
                        "ram_energy_total": 6.446837859738936e-06,
                        "total_energy_kwh": 0.006614209530848999,
                        "total_energy_joules": 23811.154311056394
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.688080879489001,
                        "joules_per_token": 1.4533175238681881,
                        "flops_per_joule": 711850033.4644215,
                        "joules_per_flop": 1.4047902690026076e-09
                    },
                    "per-process_emissions": [
                        0.0006326202099029382,
                        0.0006454235372347316,
                        0.0006247128360901016,
                        0.0006169265375491546
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0050": {
            "setup": {
                "experiment_id": "0050",
                "date_time": "April 11, 2025 at 12:57:50 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A5_Parallel_Overdrive",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 7.086054954000019,
                        "average_latency_ms_per_batch": 3543.0274770000096,
                        "throughput_queries_per_sec": 18.063647661629417,
                        "throughput_tokens_per_sec": 2312.1469006885654
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419853312,
                        "gpu_max_memory_allocated_bytes": 4419853312,
                        "gpu_current_memory_reserved_bytes": 6870269952,
                        "gpu_max_memory_reserved_bytes": 6870269952
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            18.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 3099123712
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0050",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 305.97959510892525,
                            "process_3": 0.0,
                            "process_2": 806.0764331650836,
                            "process_1": 806.553364267072
                        },
                        "ram_power": {
                            "process_0": 1.0806126594543457,
                            "process_3": 0.8699040412902833,
                            "process_2": 0.8503775596618652,
                            "process_1": 0.8721199035644531
                        },
                        "cpu_energy": {
                            "process_0": 0.00022243310946874572,
                            "process_3": 0.00025411739096875866,
                            "process_2": 0.00021831649506249564,
                            "process_1": 0.000215408917281259
                        },
                        "gpu_energy": {
                            "process_0": 0.001436343649073999,
                            "process_3": 0.001438450039648001,
                            "process_2": 0.001420120302762,
                            "process_1": 0.0014025727887240005
                        },
                        "ram_energy": {
                            "process_0": 1.8616714426552095e-06,
                            "process_3": 1.6799436705000766e-06,
                            "process_2": 1.4446461710463208e-06,
                            "process_1": 1.4605765755373294e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0016606384299854,
                            "process_3": 0.00169424737428726,
                            "process_2": 0.0016398814439955417,
                            "process_1": 0.0016194422825807967
                        },
                        "total_energy_joules": {
                            "process_0": 5978.29834794744,
                            "process_3": 6099.290547434136,
                            "process_2": 5903.57319838395,
                            "process_1": 5829.992217290868
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 479.65234813527024,
                        "ram_power_avg": 0.9182535409927368,
                        "cpu_energy_total": 0.0009102759127812591,
                        "gpu_energy_total": 0.0056974867802080005,
                        "ram_energy_total": 6.446837859738936e-06,
                        "total_energy_kwh": 0.006614209530848999,
                        "total_energy_joules": 23811.154311056394
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.688080879489001,
                        "joules_per_token": 1.4533175238681881,
                        "flops_per_joule": 711850033.4644215,
                        "joules_per_flop": 1.4047902690026076e-09
                    },
                    "per-process_emissions": [
                        0.0006326202099029382,
                        0.0006454235372347316,
                        0.0006247128360901016,
                        0.0006169265375491546
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0051": {
            "setup": {
                "experiment_id": "0051",
                "date_time": "April 11, 2025 at 12:59:14 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R4_High_Load_Cloud_API_Deployment",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 58.84743404400001,
                        "average_latency_ms_per_batch": 3677.9646277500005,
                        "throughput_queries_per_sec": 2.17511607905104,
                        "throughput_tokens_per_sec": 278.4148581185331
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419853312,
                        "gpu_max_memory_allocated_bytes": 4419853312,
                        "gpu_current_memory_reserved_bytes": 6872367104,
                        "gpu_max_memory_reserved_bytes": 6872367104
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            24.0,
                            25.0,
                            19.0,
                            24.0
                        ],
                        "cpu_usage_percent": 1.0,
                        "cpu_memory_usage_bytes": 3100430336
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0051",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 118.54351054760643
                        },
                        "ram_power": {
                            "process_0": 1.0811519622802734
                        },
                        "cpu_energy": {
                            "process_0": 0.0018045119103125493
                        },
                        "gpu_energy": {
                            "process_0": 0.002001616601292
                        },
                        "ram_energy": {
                            "process_0": 1.5268824470745704e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.003821397336075295
                        },
                        "total_energy_joules": {
                            "process_0": 13757.030409871062
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 118.54351054760643,
                        "ram_power_avg": 1.0811519622802734,
                        "cpu_energy_total": 0.0018045119103125493,
                        "gpu_energy_total": 0.002001616601292,
                        "ram_energy_total": 1.5268824470745704e-05,
                        "total_energy_kwh": 0.003821397336075295,
                        "total_energy_joules": 13757.030409871062
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.1909546982060906,
                        "joules_per_token": 0.8396625006024817,
                        "flops_per_joule": 1232095189.743123,
                        "joules_per_flop": 8.116256019216242e-10
                    },
                    "per-process_emissions": [
                        0.0014557613151778837
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0051": {
            "setup": {
                "experiment_id": "0051",
                "date_time": "April 11, 2025 at 12:59:14 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R4_High_Load_Cloud_API_Deployment",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 58.84743404400001,
                        "average_latency_ms_per_batch": 3677.9646277500005,
                        "throughput_queries_per_sec": 2.17511607905104,
                        "throughput_tokens_per_sec": 278.4148581185331
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4419853312,
                        "gpu_max_memory_allocated_bytes": 4419853312,
                        "gpu_current_memory_reserved_bytes": 6872367104,
                        "gpu_max_memory_reserved_bytes": 6872367104
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            24.0,
                            25.0,
                            19.0,
                            24.0
                        ],
                        "cpu_usage_percent": 1.0,
                        "cpu_memory_usage_bytes": 3100430336
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0051",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 118.54351054760643
                        },
                        "ram_power": {
                            "process_0": 1.0811519622802734
                        },
                        "cpu_energy": {
                            "process_0": 0.0018045119103125493
                        },
                        "gpu_energy": {
                            "process_0": 0.002001616601292
                        },
                        "ram_energy": {
                            "process_0": 1.5268824470745704e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.003821397336075295
                        },
                        "total_energy_joules": {
                            "process_0": 13757.030409871062
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 118.54351054760643,
                        "ram_power_avg": 1.0811519622802734,
                        "cpu_energy_total": 0.0018045119103125493,
                        "gpu_energy_total": 0.002001616601292,
                        "ram_energy_total": 1.5268824470745704e-05,
                        "total_energy_kwh": 0.003821397336075295,
                        "total_energy_joules": 13757.030409871062
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.1909546982060906,
                        "joules_per_token": 0.8396625006024817,
                        "flops_per_joule": 1232095189.743123,
                        "joules_per_flop": 8.116256019216242e-10
                    },
                    "per-process_emissions": [
                        0.0014557613151778837
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0055": {
            "setup": {
                "experiment_id": "0055",
                "date_time": "April 11, 2025 at 01:01:36 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R3_Balanced_Enterprise_Service",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 52.4309351500001,
                        "average_latency_ms_per_batch": 13107.733787500025,
                        "throughput_queries_per_sec": 2.441306828379157,
                        "throughput_tokens_per_sec": 312.48727403253207
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576947200,
                        "gpu_max_memory_allocated_bytes": 1576947200,
                        "gpu_current_memory_reserved_bytes": 2839543808,
                        "gpu_max_memory_reserved_bytes": 2839543808
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            18.0,
                            100.0,
                            100.0,
                            99.0
                        ],
                        "cpu_usage_percent": 3.9,
                        "cpu_memory_usage_bytes": 2711846912
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0055",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 415.3240362638392,
                            "process_2": 461.2482956208291,
                            "process_0": 407.13701635784463,
                            "process_1": 695.1893053663698
                        },
                        "ram_power": {
                            "process_3": 0.993292808532715,
                            "process_2": 0.9929351806640626,
                            "process_0": 0.9459700584411621,
                            "process_1": 0.9926776885986327
                        },
                        "cpu_energy": {
                            "process_3": 0.0015909703404687434,
                            "process_2": 0.0015788326969374896,
                            "process_0": 0.0016154946746249764,
                            "process_1": 0.0015474516619999787
                        },
                        "gpu_energy": {
                            "process_3": 0.007122172364400003,
                            "process_2": 0.007075920382953998,
                            "process_0": 0.007203044095764,
                            "process_1": 0.006942573609610005
                        },
                        "ram_energy": {
                            "process_3": 1.226055091654487e-05,
                            "process_2": 1.2073405058501136e-05,
                            "process_0": 1.1860242254180166e-05,
                            "process_1": 1.1809976878435586e-05
                        },
                        "total_energy_kwh": {
                            "process_3": 0.008725403255785292,
                            "process_2": 0.008666826484949985,
                            "process_0": 0.008830399012643153,
                            "process_1": 0.008501835248488418
                        },
                        "total_energy_joules": {
                            "process_3": 31411.45172082705,
                            "process_2": 31200.575345819947,
                            "process_0": 31789.43644551535,
                            "process_1": 30606.606894558307
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 494.7246634022207,
                        "ram_power_avg": 0.9812189340591431,
                        "cpu_energy_total": 0.006332749374031188,
                        "gpu_energy_total": 0.028343710452728006,
                        "ram_energy_total": 4.800417510766176e-05,
                        "total_energy_kwh": 0.03472446400186685,
                        "total_energy_joules": 125008.07040672067
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.13106353811152952,
                        "joules_per_token": 7.629887109785197,
                        "flops_per_joule": 8275818.710216496,
                        "joules_per_flop": 1.2083396640449605e-07
                    },
                    "per-process_emissions": [
                        0.003323942370291407,
                        0.003301627549441697,
                        0.0033639405038664093,
                        0.0032387741379116632
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0055": {
            "setup": {
                "experiment_id": "0055",
                "date_time": "April 11, 2025 at 01:01:36 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R3_Balanced_Enterprise_Service",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 52.4309351500001,
                        "average_latency_ms_per_batch": 13107.733787500025,
                        "throughput_queries_per_sec": 2.441306828379157,
                        "throughput_tokens_per_sec": 312.48727403253207
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576947200,
                        "gpu_max_memory_allocated_bytes": 1576947200,
                        "gpu_current_memory_reserved_bytes": 2839543808,
                        "gpu_max_memory_reserved_bytes": 2839543808
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            18.0,
                            100.0,
                            100.0,
                            99.0
                        ],
                        "cpu_usage_percent": 3.9,
                        "cpu_memory_usage_bytes": 2711846912
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0055",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 415.3240362638392,
                            "process_2": 461.2482956208291,
                            "process_0": 407.13701635784463,
                            "process_1": 695.1893053663698
                        },
                        "ram_power": {
                            "process_3": 0.993292808532715,
                            "process_2": 0.9929351806640626,
                            "process_0": 0.9459700584411621,
                            "process_1": 0.9926776885986327
                        },
                        "cpu_energy": {
                            "process_3": 0.0015909703404687434,
                            "process_2": 0.0015788326969374896,
                            "process_0": 0.0016154946746249764,
                            "process_1": 0.0015474516619999787
                        },
                        "gpu_energy": {
                            "process_3": 0.007122172364400003,
                            "process_2": 0.007075920382953998,
                            "process_0": 0.007203044095764,
                            "process_1": 0.006942573609610005
                        },
                        "ram_energy": {
                            "process_3": 1.226055091654487e-05,
                            "process_2": 1.2073405058501136e-05,
                            "process_0": 1.1860242254180166e-05,
                            "process_1": 1.1809976878435586e-05
                        },
                        "total_energy_kwh": {
                            "process_3": 0.008725403255785292,
                            "process_2": 0.008666826484949985,
                            "process_0": 0.008830399012643153,
                            "process_1": 0.008501835248488418
                        },
                        "total_energy_joules": {
                            "process_3": 31411.45172082705,
                            "process_2": 31200.575345819947,
                            "process_0": 31789.43644551535,
                            "process_1": 30606.606894558307
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 494.7246634022207,
                        "ram_power_avg": 0.9812189340591431,
                        "cpu_energy_total": 0.006332749374031188,
                        "gpu_energy_total": 0.028343710452728006,
                        "ram_energy_total": 4.800417510766176e-05,
                        "total_energy_kwh": 0.03472446400186685,
                        "total_energy_joules": 125008.07040672067
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.13106353811152952,
                        "joules_per_token": 7.629887109785197,
                        "flops_per_joule": 8275818.710216496,
                        "joules_per_flop": 1.2083396640449605e-07
                    },
                    "per-process_emissions": [
                        0.003323942370291407,
                        0.003301627549441697,
                        0.0033639405038664093,
                        0.0032387741379116632
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0056": {
            "setup": {
                "experiment_id": "0056",
                "date_time": "April 11, 2025 at 01:21:30 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R5_Real_Time_Mobile_Inference",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.6,
                    "simulate_burst": true,
                    "burst_interval": 5.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 14419
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 1168.121330729001,
                        "average_latency_ms_per_batch": 9125.947896320322,
                        "throughput_queries_per_sec": 0.10957765827297902,
                        "throughput_tokens_per_sec": 12.343751989360037
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576157696,
                        "gpu_max_memory_allocated_bytes": 1576157696,
                        "gpu_current_memory_reserved_bytes": 2841640960,
                        "gpu_max_memory_reserved_bytes": 2841640960
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            17.0,
                            27.0,
                            25.0,
                            27.0
                        ],
                        "cpu_usage_percent": 0.9,
                        "cpu_memory_usage_bytes": 2649198592
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0056",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 118.46996986273552
                        },
                        "ram_power": {
                            "process_0": 0.9248213768005371
                        },
                        "cpu_energy": {
                            "process_0": 0.035696834485156495
                        },
                        "gpu_energy": {
                            "process_0": 0.03487604206747799
                        },
                        "ram_energy": {
                            "process_0": 0.0002553537496743837
                        },
                        "total_energy_kwh": {
                            "process_0": 0.07082823030230896
                        },
                        "total_energy_joules": {
                            "process_0": 254981.62908831227
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 118.46996986273552,
                        "ram_power_avg": 0.9248213768005371,
                        "cpu_energy_total": 0.035696834485156495,
                        "gpu_energy_total": 0.03487604206747799,
                        "ram_energy_total": 0.0002553537496743837,
                        "total_energy_kwh": 0.07082823030230896,
                        "total_energy_joules": 254981.62908831227
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.05654917199939143,
                        "joules_per_token": 17.683724883023253,
                        "flops_per_joule": 4057328.0973182903,
                        "joules_per_flop": 2.464676200726667e-07
                    },
                    "per-process_emissions": [
                        0.026982014333664598
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0056": {
            "setup": {
                "experiment_id": "0056",
                "date_time": "April 11, 2025 at 01:21:30 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R5_Real_Time_Mobile_Inference",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.6,
                    "simulate_burst": true,
                    "burst_interval": 5.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 14419
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 1168.121330729001,
                        "average_latency_ms_per_batch": 9125.947896320322,
                        "throughput_queries_per_sec": 0.10957765827297902,
                        "throughput_tokens_per_sec": 12.343751989360037
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576157696,
                        "gpu_max_memory_allocated_bytes": 1576157696,
                        "gpu_current_memory_reserved_bytes": 2841640960,
                        "gpu_max_memory_reserved_bytes": 2841640960
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            17.0,
                            27.0,
                            25.0,
                            27.0
                        ],
                        "cpu_usage_percent": 0.9,
                        "cpu_memory_usage_bytes": 2649198592
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0056",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 118.46996986273552
                        },
                        "ram_power": {
                            "process_0": 0.9248213768005371
                        },
                        "cpu_energy": {
                            "process_0": 0.035696834485156495
                        },
                        "gpu_energy": {
                            "process_0": 0.03487604206747799
                        },
                        "ram_energy": {
                            "process_0": 0.0002553537496743837
                        },
                        "total_energy_kwh": {
                            "process_0": 0.07082823030230896
                        },
                        "total_energy_joules": {
                            "process_0": 254981.62908831227
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 118.46996986273552,
                        "ram_power_avg": 0.9248213768005371,
                        "cpu_energy_total": 0.035696834485156495,
                        "gpu_energy_total": 0.03487604206747799,
                        "ram_energy_total": 0.0002553537496743837,
                        "total_energy_kwh": 0.07082823030230896,
                        "total_energy_joules": 254981.62908831227
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.05654917199939143,
                        "joules_per_token": 17.683724883023253,
                        "flops_per_joule": 4057328.0973182903,
                        "joules_per_flop": 2.464676200726667e-07
                    },
                    "per-process_emissions": [
                        0.026982014333664598
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0057": {
            "setup": {
                "experiment_id": "0057",
                "date_time": "April 11, 2025 at 01:23:35 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R2_Low_Latency_Chatbot_Deployment",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.05,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 98.84647394600007,
                        "average_latency_ms_per_batch": 3088.952310812502,
                        "throughput_queries_per_sec": 1.2949374407621916,
                        "throughput_tokens_per_sec": 165.75199241756053
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 12035555328,
                        "gpu_max_memory_reserved_bytes": 12035555328
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            29.0,
                            25.0,
                            25.0,
                            27.0
                        ],
                        "cpu_usage_percent": 0.8,
                        "cpu_memory_usage_bytes": 2005504000
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0057",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 160.9501303576273
                        },
                        "ram_power": {
                            "process_0": 0.6992354393005371
                        },
                        "cpu_energy": {
                            "process_0": 0.0030264088215937045
                        },
                        "gpu_energy": {
                            "process_0": 0.004555453366582002
                        },
                        "ram_energy": {
                            "process_0": 1.6420306080058267e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.007598282494255767
                        },
                        "total_energy_joules": {
                            "process_0": 27353.81697932076
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 160.9501303576273,
                        "ram_power_avg": 0.6992354393005371,
                        "cpu_energy_total": 0.0030264088215937045,
                        "gpu_energy_total": 0.004555453366582002,
                        "ram_energy_total": 1.6420306080058267e-05,
                        "total_energy_kwh": 0.007598282494255767,
                        "total_energy_joules": 27353.81697932076
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5989657681919184,
                        "joules_per_token": 1.6695444933667456,
                        "flops_per_joule": 619656518.3559583,
                        "joules_per_flop": 1.6137972737753973e-09
                    },
                    "per-process_emissions": [
                        0.0028945657161867345
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0057": {
            "setup": {
                "experiment_id": "0057",
                "date_time": "April 11, 2025 at 01:23:35 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R2_Low_Latency_Chatbot_Deployment",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.05,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 98.84647394600007,
                        "average_latency_ms_per_batch": 3088.952310812502,
                        "throughput_queries_per_sec": 1.2949374407621916,
                        "throughput_tokens_per_sec": 165.75199241756053
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 12035555328,
                        "gpu_max_memory_reserved_bytes": 12035555328
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            29.0,
                            25.0,
                            25.0,
                            27.0
                        ],
                        "cpu_usage_percent": 0.8,
                        "cpu_memory_usage_bytes": 2005504000
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0057",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 160.9501303576273
                        },
                        "ram_power": {
                            "process_0": 0.6992354393005371
                        },
                        "cpu_energy": {
                            "process_0": 0.0030264088215937045
                        },
                        "gpu_energy": {
                            "process_0": 0.004555453366582002
                        },
                        "ram_energy": {
                            "process_0": 1.6420306080058267e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.007598282494255767
                        },
                        "total_energy_joules": {
                            "process_0": 27353.81697932076
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 160.9501303576273,
                        "ram_power_avg": 0.6992354393005371,
                        "cpu_energy_total": 0.0030264088215937045,
                        "gpu_energy_total": 0.004555453366582002,
                        "ram_energy_total": 1.6420306080058267e-05,
                        "total_energy_kwh": 0.007598282494255767,
                        "total_energy_joules": 27353.81697932076
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.5989657681919184,
                        "joules_per_token": 1.6695444933667456,
                        "flops_per_joule": 619656518.3559583,
                        "joules_per_flop": 1.6137972737753973e-09
                    },
                    "per-process_emissions": [
                        0.0028945657161867345
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0058": {
            "setup": {
                "experiment_id": "0058",
                "date_time": "April 11, 2025 at 01:24:19 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A1_Max_Throughput_Exploit",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 14.190496213999722,
                        "average_latency_ms_per_batch": 14190.496213999722,
                        "throughput_queries_per_sec": 9.020121500312356,
                        "throughput_tokens_per_sec": 1154.5755520399816
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1577541632,
                        "gpu_max_memory_allocated_bytes": 1577541632,
                        "gpu_current_memory_reserved_bytes": 3577741312,
                        "gpu_max_memory_reserved_bytes": 3577741312
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 2698506240
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0058",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 633.8486678975152,
                            "process_0": 359.0496503849245,
                            "process_3": 602.8890283379544,
                            "process_2": 605.9701499005001
                        },
                        "ram_power": {
                            "process_1": 0.9867196083068848,
                            "process_0": 0.9413280487060547,
                            "process_3": 0.9840617179870604,
                            "process_2": 0.9895563125610352
                        },
                        "cpu_energy": {
                            "process_1": 0.00042495803428124654,
                            "process_0": 0.00044030417337501143,
                            "process_3": 0.00043325986346874845,
                            "process_2": 0.0004276123005312798
                        },
                        "gpu_energy": {
                            "process_1": 0.0023799135705959845,
                            "process_0": 0.0024438747328759877,
                            "process_3": 0.002420380547413989,
                            "process_2": 0.0023942427487260026
                        },
                        "ram_energy": {
                            "process_1": 3.301555579632363e-06,
                            "process_0": 3.2607937969714537e-06,
                            "process_3": 3.3489897420244626e-06,
                            "process_2": 3.323926200052414e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0028081731604568633,
                            "process_0": 0.0028874397000479713,
                            "process_3": 0.002856989400624762,
                            "process_2": 0.002825178975457334
                        },
                        "total_energy_joules": {
                            "process_1": 10109.423377644707,
                            "process_0": 10394.782920172696,
                            "process_3": 10285.161842249143,
                            "process_2": 10170.644311646403
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 550.4393741302235,
                        "ram_power_avg": 0.9754164218902588,
                        "cpu_energy_total": 0.001726134371656286,
                        "gpu_energy_total": 0.009638411599611964,
                        "ram_energy_total": 1.3235265318680694e-05,
                        "total_energy_kwh": 0.01137778123658693,
                        "total_energy_joules": 40960.012451712944
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.39999987840127776,
                        "joules_per_token": 2.500000759992245,
                        "flops_per_joule": 25257417.32182348,
                        "joules_per_flop": 3.9592329938499195e-08
                    },
                    "per-process_emissions": [
                        0.001069773565476042,
                        0.0010999701537332746,
                        0.0010883701121680031,
                        0.0010762519307004714
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0058": {
            "setup": {
                "experiment_id": "0058",
                "date_time": "April 11, 2025 at 01:24:19 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A1_Max_Throughput_Exploit",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 14.190496213999722,
                        "average_latency_ms_per_batch": 14190.496213999722,
                        "throughput_queries_per_sec": 9.020121500312356,
                        "throughput_tokens_per_sec": 1154.5755520399816
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1577541632,
                        "gpu_max_memory_allocated_bytes": 1577541632,
                        "gpu_current_memory_reserved_bytes": 3577741312,
                        "gpu_max_memory_reserved_bytes": 3577741312
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 2698506240
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0058",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 633.8486678975152,
                            "process_0": 359.0496503849245,
                            "process_3": 602.8890283379544,
                            "process_2": 605.9701499005001
                        },
                        "ram_power": {
                            "process_1": 0.9867196083068848,
                            "process_0": 0.9413280487060547,
                            "process_3": 0.9840617179870604,
                            "process_2": 0.9895563125610352
                        },
                        "cpu_energy": {
                            "process_1": 0.00042495803428124654,
                            "process_0": 0.00044030417337501143,
                            "process_3": 0.00043325986346874845,
                            "process_2": 0.0004276123005312798
                        },
                        "gpu_energy": {
                            "process_1": 0.0023799135705959845,
                            "process_0": 0.0024438747328759877,
                            "process_3": 0.002420380547413989,
                            "process_2": 0.0023942427487260026
                        },
                        "ram_energy": {
                            "process_1": 3.301555579632363e-06,
                            "process_0": 3.2607937969714537e-06,
                            "process_3": 3.3489897420244626e-06,
                            "process_2": 3.323926200052414e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0028081731604568633,
                            "process_0": 0.0028874397000479713,
                            "process_3": 0.002856989400624762,
                            "process_2": 0.002825178975457334
                        },
                        "total_energy_joules": {
                            "process_1": 10109.423377644707,
                            "process_0": 10394.782920172696,
                            "process_3": 10285.161842249143,
                            "process_2": 10170.644311646403
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 550.4393741302235,
                        "ram_power_avg": 0.9754164218902588,
                        "cpu_energy_total": 0.001726134371656286,
                        "gpu_energy_total": 0.009638411599611964,
                        "ram_energy_total": 1.3235265318680694e-05,
                        "total_energy_kwh": 0.01137778123658693,
                        "total_energy_joules": 40960.012451712944
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.39999987840127776,
                        "joules_per_token": 2.500000759992245,
                        "flops_per_joule": 25257417.32182348,
                        "joules_per_flop": 3.9592329938499195e-08
                    },
                    "per-process_emissions": [
                        0.001069773565476042,
                        0.0010999701537332746,
                        0.0010883701121680031,
                        0.0010762519307004714
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0059": {
            "setup": {
                "experiment_id": "0059",
                "date_time": "April 11, 2025 at 01:24:55 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A3_Quantisation_Gaming",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 10.569793939999727,
                        "average_latency_ms_per_batch": 5284.896969999863,
                        "throughput_queries_per_sec": 12.109980641685368,
                        "throughput_tokens_per_sec": 1550.0775221357271
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1986002944,
                        "gpu_max_memory_reserved_bytes": 1986002944
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            18.0,
                            25.0,
                            25.0,
                            27.0
                        ],
                        "cpu_usage_percent": 0.8,
                        "cpu_memory_usage_bytes": 2675888128
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0059",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 201.25676016401115
                        },
                        "ram_power": {
                            "process_0": 0.934354305267334
                        },
                        "cpu_energy": {
                            "process_0": 0.0003280987878749784
                        },
                        "gpu_energy": {
                            "process_0": 0.0005345873721139954
                        },
                        "ram_energy": {
                            "process_0": 2.2492210242031757e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0008649353810131769
                        },
                        "total_energy_joules": {
                            "process_0": 3113.767371647437
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 201.25676016401115,
                        "ram_power_avg": 0.934354305267334,
                        "cpu_energy_total": 0.0003280987878749784,
                        "gpu_energy_total": 0.0005345873721139954,
                        "ram_energy_total": 2.2492210242031757e-06,
                        "total_energy_kwh": 0.0008649353810131769,
                        "total_energy_joules": 3113.767371647437
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 5.261793205614949,
                        "joules_per_token": 0.1900492780546531,
                        "flops_per_joule": 332248368.1408228,
                        "joules_per_flop": 3.0097965735565382e-09
                    },
                    "per-process_emissions": [
                        0.00032949713339696973
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0059": {
            "setup": {
                "experiment_id": "0059",
                "date_time": "April 11, 2025 at 01:24:55 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A3_Quantisation_Gaming",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 10.569793939999727,
                        "average_latency_ms_per_batch": 5284.896969999863,
                        "throughput_queries_per_sec": 12.109980641685368,
                        "throughput_tokens_per_sec": 1550.0775221357271
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 1986002944,
                        "gpu_max_memory_reserved_bytes": 1986002944
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            18.0,
                            25.0,
                            25.0,
                            27.0
                        ],
                        "cpu_usage_percent": 0.8,
                        "cpu_memory_usage_bytes": 2675888128
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0059",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 201.25676016401115
                        },
                        "ram_power": {
                            "process_0": 0.934354305267334
                        },
                        "cpu_energy": {
                            "process_0": 0.0003280987878749784
                        },
                        "gpu_energy": {
                            "process_0": 0.0005345873721139954
                        },
                        "ram_energy": {
                            "process_0": 2.2492210242031757e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0008649353810131769
                        },
                        "total_energy_joules": {
                            "process_0": 3113.767371647437
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 201.25676016401115,
                        "ram_power_avg": 0.934354305267334,
                        "cpu_energy_total": 0.0003280987878749784,
                        "gpu_energy_total": 0.0005345873721139954,
                        "ram_energy_total": 2.2492210242031757e-06,
                        "total_energy_kwh": 0.0008649353810131769,
                        "total_energy_joules": 3113.767371647437
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 5.261793205614949,
                        "joules_per_token": 0.1900492780546531,
                        "flops_per_joule": 332248368.1408228,
                        "joules_per_flop": 3.0097965735565382e-09
                    },
                    "per-process_emissions": [
                        0.00032949713339696973
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0060": {
            "setup": {
                "experiment_id": "0060",
                "date_time": "April 11, 2025 at 01:26:47 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "default",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 86.21784818200103,
                        "average_latency_ms_per_batch": 10777.23102275013,
                        "throughput_queries_per_sec": 1.484611396584605,
                        "throughput_tokens_per_sec": 190.03025876282945
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576560640,
                        "gpu_max_memory_allocated_bytes": 1576560640,
                        "gpu_current_memory_reserved_bytes": 2707423232,
                        "gpu_max_memory_reserved_bytes": 2707423232
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            19.0,
                            26.0,
                            26.0,
                            27.0
                        ],
                        "cpu_usage_percent": 0.9,
                        "cpu_memory_usage_bytes": 2684702720
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0060",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 170.02014733720915
                        },
                        "ram_power": {
                            "process_0": 0.9367575645446778
                        },
                        "cpu_energy": {
                            "process_0": 0.002636959969031268
                        },
                        "gpu_energy": {
                            "process_0": 0.0029004189869999975
                        },
                        "ram_energy": {
                            "process_0": 1.9033983535516406e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0055564129395667804
                        },
                        "total_energy_joules": {
                            "process_0": 20003.08658244041
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 170.02014733720915,
                        "ram_power_avg": 0.9367575645446778,
                        "cpu_energy_total": 0.002636959969031268,
                        "gpu_energy_total": 0.0029004189869999975,
                        "ram_energy_total": 1.9033983535516406e-05,
                        "total_energy_kwh": 0.0055564129395667804,
                        "total_energy_joules": 20003.08658244041
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.819073593091508,
                        "joules_per_token": 1.220891515041529,
                        "flops_per_joule": 51719224.61747321,
                        "joules_per_flop": 1.9335169995223646e-08
                    },
                    "per-process_emissions": [
                        0.002116715509327965
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0060": {
            "setup": {
                "experiment_id": "0060",
                "date_time": "April 11, 2025 at 01:26:47 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "default",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 86.21784818200103,
                        "average_latency_ms_per_batch": 10777.23102275013,
                        "throughput_queries_per_sec": 1.484611396584605,
                        "throughput_tokens_per_sec": 190.03025876282945
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1576560640,
                        "gpu_max_memory_allocated_bytes": 1576560640,
                        "gpu_current_memory_reserved_bytes": 2707423232,
                        "gpu_max_memory_reserved_bytes": 2707423232
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            19.0,
                            26.0,
                            26.0,
                            27.0
                        ],
                        "cpu_usage_percent": 0.9,
                        "cpu_memory_usage_bytes": 2684702720
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0060",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 170.02014733720915
                        },
                        "ram_power": {
                            "process_0": 0.9367575645446778
                        },
                        "cpu_energy": {
                            "process_0": 0.002636959969031268
                        },
                        "gpu_energy": {
                            "process_0": 0.0029004189869999975
                        },
                        "ram_energy": {
                            "process_0": 1.9033983535516406e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0055564129395667804
                        },
                        "total_energy_joules": {
                            "process_0": 20003.08658244041
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 170.02014733720915,
                        "ram_power_avg": 0.9367575645446778,
                        "cpu_energy_total": 0.002636959969031268,
                        "gpu_energy_total": 0.0029004189869999975,
                        "ram_energy_total": 1.9033983535516406e-05,
                        "total_energy_kwh": 0.0055564129395667804,
                        "total_energy_joules": 20003.08658244041
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.819073593091508,
                        "joules_per_token": 1.220891515041529,
                        "flops_per_joule": 51719224.61747321,
                        "joules_per_flop": 1.9335169995223646e-08
                    },
                    "per-process_emissions": [
                        0.002116715509327965
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0061": {
            "setup": {
                "experiment_id": "0061",
                "date_time": "April 11, 2025 at 01:27:22 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A2_Precision_Minimalist",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 5.831847961000221,
                        "average_latency_ms_per_batch": 5831.847961000221,
                        "throughput_queries_per_sec": 21.948445991045126,
                        "throughput_tokens_per_sec": 2809.401086853776
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 2621440000,
                        "gpu_max_memory_reserved_bytes": 2621440000
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            12.0,
                            100.0,
                            18.0,
                            27.0
                        ],
                        "cpu_usage_percent": 1.7,
                        "cpu_memory_usage_bytes": 2615406592
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0061",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 470.1087332300603,
                            "process_1": 490.88305992371835
                        },
                        "ram_power": {
                            "process_0": 0.9125189781188965,
                            "process_1": 0.9563555717468262
                        },
                        "cpu_energy": {
                            "process_0": 0.00018326088259375695,
                            "process_1": 0.00018151934134375835
                        },
                        "gpu_energy": {
                            "process_0": 0.0007131769594300105,
                            "process_1": 0.0007096216788080104
                        },
                        "ram_energy": {
                            "process_0": 1.259095783456445e-06,
                            "process_1": 1.3066696119839553e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0008976969378072238,
                            "process_1": 0.0008924476897637527
                        },
                        "total_energy_joules": {
                            "process_0": 3231.7089761060056,
                            "process_1": 3212.81168314951
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 480.4958965768893,
                        "ram_power_avg": 0.9344372749328613,
                        "cpu_energy_total": 0.00036478022393751533,
                        "gpu_energy_total": 0.0014227986382380209,
                        "ram_energy_total": 2.5657653954404004e-06,
                        "total_energy_kwh": 0.0017901446275709765,
                        "total_energy_joules": 6444.520659255515
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 2.542314761062883,
                        "joules_per_token": 0.3933423253940134,
                        "flops_per_joule": 160530811.00984675,
                        "joules_per_flop": 6.229333756612376e-09
                    },
                    "per-process_emissions": [
                        0.00034197764845766195,
                        0.0003399779474155016
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0061": {
            "setup": {
                "experiment_id": "0061",
                "date_time": "April 11, 2025 at 01:27:22 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A2_Precision_Minimalist",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 615606272,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 5.831847961000221,
                        "average_latency_ms_per_batch": 5831.847961000221,
                        "throughput_queries_per_sec": 21.948445991045126,
                        "throughput_tokens_per_sec": 2809.401086853776
                    }
                },
                "compute_metrics": {
                    "flops": 1034544128000,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1087688704,
                        "gpu_max_memory_allocated_bytes": 1087688704,
                        "gpu_current_memory_reserved_bytes": 2621440000,
                        "gpu_max_memory_reserved_bytes": 2621440000
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            12.0,
                            100.0,
                            18.0,
                            27.0
                        ],
                        "cpu_usage_percent": 1.7,
                        "cpu_memory_usage_bytes": 2615406592
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0061",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 470.1087332300603,
                            "process_1": 490.88305992371835
                        },
                        "ram_power": {
                            "process_0": 0.9125189781188965,
                            "process_1": 0.9563555717468262
                        },
                        "cpu_energy": {
                            "process_0": 0.00018326088259375695,
                            "process_1": 0.00018151934134375835
                        },
                        "gpu_energy": {
                            "process_0": 0.0007131769594300105,
                            "process_1": 0.0007096216788080104
                        },
                        "ram_energy": {
                            "process_0": 1.259095783456445e-06,
                            "process_1": 1.3066696119839553e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0008976969378072238,
                            "process_1": 0.0008924476897637527
                        },
                        "total_energy_joules": {
                            "process_0": 3231.7089761060056,
                            "process_1": 3212.81168314951
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 480.4958965768893,
                        "ram_power_avg": 0.9344372749328613,
                        "cpu_energy_total": 0.00036478022393751533,
                        "gpu_energy_total": 0.0014227986382380209,
                        "ram_energy_total": 2.5657653954404004e-06,
                        "total_energy_kwh": 0.0017901446275709765,
                        "total_energy_joules": 6444.520659255515
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 2.542314761062883,
                        "joules_per_token": 0.3933423253940134,
                        "flops_per_joule": 160530811.00984675,
                        "joules_per_flop": 6.229333756612376e-09
                    },
                    "per-process_emissions": [
                        0.00034197764845766195,
                        0.0003399779474155016
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0062": {
            "setup": {
                "experiment_id": "0062",
                "date_time": "April 11, 2025 at 01:31:00 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_2_nout-200",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 2,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 187.03787688399913,
                        "average_latency_ms_per_batch": 2922.4668263124863,
                        "throughput_queries_per_sec": 0.6843533627115838,
                        "throughput_tokens_per_sec": 87.59723042708272
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13220446208,
                        "gpu_max_memory_reserved_bytes": 13220446208
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            12.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 2004709376
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0062",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 829.9260040131874,
                            "process_1": 550.550380492972,
                            "process_3": 590.1961492511665,
                            "process_2": 525.368929004568
                        },
                        "ram_power": {
                            "process_0": 0.6986045837402344,
                            "process_1": 0.646120548248291,
                            "process_3": 0.6480317115783691,
                            "process_2": 0.6479573249816895
                        },
                        "cpu_energy": {
                            "process_0": 0.005772186036906347,
                            "process_1": 0.005669605937093818,
                            "process_3": 0.005654858597781189,
                            "process_2": 0.0056578272004686875
                        },
                        "gpu_energy": {
                            "process_0": 0.031963975293382,
                            "process_1": 0.03153336244889201,
                            "process_3": 0.031472416011245986,
                            "process_2": 0.03160744945260599
                        },
                        "ram_energy": {
                            "process_0": 3.094045989622956e-05,
                            "process_1": 2.8105064039021698e-05,
                            "process_3": 2.8081847117020053e-05,
                            "process_2": 2.8125150839966398e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.03776710179018461,
                            "process_1": 0.03723107345002483,
                            "process_3": 0.037155356456144185,
                            "process_2": 0.03729340180391465
                        },
                        "total_energy_joules": {
                            "process_0": 135961.56644466458,
                            "process_1": 134031.8644200894,
                            "process_3": 133759.28324211907,
                            "process_2": 134256.24649409272
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 624.0103656904735,
                        "ram_power_avg": 0.660178542137146,
                        "cpu_energy_total": 0.02275447777225004,
                        "gpu_energy_total": 0.12657720320612598,
                        "ram_energy_total": 0.00011525252189223769,
                        "total_energy_kwh": 0.14944693350026828,
                        "total_energy_joules": 538008.9606009658
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.030453024391450235,
                        "joules_per_token": 32.83746097418004,
                        "flops_per_joule": 31504997.564015616,
                        "joules_per_flop": 3.1740995947328055e-08
                    },
                    "per-process_emissions": [
                        0.014387377426970827,
                        0.01418317743078696,
                        0.014154333041968128,
                        0.014206921417201285
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0062": {
            "setup": {
                "experiment_id": "0062",
                "date_time": "April 11, 2025 at 01:31:00 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_2_nout-200",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 2,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 187.03787688399913,
                        "average_latency_ms_per_batch": 2922.4668263124863,
                        "throughput_queries_per_sec": 0.6843533627115838,
                        "throughput_tokens_per_sec": 87.59723042708272
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13220446208,
                        "gpu_max_memory_reserved_bytes": 13220446208
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            12.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 2004709376
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0062",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 829.9260040131874,
                            "process_1": 550.550380492972,
                            "process_3": 590.1961492511665,
                            "process_2": 525.368929004568
                        },
                        "ram_power": {
                            "process_0": 0.6986045837402344,
                            "process_1": 0.646120548248291,
                            "process_3": 0.6480317115783691,
                            "process_2": 0.6479573249816895
                        },
                        "cpu_energy": {
                            "process_0": 0.005772186036906347,
                            "process_1": 0.005669605937093818,
                            "process_3": 0.005654858597781189,
                            "process_2": 0.0056578272004686875
                        },
                        "gpu_energy": {
                            "process_0": 0.031963975293382,
                            "process_1": 0.03153336244889201,
                            "process_3": 0.031472416011245986,
                            "process_2": 0.03160744945260599
                        },
                        "ram_energy": {
                            "process_0": 3.094045989622956e-05,
                            "process_1": 2.8105064039021698e-05,
                            "process_3": 2.8081847117020053e-05,
                            "process_2": 2.8125150839966398e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.03776710179018461,
                            "process_1": 0.03723107345002483,
                            "process_3": 0.037155356456144185,
                            "process_2": 0.03729340180391465
                        },
                        "total_energy_joules": {
                            "process_0": 135961.56644466458,
                            "process_1": 134031.8644200894,
                            "process_3": 133759.28324211907,
                            "process_2": 134256.24649409272
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 624.0103656904735,
                        "ram_power_avg": 0.660178542137146,
                        "cpu_energy_total": 0.02275447777225004,
                        "gpu_energy_total": 0.12657720320612598,
                        "ram_energy_total": 0.00011525252189223769,
                        "total_energy_kwh": 0.14944693350026828,
                        "total_energy_joules": 538008.9606009658
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.030453024391450235,
                        "joules_per_token": 32.83746097418004,
                        "flops_per_joule": 31504997.564015616,
                        "joules_per_flop": 3.1740995947328055e-08
                    },
                    "per-process_emissions": [
                        0.014387377426970827,
                        0.01418317743078696,
                        0.014154333041968128,
                        0.014206921417201285
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0063": {
            "setup": {
                "experiment_id": "0063",
                "date_time": "April 11, 2025 at 01:34:38 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_2_nout-100",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 2,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 186.37456694000005,
                        "average_latency_ms_per_batch": 2912.1026084375007,
                        "throughput_queries_per_sec": 0.686788986832132,
                        "throughput_tokens_per_sec": 87.90899031451289
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13220446208,
                        "gpu_max_memory_reserved_bytes": 13220446208
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            18.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 2006880256
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0063",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 609.5893862282766,
                            "process_1": 562.3351865500053,
                            "process_2": 669.9288872729223,
                            "process_3": 443.33883259042926
                        },
                        "ram_power": {
                            "process_0": 0.6985788345336914,
                            "process_1": 0.6499271392822266,
                            "process_2": 0.6524848937988282,
                            "process_3": 0.6480875015258789
                        },
                        "cpu_energy": {
                            "process_0": 0.005752584203499967,
                            "process_1": 0.005756986118874935,
                            "process_2": 0.005688747776312384,
                            "process_3": 0.0057688677931563
                        },
                        "gpu_energy": {
                            "process_0": 0.03204950647291803,
                            "process_1": 0.032076741494706004,
                            "process_2": 0.031861116599984016,
                            "process_3": 0.03213997015640002
                        },
                        "ram_energy": {
                            "process_0": 3.068566884416046e-05,
                            "process_1": 2.848862463921639e-05,
                            "process_2": 2.8214648120154623e-05,
                            "process_3": 2.8455420634687686e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.03783277634526215,
                            "process_1": 0.037862216238220184,
                            "process_2": 0.037578079024416566,
                            "process_3": 0.03793729337019102
                        },
                        "total_energy_joules": {
                            "process_0": 136197.99484294374,
                            "process_1": 136303.97845759266,
                            "process_2": 135281.08448789964,
                            "process_3": 136574.25613268767
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 571.2980731604083,
                        "ram_power_avg": 0.6622695922851562,
                        "cpu_energy_total": 0.022967185891843586,
                        "gpu_energy_total": 0.12812733472400806,
                        "ram_energy_total": 0.00011584436223821915,
                        "total_energy_kwh": 0.15121036497808993,
                        "total_energy_joules": 544357.3139211237
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.030097877958105305,
                        "joules_per_token": 33.22493371100609,
                        "flops_per_joule": 31137582.90681847,
                        "joules_per_flop": 3.211553070745967e-08
                    },
                    "per-process_emissions": [
                        0.014412396148727616,
                        0.01442361127594998,
                        0.014315369204351492,
                        0.01445221190937427
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0063": {
            "setup": {
                "experiment_id": "0063",
                "date_time": "April 11, 2025 at 01:34:38 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_2_nout-100",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 2,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 186.37456694000005,
                        "average_latency_ms_per_batch": 2912.1026084375007,
                        "throughput_queries_per_sec": 0.686788986832132,
                        "throughput_tokens_per_sec": 87.90899031451289
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13220446208,
                        "gpu_max_memory_reserved_bytes": 13220446208
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            18.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 2006880256
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0063",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 609.5893862282766,
                            "process_1": 562.3351865500053,
                            "process_2": 669.9288872729223,
                            "process_3": 443.33883259042926
                        },
                        "ram_power": {
                            "process_0": 0.6985788345336914,
                            "process_1": 0.6499271392822266,
                            "process_2": 0.6524848937988282,
                            "process_3": 0.6480875015258789
                        },
                        "cpu_energy": {
                            "process_0": 0.005752584203499967,
                            "process_1": 0.005756986118874935,
                            "process_2": 0.005688747776312384,
                            "process_3": 0.0057688677931563
                        },
                        "gpu_energy": {
                            "process_0": 0.03204950647291803,
                            "process_1": 0.032076741494706004,
                            "process_2": 0.031861116599984016,
                            "process_3": 0.03213997015640002
                        },
                        "ram_energy": {
                            "process_0": 3.068566884416046e-05,
                            "process_1": 2.848862463921639e-05,
                            "process_2": 2.8214648120154623e-05,
                            "process_3": 2.8455420634687686e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.03783277634526215,
                            "process_1": 0.037862216238220184,
                            "process_2": 0.037578079024416566,
                            "process_3": 0.03793729337019102
                        },
                        "total_energy_joules": {
                            "process_0": 136197.99484294374,
                            "process_1": 136303.97845759266,
                            "process_2": 135281.08448789964,
                            "process_3": 136574.25613268767
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 571.2980731604083,
                        "ram_power_avg": 0.6622695922851562,
                        "cpu_energy_total": 0.022967185891843586,
                        "gpu_energy_total": 0.12812733472400806,
                        "ram_energy_total": 0.00011584436223821915,
                        "total_energy_kwh": 0.15121036497808993,
                        "total_energy_joules": 544357.3139211237
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.030097877958105305,
                        "joules_per_token": 33.22493371100609,
                        "flops_per_joule": 31137582.90681847,
                        "joules_per_flop": 3.211553070745967e-08
                    },
                    "per-process_emissions": [
                        0.014412396148727616,
                        0.01442361127594998,
                        0.014315369204351492,
                        0.01445221190937427
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0064": {
            "setup": {
                "experiment_id": "0064",
                "date_time": "April 11, 2025 at 01:35:25 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_32_nout-800",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 15.431408365999687,
                        "average_latency_ms_per_batch": 3857.8520914999217,
                        "throughput_queries_per_sec": 8.294771090500387,
                        "throughput_tokens_per_sec": 1061.7306995840495
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            25.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1995149312
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0064",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 834.7779391888886,
                            "process_1": 915.1373721200737,
                            "process_3": 742.3818225101627,
                            "process_2": 1097.4918875735164
                        },
                        "ram_power": {
                            "process_0": 0.6955461502075195,
                            "process_1": 0.6510272026062012,
                            "process_3": 0.6500186920166016,
                            "process_2": 0.6571483612060547
                        },
                        "cpu_energy": {
                            "process_0": 0.0004790921812499817,
                            "process_1": 0.0004762958689687337,
                            "process_3": 0.00048361783574999605,
                            "process_2": 0.00047280332662499804
                        },
                        "gpu_energy": {
                            "process_0": 0.003944072321921996,
                            "process_1": 0.003911999796264026,
                            "process_3": 0.003969213730923987,
                            "process_2": 0.003903137289174005
                        },
                        "ram_energy": {
                            "process_0": 2.6075542343306114e-06,
                            "process_1": 2.414898505446887e-06,
                            "process_3": 2.4588394603316787e-06,
                            "process_2": 2.4278290786709455e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0044257720574063086,
                            "process_1": 0.004390710563738206,
                            "process_3": 0.004455290406134315,
                            "process_2": 0.004378368444877673
                        },
                        "total_energy_joules": {
                            "process_0": 15932.77940666271,
                            "process_1": 15806.558029457543,
                            "process_3": 16039.045462083535,
                            "process_2": 15762.126401559624
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 897.4472553481603,
                        "ram_power_avg": 0.6634351015090942,
                        "cpu_energy_total": 0.0019118092125937096,
                        "gpu_energy_total": 0.015728423138284015,
                        "ram_energy_total": 9.909121278780122e-06,
                        "total_energy_kwh": 0.017650141472156503,
                        "total_energy_joules": 63540.50929976341
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.25785125395683606,
                        "joules_per_token": 3.8782049133156384,
                        "flops_per_joule": 266758500.67848152,
                        "joules_per_flop": 3.748709028790349e-09
                    },
                    "per-process_emissions": [
                        0.0016859978652689334,
                        0.0016726411892560697,
                        0.0016972428802168673,
                        0.0016679394590761497
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0064": {
            "setup": {
                "experiment_id": "0064",
                "date_time": "April 11, 2025 at 01:35:25 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_32_nout-800",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 15.431408365999687,
                        "average_latency_ms_per_batch": 3857.8520914999217,
                        "throughput_queries_per_sec": 8.294771090500387,
                        "throughput_tokens_per_sec": 1061.7306995840495
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            25.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1995149312
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0064",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 834.7779391888886,
                            "process_1": 915.1373721200737,
                            "process_3": 742.3818225101627,
                            "process_2": 1097.4918875735164
                        },
                        "ram_power": {
                            "process_0": 0.6955461502075195,
                            "process_1": 0.6510272026062012,
                            "process_3": 0.6500186920166016,
                            "process_2": 0.6571483612060547
                        },
                        "cpu_energy": {
                            "process_0": 0.0004790921812499817,
                            "process_1": 0.0004762958689687337,
                            "process_3": 0.00048361783574999605,
                            "process_2": 0.00047280332662499804
                        },
                        "gpu_energy": {
                            "process_0": 0.003944072321921996,
                            "process_1": 0.003911999796264026,
                            "process_3": 0.003969213730923987,
                            "process_2": 0.003903137289174005
                        },
                        "ram_energy": {
                            "process_0": 2.6075542343306114e-06,
                            "process_1": 2.414898505446887e-06,
                            "process_3": 2.4588394603316787e-06,
                            "process_2": 2.4278290786709455e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0044257720574063086,
                            "process_1": 0.004390710563738206,
                            "process_3": 0.004455290406134315,
                            "process_2": 0.004378368444877673
                        },
                        "total_energy_joules": {
                            "process_0": 15932.77940666271,
                            "process_1": 15806.558029457543,
                            "process_3": 16039.045462083535,
                            "process_2": 15762.126401559624
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 897.4472553481603,
                        "ram_power_avg": 0.6634351015090942,
                        "cpu_energy_total": 0.0019118092125937096,
                        "gpu_energy_total": 0.015728423138284015,
                        "ram_energy_total": 9.909121278780122e-06,
                        "total_energy_kwh": 0.017650141472156503,
                        "total_energy_joules": 63540.50929976341
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.25785125395683606,
                        "joules_per_token": 3.8782049133156384,
                        "flops_per_joule": 266758500.67848152,
                        "joules_per_flop": 3.748709028790349e-09
                    },
                    "per-process_emissions": [
                        0.0016859978652689334,
                        0.0016726411892560697,
                        0.0016972428802168673,
                        0.0016679394590761497
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0066": {
            "setup": {
                "experiment_id": "0066",
                "date_time": "April 11, 2025 at 09:03:49 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_1",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.043647837010212,
                        "average_latency_ms_per_batch": 2880.4559796262765,
                        "throughput_queries_per_sec": 5.5546761044672905,
                        "throughput_tokens_per_sec": 710.9985413718132
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 12324962304,
                        "gpu_max_memory_reserved_bytes": 12324962304
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            2.0,
                            63.0
                        ],
                        "cpu_usage_percent": 3.7,
                        "cpu_memory_usage_bytes": 2012663808
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0066",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1162.6360623551136
                        },
                        "ram_power": {
                            "process_0": 0.7012782096862794
                        },
                        "cpu_energy": {
                            "process_0": 0.0007115564082505445
                        },
                        "gpu_energy": {
                            "process_0": 0.003635247630417948
                        },
                        "ram_energy": {
                            "process_0": 4.0386983237683384e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.004350842736992261
                        },
                        "total_energy_joules": {
                            "process_0": 15663.03385317214
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1162.6360623551136,
                        "ram_power_avg": 0.7012782096862794,
                        "cpu_energy_total": 0.0007115564082505445,
                        "gpu_energy_total": 0.003635247630417948,
                        "ram_energy_total": 4.0386983237683384e-06,
                        "total_energy_kwh": 0.004350842736992261,
                        "total_energy_joules": 15663.03385317214
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.0460297892213166,
                        "joules_per_token": 0.9559957185774011,
                        "flops_per_joule": 1082163976.1519907,
                        "joules_per_flop": 9.240743750830135e-10
                    },
                    "per-process_emissions": [
                        0.0016574535406572019
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0066": {
            "setup": {
                "experiment_id": "0066",
                "date_time": "April 11, 2025 at 09:03:49 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_1",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.043647837010212,
                        "average_latency_ms_per_batch": 2880.4559796262765,
                        "throughput_queries_per_sec": 5.5546761044672905,
                        "throughput_tokens_per_sec": 710.9985413718132
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 12324962304,
                        "gpu_max_memory_reserved_bytes": 12324962304
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            2.0,
                            63.0
                        ],
                        "cpu_usage_percent": 3.7,
                        "cpu_memory_usage_bytes": 2012663808
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0066",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1162.6360623551136
                        },
                        "ram_power": {
                            "process_0": 0.7012782096862794
                        },
                        "cpu_energy": {
                            "process_0": 0.0007115564082505445
                        },
                        "gpu_energy": {
                            "process_0": 0.003635247630417948
                        },
                        "ram_energy": {
                            "process_0": 4.0386983237683384e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.004350842736992261
                        },
                        "total_energy_joules": {
                            "process_0": 15663.03385317214
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1162.6360623551136,
                        "ram_power_avg": 0.7012782096862794,
                        "cpu_energy_total": 0.0007115564082505445,
                        "gpu_energy_total": 0.003635247630417948,
                        "ram_energy_total": 4.0386983237683384e-06,
                        "total_energy_kwh": 0.004350842736992261,
                        "total_energy_joules": 15663.03385317214
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.0460297892213166,
                        "joules_per_token": 0.9559957185774011,
                        "flops_per_joule": 1082163976.1519907,
                        "joules_per_flop": 9.240743750830135e-10
                    },
                    "per-process_emissions": [
                        0.0016574535406572019
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0067": {
            "setup": {
                "experiment_id": "0067",
                "date_time": "April 11, 2025 at 09:04:47 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.57521506500416,
                        "average_latency_ms_per_batch": 2821.90188312552,
                        "throughput_queries_per_sec": 5.6699349100963445,
                        "throughput_tokens_per_sec": 725.7516684923321
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.5,
                        "cpu_memory_usage_bytes": 2013552640
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0067",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 530.905825028438,
                            "process_0": 587.958573511026
                        },
                        "ram_power": {
                            "process_1": 0.6506323814392091,
                            "process_0": 0.7014985084533691
                        },
                        "cpu_energy": {
                            "process_1": 0.0009197742600935043,
                            "process_0": 0.0006962037109062749
                        },
                        "gpu_energy": {
                            "process_1": 0.005217601951855999,
                            "process_0": 0.004114287735872024
                        },
                        "ram_energy": {
                            "process_1": 4.7165971604303475e-06,
                            "process_0": 3.788820037447402e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.006142092809109933,
                            "process_0": 0.004814280266815746
                        },
                        "total_energy_joules": {
                            "process_1": 22111.534112795758,
                            "process_0": 17331.408960536686
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 559.432199269732,
                        "ram_power_avg": 0.6760654449462891,
                        "cpu_energy_total": 0.001615977970999779,
                        "gpu_energy_total": 0.009331889687728023,
                        "ram_energy_total": 8.50541719787775e-06,
                        "total_energy_kwh": 0.010956373075925679,
                        "total_energy_joules": 39442.94307333244
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.41538482484785216,
                        "joules_per_token": 2.4074061934407007,
                        "flops_per_joule": 429733931.40665394,
                        "joules_per_flop": 2.327021272736566e-09
                    },
                    "per-process_emissions": [
                        0.002339830255630429,
                        0.0018340000676434587
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0067": {
            "setup": {
                "experiment_id": "0067",
                "date_time": "April 11, 2025 at 09:04:47 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 22.57521506500416,
                        "average_latency_ms_per_batch": 2821.90188312552,
                        "throughput_queries_per_sec": 5.6699349100963445,
                        "throughput_tokens_per_sec": 725.7516684923321
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.5,
                        "cpu_memory_usage_bytes": 2013552640
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0067",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 530.905825028438,
                            "process_0": 587.958573511026
                        },
                        "ram_power": {
                            "process_1": 0.6506323814392091,
                            "process_0": 0.7014985084533691
                        },
                        "cpu_energy": {
                            "process_1": 0.0009197742600935043,
                            "process_0": 0.0006962037109062749
                        },
                        "gpu_energy": {
                            "process_1": 0.005217601951855999,
                            "process_0": 0.004114287735872024
                        },
                        "ram_energy": {
                            "process_1": 4.7165971604303475e-06,
                            "process_0": 3.788820037447402e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.006142092809109933,
                            "process_0": 0.004814280266815746
                        },
                        "total_energy_joules": {
                            "process_1": 22111.534112795758,
                            "process_0": 17331.408960536686
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 559.432199269732,
                        "ram_power_avg": 0.6760654449462891,
                        "cpu_energy_total": 0.001615977970999779,
                        "gpu_energy_total": 0.009331889687728023,
                        "ram_energy_total": 8.50541719787775e-06,
                        "total_energy_kwh": 0.010956373075925679,
                        "total_energy_joules": 39442.94307333244
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.41538482484785216,
                        "joules_per_token": 2.4074061934407007,
                        "flops_per_joule": 429733931.40665394,
                        "joules_per_flop": 2.327021272736566e-09
                    },
                    "per-process_emissions": [
                        0.002339830255630429,
                        0.0018340000676434587
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0068": {
            "setup": {
                "experiment_id": "0068",
                "date_time": "April 11, 2025 at 09:05:46 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_3",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 3
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.74392720900505,
                        "average_latency_ms_per_batch": 2967.9909011256314,
                        "throughput_queries_per_sec": 5.390852106026298,
                        "throughput_tokens_per_sec": 690.0290695713661
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.8,
                        "cpu_memory_usage_bytes": 2013167616
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0068",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 696.5594731562404,
                            "process_1": 476.803636788845,
                            "process_2": 730.4733323093229
                        },
                        "ram_power": {
                            "process_0": 0.7012767791748047,
                            "process_1": 0.6507582664489746,
                            "process_2": 0.6517009735107423
                        },
                        "cpu_energy": {
                            "process_0": 0.0007330277759684807,
                            "process_1": 0.0009372378677501274,
                            "process_2": 0.0009008104859688046
                        },
                        "gpu_energy": {
                            "process_0": 0.004943030898865941,
                            "process_1": 0.006087661536791966,
                            "process_2": 0.00592325001637406
                        },
                        "ram_energy": {
                            "process_0": 3.950379154920785e-06,
                            "process_1": 4.776331069454824e-06,
                            "process_2": 4.5716254748404535e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005680009053989341,
                            "process_1": 0.00702967573561155,
                            "process_2": 0.006828632127817705
                        },
                        "total_energy_joules": {
                            "process_0": 20448.03259436163,
                            "process_1": 25306.83264820158,
                            "process_2": 24583.075660143735
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 634.612147418136,
                        "ram_power_avg": 0.6679120063781738,
                        "cpu_energy_total": 0.002571076129687413,
                        "gpu_energy_total": 0.016953942452031967,
                        "ram_energy_total": 1.3298335699216062e-05,
                        "total_energy_kwh": 0.019538316917418596,
                        "total_energy_joules": 70337.94090270695
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.23293260777512273,
                        "joules_per_token": 4.293087213299985,
                        "flops_per_joule": 240979061.59348038,
                        "joules_per_flop": 4.149738128231863e-09
                    },
                    "per-process_emissions": [
                        0.0021637994491172396,
                        0.00267795497148122,
                        0.0026013674090921545
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0068": {
            "setup": {
                "experiment_id": "0068",
                "date_time": "April 11, 2025 at 09:05:46 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_3",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 3
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.74392720900505,
                        "average_latency_ms_per_batch": 2967.9909011256314,
                        "throughput_queries_per_sec": 5.390852106026298,
                        "throughput_tokens_per_sec": 690.0290695713661
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.8,
                        "cpu_memory_usage_bytes": 2013167616
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0068",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 696.5594731562404,
                            "process_1": 476.803636788845,
                            "process_2": 730.4733323093229
                        },
                        "ram_power": {
                            "process_0": 0.7012767791748047,
                            "process_1": 0.6507582664489746,
                            "process_2": 0.6517009735107423
                        },
                        "cpu_energy": {
                            "process_0": 0.0007330277759684807,
                            "process_1": 0.0009372378677501274,
                            "process_2": 0.0009008104859688046
                        },
                        "gpu_energy": {
                            "process_0": 0.004943030898865941,
                            "process_1": 0.006087661536791966,
                            "process_2": 0.00592325001637406
                        },
                        "ram_energy": {
                            "process_0": 3.950379154920785e-06,
                            "process_1": 4.776331069454824e-06,
                            "process_2": 4.5716254748404535e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.005680009053989341,
                            "process_1": 0.00702967573561155,
                            "process_2": 0.006828632127817705
                        },
                        "total_energy_joules": {
                            "process_0": 20448.03259436163,
                            "process_1": 25306.83264820158,
                            "process_2": 24583.075660143735
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 634.612147418136,
                        "ram_power_avg": 0.6679120063781738,
                        "cpu_energy_total": 0.002571076129687413,
                        "gpu_energy_total": 0.016953942452031967,
                        "ram_energy_total": 1.3298335699216062e-05,
                        "total_energy_kwh": 0.019538316917418596,
                        "total_energy_joules": 70337.94090270695
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.23293260777512273,
                        "joules_per_token": 4.293087213299985,
                        "flops_per_joule": 240979061.59348038,
                        "joules_per_flop": 4.149738128231863e-09
                    },
                    "per-process_emissions": [
                        0.0021637994491172396,
                        0.00267795497148122,
                        0.0026013674090921545
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0069": {
            "setup": {
                "experiment_id": "0069",
                "date_time": "April 11, 2025 at 09:06:52 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.71341767800186,
                        "average_latency_ms_per_batch": 2964.1772097502326,
                        "throughput_queries_per_sec": 5.397787941750011,
                        "throughput_tokens_per_sec": 690.9168565440015
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.2,
                        "cpu_memory_usage_bytes": 2011045888
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0069",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 886.532394200152,
                            "process_2": 735.7093176450322,
                            "process_1": 686.3411294947002,
                            "process_3": 480.44944979498536
                        },
                        "ram_power": {
                            "process_0": 0.7009191513061523,
                            "process_2": 0.6467885971069336,
                            "process_1": 0.6509342193603516,
                            "process_3": 0.6532273292541504
                        },
                        "cpu_energy": {
                            "process_0": 0.0007321088768118216,
                            "process_2": 0.0009018179621873513,
                            "process_1": 0.0009317515727191222,
                            "process_3": 0.0011155977370622167
                        },
                        "gpu_energy": {
                            "process_0": 0.005337280936488109,
                            "process_2": 0.006421940137548043,
                            "process_1": 0.006590352216721995,
                            "process_3": 0.007439231784714007
                        },
                        "ram_energy": {
                            "process_0": 3.95148232553238e-06,
                            "process_2": 4.546154257272068e-06,
                            "process_1": 4.786892482210701e-06,
                            "process_3": 5.754955435361401e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.006073341295625461,
                            "process_2": 0.007328304253992669,
                            "process_1": 0.007526890681923329,
                            "process_3": 0.008560584477211586
                        },
                        "total_energy_joules": {
                            "process_0": 21864.02866425166,
                            "process_2": 26381.895314373607,
                            "process_1": 27096.806454923986,
                            "process_3": 30818.10411796171
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 697.2580727837175,
                        "ram_power_avg": 0.662967324256897,
                        "cpu_energy_total": 0.003681276148780512,
                        "gpu_energy_total": 0.025788805075472154,
                        "ram_energy_total": 1.903948450037655e-05,
                        "total_energy_kwh": 0.029489120708753043,
                        "total_energy_joules": 106160.83455151095
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.15433186889700098,
                        "joules_per_token": 6.47954312448187,
                        "flops_per_joule": 159663128.7306582,
                        "joules_per_flop": 6.263186798042383e-09
                    },
                    "per-process_emissions": [
                        0.0023136393665685196,
                        0.0027917175055585074,
                        0.0028673690052786922,
                        0.003261154656593754
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0069": {
            "setup": {
                "experiment_id": "0069",
                "date_time": "April 11, 2025 at 09:06:52 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "num_processes_4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 23.71341767800186,
                        "average_latency_ms_per_batch": 2964.1772097502326,
                        "throughput_queries_per_sec": 5.397787941750011,
                        "throughput_tokens_per_sec": 690.9168565440015
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13212057600,
                        "gpu_max_memory_reserved_bytes": 13212057600
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.2,
                        "cpu_memory_usage_bytes": 2011045888
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0069",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 886.532394200152,
                            "process_2": 735.7093176450322,
                            "process_1": 686.3411294947002,
                            "process_3": 480.44944979498536
                        },
                        "ram_power": {
                            "process_0": 0.7009191513061523,
                            "process_2": 0.6467885971069336,
                            "process_1": 0.6509342193603516,
                            "process_3": 0.6532273292541504
                        },
                        "cpu_energy": {
                            "process_0": 0.0007321088768118216,
                            "process_2": 0.0009018179621873513,
                            "process_1": 0.0009317515727191222,
                            "process_3": 0.0011155977370622167
                        },
                        "gpu_energy": {
                            "process_0": 0.005337280936488109,
                            "process_2": 0.006421940137548043,
                            "process_1": 0.006590352216721995,
                            "process_3": 0.007439231784714007
                        },
                        "ram_energy": {
                            "process_0": 3.95148232553238e-06,
                            "process_2": 4.546154257272068e-06,
                            "process_1": 4.786892482210701e-06,
                            "process_3": 5.754955435361401e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.006073341295625461,
                            "process_2": 0.007328304253992669,
                            "process_1": 0.007526890681923329,
                            "process_3": 0.008560584477211586
                        },
                        "total_energy_joules": {
                            "process_0": 21864.02866425166,
                            "process_2": 26381.895314373607,
                            "process_1": 27096.806454923986,
                            "process_3": 30818.10411796171
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 697.2580727837175,
                        "ram_power_avg": 0.662967324256897,
                        "cpu_energy_total": 0.003681276148780512,
                        "gpu_energy_total": 0.025788805075472154,
                        "ram_energy_total": 1.903948450037655e-05,
                        "total_energy_kwh": 0.029489120708753043,
                        "total_energy_joules": 106160.83455151095
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.15433186889700098,
                        "joules_per_token": 6.47954312448187,
                        "flops_per_joule": 159663128.7306582,
                        "joules_per_flop": 6.263186798042383e-09
                    },
                    "per-process_emissions": [
                        0.0023136393665685196,
                        0.0027917175055585074,
                        0.0028673690052786922,
                        0.003261154656593754
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0070": {
            "setup": {
                "experiment_id": "0070",
                "date_time": "April 11, 2025 at 09:13:16 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_1",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 14722
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 298.1688564670077,
                        "average_latency_ms_per_batch": 2329.4441911484973,
                        "throughput_queries_per_sec": 0.42928695342856227,
                        "throughput_tokens_per_sec": 49.374707252931984
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818202112,
                        "gpu_max_memory_allocated_bytes": 8818202112,
                        "gpu_current_memory_reserved_bytes": 13214154752,
                        "gpu_max_memory_reserved_bytes": 13214154752
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.3,
                        "cpu_memory_usage_bytes": 1999503360
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0070",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 792.8090915439782,
                            "process_1": 610.7814202525254,
                            "process_3": 457.44408509521094,
                            "process_2": 562.3781273370095
                        },
                        "ram_power": {
                            "process_0": 0.6964187622070312,
                            "process_1": 0.6580767631530762,
                            "process_3": 0.6526422500610352,
                            "process_2": 0.6436114311218262
                        },
                        "cpu_energy": {
                            "process_0": 0.009119059069343962,
                            "process_1": 0.01030751454787651,
                            "process_3": 0.010823382378186695,
                            "process_2": 0.010057270109968158
                        },
                        "gpu_energy": {
                            "process_0": 0.04585374946074794,
                            "process_1": 0.05152481677537596,
                            "process_3": 0.05363659596479803,
                            "process_2": 0.050350029724436085
                        },
                        "ram_energy": {
                            "process_0": 4.844282915427267e-05,
                            "process_1": 5.243762714922914e-05,
                            "process_3": 5.508446007562415e-05,
                            "process_2": 5.000957425957915e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.05502125135924615,
                            "process_1": 0.06188476895040172,
                            "process_3": 0.0645150628030603,
                            "process_2": 0.06045730940866382
                        },
                        "total_energy_joules": {
                            "process_0": 198076.50489328612,
                            "process_1": 222785.1682214462,
                            "process_3": 232254.22609101707,
                            "process_2": 217646.31387118975
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 605.853181057181,
                        "ram_power_avg": 0.6626873016357422,
                        "cpu_energy_total": 0.04030722610537533,
                        "gpu_energy_total": 0.201365191925358,
                        "ram_energy_total": 0.00020597449063870514,
                        "total_energy_kwh": 0.24187839252137197,
                        "total_energy_joules": 870762.2130769391
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.016907026716258287,
                        "joules_per_token": 59.14700537134486,
                        "flops_per_joule": 19465671.268918887,
                        "joules_per_flop": 5.1372489866132394e-08
                    },
                    "per-process_emissions": [
                        0.02096034570530482,
                        0.023575002731655536,
                        0.024577013174825822,
                        0.023031212019230484
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0070": {
            "setup": {
                "experiment_id": "0070",
                "date_time": "April 11, 2025 at 09:13:16 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_1",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 14722
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 298.1688564670077,
                        "average_latency_ms_per_batch": 2329.4441911484973,
                        "throughput_queries_per_sec": 0.42928695342856227,
                        "throughput_tokens_per_sec": 49.374707252931984
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818202112,
                        "gpu_max_memory_allocated_bytes": 8818202112,
                        "gpu_current_memory_reserved_bytes": 13214154752,
                        "gpu_max_memory_reserved_bytes": 13214154752
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.3,
                        "cpu_memory_usage_bytes": 1999503360
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0070",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 792.8090915439782,
                            "process_1": 610.7814202525254,
                            "process_3": 457.44408509521094,
                            "process_2": 562.3781273370095
                        },
                        "ram_power": {
                            "process_0": 0.6964187622070312,
                            "process_1": 0.6580767631530762,
                            "process_3": 0.6526422500610352,
                            "process_2": 0.6436114311218262
                        },
                        "cpu_energy": {
                            "process_0": 0.009119059069343962,
                            "process_1": 0.01030751454787651,
                            "process_3": 0.010823382378186695,
                            "process_2": 0.010057270109968158
                        },
                        "gpu_energy": {
                            "process_0": 0.04585374946074794,
                            "process_1": 0.05152481677537596,
                            "process_3": 0.05363659596479803,
                            "process_2": 0.050350029724436085
                        },
                        "ram_energy": {
                            "process_0": 4.844282915427267e-05,
                            "process_1": 5.243762714922914e-05,
                            "process_3": 5.508446007562415e-05,
                            "process_2": 5.000957425957915e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.05502125135924615,
                            "process_1": 0.06188476895040172,
                            "process_3": 0.0645150628030603,
                            "process_2": 0.06045730940866382
                        },
                        "total_energy_joules": {
                            "process_0": 198076.50489328612,
                            "process_1": 222785.1682214462,
                            "process_3": 232254.22609101707,
                            "process_2": 217646.31387118975
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 605.853181057181,
                        "ram_power_avg": 0.6626873016357422,
                        "cpu_energy_total": 0.04030722610537533,
                        "gpu_energy_total": 0.201365191925358,
                        "ram_energy_total": 0.00020597449063870514,
                        "total_energy_kwh": 0.24187839252137197,
                        "total_energy_joules": 870762.2130769391
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.016907026716258287,
                        "joules_per_token": 59.14700537134486,
                        "flops_per_joule": 19465671.268918887,
                        "joules_per_flop": 5.1372489866132394e-08
                    },
                    "per-process_emissions": [
                        0.02096034570530482,
                        0.023575002731655536,
                        0.024577013174825822,
                        0.023031212019230484
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0071": {
            "setup": {
                "experiment_id": "0071",
                "date_time": "April 11, 2025 at 09:17:33 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 2,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 165.46165985699918,
                        "average_latency_ms_per_batch": 2585.338435265612,
                        "throughput_queries_per_sec": 0.773593109791262,
                        "throughput_tokens_per_sec": 99.01991805328153
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13220446208,
                        "gpu_max_memory_reserved_bytes": 13220446208
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.0,
                        "cpu_memory_usage_bytes": 1984512000
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0071",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 565.6576801301723,
                            "process_0": 669.1578727355367,
                            "process_2": 671.1701166194744,
                            "process_1": 531.0568240128713
                        },
                        "ram_power": {
                            "process_3": 0.651914119720459,
                            "process_0": 0.6908969879150391,
                            "process_2": 0.6480259895324707,
                            "process_1": 0.650233268737793
                        },
                        "cpu_energy": {
                            "process_3": 0.006966984916218281,
                            "process_0": 0.005088678707624922,
                            "process_2": 0.0061333873122504205,
                            "process_1": 0.006254791232625165
                        },
                        "gpu_energy": {
                            "process_3": 0.03948306436420401,
                            "process_0": 0.030085957402079977,
                            "process_2": 0.03576100916434005,
                            "process_1": 0.036370140762755965
                        },
                        "ram_energy": {
                            "process_3": 3.572022008819491e-05,
                            "process_0": 2.703350302215389e-05,
                            "process_2": 3.097454369282929e-05,
                            "process_1": 3.1744979851752714e-05
                        },
                        "total_energy_kwh": {
                            "process_3": 0.04648576950051047,
                            "process_0": 0.03520166961272707,
                            "process_2": 0.0419253710202833,
                            "process_1": 0.04265667697523283
                        },
                        "total_energy_joules": {
                            "process_3": 167348.7702018377,
                            "process_0": 126726.01060581744,
                            "process_2": 150931.33567301987,
                            "process_1": 153564.0371108382
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 609.2606233745137,
                        "ram_power_avg": 0.6602675914764404,
                        "cpu_energy_total": 0.02444384216871879,
                        "gpu_energy_total": 0.14170017169338,
                        "ram_energy_total": 0.00012547324665493082,
                        "total_energy_kwh": 0.16626948710875367,
                        "total_energy_joules": 598570.1535915132
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.027371896012010413,
                        "joules_per_token": 36.533822851044505,
                        "flops_per_joule": 28317434.29145199,
                        "joules_per_flop": 3.531393380161789e-08
                    },
                    "per-process_emissions": [
                        0.017708753891219463,
                        0.013410076038968377,
                        0.015971470090176923,
                        0.016250061093714945
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0071": {
            "setup": {
                "experiment_id": "0071",
                "date_time": "April 11, 2025 at 09:17:33 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 2,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 165.46165985699918,
                        "average_latency_ms_per_batch": 2585.338435265612,
                        "throughput_queries_per_sec": 0.773593109791262,
                        "throughput_tokens_per_sec": 99.01991805328153
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13220446208,
                        "gpu_max_memory_reserved_bytes": 13220446208
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.0,
                        "cpu_memory_usage_bytes": 1984512000
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0071",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_3": 565.6576801301723,
                            "process_0": 669.1578727355367,
                            "process_2": 671.1701166194744,
                            "process_1": 531.0568240128713
                        },
                        "ram_power": {
                            "process_3": 0.651914119720459,
                            "process_0": 0.6908969879150391,
                            "process_2": 0.6480259895324707,
                            "process_1": 0.650233268737793
                        },
                        "cpu_energy": {
                            "process_3": 0.006966984916218281,
                            "process_0": 0.005088678707624922,
                            "process_2": 0.0061333873122504205,
                            "process_1": 0.006254791232625165
                        },
                        "gpu_energy": {
                            "process_3": 0.03948306436420401,
                            "process_0": 0.030085957402079977,
                            "process_2": 0.03576100916434005,
                            "process_1": 0.036370140762755965
                        },
                        "ram_energy": {
                            "process_3": 3.572022008819491e-05,
                            "process_0": 2.703350302215389e-05,
                            "process_2": 3.097454369282929e-05,
                            "process_1": 3.1744979851752714e-05
                        },
                        "total_energy_kwh": {
                            "process_3": 0.04648576950051047,
                            "process_0": 0.03520166961272707,
                            "process_2": 0.0419253710202833,
                            "process_1": 0.04265667697523283
                        },
                        "total_energy_joules": {
                            "process_3": 167348.7702018377,
                            "process_0": 126726.01060581744,
                            "process_2": 150931.33567301987,
                            "process_1": 153564.0371108382
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 609.2606233745137,
                        "ram_power_avg": 0.6602675914764404,
                        "cpu_energy_total": 0.02444384216871879,
                        "gpu_energy_total": 0.14170017169338,
                        "ram_energy_total": 0.00012547324665493082,
                        "total_energy_kwh": 0.16626948710875367,
                        "total_energy_joules": 598570.1535915132
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.027371896012010413,
                        "joules_per_token": 36.533822851044505,
                        "flops_per_joule": 28317434.29145199,
                        "joules_per_flop": 3.531393380161789e-08
                    },
                    "per-process_emissions": [
                        0.017708753891219463,
                        0.013410076038968377,
                        0.015971470090176923,
                        0.016250061093714945
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0072": {
            "setup": {
                "experiment_id": "0072",
                "date_time": "April 11, 2025 at 09:19:59 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 86.7260356930019,
                        "average_latency_ms_per_batch": 2710.1886154063095,
                        "throughput_queries_per_sec": 1.4759120369931606,
                        "throughput_tokens_per_sec": 188.91674073512456
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13230931968,
                        "gpu_max_memory_reserved_bytes": 13230931968
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.8,
                        "cpu_memory_usage_bytes": 2007928832
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0072",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 731.5103549572807,
                            "process_1": 559.4469908165999,
                            "process_3": 455.082934684674,
                            "process_2": 649.1991228814225
                        },
                        "ram_power": {
                            "process_0": 0.6995115280151367,
                            "process_1": 0.6521029472351074,
                            "process_3": 0.6526479721069336,
                            "process_2": 0.6461706161499023
                        },
                        "cpu_energy": {
                            "process_0": 0.002654777627437056,
                            "process_1": 0.0031918680300933597,
                            "process_3": 0.0035616769828122874,
                            "process_2": 0.0031312319080310552
                        },
                        "gpu_energy": {
                            "process_0": 0.016468757897218023,
                            "process_1": 0.01944544805634596,
                            "process_3": 0.02108570325743797,
                            "process_2": 0.019173027838410017
                        },
                        "ram_energy": {
                            "process_0": 1.425204083776244e-05,
                            "process_1": 1.6140649953053134e-05,
                            "process_3": 1.83035661950528e-05,
                            "process_2": 1.5728866745446024e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.019137787565492847,
                            "process_1": 0.02265345673639237,
                            "process_3": 0.024665683806445304,
                            "process_2": 0.02231998861318652
                        },
                        "total_energy_joules": {
                            "process_0": 68896.03523577425,
                            "process_1": 81552.44425101254,
                            "process_3": 88796.4617032031,
                            "process_2": 80351.95900747148
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 598.8098508349942,
                        "ram_power_avg": 0.66260826587677,
                        "cpu_energy_total": 0.012539554548373757,
                        "gpu_energy_total": 0.07617293704941197,
                        "ram_energy_total": 6.44251237313144e-05,
                        "total_energy_kwh": 0.08877691672151705,
                        "total_energy_joules": 319596.90019746136
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.051264577315603584,
                        "joules_per_token": 19.50664674056771,
                        "flops_per_joule": 53035467.43625969,
                        "joules_per_flop": 1.8855306615367217e-08
                    },
                    "per-process_emissions": [
                        0.0072905401730745,
                        0.008629834343728673,
                        0.009396392246065338,
                        0.008502799662193406
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0072": {
            "setup": {
                "experiment_id": "0072",
                "date_time": "April 11, 2025 at 09:19:59 AM",
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 1034544128000
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 86.7260356930019,
                        "average_latency_ms_per_batch": 2710.1886154063095,
                        "throughput_queries_per_sec": 1.4759120369931606,
                        "throughput_tokens_per_sec": 188.91674073512456
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 8818209280,
                        "gpu_max_memory_allocated_bytes": 8818209280,
                        "gpu_current_memory_reserved_bytes": 13230931968,
                        "gpu_max_memory_reserved_bytes": 13230931968
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 5.8,
                        "cpu_memory_usage_bytes": 2007928832
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0072",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 731.5103549572807,
                            "process_1": 559.4469908165999,
                            "process_3": 455.082934684674,
                            "process_2": 649.1991228814225
                        },
                        "ram_power": {
                            "process_0": 0.6995115280151367,
                            "process_1": 0.6521029472351074,
                            "process_3": 0.6526479721069336,
                            "process_2": 0.6461706161499023
                        },
                        "cpu_energy": {
                            "process_0": 0.002654777627437056,
                            "process_1": 0.0031918680300933597,
                            "process_3": 0.0035616769828122874,
                            "process_2": 0.0031312319080310552
                        },
                        "gpu_energy": {
                            "process_0": 0.016468757897218023,
                            "process_1": 0.01944544805634596,
                            "process_3": 0.02108570325743797,
                            "process_2": 0.019173027838410017
                        },
                        "ram_energy": {
                            "process_0": 1.425204083776244e-05,
                            "process_1": 1.6140649953053134e-05,
                            "process_3": 1.83035661950528e-05,
                            "process_2": 1.5728866745446024e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.019137787565492847,
                            "process_1": 0.02265345673639237,
                            "process_3": 0.024665683806445304,
                            "process_2": 0.02231998861318652
                        },
                        "total_energy_joules": {
                            "process_0": 68896.03523577425,
                            "process_1": 81552.44425101254,
                            "process_3": 88796.4617032031,
                            "process_2": 80351.95900747148
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 598.8098508349942,
                        "ram_power_avg": 0.66260826587677,
                        "cpu_energy_total": 0.012539554548373757,
                        "gpu_energy_total": 0.07617293704941197,
                        "ram_energy_total": 6.44251237313144e-05,
                        "total_energy_kwh": 0.08877691672151705,
                        "total_energy_joules": 319596.90019746136
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.051264577315603584,
                        "joules_per_token": 19.50664674056771,
                        "flops_per_joule": 53035467.43625969,
                        "joules_per_flop": 1.8855306615367217e-08
                    },
                    "per-process_emissions": [
                        0.0072905401730745,
                        0.008629834343728673,
                        0.009396392246065338,
                        0.008502799662193406
                    ]
                },
                "local_energy_results": {}
            }
        }
    }
]