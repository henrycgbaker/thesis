{
  "config_name": "R2_Low_Latency_Chatbot_Deployment",
  "max_input_tokens": 128,
  "max_output_tokens": 128,
  "number_input_prompts": 128,
  "decode_token_to_text": true,
  "decoder_config": {
    "decoder_temperature": 1.0,
    "decoder_top_k": 0,
    "decoder_top_p": 0.9,
    "decoding_mode": "top_p"
  },
  "query_rate": 1.0,
  "latency_simulation": {
    "simulate": true,
    "delay_min": 0.01,
    "delay_max": 0.05,
    "simulate_burst": false,
    "burst_interval": 0.0,
    "burst_size": 0
  },
  "fp_precision": "torch.float32",
  "quantisation": {
    "quantization": false,
    "load_in_8bit": false,
    "load_in_4bit": false,
    "cached_flops_for_quantised_models": 16949970993152
  },
  "batching_options": {
    "batch_size___fixed_batching": 4,
    "adaptive_batching": false,
    "adaptive_max_tokens": 0,
    "max_batch_size___adaptive_batching": 0
  },
  "sharding_config": {
    "fsdp_config": {
      "use_orig_params": false,
      "cpu_offload": false
    },
    "sharding_strategy": "NO_SHARD"
  },
  "accelerate_config": {
    "distributed_type": "DistributedType.MULTI_GPU",
    "num_processes": 1
  },
  "inference_type": "pure_generative",
  "backend": "pytorch"
}