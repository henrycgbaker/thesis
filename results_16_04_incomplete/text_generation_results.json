[
    {
        "CONFIGURATION_RUN_#0442": {
            "setup": {
                "experiment_id": "0442",
                "date_time": "April 16, 2025 at 09:30:16 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R2_Low_Latency_Chatbot_Deployment",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.05,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 15592
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 67.81157205294585,
                        "average_latency_ms_per_batch": 2119.1116266545578,
                        "throughput_queries_per_sec": 1.8875834334007224,
                        "throughput_tokens_per_sec": 229.9312569811255
                    }
                },
                "compute_metrics": {
                    "flops": 20248623316992,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 9905836032,
                        "gpu_max_memory_allocated_bytes": 9905836032,
                        "gpu_current_memory_reserved_bytes": 10435428352,
                        "gpu_max_memory_reserved_bytes": 10435428352
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.1,
                        "cpu_memory_usage_bytes": 1897975808
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0442",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 160.88880963052873
                        },
                        "ram_power": {
                            "process_0": 0.661259651184082
                        },
                        "cpu_energy": {
                            "process_0": 0.0018967691408179235
                        },
                        "gpu_energy": {
                            "process_0": 0.002785060839157971
                        },
                        "ram_energy": {
                            "process_0": 1.1138508209407709e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0046929684881853026
                        },
                        "total_energy_joules": {
                            "process_0": 16894.68655746709
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 160.88880963052873,
                        "ram_power_avg": 0.661259651184082,
                        "cpu_energy_total": 0.0018967691408179235,
                        "gpu_energy_total": 0.002785060839157971,
                        "ram_energy_total": 1.1138508209407709e-05,
                        "total_energy_kwh": 0.0046929684881853026,
                        "total_energy_joules": 16894.68655746709
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.9228937125861427,
                        "joules_per_token": 1.0835483938857804,
                        "flops_per_joule": 1198520212.145785,
                        "joules_per_flop": 8.343622325814913e-10
                    },
                    "per-process_emissions": [
                        0.001787786345574191
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0442": {
            "setup": {
                "experiment_id": "0442",
                "date_time": "April 16, 2025 at 09:30:16 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R2_Low_Latency_Chatbot_Deployment",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.05,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float32",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 4,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 15592
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 67.81157205294585,
                        "average_latency_ms_per_batch": 2119.1116266545578,
                        "throughput_queries_per_sec": 1.8875834334007224,
                        "throughput_tokens_per_sec": 229.9312569811255
                    }
                },
                "compute_metrics": {
                    "flops": 20248623316992,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 9905836032,
                        "gpu_max_memory_allocated_bytes": 9905836032,
                        "gpu_current_memory_reserved_bytes": 10435428352,
                        "gpu_max_memory_reserved_bytes": 10435428352
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            11.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.1,
                        "cpu_memory_usage_bytes": 1897975808
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0442",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 160.88880963052873
                        },
                        "ram_power": {
                            "process_0": 0.661259651184082
                        },
                        "cpu_energy": {
                            "process_0": 0.0018967691408179235
                        },
                        "gpu_energy": {
                            "process_0": 0.002785060839157971
                        },
                        "ram_energy": {
                            "process_0": 1.1138508209407709e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0046929684881853026
                        },
                        "total_energy_joules": {
                            "process_0": 16894.68655746709
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 160.88880963052873,
                        "ram_power_avg": 0.661259651184082,
                        "cpu_energy_total": 0.0018967691408179235,
                        "gpu_energy_total": 0.002785060839157971,
                        "ram_energy_total": 1.1138508209407709e-05,
                        "total_energy_kwh": 0.0046929684881853026,
                        "total_energy_joules": 16894.68655746709
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.9228937125861427,
                        "joules_per_token": 1.0835483938857804,
                        "flops_per_joule": 1198520212.145785,
                        "joules_per_flop": 8.343622325814913e-10
                    },
                    "per-process_emissions": [
                        0.001787786345574191
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0443": {
            "setup": {
                "experiment_id": "0443",
                "date_time": "April 16, 2025 at 09:31:31 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R3_Balanced_Enterprise_Service",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 30.84061127406312,
                        "average_latency_ms_per_batch": 7710.15281851578,
                        "throughput_queries_per_sec": 4.150371692134639,
                        "throughput_tokens_per_sec": 531.2475765932338
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 2039428096,
                        "gpu_max_memory_allocated_bytes": 2039428096,
                        "gpu_current_memory_reserved_bytes": 3024093184,
                        "gpu_max_memory_reserved_bytes": 3024093184
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 2006937600
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0443",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 93.37944621590394,
                            "process_0": 302.61368260629615,
                            "process_2": 346.1412439347654,
                            "process_3": 305.6391335301352
                        },
                        "ram_power": {
                            "process_1": 0.7350926399230957,
                            "process_0": 0.700631618499756,
                            "process_2": 0.7395157814025879,
                            "process_3": 0.7326221466064453
                        },
                        "cpu_energy": {
                            "process_1": 0.0008424974541212577,
                            "process_0": 0.0008796840372488078,
                            "process_2": 0.0008733085085968925,
                            "process_3": 0.0008235396171876346
                        },
                        "gpu_energy": {
                            "process_1": 0.002531093413762342,
                            "process_0": 0.0025875317922468,
                            "process_2": 0.002548634816682771,
                            "process_3": 0.002594442075550596
                        },
                        "ram_energy": {
                            "process_1": 5.28080378627525e-06,
                            "process_0": 6.282518138529292e-06,
                            "process_2": 6.553947156065152e-06,
                            "process_3": 6.155037501781725e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0033788716716698745,
                            "process_0": 0.003473498347634137,
                            "process_2": 0.003428497272435728,
                            "process_3": 0.0034241367302400125
                        },
                        "total_energy_joules": {
                            "process_1": 12163.938018011548,
                            "process_0": 12504.594051482893,
                            "process_2": 12342.59018076862,
                            "process_3": 12326.892228864044
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 261.94337657177516,
                        "ram_power_avg": 0.7269655466079712,
                        "cpu_energy_total": 0.0034190296171545927,
                        "gpu_energy_total": 0.010261702098242509,
                        "ram_energy_total": 2.4272306582651418e-05,
                        "total_energy_kwh": 0.013705004021979751,
                        "total_energy_joules": 49338.0144791271
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.3320765979938532,
                        "joules_per_token": 3.0113534227982854,
                        "flops_per_joule": 343547894.50075746,
                        "joules_per_flop": 2.9108022957124985e-09
                    },
                    "per-process_emissions": [
                        0.0012871811633226387,
                        0.0013232291955312244,
                        0.0013060860359343906,
                        0.0013044248873849329
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0443": {
            "setup": {
                "experiment_id": "0443",
                "date_time": "April 16, 2025 at 09:31:31 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R3_Balanced_Enterprise_Service",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.5,
                    "delay_max": 1.5,
                    "simulate_burst": true,
                    "burst_interval": 4.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 30.84061127406312,
                        "average_latency_ms_per_batch": 7710.15281851578,
                        "throughput_queries_per_sec": 4.150371692134639,
                        "throughput_tokens_per_sec": 531.2475765932338
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 2039428096,
                        "gpu_max_memory_allocated_bytes": 2039428096,
                        "gpu_current_memory_reserved_bytes": 3024093184,
                        "gpu_max_memory_reserved_bytes": 3024093184
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 2006937600
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0443",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 93.37944621590394,
                            "process_0": 302.61368260629615,
                            "process_2": 346.1412439347654,
                            "process_3": 305.6391335301352
                        },
                        "ram_power": {
                            "process_1": 0.7350926399230957,
                            "process_0": 0.700631618499756,
                            "process_2": 0.7395157814025879,
                            "process_3": 0.7326221466064453
                        },
                        "cpu_energy": {
                            "process_1": 0.0008424974541212577,
                            "process_0": 0.0008796840372488078,
                            "process_2": 0.0008733085085968925,
                            "process_3": 0.0008235396171876346
                        },
                        "gpu_energy": {
                            "process_1": 0.002531093413762342,
                            "process_0": 0.0025875317922468,
                            "process_2": 0.002548634816682771,
                            "process_3": 0.002594442075550596
                        },
                        "ram_energy": {
                            "process_1": 5.28080378627525e-06,
                            "process_0": 6.282518138529292e-06,
                            "process_2": 6.553947156065152e-06,
                            "process_3": 6.155037501781725e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0033788716716698745,
                            "process_0": 0.003473498347634137,
                            "process_2": 0.003428497272435728,
                            "process_3": 0.0034241367302400125
                        },
                        "total_energy_joules": {
                            "process_1": 12163.938018011548,
                            "process_0": 12504.594051482893,
                            "process_2": 12342.59018076862,
                            "process_3": 12326.892228864044
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 261.94337657177516,
                        "ram_power_avg": 0.7269655466079712,
                        "cpu_energy_total": 0.0034190296171545927,
                        "gpu_energy_total": 0.010261702098242509,
                        "ram_energy_total": 2.4272306582651418e-05,
                        "total_energy_kwh": 0.013705004021979751,
                        "total_energy_joules": 49338.0144791271
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.3320765979938532,
                        "joules_per_token": 3.0113534227982854,
                        "flops_per_joule": 343547894.50075746,
                        "joules_per_flop": 2.9108022957124985e-09
                    },
                    "per-process_emissions": [
                        0.0012871811633226387,
                        0.0013232291955312244,
                        0.0013060860359343906,
                        0.0013044248873849329
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0444": {
            "setup": {
                "experiment_id": "0444",
                "date_time": "April 16, 2025 at 09:32:17 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A5_Parallel_Overdrive",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 6.476906926953234,
                        "average_latency_ms_per_batch": 3238.453463476617,
                        "throughput_queries_per_sec": 19.762519585905455,
                        "throughput_tokens_per_sec": 2529.6025069958982
                    }
                },
                "compute_metrics": {
                    "flops": 20248623316992,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4963390464,
                        "gpu_max_memory_allocated_bytes": 4963390464,
                        "gpu_current_memory_reserved_bytes": 7029653504,
                        "gpu_max_memory_reserved_bytes": 7029653504
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1927262208
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0444",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 868.8401850842397,
                            "process_2": 63.07747828676721,
                            "process_0": 1746.672977597047,
                            "process_3": 901.5967430474234
                        },
                        "ram_power": {
                            "process_1": 0.6791524887084961,
                            "process_2": 0.674795150756836,
                            "process_0": 0.6718111038208008,
                            "process_3": 0.6737194061279297
                        },
                        "cpu_energy": {
                            "process_1": 0.00013748372612462845,
                            "process_2": 0.0001766973674402834,
                            "process_0": 0.0001431818021555955,
                            "process_3": 0.00013726267506353906
                        },
                        "gpu_energy": {
                            "process_1": 0.00111331727954056,
                            "process_2": 0.0011392050780294483,
                            "process_0": 0.0011589778716247778,
                            "process_3": 0.00111331727954056
                        },
                        "ram_energy": {
                            "process_1": 1.031968198808262e-06,
                            "process_2": 1.245090436034586e-06,
                            "process_0": 1.0395484188547416e-06,
                            "process_3": 1.0362656745226373e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0012518329738639966,
                            "process_2": 0.0013171475359057658,
                            "process_0": 0.0013031992221992279,
                            "process_3": 0.0012516162202786214
                        },
                        "total_energy_joules": {
                            "process_1": 4506.598705910388,
                            "process_2": 4741.7311292607565,
                            "process_0": 4691.517199917221,
                            "process_3": 4505.818393003037
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 895.0468460038693,
                        "ram_power_avg": 0.6748695373535156,
                        "cpu_energy_total": 0.0005946255707840464,
                        "gpu_energy_total": 0.004524817508735346,
                        "ram_energy_total": 4.352872728220227e-06,
                        "total_energy_kwh": 0.005123795952247611,
                        "total_energy_joules": 18445.6654280914
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.8882303576345023,
                        "joules_per_token": 1.1258340715387818,
                        "flops_per_joule": 1097744258.4508133,
                        "joules_per_flop": 9.109589891285293e-10
                    },
                    "per-process_emissions": [
                        0.00047688577139348953,
                        0.0005017673538033015,
                        0.0004964537436967959,
                        0.00047680319911514086
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0444": {
            "setup": {
                "experiment_id": "0444",
                "date_time": "April 16, 2025 at 09:32:17 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A5_Parallel_Overdrive",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 6.476906926953234,
                        "average_latency_ms_per_batch": 3238.453463476617,
                        "throughput_queries_per_sec": 19.762519585905455,
                        "throughput_tokens_per_sec": 2529.6025069958982
                    }
                },
                "compute_metrics": {
                    "flops": 20248623316992,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4963390464,
                        "gpu_max_memory_allocated_bytes": 4963390464,
                        "gpu_current_memory_reserved_bytes": 7029653504,
                        "gpu_max_memory_reserved_bytes": 7029653504
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1927262208
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0444",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 868.8401850842397,
                            "process_2": 63.07747828676721,
                            "process_0": 1746.672977597047,
                            "process_3": 901.5967430474234
                        },
                        "ram_power": {
                            "process_1": 0.6791524887084961,
                            "process_2": 0.674795150756836,
                            "process_0": 0.6718111038208008,
                            "process_3": 0.6737194061279297
                        },
                        "cpu_energy": {
                            "process_1": 0.00013748372612462845,
                            "process_2": 0.0001766973674402834,
                            "process_0": 0.0001431818021555955,
                            "process_3": 0.00013726267506353906
                        },
                        "gpu_energy": {
                            "process_1": 0.00111331727954056,
                            "process_2": 0.0011392050780294483,
                            "process_0": 0.0011589778716247778,
                            "process_3": 0.00111331727954056
                        },
                        "ram_energy": {
                            "process_1": 1.031968198808262e-06,
                            "process_2": 1.245090436034586e-06,
                            "process_0": 1.0395484188547416e-06,
                            "process_3": 1.0362656745226373e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0012518329738639966,
                            "process_2": 0.0013171475359057658,
                            "process_0": 0.0013031992221992279,
                            "process_3": 0.0012516162202786214
                        },
                        "total_energy_joules": {
                            "process_1": 4506.598705910388,
                            "process_2": 4741.7311292607565,
                            "process_0": 4691.517199917221,
                            "process_3": 4505.818393003037
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 895.0468460038693,
                        "ram_power_avg": 0.6748695373535156,
                        "cpu_energy_total": 0.0005946255707840464,
                        "gpu_energy_total": 0.004524817508735346,
                        "ram_energy_total": 4.352872728220227e-06,
                        "total_energy_kwh": 0.005123795952247611,
                        "total_energy_joules": 18445.6654280914
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.8882303576345023,
                        "joules_per_token": 1.1258340715387818,
                        "flops_per_joule": 1097744258.4508133,
                        "joules_per_flop": 9.109589891285293e-10
                    },
                    "per-process_emissions": [
                        0.00047688577139348953,
                        0.0005017673538033015,
                        0.0004964537436967959,
                        0.00047680319911514086
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0445": {
            "setup": {
                "experiment_id": "0445",
                "date_time": "April 16, 2025 at 09:33:39 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "default",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 49.6338397989166,
                        "average_latency_ms_per_batch": 6204.229974864575,
                        "throughput_queries_per_sec": 2.578885706174882,
                        "throughput_tokens_per_sec": 330.0973703903849
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 2039254528,
                        "gpu_max_memory_allocated_bytes": 2039254528,
                        "gpu_current_memory_reserved_bytes": 2751463424,
                        "gpu_max_memory_reserved_bytes": 2751463424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.2,
                        "cpu_memory_usage_bytes": 1974308864
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0445",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1000.4392545221945
                        },
                        "ram_power": {
                            "process_0": 0.6892361640930176
                        },
                        "cpu_energy": {
                            "process_0": 0.001418401973622167
                        },
                        "gpu_energy": {
                            "process_0": 0.0010491544504338535
                        },
                        "ram_energy": {
                            "process_0": 8.427432393539506e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0024759838564495595
                        },
                        "total_energy_joules": {
                            "process_0": 8913.541883218413
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1000.4392545221945,
                        "ram_power_avg": 0.6892361640930176,
                        "cpu_energy_total": 0.001418401973622167,
                        "gpu_energy_total": 0.0010491544504338535,
                        "ram_energy_total": 8.427432393539506e-06,
                        "total_energy_kwh": 0.0024759838564495595,
                        "total_energy_joules": 8913.541883218413
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.838102093943853,
                        "joules_per_token": 0.5440394215831551,
                        "flops_per_joule": 1901597727.9541175,
                        "joules_per_flop": 5.258735774131765e-10
                    },
                    "per-process_emissions": [
                        0.0009432260501144597
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0445": {
            "setup": {
                "experiment_id": "0445",
                "date_time": "April 16, 2025 at 09:33:39 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "default",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 49.6338397989166,
                        "average_latency_ms_per_batch": 6204.229974864575,
                        "throughput_queries_per_sec": 2.578885706174882,
                        "throughput_tokens_per_sec": 330.0973703903849
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 2039254528,
                        "gpu_max_memory_allocated_bytes": 2039254528,
                        "gpu_current_memory_reserved_bytes": 2751463424,
                        "gpu_max_memory_reserved_bytes": 2751463424
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.2,
                        "cpu_memory_usage_bytes": 1974308864
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0445",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 1000.4392545221945
                        },
                        "ram_power": {
                            "process_0": 0.6892361640930176
                        },
                        "cpu_energy": {
                            "process_0": 0.001418401973622167
                        },
                        "gpu_energy": {
                            "process_0": 0.0010491544504338535
                        },
                        "ram_energy": {
                            "process_0": 8.427432393539506e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0024759838564495595
                        },
                        "total_energy_joules": {
                            "process_0": 8913.541883218413
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 1000.4392545221945,
                        "ram_power_avg": 0.6892361640930176,
                        "cpu_energy_total": 0.001418401973622167,
                        "gpu_energy_total": 0.0010491544504338535,
                        "ram_energy_total": 8.427432393539506e-06,
                        "total_energy_kwh": 0.0024759838564495595,
                        "total_energy_joules": 8913.541883218413
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.838102093943853,
                        "joules_per_token": 0.5440394215831551,
                        "flops_per_joule": 1901597727.9541175,
                        "joules_per_flop": 5.258735774131765e-10
                    },
                    "per-process_emissions": [
                        0.0009432260501144597
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0446": {
            "setup": {
                "experiment_id": "0446",
                "date_time": "April 16, 2025 at 09:34:21 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A3_Quantisation_Gaming",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 7.916800273000263,
                        "average_latency_ms_per_batch": 3958.4001365001313,
                        "throughput_queries_per_sec": 16.1681481894315,
                        "throughput_tokens_per_sec": 2069.522968247232
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2850029568,
                        "gpu_max_memory_reserved_bytes": 2850029568
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 0.9,
                        "cpu_memory_usage_bytes": 1919307776
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0446",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 180.73405173703352
                        },
                        "ram_power": {
                            "process_0": 0.6699328422546387
                        },
                        "cpu_energy": {
                            "process_0": 0.00022756926784313691
                        },
                        "gpu_energy": {
                            "process_0": 0.00036314890163069435
                        },
                        "ram_energy": {
                            "process_0": 1.3943774325976654e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.000592112546906429
                        },
                        "total_energy_joules": {
                            "process_0": 2131.605168863144
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 180.73405173703352,
                        "ram_power_avg": 0.6699328422546387,
                        "cpu_energy_total": 0.00022756926784313691,
                        "gpu_energy_total": 0.00036314890163069435,
                        "ram_energy_total": 1.3943774325976654e-06,
                        "total_energy_kwh": 0.000592112546906429,
                        "total_energy_joules": 2131.605168863144
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 7.686226435985859,
                        "joules_per_token": 0.13010285454486964,
                        "flops_per_joule": 7951740425.8275385,
                        "joules_per_flop": 1.2575863225514306e-10
                    },
                    "per-process_emissions": [
                        0.00022556527474400413
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0446": {
            "setup": {
                "experiment_id": "0446",
                "date_time": "April 16, 2025 at 09:34:21 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A3_Quantisation_Gaming",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 64,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 7.916800273000263,
                        "average_latency_ms_per_batch": 3958.4001365001313,
                        "throughput_queries_per_sec": 16.1681481894315,
                        "throughput_tokens_per_sec": 2069.522968247232
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2850029568,
                        "gpu_max_memory_reserved_bytes": 2850029568
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 0.9,
                        "cpu_memory_usage_bytes": 1919307776
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0446",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 180.73405173703352
                        },
                        "ram_power": {
                            "process_0": 0.6699328422546387
                        },
                        "cpu_energy": {
                            "process_0": 0.00022756926784313691
                        },
                        "gpu_energy": {
                            "process_0": 0.00036314890163069435
                        },
                        "ram_energy": {
                            "process_0": 1.3943774325976654e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.000592112546906429
                        },
                        "total_energy_joules": {
                            "process_0": 2131.605168863144
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 180.73405173703352,
                        "ram_power_avg": 0.6699328422546387,
                        "cpu_energy_total": 0.00022756926784313691,
                        "gpu_energy_total": 0.00036314890163069435,
                        "ram_energy_total": 1.3943774325976654e-06,
                        "total_energy_kwh": 0.000592112546906429,
                        "total_energy_joules": 2131.605168863144
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 7.686226435985859,
                        "joules_per_token": 0.13010285454486964,
                        "flops_per_joule": 7951740425.8275385,
                        "joules_per_flop": 1.2575863225514306e-10
                    },
                    "per-process_emissions": [
                        0.00022556527474400413
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0447": {
            "setup": {
                "experiment_id": "0447",
                "date_time": "April 16, 2025 at 09:45:29 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R5_Real_Time_Mobile_Inference",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.6,
                    "simulate_burst": true,
                    "burst_interval": 5.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 12111
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 632.9420917327516,
                        "average_latency_ms_per_batch": 4944.860091662122,
                        "throughput_queries_per_sec": 0.20223019083718277,
                        "throughput_tokens_per_sec": 19.134451884602505
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 2038922240,
                        "gpu_max_memory_allocated_bytes": 2038922240,
                        "gpu_current_memory_reserved_bytes": 2766143488,
                        "gpu_max_memory_reserved_bytes": 2766143488
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.2,
                        "cpu_memory_usage_bytes": 1965412352
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0447",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 25.21011911896497
                        },
                        "ram_power": {
                            "process_0": 0.6862235069274902
                        },
                        "cpu_energy": {
                            "process_0": 0.01833473855569717
                        },
                        "gpu_energy": {
                            "process_0": 0.009615778248172724
                        },
                        "ram_energy": {
                            "process_0": 0.00010354213640189434
                        },
                        "total_energy_kwh": {
                            "process_0": 0.02805405894027185
                        },
                        "total_energy_joules": {
                            "process_0": 100994.61218497866
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 25.21011911896497,
                        "ram_power_avg": 0.6862235069274902,
                        "cpu_energy_total": 0.01833473855569717,
                        "gpu_energy_total": 0.009615778248172724,
                        "ram_energy_total": 0.00010354213640189434,
                        "total_energy_kwh": 0.02805405894027185,
                        "total_energy_joules": 100994.61218497866
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.11991728804124581,
                        "joules_per_token": 8.339081181155864,
                        "flops_per_joule": 167830447.8471282,
                        "joules_per_flop": 5.9583943964141144e-09
                    },
                    "per-process_emissions": [
                        0.010687193753296562
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0447": {
            "setup": {
                "experiment_id": "0447",
                "date_time": "April 16, 2025 at 09:45:29 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R5_Real_Time_Mobile_Inference",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.2,
                    "delay_max": 0.6,
                    "simulate_burst": true,
                    "burst_interval": 5.0,
                    "burst_size": 8
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 12111
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 632.9420917327516,
                        "average_latency_ms_per_batch": 4944.860091662122,
                        "throughput_queries_per_sec": 0.20223019083718277,
                        "throughput_tokens_per_sec": 19.134451884602505
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 2038922240,
                        "gpu_max_memory_allocated_bytes": 2038922240,
                        "gpu_current_memory_reserved_bytes": 2766143488,
                        "gpu_max_memory_reserved_bytes": 2766143488
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 1.2,
                        "cpu_memory_usage_bytes": 1965412352
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0447",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 25.21011911896497
                        },
                        "ram_power": {
                            "process_0": 0.6862235069274902
                        },
                        "cpu_energy": {
                            "process_0": 0.01833473855569717
                        },
                        "gpu_energy": {
                            "process_0": 0.009615778248172724
                        },
                        "ram_energy": {
                            "process_0": 0.00010354213640189434
                        },
                        "total_energy_kwh": {
                            "process_0": 0.02805405894027185
                        },
                        "total_energy_joules": {
                            "process_0": 100994.61218497866
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 25.21011911896497,
                        "ram_power_avg": 0.6862235069274902,
                        "cpu_energy_total": 0.01833473855569717,
                        "gpu_energy_total": 0.009615778248172724,
                        "ram_energy_total": 0.00010354213640189434,
                        "total_energy_kwh": 0.02805405894027185,
                        "total_energy_joules": 100994.61218497866
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.11991728804124581,
                        "joules_per_token": 8.339081181155864,
                        "flops_per_joule": 167830447.8471282,
                        "joules_per_flop": 5.9583943964141144e-09
                    },
                    "per-process_emissions": [
                        0.010687193753296562
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0448": {
            "setup": {
                "experiment_id": "0448",
                "date_time": "April 16, 2025 at 09:46:22 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A1_Max_Throughput_Exploit",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 8.822531022015028,
                        "average_latency_ms_per_batch": 8822.531022015028,
                        "throughput_queries_per_sec": 14.508308293912393,
                        "throughput_tokens_per_sec": 1857.0634616207863
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 2039678464,
                        "gpu_max_memory_allocated_bytes": 2039678464,
                        "gpu_current_memory_reserved_bytes": 4636803072,
                        "gpu_max_memory_reserved_bytes": 4636803072
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1913118720
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0448",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 615.8340550283847,
                            "process_0": 608.5960295174214,
                            "process_3": 635.2336633021536,
                            "process_1": 612.5787005260511
                        },
                        "ram_power": {
                            "process_2": 0.7025055885314941,
                            "process_0": 0.667771339416504,
                            "process_3": 0.7054896354675293,
                            "process_1": 0.6995015144348145
                        },
                        "cpu_energy": {
                            "process_2": 0.0003467913317472267,
                            "process_0": 0.00034915217303023384,
                            "process_3": 0.00034700104522198674,
                            "process_1": 0.0003474019765653793
                        },
                        "gpu_energy": {
                            "process_2": 0.001296368537093251,
                            "process_0": 0.0012970546487549584,
                            "process_3": 0.001296368537093251,
                            "process_1": 0.0012987552056697993
                        },
                        "ram_energy": {
                            "process_2": 1.8564581527020024e-06,
                            "process_0": 1.7739820244691185e-06,
                            "process_3": 1.8571214222016635e-06,
                            "process_1": 1.838734894932872e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0016450163269931795,
                            "process_0": 0.0016479808038096615,
                            "process_3": 0.0016452267037374397,
                            "process_1": 0.0016479959171301116
                        },
                        "total_energy_joules": {
                            "process_2": 5922.058777175446,
                            "process_0": 5932.730893714781,
                            "process_3": 5922.816133454783,
                            "process_1": 5932.785301668402
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 618.0606120935026,
                        "ram_power_avg": 0.6938170194625854,
                        "cpu_energy_total": 0.0013903465265648267,
                        "gpu_energy_total": 0.0051885469286112595,
                        "ram_energy_total": 7.326296494305656e-06,
                        "total_energy_kwh": 0.0065862197516703925,
                        "total_energy_joules": 23710.391106013412
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.6910050503487773,
                        "joules_per_token": 1.447167425904139,
                        "flops_per_joule": 714875217.2566719,
                        "joules_per_flop": 1.3988455269683178e-09
                    },
                    "per-process_emissions": [
                        0.0006266689697680517,
                        0.0006277982872112906,
                        0.0006267491127887777,
                        0.000627804044630716
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0448": {
            "setup": {
                "experiment_id": "0448",
                "date_time": "April 16, 2025 at 09:46:22 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A1_Max_Throughput_Exploit",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": true,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 256,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 8.822531022015028,
                        "average_latency_ms_per_batch": 8822.531022015028,
                        "throughput_queries_per_sec": 14.508308293912393,
                        "throughput_tokens_per_sec": 1857.0634616207863
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 2039678464,
                        "gpu_max_memory_allocated_bytes": 2039678464,
                        "gpu_current_memory_reserved_bytes": 4636803072,
                        "gpu_max_memory_reserved_bytes": 4636803072
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1913118720
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0448",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 615.8340550283847,
                            "process_0": 608.5960295174214,
                            "process_3": 635.2336633021536,
                            "process_1": 612.5787005260511
                        },
                        "ram_power": {
                            "process_2": 0.7025055885314941,
                            "process_0": 0.667771339416504,
                            "process_3": 0.7054896354675293,
                            "process_1": 0.6995015144348145
                        },
                        "cpu_energy": {
                            "process_2": 0.0003467913317472267,
                            "process_0": 0.00034915217303023384,
                            "process_3": 0.00034700104522198674,
                            "process_1": 0.0003474019765653793
                        },
                        "gpu_energy": {
                            "process_2": 0.001296368537093251,
                            "process_0": 0.0012970546487549584,
                            "process_3": 0.001296368537093251,
                            "process_1": 0.0012987552056697993
                        },
                        "ram_energy": {
                            "process_2": 1.8564581527020024e-06,
                            "process_0": 1.7739820244691185e-06,
                            "process_3": 1.8571214222016635e-06,
                            "process_1": 1.838734894932872e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.0016450163269931795,
                            "process_0": 0.0016479808038096615,
                            "process_3": 0.0016452267037374397,
                            "process_1": 0.0016479959171301116
                        },
                        "total_energy_joules": {
                            "process_2": 5922.058777175446,
                            "process_0": 5932.730893714781,
                            "process_3": 5922.816133454783,
                            "process_1": 5932.785301668402
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 618.0606120935026,
                        "ram_power_avg": 0.6938170194625854,
                        "cpu_energy_total": 0.0013903465265648267,
                        "gpu_energy_total": 0.0051885469286112595,
                        "ram_energy_total": 7.326296494305656e-06,
                        "total_energy_kwh": 0.0065862197516703925,
                        "total_energy_joules": 23710.391106013412
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.6910050503487773,
                        "joules_per_token": 1.447167425904139,
                        "flops_per_joule": 714875217.2566719,
                        "joules_per_flop": 1.3988455269683178e-09
                    },
                    "per-process_emissions": [
                        0.0006266689697680517,
                        0.0006277982872112906,
                        0.0006267491127887777,
                        0.000627804044630716
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0449": {
            "setup": {
                "experiment_id": "0449",
                "date_time": "April 16, 2025 at 09:47:11 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A2_Precision_Minimalist",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 6.550481713027693,
                        "average_latency_ms_per_batch": 6550.481713027693,
                        "throughput_queries_per_sec": 19.540547643302588,
                        "throughput_tokens_per_sec": 2501.1900983427313
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 4001366016,
                        "gpu_max_memory_reserved_bytes": 4001366016
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 2.0,
                        "cpu_memory_usage_bytes": 1882079232
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0449",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 813.1643226810584,
                            "process_1": 488.75462239358245
                        },
                        "ram_power": {
                            "process_0": 0.6569309234619141,
                            "process_1": 0.6854109764099121
                        },
                        "cpu_energy": {
                            "process_0": 0.0001524344359677343,
                            "process_1": 0.00011332335446968502
                        },
                        "gpu_energy": {
                            "process_0": 0.0006190954952760563,
                            "process_1": 0.0005077829062258843
                        },
                        "ram_energy": {
                            "process_0": 1.0265368496679412e-06,
                            "process_1": 8.921136432927734e-07
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0007725564680934586,
                            "process_1": 0.0006219983743388621
                        },
                        "total_energy_joules": {
                            "process_0": 2781.203285136451,
                            "process_1": 2239.1941476199036
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 650.9594725373204,
                        "ram_power_avg": 0.6711709499359131,
                        "cpu_energy_total": 0.00026575779043741933,
                        "gpu_energy_total": 0.0011268784015019406,
                        "ram_energy_total": 1.9186504929607144e-06,
                        "total_energy_kwh": 0.0013945548424323206,
                        "total_energy_joules": 5020.397432756355
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 3.263486650100662,
                        "joules_per_token": 0.306420741745383,
                        "flops_per_joule": 3376220950.6680307,
                        "joules_per_flop": 2.9618914597462484e-10
                    },
                    "per-process_emissions": [
                        0.0002943053865202031,
                        0.00023695028070438954
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0449": {
            "setup": {
                "experiment_id": "0449",
                "date_time": "April 16, 2025 at 09:47:11 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "A2_Precision_Minimalist",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 128,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 6.550481713027693,
                        "average_latency_ms_per_batch": 6550.481713027693,
                        "throughput_queries_per_sec": 19.540547643302588,
                        "throughput_tokens_per_sec": 2501.1900983427313
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 4001366016,
                        "gpu_max_memory_reserved_bytes": 4001366016
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 2.0,
                        "cpu_memory_usage_bytes": 1882079232
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0449",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 813.1643226810584,
                            "process_1": 488.75462239358245
                        },
                        "ram_power": {
                            "process_0": 0.6569309234619141,
                            "process_1": 0.6854109764099121
                        },
                        "cpu_energy": {
                            "process_0": 0.0001524344359677343,
                            "process_1": 0.00011332335446968502
                        },
                        "gpu_energy": {
                            "process_0": 0.0006190954952760563,
                            "process_1": 0.0005077829062258843
                        },
                        "ram_energy": {
                            "process_0": 1.0265368496679412e-06,
                            "process_1": 8.921136432927734e-07
                        },
                        "total_energy_kwh": {
                            "process_0": 0.0007725564680934586,
                            "process_1": 0.0006219983743388621
                        },
                        "total_energy_joules": {
                            "process_0": 2781.203285136451,
                            "process_1": 2239.1941476199036
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 650.9594725373204,
                        "ram_power_avg": 0.6711709499359131,
                        "cpu_energy_total": 0.00026575779043741933,
                        "gpu_energy_total": 0.0011268784015019406,
                        "ram_energy_total": 1.9186504929607144e-06,
                        "total_energy_kwh": 0.0013945548424323206,
                        "total_energy_joules": 5020.397432756355
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 3.263486650100662,
                        "joules_per_token": 0.306420741745383,
                        "flops_per_joule": 3376220950.6680307,
                        "joules_per_flop": 2.9618914597462484e-10
                    },
                    "per-process_emissions": [
                        0.0002943053865202031,
                        0.00023695028070438954
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0450": {
            "setup": {
                "experiment_id": "0450",
                "date_time": "April 16, 2025 at 09:48:00 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R6_Medium_Scale_Language_Model_Serving",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.1,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 10.149603599915281,
                        "average_latency_ms_per_batch": 2537.4008999788202,
                        "throughput_queries_per_sec": 12.611329963770055,
                        "throughput_tokens_per_sec": 1614.250235362567
                    }
                },
                "compute_metrics": {
                    "flops": 20248623316992,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4963390464,
                        "gpu_max_memory_allocated_bytes": 4963390464,
                        "gpu_current_memory_reserved_bytes": 7029653504,
                        "gpu_max_memory_reserved_bytes": 7029653504
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1929367552
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0450",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 468.1498913140015,
                            "process_3": 536.4754171379233,
                            "process_1": 512.7715934242436,
                            "process_2": 566.5978951382236
                        },
                        "ram_power": {
                            "process_0": 0.6725907325744629,
                            "process_3": 0.6803855895996094,
                            "process_1": 0.6802582740783691,
                            "process_2": 0.6795802116394043
                        },
                        "cpu_energy": {
                            "process_0": 0.00032112338656224893,
                            "process_3": 0.00032419117040808493,
                            "process_1": 0.0003258183600664779,
                            "process_2": 0.0003246141414019803
                        },
                        "gpu_energy": {
                            "process_0": 0.0015046834259706543,
                            "process_3": 0.0015053106486933032,
                            "process_1": 0.0015093475963672276,
                            "process_2": 0.0015053106486933032
                        },
                        "ram_energy": {
                            "process_0": 1.5989952175859596e-06,
                            "process_3": 1.6279687032056357e-06,
                            "process_1": 1.639618256785003e-06,
                            "process_2": 1.6271225421892948e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.001827405807750489,
                            "process_3": 0.0018311297878045936,
                            "process_1": 0.0018368055746904907,
                            "process_2": 0.0018315519126374728
                        },
                        "total_energy_joules": {
                            "process_0": 6578.66090790176,
                            "process_3": 6592.067236096537,
                            "process_1": 6612.500068885766,
                            "process_2": 6593.586885494902
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 520.998699253598,
                        "ram_power_avg": 0.6782037019729614,
                        "cpu_energy_total": 0.001295747058438792,
                        "gpu_energy_total": 0.006024652319724488,
                        "ram_energy_total": 6.493704719765893e-06,
                        "total_energy_kwh": 0.007326893082883047,
                        "total_energy_joules": 26376.815098378967
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.6211515658312707,
                        "joules_per_token": 1.6099130309069194,
                        "flops_per_joule": 767667485.3074439,
                        "joules_per_flop": 1.3026473299172089e-09
                    },
                    "per-process_emissions": [
                        0.0006961502424625488,
                        0.0006975688926641599,
                        0.0006997310836783425,
                        0.0006977297011192452
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0450": {
            "setup": {
                "experiment_id": "0450",
                "date_time": "April 16, 2025 at 09:48:00 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R6_Medium_Scale_Language_Model_Serving",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.01,
                    "delay_max": 0.1,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 32,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 10.149603599915281,
                        "average_latency_ms_per_batch": 2537.4008999788202,
                        "throughput_queries_per_sec": 12.611329963770055,
                        "throughput_tokens_per_sec": 1614.250235362567
                    }
                },
                "compute_metrics": {
                    "flops": 20248623316992,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4963390464,
                        "gpu_max_memory_allocated_bytes": 4963390464,
                        "gpu_current_memory_reserved_bytes": 7029653504,
                        "gpu_max_memory_reserved_bytes": 7029653504
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.2,
                        "cpu_memory_usage_bytes": 1929367552
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0450",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 468.1498913140015,
                            "process_3": 536.4754171379233,
                            "process_1": 512.7715934242436,
                            "process_2": 566.5978951382236
                        },
                        "ram_power": {
                            "process_0": 0.6725907325744629,
                            "process_3": 0.6803855895996094,
                            "process_1": 0.6802582740783691,
                            "process_2": 0.6795802116394043
                        },
                        "cpu_energy": {
                            "process_0": 0.00032112338656224893,
                            "process_3": 0.00032419117040808493,
                            "process_1": 0.0003258183600664779,
                            "process_2": 0.0003246141414019803
                        },
                        "gpu_energy": {
                            "process_0": 0.0015046834259706543,
                            "process_3": 0.0015053106486933032,
                            "process_1": 0.0015093475963672276,
                            "process_2": 0.0015053106486933032
                        },
                        "ram_energy": {
                            "process_0": 1.5989952175859596e-06,
                            "process_3": 1.6279687032056357e-06,
                            "process_1": 1.639618256785003e-06,
                            "process_2": 1.6271225421892948e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.001827405807750489,
                            "process_3": 0.0018311297878045936,
                            "process_1": 0.0018368055746904907,
                            "process_2": 0.0018315519126374728
                        },
                        "total_energy_joules": {
                            "process_0": 6578.66090790176,
                            "process_3": 6592.067236096537,
                            "process_1": 6612.500068885766,
                            "process_2": 6593.586885494902
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 520.998699253598,
                        "ram_power_avg": 0.6782037019729614,
                        "cpu_energy_total": 0.001295747058438792,
                        "gpu_energy_total": 0.006024652319724488,
                        "ram_energy_total": 6.493704719765893e-06,
                        "total_energy_kwh": 0.007326893082883047,
                        "total_energy_joules": 26376.815098378967
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.6211515658312707,
                        "joules_per_token": 1.6099130309069194,
                        "flops_per_joule": 767667485.3074439,
                        "joules_per_flop": 1.3026473299172089e-09
                    },
                    "per-process_emissions": [
                        0.0006961502424625488,
                        0.0006975688926641599,
                        0.0006997310836783425,
                        0.0006977297011192452
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0451": {
            "setup": {
                "experiment_id": "0451",
                "date_time": "April 16, 2025 at 09:49:14 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R4_High_Load_Cloud_API_Deployment",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 15368
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 42.6497334059095,
                        "average_latency_ms_per_batch": 2665.608337869344,
                        "throughput_queries_per_sec": 3.0011910926098415,
                        "throughput_tokens_per_sec": 360.3305055564691
                    }
                },
                "compute_metrics": {
                    "flops": 20248623316992,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4963382784,
                        "gpu_max_memory_allocated_bytes": 4963382784,
                        "gpu_current_memory_reserved_bytes": 6727663616,
                        "gpu_max_memory_reserved_bytes": 6727663616
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 0.9,
                        "cpu_memory_usage_bytes": 1933840384
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0451",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 94.54588889016779
                        },
                        "ram_power": {
                            "process_0": 0.6742844581604004
                        },
                        "cpu_energy": {
                            "process_0": 0.0012112509238995698
                        },
                        "gpu_energy": {
                            "process_0": 0.0009977391315239004
                        },
                        "ram_energy": {
                            "process_0": 7.1259265215985285e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.002216115981945069
                        },
                        "total_energy_joules": {
                            "process_0": 7978.017535002248
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 94.54588889016779,
                        "ram_power_avg": 0.6742844581604004,
                        "cpu_energy_total": 0.0012112509238995698,
                        "gpu_energy_total": 0.0009977391315239004,
                        "ram_energy_total": 7.1259265215985285e-06,
                        "total_energy_kwh": 0.002216115981945069,
                        "total_energy_joules": 7978.017535002248
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.9262930837862178,
                        "joules_per_token": 0.5191318021214373,
                        "flops_per_joule": 2538051994.5154886,
                        "joules_per_flop": 3.9400296060164003e-10
                    },
                    "per-process_emissions": [
                        0.000844229383321974
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0451": {
            "setup": {
                "experiment_id": "0451",
                "date_time": "April 16, 2025 at 09:49:14 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 1,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "R4_High_Load_Cloud_API_Deployment",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": true,
                    "delay_min": 0.05,
                    "delay_max": 0.2,
                    "simulate_burst": true,
                    "burst_interval": 2.0,
                    "burst_size": 5
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": false,
                    "load_in_8bit": false,
                    "load_in_4bit": false,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 8,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1235814400,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 15368
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 42.6497334059095,
                        "average_latency_ms_per_batch": 2665.608337869344,
                        "throughput_queries_per_sec": 3.0011910926098415,
                        "throughput_tokens_per_sec": 360.3305055564691
                    }
                },
                "compute_metrics": {
                    "flops": 20248623316992,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 4963382784,
                        "gpu_max_memory_allocated_bytes": 4963382784,
                        "gpu_current_memory_reserved_bytes": 6727663616,
                        "gpu_max_memory_reserved_bytes": 6727663616
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            0.0,
                            0.0,
                            0.0
                        ],
                        "cpu_usage_percent": 0.9,
                        "cpu_memory_usage_bytes": 1933840384
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0451",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_0": 94.54588889016779
                        },
                        "ram_power": {
                            "process_0": 0.6742844581604004
                        },
                        "cpu_energy": {
                            "process_0": 0.0012112509238995698
                        },
                        "gpu_energy": {
                            "process_0": 0.0009977391315239004
                        },
                        "ram_energy": {
                            "process_0": 7.1259265215985285e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.002216115981945069
                        },
                        "total_energy_joules": {
                            "process_0": 7978.017535002248
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 94.54588889016779,
                        "ram_power_avg": 0.6742844581604004,
                        "cpu_energy_total": 0.0012112509238995698,
                        "gpu_energy_total": 0.0009977391315239004,
                        "ram_energy_total": 7.1259265215985285e-06,
                        "total_energy_kwh": 0.002216115981945069,
                        "total_energy_joules": 7978.017535002248
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 1.9262930837862178,
                        "joules_per_token": 0.5191318021214373,
                        "flops_per_joule": 2538051994.5154886,
                        "joules_per_flop": 3.9400296060164003e-10
                    },
                    "per-process_emissions": [
                        0.000844229383321974
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0455": {
            "setup": {
                "experiment_id": "0455",
                "date_time": "April 16, 2025 at 09:51:30 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.252225368865766,
                        "average_latency_ms_per_batch": 3656.5281711082207,
                        "throughput_queries_per_sec": 4.375735465795883,
                        "throughput_tokens_per_sec": 560.0941396218731
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            99.0,
                            100.0,
                            84.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 1927114752
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0455",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 272.9581216842055,
                            "process_1": 408.3517974005585,
                            "process_3": 363.55680813829474,
                            "process_2": 131.958004805838
                        },
                        "ram_power": {
                            "process_0": 0.6727538108825684,
                            "process_1": 0.7082633972167969,
                            "process_3": 0.705256462097168,
                            "process_2": 0.7050819396972656
                        },
                        "cpu_energy": {
                            "process_0": 0.0007194700475938589,
                            "process_1": 0.0007219315634374652,
                            "process_3": 0.0007446282640030403,
                            "process_2": 0.0007626239686214831
                        },
                        "gpu_energy": {
                            "process_0": 0.0038467036329139503,
                            "process_1": 0.0038390697379213634,
                            "process_3": 0.0038706253187195117,
                            "process_2": 0.003824504726267719
                        },
                        "ram_energy": {
                            "process_0": 4.962378052984491e-06,
                            "process_1": 4.9129691748193405e-06,
                            "process_3": 4.9733832103227394e-06,
                            "process_2": 5.18862819747832e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.004571136058560795,
                            "process_1": 0.004565914270533649,
                            "process_3": 0.004620226965932875,
                            "process_2": 0.00459231732308668
                        },
                        "total_energy_joules": {
                            "process_0": 16456.089810818863,
                            "process_1": 16437.291373921136,
                            "process_3": 16632.81707735835,
                            "process_2": 16532.34236311205
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 294.2061830072242,
                        "ram_power_avg": 0.6978389024734497,
                        "cpu_energy_total": 0.0029486538436558473,
                        "gpu_energy_total": 0.015380903415822544,
                        "ram_energy_total": 2.0037358635604888e-05,
                        "total_energy_kwh": 0.018349594618113998,
                        "total_energy_joules": 66058.5406252104
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.24802243351024403,
                        "joules_per_token": 4.0318933487066895,
                        "flops_per_joule": 256590152.2002934,
                        "joules_per_flop": 3.897265703398482e-09
                    },
                    "per-process_emissions": [
                        0.001741374281508735,
                        0.0017393850413597938,
                        0.001760075462672129,
                        0.0017494432842298708
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0455": {
            "setup": {
                "experiment_id": "0455",
                "date_time": "April 16, 2025 at 09:51:30 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.252225368865766,
                        "average_latency_ms_per_batch": 3656.5281711082207,
                        "throughput_queries_per_sec": 4.375735465795883,
                        "throughput_tokens_per_sec": 560.0941396218731
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            99.0,
                            100.0,
                            84.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 1927114752
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0455",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_0": 272.9581216842055,
                            "process_1": 408.3517974005585,
                            "process_3": 363.55680813829474,
                            "process_2": 131.958004805838
                        },
                        "ram_power": {
                            "process_0": 0.6727538108825684,
                            "process_1": 0.7082633972167969,
                            "process_3": 0.705256462097168,
                            "process_2": 0.7050819396972656
                        },
                        "cpu_energy": {
                            "process_0": 0.0007194700475938589,
                            "process_1": 0.0007219315634374652,
                            "process_3": 0.0007446282640030403,
                            "process_2": 0.0007626239686214831
                        },
                        "gpu_energy": {
                            "process_0": 0.0038467036329139503,
                            "process_1": 0.0038390697379213634,
                            "process_3": 0.0038706253187195117,
                            "process_2": 0.003824504726267719
                        },
                        "ram_energy": {
                            "process_0": 4.962378052984491e-06,
                            "process_1": 4.9129691748193405e-06,
                            "process_3": 4.9733832103227394e-06,
                            "process_2": 5.18862819747832e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.004571136058560795,
                            "process_1": 0.004565914270533649,
                            "process_3": 0.004620226965932875,
                            "process_2": 0.00459231732308668
                        },
                        "total_energy_joules": {
                            "process_0": 16456.089810818863,
                            "process_1": 16437.291373921136,
                            "process_3": 16632.81707735835,
                            "process_2": 16532.34236311205
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 294.2061830072242,
                        "ram_power_avg": 0.6978389024734497,
                        "cpu_energy_total": 0.0029486538436558473,
                        "gpu_energy_total": 0.015380903415822544,
                        "ram_energy_total": 2.0037358635604888e-05,
                        "total_energy_kwh": 0.018349594618113998,
                        "total_energy_joules": 66058.5406252104
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.24802243351024403,
                        "joules_per_token": 4.0318933487066895,
                        "flops_per_joule": 256590152.2002934,
                        "joules_per_flop": 3.897265703398482e-09
                    },
                    "per-process_emissions": [
                        0.001741374281508735,
                        0.0017393850413597938,
                        0.001760075462672129,
                        0.0017494432842298708
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0456": {
            "setup": {
                "experiment_id": "0456",
                "date_time": "April 16, 2025 at 09:52:41 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.431059452937916,
                        "average_latency_ms_per_batch": 3678.8824316172395,
                        "throughput_queries_per_sec": 4.34914686658426,
                        "throughput_tokens_per_sec": 556.6907989227852
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.9,
                        "cpu_memory_usage_bytes": 1975275520
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0456",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 447.5341004900419,
                            "process_1": 479.4503072626376,
                            "process_2": 416.44051395115423,
                            "process_3": 394.2464978286182
                        },
                        "ram_power": {
                            "process_0": 0.6895737648010254,
                            "process_1": 0.7184872627258301,
                            "process_2": 0.7192840576171875,
                            "process_3": 0.7212238311767578
                        },
                        "cpu_energy": {
                            "process_0": 0.0007636403119067836,
                            "process_1": 0.0007200531098824285,
                            "process_2": 0.0007594965264324854,
                            "process_3": 0.0007640128006587475
                        },
                        "gpu_energy": {
                            "process_0": 0.003874792544277028,
                            "process_1": 0.0038574939193249946,
                            "process_2": 0.0038658944816010177,
                            "process_3": 0.0038870147762777307
                        },
                        "ram_energy": {
                            "process_0": 5.33742234538605e-06,
                            "process_1": 5.260042265741002e-06,
                            "process_2": 5.545471581601961e-06,
                            "process_3": 5.544840268798289e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.004643770278529198,
                            "process_1": 0.004582807071473162,
                            "process_2": 0.004630936479615104,
                            "process_3": 0.004656572417205277
                        },
                        "total_energy_joules": {
                            "process_0": 16717.573002705114,
                            "process_1": 16498.105457303383,
                            "process_2": 16671.371326614375,
                            "process_3": 16763.660701938996
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 434.41785488311297,
                        "ram_power_avg": 0.7121422290802002,
                        "cpu_energy_total": 0.003007202748880445,
                        "gpu_energy_total": 0.015485195721480771,
                        "ram_energy_total": 2.16877764615273e-05,
                        "total_energy_kwh": 0.018514086246822742,
                        "total_energy_joules": 66650.71048856186
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.24581883493667647,
                        "joules_per_token": 4.068036528842887,
                        "flops_per_joule": 254310432.2355399,
                        "joules_per_flop": 3.932202038309657e-09
                    },
                    "per-process_emissions": [
                        0.0017690442876056982,
                        0.0017458203538777012,
                        0.001764155251909374,
                        0.0017739212623343501
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0456": {
            "setup": {
                "experiment_id": "0456",
                "date_time": "April 16, 2025 at 09:52:41 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.431059452937916,
                        "average_latency_ms_per_batch": 3678.8824316172395,
                        "throughput_queries_per_sec": 4.34914686658426,
                        "throughput_tokens_per_sec": 556.6907989227852
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.9,
                        "cpu_memory_usage_bytes": 1975275520
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0456",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_0": 447.5341004900419,
                            "process_1": 479.4503072626376,
                            "process_2": 416.44051395115423,
                            "process_3": 394.2464978286182
                        },
                        "ram_power": {
                            "process_0": 0.6895737648010254,
                            "process_1": 0.7184872627258301,
                            "process_2": 0.7192840576171875,
                            "process_3": 0.7212238311767578
                        },
                        "cpu_energy": {
                            "process_0": 0.0007636403119067836,
                            "process_1": 0.0007200531098824285,
                            "process_2": 0.0007594965264324854,
                            "process_3": 0.0007640128006587475
                        },
                        "gpu_energy": {
                            "process_0": 0.003874792544277028,
                            "process_1": 0.0038574939193249946,
                            "process_2": 0.0038658944816010177,
                            "process_3": 0.0038870147762777307
                        },
                        "ram_energy": {
                            "process_0": 5.33742234538605e-06,
                            "process_1": 5.260042265741002e-06,
                            "process_2": 5.545471581601961e-06,
                            "process_3": 5.544840268798289e-06
                        },
                        "total_energy_kwh": {
                            "process_0": 0.004643770278529198,
                            "process_1": 0.004582807071473162,
                            "process_2": 0.004630936479615104,
                            "process_3": 0.004656572417205277
                        },
                        "total_energy_joules": {
                            "process_0": 16717.573002705114,
                            "process_1": 16498.105457303383,
                            "process_2": 16671.371326614375,
                            "process_3": 16763.660701938996
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 434.41785488311297,
                        "ram_power_avg": 0.7121422290802002,
                        "cpu_energy_total": 0.003007202748880445,
                        "gpu_energy_total": 0.015485195721480771,
                        "ram_energy_total": 2.16877764615273e-05,
                        "total_energy_kwh": 0.018514086246822742,
                        "total_energy_joules": 66650.71048856186
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.24581883493667647,
                        "joules_per_token": 4.068036528842887,
                        "flops_per_joule": 254310432.2355399,
                        "joules_per_flop": 3.932202038309657e-09
                    },
                    "per-process_emissions": [
                        0.0017690442876056982,
                        0.0017458203538777012,
                        0.001764155251909374,
                        0.0017739212623343501
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0457": {
            "setup": {
                "experiment_id": "0457",
                "date_time": "April 16, 2025 at 09:53:52 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.328921667067334,
                        "average_latency_ms_per_batch": 3666.1152083834168,
                        "throughput_queries_per_sec": 4.364292743286494,
                        "throughput_tokens_per_sec": 558.6294711406713
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            83.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1980977152
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0457",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 78.3426634470924,
                            "process_0": 70.66290289749874,
                            "process_1": 80.73701854650145,
                            "process_2": 78.01577380863054
                        },
                        "ram_power": {
                            "process_3": 0.7226300239562988,
                            "process_0": 0.6915650367736816,
                            "process_1": 0.7239489555358887,
                            "process_2": 0.7190937995910645
                        },
                        "cpu_energy": {
                            "process_3": 0.0009051493529641447,
                            "process_0": 0.0009075158269024541,
                            "process_1": 0.0009238190551877779,
                            "process_2": 0.000926668992593477
                        },
                        "gpu_energy": {
                            "process_3": 0.003935055370264173,
                            "process_0": 0.003935055370264173,
                            "process_1": 0.003936659260435071,
                            "process_2": 0.003939579818327488
                        },
                        "ram_energy": {
                            "process_3": 6.142912100289851e-06,
                            "process_0": 5.865945980055019e-06,
                            "process_1": 6.2713508396135465e-06,
                            "process_2": 6.231549436596727e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.004846347635328607,
                            "process_0": 0.004848437143146684,
                            "process_1": 0.004866749666462464,
                            "process_2": 0.004872480360357562
                        },
                        "total_energy_joules": {
                            "process_3": 17446.851487182987,
                            "process_0": 17454.373715328064,
                            "process_1": 17520.29879926487,
                            "process_2": 17540.92929728722
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 76.93958967493079,
                        "ram_power_avg": 0.7143094539642334,
                        "cpu_energy_total": 0.003663153227647854,
                        "gpu_energy_total": 0.015746349819290906,
                        "ram_energy_total": 2.4511758356555144e-05,
                        "total_energy_kwh": 0.01943401480529532,
                        "total_energy_joules": 69962.45329906314
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.23418275414048975,
                        "joules_per_token": 4.270169268741647,
                        "flops_per_joule": 242272393.17491135,
                        "joules_per_flop": 4.127585429339605e-09
                    },
                    "per-process_emissions": [
                        0.001846216131678433,
                        0.0018470121296817294,
                        0.0018539882854388759,
                        0.0018561713932782132
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0457": {
            "setup": {
                "experiment_id": "0457",
                "date_time": "April 16, 2025 at 09:53:52 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0.6",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.6,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.328921667067334,
                        "average_latency_ms_per_batch": 3666.1152083834168,
                        "throughput_queries_per_sec": 4.364292743286494,
                        "throughput_tokens_per_sec": 558.6294711406713
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            83.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1980977152
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0457",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_1": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 78.3426634470924,
                            "process_0": 70.66290289749874,
                            "process_1": 80.73701854650145,
                            "process_2": 78.01577380863054
                        },
                        "ram_power": {
                            "process_3": 0.7226300239562988,
                            "process_0": 0.6915650367736816,
                            "process_1": 0.7239489555358887,
                            "process_2": 0.7190937995910645
                        },
                        "cpu_energy": {
                            "process_3": 0.0009051493529641447,
                            "process_0": 0.0009075158269024541,
                            "process_1": 0.0009238190551877779,
                            "process_2": 0.000926668992593477
                        },
                        "gpu_energy": {
                            "process_3": 0.003935055370264173,
                            "process_0": 0.003935055370264173,
                            "process_1": 0.003936659260435071,
                            "process_2": 0.003939579818327488
                        },
                        "ram_energy": {
                            "process_3": 6.142912100289851e-06,
                            "process_0": 5.865945980055019e-06,
                            "process_1": 6.2713508396135465e-06,
                            "process_2": 6.231549436596727e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.004846347635328607,
                            "process_0": 0.004848437143146684,
                            "process_1": 0.004866749666462464,
                            "process_2": 0.004872480360357562
                        },
                        "total_energy_joules": {
                            "process_3": 17446.851487182987,
                            "process_0": 17454.373715328064,
                            "process_1": 17520.29879926487,
                            "process_2": 17540.92929728722
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 76.93958967493079,
                        "ram_power_avg": 0.7143094539642334,
                        "cpu_energy_total": 0.003663153227647854,
                        "gpu_energy_total": 0.015746349819290906,
                        "ram_energy_total": 2.4511758356555144e-05,
                        "total_energy_kwh": 0.01943401480529532,
                        "total_energy_joules": 69962.45329906314
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.23418275414048975,
                        "joules_per_token": 4.270169268741647,
                        "flops_per_joule": 242272393.17491135,
                        "joules_per_flop": 4.127585429339605e-09
                    },
                    "per-process_emissions": [
                        0.001846216131678433,
                        0.0018470121296817294,
                        0.0018539882854388759,
                        0.0018561713932782132
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0459": {
            "setup": {
                "experiment_id": "0459",
                "date_time": "April 16, 2025 at 09:59:04 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_1",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 11147
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 228.46023768710438,
                        "average_latency_ms_per_batch": 1784.845606930503,
                        "throughput_queries_per_sec": 0.560272550251422,
                        "throughput_tokens_per_sec": 48.79186029416094
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609948672,
                        "gpu_max_memory_allocated_bytes": 1609948672,
                        "gpu_current_memory_reserved_bytes": 2143289344,
                        "gpu_max_memory_reserved_bytes": 2143289344
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1894768640
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0459",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 618.5369724119101,
                            "process_3": 281.0634771452549,
                            "process_2": 85.97048507517535,
                            "process_1": 288.5212071483749
                        },
                        "ram_power": {
                            "process_0": 0.6615514755249023,
                            "process_3": 0.6924562454223633,
                            "process_2": 0.699188232421875,
                            "process_1": 0.6960768699645996
                        },
                        "cpu_energy": {
                            "process_0": 0.006834305859825694,
                            "process_3": 0.006790802156325295,
                            "process_2": 0.006840965800261986,
                            "process_1": 0.006804216947097306
                        },
                        "gpu_energy": {
                            "process_0": 0.017174821239845706,
                            "process_3": 0.017104042294332267,
                            "process_2": 0.016879057669900277,
                            "process_1": 0.016958953289374357
                        },
                        "ram_energy": {
                            "process_0": 3.6917144858110423e-05,
                            "process_3": 3.839607401880737e-05,
                            "process_2": 3.916727568929606e-05,
                            "process_1": 3.8223420559360066e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.02404604424452952,
                            "process_3": 0.02393324052467638,
                            "process_2": 0.023759190745851547,
                            "process_1": 0.02380139365703103
                        },
                        "total_energy_joules": {
                            "process_0": 86565.75928030627,
                            "process_3": 86159.66588883496,
                            "process_2": 85533.08668506557,
                            "process_1": 85685.01716531171
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 318.52303544517883,
                        "ram_power_avg": 0.6873182058334351,
                        "cpu_energy_total": 0.02727029076351028,
                        "gpu_energy_total": 0.0681168744934526,
                        "ram_energy_total": 0.00015270391512557392,
                        "total_energy_kwh": 0.09553986917208848,
                        "total_energy_joules": 343943.5290195185
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.032409390087311155,
                        "joules_per_token": 30.85525513766202,
                        "flops_per_joule": 49281261.49508137,
                        "joules_per_flop": 2.029168835501111e-08
                    },
                    "per-process_emissions": [
                        0.009160340554953522,
                        0.009117367977875468,
                        0.009051063714632147,
                        0.009067140913645971
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0459": {
            "setup": {
                "experiment_id": "0459",
                "date_time": "April 16, 2025 at 09:59:04 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "batching_1",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 11147
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 228.46023768710438,
                        "average_latency_ms_per_batch": 1784.845606930503,
                        "throughput_queries_per_sec": 0.560272550251422,
                        "throughput_tokens_per_sec": 48.79186029416094
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609948672,
                        "gpu_max_memory_allocated_bytes": 1609948672,
                        "gpu_current_memory_reserved_bytes": 2143289344,
                        "gpu_max_memory_reserved_bytes": 2143289344
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1894768640
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0459",
                    "local_process_results": {
                        "cpu_power": {
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_0": 618.5369724119101,
                            "process_3": 281.0634771452549,
                            "process_2": 85.97048507517535,
                            "process_1": 288.5212071483749
                        },
                        "ram_power": {
                            "process_0": 0.6615514755249023,
                            "process_3": 0.6924562454223633,
                            "process_2": 0.699188232421875,
                            "process_1": 0.6960768699645996
                        },
                        "cpu_energy": {
                            "process_0": 0.006834305859825694,
                            "process_3": 0.006790802156325295,
                            "process_2": 0.006840965800261986,
                            "process_1": 0.006804216947097306
                        },
                        "gpu_energy": {
                            "process_0": 0.017174821239845706,
                            "process_3": 0.017104042294332267,
                            "process_2": 0.016879057669900277,
                            "process_1": 0.016958953289374357
                        },
                        "ram_energy": {
                            "process_0": 3.6917144858110423e-05,
                            "process_3": 3.839607401880737e-05,
                            "process_2": 3.916727568929606e-05,
                            "process_1": 3.8223420559360066e-05
                        },
                        "total_energy_kwh": {
                            "process_0": 0.02404604424452952,
                            "process_3": 0.02393324052467638,
                            "process_2": 0.023759190745851547,
                            "process_1": 0.02380139365703103
                        },
                        "total_energy_joules": {
                            "process_0": 86565.75928030627,
                            "process_3": 86159.66588883496,
                            "process_2": 85533.08668506557,
                            "process_1": 85685.01716531171
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 318.52303544517883,
                        "ram_power_avg": 0.6873182058334351,
                        "cpu_energy_total": 0.02727029076351028,
                        "gpu_energy_total": 0.0681168744934526,
                        "ram_energy_total": 0.00015270391512557392,
                        "total_energy_kwh": 0.09553986917208848,
                        "total_energy_joules": 343943.5290195185
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.032409390087311155,
                        "joules_per_token": 30.85525513766202,
                        "flops_per_joule": 49281261.49508137,
                        "joules_per_flop": 2.029168835501111e-08
                    },
                    "per-process_emissions": [
                        0.009160340554953522,
                        0.009117367977875468,
                        0.009051063714632147,
                        0.009067140913645971
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0460": {
            "setup": {
                "experiment_id": "0460",
                "date_time": "April 16, 2025 at 10:00:15 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 28.09462890692521,
                        "average_latency_ms_per_batch": 3511.828613365651,
                        "throughput_queries_per_sec": 4.556030991690676,
                        "throughput_tokens_per_sec": 583.1719669364065
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1815089152
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0460",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 914.808145756501,
                            "process_1": 27.621772559665764,
                            "process_0": 33.39090707142709,
                            "process_2": 35.77380319931805
                        },
                        "ram_power": {
                            "process_3": 0.6660361289978027,
                            "process_1": 0.6728239059448243,
                            "process_0": 0.6336264610290527,
                            "process_2": 0.6774301528930664
                        },
                        "cpu_energy": {
                            "process_3": 0.000651104754253538,
                            "process_1": 0.0007032603271236442,
                            "process_0": 0.0006963172895902973,
                            "process_2": 0.000747117916969728
                        },
                        "gpu_energy": {
                            "process_3": 0.0037363504890750576,
                            "process_1": 0.0037482860541793173,
                            "process_0": 0.0037482860541793173,
                            "process_2": 0.003746953830892963
                        },
                        "ram_energy": {
                            "process_3": 4.3012063499291575e-06,
                            "process_1": 4.5622469442705425e-06,
                            "process_0": 4.277447910852756e-06,
                            "process_2": 4.804450804669789e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.004391756449678524,
                            "process_1": 0.0044561086282472314,
                            "process_0": 0.0044488807916804665,
                            "process_2": 0.004498876198667361
                        },
                        "total_energy_joules": {
                            "process_3": 15810.323218842685,
                            "process_1": 16041.991061690032,
                            "process_0": 16015.97085004968,
                            "process_2": 16195.9543152025
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 252.89865714672797,
                        "ram_power_avg": 0.6624791622161865,
                        "cpu_energy_total": 0.002797800287937207,
                        "gpu_energy_total": 0.014979876428326655,
                        "ram_energy_total": 1.7945352009722246e-05,
                        "total_energy_kwh": 0.017795622068273585,
                        "total_energy_joules": 64064.23944578489
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2557432998773856,
                        "joules_per_token": 3.9101708646108944,
                        "flops_per_joule": 264577729.16349238,
                        "joules_per_flop": 3.779607615356253e-09
                    },
                    "per-process_emissions": [
                        0.0016730396195050337,
                        0.0016975545819307828,
                        0.0016948011375906738,
                        0.0017138468878823312
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0460": {
            "setup": {
                "experiment_id": "0460",
                "date_time": "April 16, 2025 at 10:00:15 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_200_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 200,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 28.09462890692521,
                        "average_latency_ms_per_batch": 3511.828613365651,
                        "throughput_queries_per_sec": 4.556030991690676,
                        "throughput_tokens_per_sec": 583.1719669364065
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.6,
                        "cpu_memory_usage_bytes": 1815089152
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0460",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_3": 914.808145756501,
                            "process_1": 27.621772559665764,
                            "process_0": 33.39090707142709,
                            "process_2": 35.77380319931805
                        },
                        "ram_power": {
                            "process_3": 0.6660361289978027,
                            "process_1": 0.6728239059448243,
                            "process_0": 0.6336264610290527,
                            "process_2": 0.6774301528930664
                        },
                        "cpu_energy": {
                            "process_3": 0.000651104754253538,
                            "process_1": 0.0007032603271236442,
                            "process_0": 0.0006963172895902973,
                            "process_2": 0.000747117916969728
                        },
                        "gpu_energy": {
                            "process_3": 0.0037363504890750576,
                            "process_1": 0.0037482860541793173,
                            "process_0": 0.0037482860541793173,
                            "process_2": 0.003746953830892963
                        },
                        "ram_energy": {
                            "process_3": 4.3012063499291575e-06,
                            "process_1": 4.5622469442705425e-06,
                            "process_0": 4.277447910852756e-06,
                            "process_2": 4.804450804669789e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.004391756449678524,
                            "process_1": 0.0044561086282472314,
                            "process_0": 0.0044488807916804665,
                            "process_2": 0.004498876198667361
                        },
                        "total_energy_joules": {
                            "process_3": 15810.323218842685,
                            "process_1": 16041.991061690032,
                            "process_0": 16015.97085004968,
                            "process_2": 16195.9543152025
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 252.89865714672797,
                        "ram_power_avg": 0.6624791622161865,
                        "cpu_energy_total": 0.002797800287937207,
                        "gpu_energy_total": 0.014979876428326655,
                        "ram_energy_total": 1.7945352009722246e-05,
                        "total_energy_kwh": 0.017795622068273585,
                        "total_energy_joules": 64064.23944578489
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2557432998773856,
                        "joules_per_token": 3.9101708646108944,
                        "flops_per_joule": 264577729.16349238,
                        "joules_per_flop": 3.779607615356253e-09
                    },
                    "per-process_emissions": [
                        0.0016730396195050337,
                        0.0016975545819307828,
                        0.0016948011375906738,
                        0.0017138468878823312
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0461": {
            "setup": {
                "experiment_id": "0461",
                "date_time": "April 16, 2025 at 10:01:26 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.30379613302648,
                        "average_latency_ms_per_batch": 3662.97451662831,
                        "throughput_queries_per_sec": 4.368034756279894,
                        "throughput_tokens_per_sec": 559.1084488038264
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            97.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.0,
                        "cpu_memory_usage_bytes": 1973387264
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0461",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 415.3080968807427,
                            "process_1": 0.0,
                            "process_0": 469.52056450315615,
                            "process_3": 351.52979913108123
                        },
                        "ram_power": {
                            "process_2": 0.7280302047729492,
                            "process_1": 0.7240090370178223,
                            "process_0": 0.6890087127685548,
                            "process_3": 0.7210292816162109
                        },
                        "cpu_energy": {
                            "process_2": 0.0007392948794986296,
                            "process_1": 0.0006781019314021249,
                            "process_0": 0.000687637390978125,
                            "process_3": 0.0007085292273768572
                        },
                        "gpu_energy": {
                            "process_2": 0.003880079492949662,
                            "process_1": 0.003848170578534038,
                            "process_0": 0.0038697214291083526,
                            "process_3": 0.0038715030971996356
                        },
                        "ram_energy": {
                            "process_2": 5.199082848351798e-06,
                            "process_1": 5.156357434554002e-06,
                            "process_0": 4.90622275707511e-06,
                            "process_3": 4.928041868322197e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.004624573455296643,
                            "process_1": 0.0045314288673707175,
                            "process_0": 0.004562265042843554,
                            "process_3": 0.004584960366444814
                        },
                        "total_energy_joules": {
                            "process_2": 16648.464439067917,
                            "process_1": 16313.143922534582,
                            "process_0": 16424.154154236792,
                            "process_3": 16505.85731920133
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 309.08961512874504,
                        "ram_power_avg": 0.7155193090438843,
                        "cpu_energy_total": 0.0028135634292557367,
                        "gpu_energy_total": 0.015469474597791688,
                        "ram_energy_total": 2.0189704908303108e-05,
                        "total_energy_kwh": 0.01830322773195573,
                        "total_energy_joules": 65891.61983504062
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.24865073951766964,
                        "joules_per_token": 4.021705312197303,
                        "flops_per_joule": 257240162.4908627,
                        "joules_per_flop": 3.8874178523173675e-09
                    },
                    "per-process_emissions": [
                        0.0017617312577952562,
                        0.0017262478270248748,
                        0.001737994868071252,
                        0.0017466406515971517
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0461": {
            "setup": {
                "experiment_id": "0461",
                "date_time": "April 16, 2025 at 10:01:26 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_100_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 100,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.30379613302648,
                        "average_latency_ms_per_batch": 3662.97451662831,
                        "throughput_queries_per_sec": 4.368034756279894,
                        "throughput_tokens_per_sec": 559.1084488038264
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            97.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.0,
                        "cpu_memory_usage_bytes": 1973387264
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0461",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 415.3080968807427,
                            "process_1": 0.0,
                            "process_0": 469.52056450315615,
                            "process_3": 351.52979913108123
                        },
                        "ram_power": {
                            "process_2": 0.7280302047729492,
                            "process_1": 0.7240090370178223,
                            "process_0": 0.6890087127685548,
                            "process_3": 0.7210292816162109
                        },
                        "cpu_energy": {
                            "process_2": 0.0007392948794986296,
                            "process_1": 0.0006781019314021249,
                            "process_0": 0.000687637390978125,
                            "process_3": 0.0007085292273768572
                        },
                        "gpu_energy": {
                            "process_2": 0.003880079492949662,
                            "process_1": 0.003848170578534038,
                            "process_0": 0.0038697214291083526,
                            "process_3": 0.0038715030971996356
                        },
                        "ram_energy": {
                            "process_2": 5.199082848351798e-06,
                            "process_1": 5.156357434554002e-06,
                            "process_0": 4.90622275707511e-06,
                            "process_3": 4.928041868322197e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.004624573455296643,
                            "process_1": 0.0045314288673707175,
                            "process_0": 0.004562265042843554,
                            "process_3": 0.004584960366444814
                        },
                        "total_energy_joules": {
                            "process_2": 16648.464439067917,
                            "process_1": 16313.143922534582,
                            "process_0": 16424.154154236792,
                            "process_3": 16505.85731920133
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 309.08961512874504,
                        "ram_power_avg": 0.7155193090438843,
                        "cpu_energy_total": 0.0028135634292557367,
                        "gpu_energy_total": 0.015469474597791688,
                        "ram_energy_total": 2.0189704908303108e-05,
                        "total_energy_kwh": 0.01830322773195573,
                        "total_energy_joules": 65891.61983504062
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.24865073951766964,
                        "joules_per_token": 4.021705312197303,
                        "flops_per_joule": 257240162.4908627,
                        "joules_per_flop": 3.8874178523173675e-09
                    },
                    "per-process_emissions": [
                        0.0017617312577952562,
                        0.0017262478270248748,
                        0.001737994868071252,
                        0.0017466406515971517
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0462": {
            "setup": {
                "experiment_id": "0462",
                "date_time": "April 16, 2025 at 10:02:38 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.17176663596183,
                        "average_latency_ms_per_batch": 3646.470829495229,
                        "throughput_queries_per_sec": 4.387804194285804,
                        "throughput_tokens_per_sec": 561.638936868583
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            95.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1954217984
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0462",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 443.85478659103114,
                            "process_1": 470.04036491313,
                            "process_3": 26.52130275902129,
                            "process_0": 482.79391558704987
                        },
                        "ram_power": {
                            "process_2": 0.7109026908874512,
                            "process_1": 0.7082433700561523,
                            "process_3": 0.7088341712951661,
                            "process_0": 0.6822195053100586
                        },
                        "cpu_energy": {
                            "process_2": 0.0007703578665623353,
                            "process_1": 0.0007623563351571648,
                            "process_3": 0.0008376879899988126,
                            "process_0": 0.0007702733633068419
                        },
                        "gpu_energy": {
                            "process_2": 0.003866988649144254,
                            "process_1": 0.0038563411406240533,
                            "process_3": 0.003973288178626078,
                            "process_0": 0.0038268894503961093
                        },
                        "ram_energy": {
                            "process_2": 5.1650292699263126e-06,
                            "process_1": 5.139679896171003e-06,
                            "process_3": 5.229064090274442e-06,
                            "process_0": 5.001484504453773e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.004642511544976516,
                            "process_1": 0.00462383715567739,
                            "process_3": 0.004816205232715164,
                            "process_0": 0.004602164298207405
                        },
                        "total_energy_joules": {
                            "process_2": 16713.04156191546,
                            "process_1": 16645.813760438603,
                            "process_3": 17338.33883777459,
                            "process_0": 16567.79147354666
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 355.8025924625581,
                        "ram_power_avg": 0.702549934387207,
                        "cpu_energy_total": 0.0031406755550251546,
                        "gpu_energy_total": 0.015523507418790494,
                        "ram_energy_total": 2.053525776082553e-05,
                        "total_energy_kwh": 0.018684718231576476,
                        "total_energy_joules": 67264.98563367531
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.243573976053859,
                        "joules_per_token": 4.105528908305378,
                        "flops_per_joule": 251988026.66013244,
                        "joules_per_flop": 3.968442521869283e-09
                    },
                    "per-process_emissions": [
                        0.001768564773058804,
                        0.0017614507644553017,
                        0.0018347333834028417,
                        0.0017531944894021109
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0462": {
            "setup": {
                "experiment_id": "0462",
                "date_time": "April 16, 2025 at 10:02:38 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.8_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.8,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.17176663596183,
                        "average_latency_ms_per_batch": 3646.470829495229,
                        "throughput_queries_per_sec": 4.387804194285804,
                        "throughput_tokens_per_sec": 561.638936868583
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            95.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1954217984
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0462",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_2": 443.85478659103114,
                            "process_1": 470.04036491313,
                            "process_3": 26.52130275902129,
                            "process_0": 482.79391558704987
                        },
                        "ram_power": {
                            "process_2": 0.7109026908874512,
                            "process_1": 0.7082433700561523,
                            "process_3": 0.7088341712951661,
                            "process_0": 0.6822195053100586
                        },
                        "cpu_energy": {
                            "process_2": 0.0007703578665623353,
                            "process_1": 0.0007623563351571648,
                            "process_3": 0.0008376879899988126,
                            "process_0": 0.0007702733633068419
                        },
                        "gpu_energy": {
                            "process_2": 0.003866988649144254,
                            "process_1": 0.0038563411406240533,
                            "process_3": 0.003973288178626078,
                            "process_0": 0.0038268894503961093
                        },
                        "ram_energy": {
                            "process_2": 5.1650292699263126e-06,
                            "process_1": 5.139679896171003e-06,
                            "process_3": 5.229064090274442e-06,
                            "process_0": 5.001484504453773e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.004642511544976516,
                            "process_1": 0.00462383715567739,
                            "process_3": 0.004816205232715164,
                            "process_0": 0.004602164298207405
                        },
                        "total_energy_joules": {
                            "process_2": 16713.04156191546,
                            "process_1": 16645.813760438603,
                            "process_3": 17338.33883777459,
                            "process_0": 16567.79147354666
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 355.8025924625581,
                        "ram_power_avg": 0.702549934387207,
                        "cpu_energy_total": 0.0031406755550251546,
                        "gpu_energy_total": 0.015523507418790494,
                        "ram_energy_total": 2.053525776082553e-05,
                        "total_energy_kwh": 0.018684718231576476,
                        "total_energy_joules": 67264.98563367531
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.243573976053859,
                        "joules_per_token": 4.105528908305378,
                        "flops_per_joule": 251988026.66013244,
                        "joules_per_flop": 3.968442521869283e-09
                    },
                    "per-process_emissions": [
                        0.001768564773058804,
                        0.0017614507644553017,
                        0.0018347333834028417,
                        0.0017531944894021109
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0463": {
            "setup": {
                "experiment_id": "0463",
                "date_time": "April 16, 2025 at 10:03:51 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 30.71089884196408,
                        "average_latency_ms_per_batch": 3838.86235524551,
                        "throughput_queries_per_sec": 4.167901456049142,
                        "throughput_tokens_per_sec": 533.4913863742902
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1939144704
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0463",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 476.3089847835034,
                            "process_2": 613.6088771557557,
                            "process_3": 518.8485817768586,
                            "process_0": 447.2764375710716
                        },
                        "ram_power": {
                            "process_1": 0.7043352127075195,
                            "process_2": 0.7042551040649414,
                            "process_3": 0.7036557197570801,
                            "process_0": 0.6769552230834961
                        },
                        "cpu_energy": {
                            "process_1": 0.0007701870351284015,
                            "process_2": 0.0007850867375236703,
                            "process_3": 0.0007596981654060074,
                            "process_0": 0.0008059175528160267
                        },
                        "gpu_energy": {
                            "process_1": 0.0038891903335747102,
                            "process_2": 0.003980848740234855,
                            "process_3": 0.003838409737394244,
                            "process_0": 0.003986912356197081
                        },
                        "ram_energy": {
                            "process_1": 5.17709914503042e-06,
                            "process_2": 5.256982953910937e-06,
                            "process_3": 5.087412434989377e-06,
                            "process_0": 5.15002912313296e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0046645544678481435,
                            "process_2": 0.004771192460712435,
                            "process_3": 0.004603195315235239,
                            "process_0": 0.0047979799381362405
                        },
                        "total_energy_joules": {
                            "process_1": 16792.396084253316,
                            "process_2": 17176.292858564764,
                            "process_3": 16571.503134846862,
                            "process_0": 17272.727777290467
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 514.0107203217974,
                        "ram_power_avg": 0.6973003149032593,
                        "cpu_energy_total": 0.003120889490874106,
                        "gpu_energy_total": 0.01569536116740089,
                        "ram_energy_total": 2.0671523657063694e-05,
                        "total_energy_kwh": 0.018836922181932055,
                        "total_energy_joules": 67812.91985495541
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.24160587739096953,
                        "joules_per_token": 4.138972159115931,
                        "flops_per_joule": 249951941.7451155,
                        "joules_per_flop": 4.000769079920708e-09
                    },
                    "per-process_emissions": [
                        0.0017769620245267504,
                        0.001817585767908402,
                        0.0017535872553388644,
                        0.001827790457433001
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0463": {
            "setup": {
                "experiment_id": "0463",
                "date_time": "April 16, 2025 at 10:03:51 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 30.71089884196408,
                        "average_latency_ms_per_batch": 3838.86235524551,
                        "throughput_queries_per_sec": 4.167901456049142,
                        "throughput_tokens_per_sec": 533.4913863742902
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1939144704
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0463",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 476.3089847835034,
                            "process_2": 613.6088771557557,
                            "process_3": 518.8485817768586,
                            "process_0": 447.2764375710716
                        },
                        "ram_power": {
                            "process_1": 0.7043352127075195,
                            "process_2": 0.7042551040649414,
                            "process_3": 0.7036557197570801,
                            "process_0": 0.6769552230834961
                        },
                        "cpu_energy": {
                            "process_1": 0.0007701870351284015,
                            "process_2": 0.0007850867375236703,
                            "process_3": 0.0007596981654060074,
                            "process_0": 0.0008059175528160267
                        },
                        "gpu_energy": {
                            "process_1": 0.0038891903335747102,
                            "process_2": 0.003980848740234855,
                            "process_3": 0.003838409737394244,
                            "process_0": 0.003986912356197081
                        },
                        "ram_energy": {
                            "process_1": 5.17709914503042e-06,
                            "process_2": 5.256982953910937e-06,
                            "process_3": 5.087412434989377e-06,
                            "process_0": 5.15002912313296e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.0046645544678481435,
                            "process_2": 0.004771192460712435,
                            "process_3": 0.004603195315235239,
                            "process_0": 0.0047979799381362405
                        },
                        "total_energy_joules": {
                            "process_1": 16792.396084253316,
                            "process_2": 17176.292858564764,
                            "process_3": 16571.503134846862,
                            "process_0": 17272.727777290467
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 514.0107203217974,
                        "ram_power_avg": 0.6973003149032593,
                        "cpu_energy_total": 0.003120889490874106,
                        "gpu_energy_total": 0.01569536116740089,
                        "ram_energy_total": 2.0671523657063694e-05,
                        "total_energy_kwh": 0.018836922181932055,
                        "total_energy_joules": 67812.91985495541
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.24160587739096953,
                        "joules_per_token": 4.138972159115931,
                        "flops_per_joule": 249951941.7451155,
                        "joules_per_flop": 4.000769079920708e-09
                    },
                    "per-process_emissions": [
                        0.0017769620245267504,
                        0.001817585767908402,
                        0.0017535872553388644,
                        0.001827790457433001
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0464": {
            "setup": {
                "experiment_id": "0464",
                "date_time": "April 16, 2025 at 10:05:05 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.520197120960802,
                        "average_latency_ms_per_batch": 3690.0246401201002,
                        "throughput_queries_per_sec": 4.3360144065269015,
                        "throughput_tokens_per_sec": 555.0098440354434
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            1.0,
                            100.0,
                            85.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1929502720
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0464",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 77.23675637920903,
                            "process_1": 61.7211826610488,
                            "process_0": 83.6423140606693,
                            "process_3": 74.65477759195964
                        },
                        "ram_power": {
                            "process_2": 0.7042407989501953,
                            "process_1": 0.7049603462219238,
                            "process_0": 0.6735877990722656,
                            "process_3": 0.7081475257873535
                        },
                        "cpu_energy": {
                            "process_2": 0.0009588426752488885,
                            "process_1": 0.0009519811325371848,
                            "process_0": 0.0009569471642844293,
                            "process_3": 0.0009860566309635031
                        },
                        "gpu_energy": {
                            "process_2": 0.003977726515509872,
                            "process_1": 0.003962953170357331,
                            "process_0": 0.003976609847950563,
                            "process_3": 0.003977769848879653
                        },
                        "ram_energy": {
                            "process_2": 5.9711800363732736e-06,
                            "process_1": 5.898786035729458e-06,
                            "process_0": 5.661122403484264e-06,
                            "process_3": 4.736594500022273e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.004942540370795132,
                            "process_1": 0.004920833088930245,
                            "process_0": 0.004939218134638476,
                            "process_3": 0.004968563074343178
                        },
                        "total_energy_joules": {
                            "process_2": 17793.145334862475,
                            "process_1": 17714.99912014888,
                            "process_0": 17781.185284698513,
                            "process_3": 17886.82706763544
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 74.3137576732217,
                        "ram_power_avg": 0.6977341175079346,
                        "cpu_energy_total": 0.0038538276030340058,
                        "gpu_energy_total": 0.01589505938269742,
                        "ram_energy_total": 2.226768297560927e-05,
                        "total_energy_kwh": 0.019771154668707033,
                        "total_energy_joules": 71176.15680734531
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.23018944454035467,
                        "joules_per_token": 4.344247852010822,
                        "flops_per_joule": 238141138.1768056,
                        "joules_per_flop": 4.199190478620958e-09
                    },
                    "per-process_emissions": [
                        0.0018828607542544055,
                        0.0018745913652279767,
                        0.0018815951483905275,
                        0.0018927741031710336
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0464": {
            "setup": {
                "experiment_id": "0464",
                "date_time": "April 16, 2025 at 10:05:05 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0.4",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.4,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.520197120960802,
                        "average_latency_ms_per_batch": 3690.0246401201002,
                        "throughput_queries_per_sec": 4.3360144065269015,
                        "throughput_tokens_per_sec": 555.0098440354434
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            1.0,
                            100.0,
                            85.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1929502720
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0464",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 77.23675637920903,
                            "process_1": 61.7211826610488,
                            "process_0": 83.6423140606693,
                            "process_3": 74.65477759195964
                        },
                        "ram_power": {
                            "process_2": 0.7042407989501953,
                            "process_1": 0.7049603462219238,
                            "process_0": 0.6735877990722656,
                            "process_3": 0.7081475257873535
                        },
                        "cpu_energy": {
                            "process_2": 0.0009588426752488885,
                            "process_1": 0.0009519811325371848,
                            "process_0": 0.0009569471642844293,
                            "process_3": 0.0009860566309635031
                        },
                        "gpu_energy": {
                            "process_2": 0.003977726515509872,
                            "process_1": 0.003962953170357331,
                            "process_0": 0.003976609847950563,
                            "process_3": 0.003977769848879653
                        },
                        "ram_energy": {
                            "process_2": 5.9711800363732736e-06,
                            "process_1": 5.898786035729458e-06,
                            "process_0": 5.661122403484264e-06,
                            "process_3": 4.736594500022273e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.004942540370795132,
                            "process_1": 0.004920833088930245,
                            "process_0": 0.004939218134638476,
                            "process_3": 0.004968563074343178
                        },
                        "total_energy_joules": {
                            "process_2": 17793.145334862475,
                            "process_1": 17714.99912014888,
                            "process_0": 17781.185284698513,
                            "process_3": 17886.82706763544
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 74.3137576732217,
                        "ram_power_avg": 0.6977341175079346,
                        "cpu_energy_total": 0.0038538276030340058,
                        "gpu_energy_total": 0.01589505938269742,
                        "ram_energy_total": 2.226768297560927e-05,
                        "total_energy_kwh": 0.019771154668707033,
                        "total_energy_joules": 71176.15680734531
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.23018944454035467,
                        "joules_per_token": 4.344247852010822,
                        "flops_per_joule": 238141138.1768056,
                        "joules_per_flop": 4.199190478620958e-09
                    },
                    "per-process_emissions": [
                        0.0018828607542544055,
                        0.0018745913652279767,
                        0.0018815951483905275,
                        0.0018927741031710336
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0465": {
            "setup": {
                "experiment_id": "0465",
                "date_time": "April 16, 2025 at 10:06:17 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.30849027406657,
                        "average_latency_ms_per_batch": 3663.5612842583214,
                        "throughput_queries_per_sec": 4.367335157937493,
                        "throughput_tokens_per_sec": 559.0189002159991
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            91.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1940324352
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0465",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 699.2141291557597,
                            "process_1": 43.087610512329576,
                            "process_0": 68.9115609963859,
                            "process_3": 74.79588888336274
                        },
                        "ram_power": {
                            "process_2": 0.7021880149841309,
                            "process_1": 0.7042036056518555,
                            "process_0": 0.6774616241455078,
                            "process_3": 0.7074551582336426
                        },
                        "cpu_energy": {
                            "process_2": 0.000852772029780681,
                            "process_1": 0.0008795877125376137,
                            "process_0": 0.0009104481243211922,
                            "process_3": 0.0008947495836309829
                        },
                        "gpu_energy": {
                            "process_2": 0.003964363727043896,
                            "process_1": 0.003951601772389068,
                            "process_0": 0.003956392887333848,
                            "process_3": 0.003959347611920627
                        },
                        "ram_energy": {
                            "process_2": 5.022281451478466e-06,
                            "process_1": 5.820666865404446e-06,
                            "process_0": 5.4302695665528875e-06,
                            "process_3": 5.9349072435542635e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.004822158038276056,
                            "process_1": 0.004837010151792085,
                            "process_0": 0.004872271281221592,
                            "process_3": 0.004860032102795166
                        },
                        "total_energy_joules": {
                            "process_2": 17359.7689377938,
                            "process_1": 17413.236546451506,
                            "process_0": 17540.176612397732,
                            "process_3": 17496.115570062597
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 221.50229738695947,
                        "ram_power_avg": 0.6978271007537842,
                        "cpu_energy_total": 0.0035375574502704695,
                        "gpu_energy_total": 0.01583170599868744,
                        "ram_energy_total": 2.2208125126990063e-05,
                        "total_energy_kwh": 0.019391471574084898,
                        "total_energy_joules": 69809.29766670564
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.23469653108706282,
                        "joules_per_token": 4.260821390790139,
                        "flops_per_joule": 242803918.0980903,
                        "joules_per_flop": 4.118549683353999e-09
                    },
                    "per-process_emissions": [
                        0.0018370011046812637,
                        0.0018426590173251948,
                        0.0018560917445813656,
                        0.0018514292295598186
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0465": {
            "setup": {
                "experiment_id": "0465",
                "date_time": "April 16, 2025 at 10:06:17 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_greedy_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "greedy"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.30849027406657,
                        "average_latency_ms_per_batch": 3663.5612842583214,
                        "throughput_queries_per_sec": 4.367335157937493,
                        "throughput_tokens_per_sec": 559.0189002159991
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            91.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1940324352
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0465",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_2": 699.2141291557597,
                            "process_1": 43.087610512329576,
                            "process_0": 68.9115609963859,
                            "process_3": 74.79588888336274
                        },
                        "ram_power": {
                            "process_2": 0.7021880149841309,
                            "process_1": 0.7042036056518555,
                            "process_0": 0.6774616241455078,
                            "process_3": 0.7074551582336426
                        },
                        "cpu_energy": {
                            "process_2": 0.000852772029780681,
                            "process_1": 0.0008795877125376137,
                            "process_0": 0.0009104481243211922,
                            "process_3": 0.0008947495836309829
                        },
                        "gpu_energy": {
                            "process_2": 0.003964363727043896,
                            "process_1": 0.003951601772389068,
                            "process_0": 0.003956392887333848,
                            "process_3": 0.003959347611920627
                        },
                        "ram_energy": {
                            "process_2": 5.022281451478466e-06,
                            "process_1": 5.820666865404446e-06,
                            "process_0": 5.4302695665528875e-06,
                            "process_3": 5.9349072435542635e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.004822158038276056,
                            "process_1": 0.004837010151792085,
                            "process_0": 0.004872271281221592,
                            "process_3": 0.004860032102795166
                        },
                        "total_energy_joules": {
                            "process_2": 17359.7689377938,
                            "process_1": 17413.236546451506,
                            "process_0": 17540.176612397732,
                            "process_3": 17496.115570062597
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 221.50229738695947,
                        "ram_power_avg": 0.6978271007537842,
                        "cpu_energy_total": 0.0035375574502704695,
                        "gpu_energy_total": 0.01583170599868744,
                        "ram_energy_total": 2.2208125126990063e-05,
                        "total_energy_kwh": 0.019391471574084898,
                        "total_energy_joules": 69809.29766670564
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.23469653108706282,
                        "joules_per_token": 4.260821390790139,
                        "flops_per_joule": 242803918.0980903,
                        "joules_per_flop": 4.118549683353999e-09
                    },
                    "per-process_emissions": [
                        0.0018370011046812637,
                        0.0018426590173251948,
                        0.0018560917445813656,
                        0.0018514292295598186
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0466": {
            "setup": {
                "experiment_id": "0466",
                "date_time": "April 16, 2025 at 10:07:32 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.532365761115216,
                        "average_latency_ms_per_batch": 3691.545720139402,
                        "throughput_queries_per_sec": 4.334227776920449,
                        "throughput_tokens_per_sec": 554.7811554458175
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.9,
                        "cpu_memory_usage_bytes": 1982394368
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0466",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 400.80742892447574,
                            "process_0": 478.02699099025034,
                            "process_3": 410.38513307643836,
                            "process_1": 448.9022741597065
                        },
                        "ram_power": {
                            "process_2": 0.7305750846862793,
                            "process_0": 0.6920599937438965,
                            "process_3": 0.7216300964355469,
                            "process_1": 0.718996524810791
                        },
                        "cpu_energy": {
                            "process_2": 0.0006806663463212317,
                            "process_0": 0.0006524254510950414,
                            "process_3": 0.0006718427048399463,
                            "process_1": 0.000653064845027984
                        },
                        "gpu_energy": {
                            "process_2": 0.003934576203215023,
                            "process_0": 0.003906322013945918,
                            "process_3": 0.003918823690611362,
                            "process_1": 0.003880809493532311
                        },
                        "ram_energy": {
                            "process_2": 4.8213632005115355e-06,
                            "process_0": 4.8145036536310165e-06,
                            "process_3": 5.083937985588299e-06,
                            "process_1": 4.9897768192202836e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.004620063912736767,
                            "process_0": 0.0045635619686945905,
                            "process_3": 0.004595750333436896,
                            "process_1": 0.004538864115379515
                        },
                        "total_energy_joules": {
                            "process_2": 16632.23008585236,
                            "process_0": 16428.823087300527,
                            "process_3": 16544.701200372827,
                            "process_1": 16339.910815366255
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 434.53045678771775,
                        "ram_power_avg": 0.7158154249191284,
                        "cpu_energy_total": 0.002657999347284203,
                        "gpu_energy_total": 0.015640531401304614,
                        "ram_energy_total": 1.9709581658951134e-05,
                        "total_energy_kwh": 0.01831824033024777,
                        "total_energy_joules": 65945.66518889197
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.24844695937284678,
                        "joules_per_token": 4.025003978814207,
                        "flops_per_joule": 257029342.9386332,
                        "joules_per_flop": 3.890606374225351e-09
                    },
                    "per-process_emissions": [
                        0.0017600133475570714,
                        0.0017384889319742042,
                        0.0017507510895227857,
                        0.0017290802847538262
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0466": {
            "setup": {
                "experiment_id": "0466",
                "date_time": "April 16, 2025 at 10:07:32 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_1.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.2,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.532365761115216,
                        "average_latency_ms_per_batch": 3691.545720139402,
                        "throughput_queries_per_sec": 4.334227776920449,
                        "throughput_tokens_per_sec": 554.7811554458175
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.9,
                        "cpu_memory_usage_bytes": 1982394368
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0466",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 400.80742892447574,
                            "process_0": 478.02699099025034,
                            "process_3": 410.38513307643836,
                            "process_1": 448.9022741597065
                        },
                        "ram_power": {
                            "process_2": 0.7305750846862793,
                            "process_0": 0.6920599937438965,
                            "process_3": 0.7216300964355469,
                            "process_1": 0.718996524810791
                        },
                        "cpu_energy": {
                            "process_2": 0.0006806663463212317,
                            "process_0": 0.0006524254510950414,
                            "process_3": 0.0006718427048399463,
                            "process_1": 0.000653064845027984
                        },
                        "gpu_energy": {
                            "process_2": 0.003934576203215023,
                            "process_0": 0.003906322013945918,
                            "process_3": 0.003918823690611362,
                            "process_1": 0.003880809493532311
                        },
                        "ram_energy": {
                            "process_2": 4.8213632005115355e-06,
                            "process_0": 4.8145036536310165e-06,
                            "process_3": 5.083937985588299e-06,
                            "process_1": 4.9897768192202836e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.004620063912736767,
                            "process_0": 0.0045635619686945905,
                            "process_3": 0.004595750333436896,
                            "process_1": 0.004538864115379515
                        },
                        "total_energy_joules": {
                            "process_2": 16632.23008585236,
                            "process_0": 16428.823087300527,
                            "process_3": 16544.701200372827,
                            "process_1": 16339.910815366255
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 434.53045678771775,
                        "ram_power_avg": 0.7158154249191284,
                        "cpu_energy_total": 0.002657999347284203,
                        "gpu_energy_total": 0.015640531401304614,
                        "ram_energy_total": 1.9709581658951134e-05,
                        "total_energy_kwh": 0.01831824033024777,
                        "total_energy_joules": 65945.66518889197
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.24844695937284678,
                        "joules_per_token": 4.025003978814207,
                        "flops_per_joule": 257029342.9386332,
                        "joules_per_flop": 3.890606374225351e-09
                    },
                    "per-process_emissions": [
                        0.0017600133475570714,
                        0.0017384889319742042,
                        0.0017507510895227857,
                        0.0017290802847538262
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0467": {
            "setup": {
                "experiment_id": "0467",
                "date_time": "April 16, 2025 at 10:08:43 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.154222639102954,
                        "average_latency_ms_per_batch": 3644.2778298878693,
                        "throughput_queries_per_sec": 4.390444622190703,
                        "throughput_tokens_per_sec": 561.97691164041
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.4,
                        "cpu_memory_usage_bytes": 1925251072
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0467",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 354.94355331581215,
                            "process_0": 775.5740300781088,
                            "process_3": 430.4037031729032,
                            "process_1": 0.0
                        },
                        "ram_power": {
                            "process_2": 0.7068886756896973,
                            "process_0": 0.6721973419189453,
                            "process_3": 0.7050375938415528,
                            "process_1": 0.7086582183837891
                        },
                        "cpu_energy": {
                            "process_2": 0.0006744479529352248,
                            "process_0": 0.0006686690579990683,
                            "process_3": 0.0006785253689340606,
                            "process_1": 0.0006667251773687894
                        },
                        "gpu_energy": {
                            "process_2": 0.003949654826388205,
                            "process_0": 0.003913251186157041,
                            "process_3": 0.00392311980516169,
                            "process_1": 0.00390362840067926
                        },
                        "ram_energy": {
                            "process_2": 5.025826018740013e-06,
                            "process_0": 4.7075922416059265e-06,
                            "process_3": 4.953028285281093e-06,
                            "process_1": 5.331798670391966e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.004629128605342169,
                            "process_0": 0.004586627836397715,
                            "process_3": 0.004606598202381032,
                            "process_1": 0.004575685376718441
                        },
                        "total_energy_joules": {
                            "process_2": 16664.86297923181,
                            "process_0": 16511.860211031773,
                            "process_3": 16583.753528571717,
                            "process_1": 16472.467356186386
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 390.23032164170604,
                        "ram_power_avg": 0.6981954574584961,
                        "cpu_energy_total": 0.002688367557237143,
                        "gpu_energy_total": 0.015689654218386195,
                        "ram_energy_total": 2.0018245216019e-05,
                        "total_energy_kwh": 0.018398040020839355,
                        "total_energy_joules": 66232.94407502169
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2473693451017659,
                        "joules_per_token": 4.0425380905164605,
                        "flops_per_joule": 255914503.42223746,
                        "joules_per_flop": 3.907555010081175e-09
                    },
                    "per-process_emissions": [
                        0.0017634665422050994,
                        0.0017472758742757097,
                        0.0017548835851970543,
                        0.00174310734426089
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0467": {
            "setup": {
                "experiment_id": "0467",
                "date_time": "April 16, 2025 at 10:08:43 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.98_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.98,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.154222639102954,
                        "average_latency_ms_per_batch": 3644.2778298878693,
                        "throughput_queries_per_sec": 4.390444622190703,
                        "throughput_tokens_per_sec": 561.97691164041
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 4.4,
                        "cpu_memory_usage_bytes": 1925251072
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0467",
                    "local_process_results": {
                        "cpu_power": {
                            "process_2": 112.5,
                            "process_0": 112.5,
                            "process_3": 112.5,
                            "process_1": 112.5
                        },
                        "gpu_power": {
                            "process_2": 354.94355331581215,
                            "process_0": 775.5740300781088,
                            "process_3": 430.4037031729032,
                            "process_1": 0.0
                        },
                        "ram_power": {
                            "process_2": 0.7068886756896973,
                            "process_0": 0.6721973419189453,
                            "process_3": 0.7050375938415528,
                            "process_1": 0.7086582183837891
                        },
                        "cpu_energy": {
                            "process_2": 0.0006744479529352248,
                            "process_0": 0.0006686690579990683,
                            "process_3": 0.0006785253689340606,
                            "process_1": 0.0006667251773687894
                        },
                        "gpu_energy": {
                            "process_2": 0.003949654826388205,
                            "process_0": 0.003913251186157041,
                            "process_3": 0.00392311980516169,
                            "process_1": 0.00390362840067926
                        },
                        "ram_energy": {
                            "process_2": 5.025826018740013e-06,
                            "process_0": 4.7075922416059265e-06,
                            "process_3": 4.953028285281093e-06,
                            "process_1": 5.331798670391966e-06
                        },
                        "total_energy_kwh": {
                            "process_2": 0.004629128605342169,
                            "process_0": 0.004586627836397715,
                            "process_3": 0.004606598202381032,
                            "process_1": 0.004575685376718441
                        },
                        "total_energy_joules": {
                            "process_2": 16664.86297923181,
                            "process_0": 16511.860211031773,
                            "process_3": 16583.753528571717,
                            "process_1": 16472.467356186386
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 390.23032164170604,
                        "ram_power_avg": 0.6981954574584961,
                        "cpu_energy_total": 0.002688367557237143,
                        "gpu_energy_total": 0.015689654218386195,
                        "ram_energy_total": 2.0018245216019e-05,
                        "total_energy_kwh": 0.018398040020839355,
                        "total_energy_joules": 66232.94407502169
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2473693451017659,
                        "joules_per_token": 4.0425380905164605,
                        "flops_per_joule": 255914503.42223746,
                        "joules_per_flop": 3.907555010081175e-09
                    },
                    "per-process_emissions": [
                        0.0017634665422050994,
                        0.0017472758742757097,
                        0.0017548835851970543,
                        0.00174310734426089
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0470": {
            "setup": {
                "experiment_id": "0470",
                "date_time": "April 16, 2025 at 10:11:17 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.236402759037446,
                        "average_latency_ms_per_batch": 3654.550344879681,
                        "throughput_queries_per_sec": 4.378103594171931,
                        "throughput_tokens_per_sec": 560.3972600540071
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            16.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 1966923776
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0470",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 443.95288640820047,
                            "process_3": 420.6641865054338,
                            "process_0": 604.4449207325358,
                            "process_2": 428.21144051200054
                        },
                        "ram_power": {
                            "process_1": 0.7208633422851562,
                            "process_3": 0.7209577560424805,
                            "process_0": 0.6866569519042969,
                            "process_2": 0.7190694808959961
                        },
                        "cpu_energy": {
                            "process_1": 0.0006635164261224418,
                            "process_3": 0.0006889543399047399,
                            "process_0": 0.0006537670753768908,
                            "process_2": 0.0006858342575633288
                        },
                        "gpu_energy": {
                            "process_1": 0.0038731928207740296,
                            "process_3": 0.0038813631050880204,
                            "process_0": 0.003849183912677745,
                            "process_2": 0.0038767172680378437
                        },
                        "ram_energy": {
                            "process_1": 5.051211306360498e-06,
                            "process_3": 4.79159582625632e-06,
                            "process_0": 4.752247934539705e-06,
                            "process_2": 4.790385857180775e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.004541760458202832,
                            "process_3": 0.004575109040819017,
                            "process_0": 0.0045077032359891744,
                            "process_2": 0.004567341911458351
                        },
                        "total_energy_joules": {
                            "process_1": 16350.337649530196,
                            "process_3": 16470.392546948464,
                            "process_0": 16227.731649561028,
                            "process_2": 16442.43088125006
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 474.31835853954266,
                        "ram_power_avg": 0.7118868827819824,
                        "cpu_energy_total": 0.0026920720989674013,
                        "gpu_energy_total": 0.015480457106577639,
                        "ram_energy_total": 1.9385440924337296e-05,
                        "total_energy_kwh": 0.018191914646469375,
                        "total_energy_joules": 65490.89272728975
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.25017218910459077,
                        "joules_per_token": 3.9972468705621185,
                        "flops_per_joule": 258814169.22705996,
                        "joules_per_flop": 3.863776094587353e-09
                    },
                    "per-process_emissions": [
                        0.0017301836465523689,
                        0.0017428877891000048,
                        0.0017172095477500761,
                        0.0017399289011700587
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0470": {
            "setup": {
                "experiment_id": "0470",
                "date_time": "April 16, 2025 at 10:11:17 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_50_decoder_temperature_0.2",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.2,
                    "decoder_top_k": 50,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.236402759037446,
                        "average_latency_ms_per_batch": 3654.550344879681,
                        "throughput_queries_per_sec": 4.378103594171931,
                        "throughput_tokens_per_sec": 560.3972600540071
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            16.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 1966923776
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0470",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5
                        },
                        "gpu_power": {
                            "process_1": 443.95288640820047,
                            "process_3": 420.6641865054338,
                            "process_0": 604.4449207325358,
                            "process_2": 428.21144051200054
                        },
                        "ram_power": {
                            "process_1": 0.7208633422851562,
                            "process_3": 0.7209577560424805,
                            "process_0": 0.6866569519042969,
                            "process_2": 0.7190694808959961
                        },
                        "cpu_energy": {
                            "process_1": 0.0006635164261224418,
                            "process_3": 0.0006889543399047399,
                            "process_0": 0.0006537670753768908,
                            "process_2": 0.0006858342575633288
                        },
                        "gpu_energy": {
                            "process_1": 0.0038731928207740296,
                            "process_3": 0.0038813631050880204,
                            "process_0": 0.003849183912677745,
                            "process_2": 0.0038767172680378437
                        },
                        "ram_energy": {
                            "process_1": 5.051211306360498e-06,
                            "process_3": 4.79159582625632e-06,
                            "process_0": 4.752247934539705e-06,
                            "process_2": 4.790385857180775e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.004541760458202832,
                            "process_3": 0.004575109040819017,
                            "process_0": 0.0045077032359891744,
                            "process_2": 0.004567341911458351
                        },
                        "total_energy_joules": {
                            "process_1": 16350.337649530196,
                            "process_3": 16470.392546948464,
                            "process_0": 16227.731649561028,
                            "process_2": 16442.43088125006
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 474.31835853954266,
                        "ram_power_avg": 0.7118868827819824,
                        "cpu_energy_total": 0.0026920720989674013,
                        "gpu_energy_total": 0.015480457106577639,
                        "ram_energy_total": 1.9385440924337296e-05,
                        "total_energy_kwh": 0.018191914646469375,
                        "total_energy_joules": 65490.89272728975
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.25017218910459077,
                        "joules_per_token": 3.9972468705621185,
                        "flops_per_joule": 258814169.22705996,
                        "joules_per_flop": 3.863776094587353e-09
                    },
                    "per-process_emissions": [
                        0.0017301836465523689,
                        0.0017428877891000048,
                        0.0017172095477500761,
                        0.0017399289011700587
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0471": {
            "setup": {
                "experiment_id": "0471",
                "date_time": "April 16, 2025 at 10:12:29 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.3044085719157,
                        "average_latency_ms_per_batch": 3663.0510714894626,
                        "throughput_queries_per_sec": 4.3679434678190585,
                        "throughput_tokens_per_sec": 559.0967638808395
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            8.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 1927794688
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0471",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 373.12997812845583,
                            "process_2": 169.9176747164201,
                            "process_1": 533.6535506494008,
                            "process_0": 491.7658077946189
                        },
                        "ram_power": {
                            "process_3": 0.7074580192565918,
                            "process_2": 0.7034754753112793,
                            "process_1": 0.7049160003662109,
                            "process_0": 0.6730856895446777
                        },
                        "cpu_energy": {
                            "process_3": 0.0006591639266243874,
                            "process_2": 0.0006535476508433932,
                            "process_1": 0.0006683404628402057,
                            "process_0": 0.0006570236186271357
                        },
                        "gpu_energy": {
                            "process_3": 0.003911797573877607,
                            "process_2": 0.003883536440158153,
                            "process_1": 0.003901154232031523,
                            "process_0": 0.0039060339581578063
                        },
                        "ram_energy": {
                            "process_3": 4.954425562988613e-06,
                            "process_2": 5.258171028961924e-06,
                            "process_1": 4.937228355266901e-06,
                            "process_0": 4.668893481430283e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.004575915926064981,
                            "process_2": 0.004542342262030509,
                            "process_1": 0.004574431923226998,
                            "process_0": 0.004567726470266374
                        },
                        "total_energy_joules": {
                            "process_3": 16473.29733383393,
                            "process_2": 16352.432143309832,
                            "process_1": 16467.95492361719,
                            "process_0": 16443.815292958945
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 392.11675282222393,
                        "ram_power_avg": 0.6972337961196899,
                        "cpu_energy_total": 0.002638075658935122,
                        "gpu_energy_total": 0.015602522204225089,
                        "ram_energy_total": 1.981871842864772e-05,
                        "total_energy_kwh": 0.018260416581588862,
                        "total_energy_joules": 65737.49969371989
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2492336957799631,
                        "joules_per_token": 4.0122985652905205,
                        "flops_per_joule": 257843256.4688992,
                        "joules_per_flop": 3.878325203050711e-09
                    },
                    "per-process_emissions": [
                        0.0017431951720344545,
                        0.0017304052847205223,
                        0.0017426298411533248,
                        0.0017400753988479753
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0471": {
            "setup": {
                "experiment_id": "0471",
                "date_time": "April 16, 2025 at 10:12:29 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_1.0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 1.0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 29.3044085719157,
                        "average_latency_ms_per_batch": 3663.0510714894626,
                        "throughput_queries_per_sec": 4.3679434678190585,
                        "throughput_tokens_per_sec": 559.0967638808395
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            8.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.8,
                        "cpu_memory_usage_bytes": 1927794688
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0471",
                    "local_process_results": {
                        "cpu_power": {
                            "process_3": 112.5,
                            "process_2": 112.5,
                            "process_1": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_3": 373.12997812845583,
                            "process_2": 169.9176747164201,
                            "process_1": 533.6535506494008,
                            "process_0": 491.7658077946189
                        },
                        "ram_power": {
                            "process_3": 0.7074580192565918,
                            "process_2": 0.7034754753112793,
                            "process_1": 0.7049160003662109,
                            "process_0": 0.6730856895446777
                        },
                        "cpu_energy": {
                            "process_3": 0.0006591639266243874,
                            "process_2": 0.0006535476508433932,
                            "process_1": 0.0006683404628402057,
                            "process_0": 0.0006570236186271357
                        },
                        "gpu_energy": {
                            "process_3": 0.003911797573877607,
                            "process_2": 0.003883536440158153,
                            "process_1": 0.003901154232031523,
                            "process_0": 0.0039060339581578063
                        },
                        "ram_energy": {
                            "process_3": 4.954425562988613e-06,
                            "process_2": 5.258171028961924e-06,
                            "process_1": 4.937228355266901e-06,
                            "process_0": 4.668893481430283e-06
                        },
                        "total_energy_kwh": {
                            "process_3": 0.004575915926064981,
                            "process_2": 0.004542342262030509,
                            "process_1": 0.004574431923226998,
                            "process_0": 0.004567726470266374
                        },
                        "total_energy_joules": {
                            "process_3": 16473.29733383393,
                            "process_2": 16352.432143309832,
                            "process_1": 16467.95492361719,
                            "process_0": 16443.815292958945
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 392.11675282222393,
                        "ram_power_avg": 0.6972337961196899,
                        "cpu_energy_total": 0.002638075658935122,
                        "gpu_energy_total": 0.015602522204225089,
                        "ram_energy_total": 1.981871842864772e-05,
                        "total_energy_kwh": 0.018260416581588862,
                        "total_energy_joules": 65737.49969371989
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2492336957799631,
                        "joules_per_token": 4.0122985652905205,
                        "flops_per_joule": 257843256.4688992,
                        "joules_per_flop": 3.878325203050711e-09
                    },
                    "per-process_emissions": [
                        0.0017431951720344545,
                        0.0017304052847205223,
                        0.0017426298411533248,
                        0.0017400753988479753
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0472": {
            "setup": {
                "experiment_id": "0472",
                "date_time": "April 16, 2025 at 10:13:40 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 27.808259066019673,
                        "average_latency_ms_per_batch": 3476.032383252459,
                        "throughput_queries_per_sec": 4.602949062582983,
                        "throughput_tokens_per_sec": 589.1774800106218
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            12.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 1828990976
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0472",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 54.593689817408446,
                            "process_2": 29.612909402013223,
                            "process_3": 750.0989980667636,
                            "process_0": 756.1160393518784
                        },
                        "ram_power": {
                            "process_1": 0.6675124168395996,
                            "process_2": 0.6699671745300294,
                            "process_3": 0.675358772277832,
                            "process_0": 0.638390064239502
                        },
                        "cpu_energy": {
                            "process_1": 0.0009066923042482813,
                            "process_2": 0.0008570339344769309,
                            "process_3": 0.0008136882170510945,
                            "process_0": 0.0008190258670983895
                        },
                        "gpu_energy": {
                            "process_1": 0.003756471894067026,
                            "process_2": 0.003747368553450592,
                            "process_3": 0.003740708270347781,
                            "process_0": 0.0037393632692719336
                        },
                        "ram_energy": {
                            "process_1": 4.352741122653828e-06,
                            "process_2": 4.075320337946321e-06,
                            "process_3": 4.26012159396747e-06,
                            "process_0": 4.066127194237721e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.004667516939437962,
                            "process_2": 0.004608477808265471,
                            "process_3": 0.004558656608992844,
                            "process_0": 0.004562455263564561
                        },
                        "total_energy_joules": {
                            "process_1": 16803.060981976665,
                            "process_2": 16590.520109755696,
                            "process_3": 16411.16379237424,
                            "process_0": 16424.838948832417
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 397.60540915951594,
                        "ram_power_avg": 0.6628071069717407,
                        "cpu_energy_total": 0.0033964403228746963,
                        "gpu_energy_total": 0.014983911987137333,
                        "ram_energy_total": 1.675431024880534e-05,
                        "total_energy_kwh": 0.01839710662026084,
                        "total_energy_joules": 66229.58383293901
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2473818956997807,
                        "joules_per_token": 4.042332997615906,
                        "flops_per_joule": 255927487.56971657,
                        "joules_per_flop": 3.907356765371256e-09
                    },
                    "per-process_emissions": [
                        0.0017780905780788918,
                        0.0017555996210587312,
                        0.001736620235195824,
                        0.0017380673326549195
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0472": {
            "setup": {
                "experiment_id": "0472",
                "date_time": "April 16, 2025 at 10:13:40 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_p_decoder_top_p_0.9_decoder_temperature_0",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0,
                    "decoder_top_k": 0,
                    "decoder_top_p": 0.9,
                    "decoding_mode": "top_p"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 27.808259066019673,
                        "average_latency_ms_per_batch": 3476.032383252459,
                        "throughput_queries_per_sec": 4.602949062582983,
                        "throughput_tokens_per_sec": 589.1774800106218
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            12.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.4,
                        "cpu_memory_usage_bytes": 1828990976
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0472",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5,
                            "process_0": 112.5
                        },
                        "gpu_power": {
                            "process_1": 54.593689817408446,
                            "process_2": 29.612909402013223,
                            "process_3": 750.0989980667636,
                            "process_0": 756.1160393518784
                        },
                        "ram_power": {
                            "process_1": 0.6675124168395996,
                            "process_2": 0.6699671745300294,
                            "process_3": 0.675358772277832,
                            "process_0": 0.638390064239502
                        },
                        "cpu_energy": {
                            "process_1": 0.0009066923042482813,
                            "process_2": 0.0008570339344769309,
                            "process_3": 0.0008136882170510945,
                            "process_0": 0.0008190258670983895
                        },
                        "gpu_energy": {
                            "process_1": 0.003756471894067026,
                            "process_2": 0.003747368553450592,
                            "process_3": 0.003740708270347781,
                            "process_0": 0.0037393632692719336
                        },
                        "ram_energy": {
                            "process_1": 4.352741122653828e-06,
                            "process_2": 4.075320337946321e-06,
                            "process_3": 4.26012159396747e-06,
                            "process_0": 4.066127194237721e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.004667516939437962,
                            "process_2": 0.004608477808265471,
                            "process_3": 0.004558656608992844,
                            "process_0": 0.004562455263564561
                        },
                        "total_energy_joules": {
                            "process_1": 16803.060981976665,
                            "process_2": 16590.520109755696,
                            "process_3": 16411.16379237424,
                            "process_0": 16424.838948832417
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 397.60540915951594,
                        "ram_power_avg": 0.6628071069717407,
                        "cpu_energy_total": 0.0033964403228746963,
                        "gpu_energy_total": 0.014983911987137333,
                        "ram_energy_total": 1.675431024880534e-05,
                        "total_energy_kwh": 0.01839710662026084,
                        "total_energy_joules": 66229.58383293901
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.2473818956997807,
                        "joules_per_token": 4.042332997615906,
                        "flops_per_joule": 255927487.56971657,
                        "joules_per_flop": 3.907356765371256e-09
                    },
                    "per-process_emissions": [
                        0.0017780905780788918,
                        0.0017555996210587312,
                        0.001736620235195824,
                        0.0017380673326549195
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0474": {
            "setup": {
                "experiment_id": "0474",
                "date_time": "April 16, 2025 at 10:15:37 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 30.884632632893045,
                        "average_latency_ms_per_batch": 3860.5790791116306,
                        "throughput_queries_per_sec": 4.144455966870599,
                        "throughput_tokens_per_sec": 530.4903637594367
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1971986432
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0474",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 56.33185209865598,
                            "process_0": 57.2855369384544,
                            "process_2": 58.73932524953022,
                            "process_3": 461.3189990631514
                        },
                        "ram_power": {
                            "process_1": 0.7230033874511719,
                            "process_0": 0.6884250640869141,
                            "process_2": 0.7257485389709473,
                            "process_3": 0.7210006713867188
                        },
                        "cpu_energy": {
                            "process_1": 0.0009386419188522266,
                            "process_0": 0.0009857602685569872,
                            "process_2": 0.0009880314423189702,
                            "process_3": 0.0009057407684031206
                        },
                        "gpu_energy": {
                            "process_1": 0.0038945845045521565,
                            "process_0": 0.003979705683761381,
                            "process_2": 0.003978323182655785,
                            "process_3": 0.0038080983242529953
                        },
                        "ram_energy": {
                            "process_1": 5.611386302963144e-06,
                            "process_0": 4.900715109001318e-06,
                            "process_2": 5.197444947025445e-06,
                            "process_3": 5.0267608694561765e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.004838837809707346,
                            "process_0": 0.004970366667427369,
                            "process_2": 0.00497155206992178,
                            "process_3": 0.0047188658535255714
                        },
                        "total_energy_joules": {
                            "process_1": 17419.816114946443,
                            "process_0": 17893.32000273853,
                            "process_2": 17897.58745171841,
                            "process_3": 16987.917072692057
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 158.418928337448,
                        "ram_power_avg": 0.714544415473938,
                        "cpu_energy_total": 0.0038181743981313045,
                        "gpu_energy_total": 0.015660711695222318,
                        "ram_energy_total": 2.0736307228446083e-05,
                        "total_energy_kwh": 0.01949962240058207,
                        "total_energy_joules": 70198.64064209544
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.23339483286482818,
                        "joules_per_token": 4.284585000127896,
                        "flops_per_joule": 241457253.8458494,
                        "joules_per_flop": 4.141519809706847e-09
                    },
                    "per-process_emissions": [
                        0.0018433552636080135,
                        0.0018934611819564563,
                        0.0018939127610367022,
                        0.0017976519469005666
                    ]
                },
                "local_energy_results": {}
            }
        }
    },
    {
        "CONFIGURATION_RUN_#0474": {
            "setup": {
                "experiment_id": "0474",
                "date_time": "April 16, 2025 at 10:15:37 AM",
                "model": "meta-llama/Llama-3.2-1B",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "available_gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "available_cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony"
            },
            "variables": {
                "config_name": "decoding_top_k_decoder_top_k_500_decoder_temperature_0.8",
                "max_input_tokens": 128,
                "max_output_tokens": 128,
                "number_input_prompts": 128,
                "decode_token_to_text": true,
                "decoder_config": {
                    "decoder_temperature": 0.8,
                    "decoder_top_k": 500,
                    "decoder_top_p": 0.0,
                    "decoding_mode": "top_k"
                },
                "query_rate": 1.0,
                "latency_simulation": {
                    "simulate": false,
                    "delay_min": 0,
                    "delay_max": 0,
                    "simulate_burst": false,
                    "burst_interval": 0.0,
                    "burst_size": 0
                },
                "fp_precision": "torch.float16",
                "quantisation": {
                    "quantization": true,
                    "load_in_8bit": false,
                    "load_in_4bit": true,
                    "cached_flops_for_quantised_models": 16949970993152
                },
                "batching_options": {
                    "batch_size___fixed_batching": 16,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 0,
                    "max_batch_size___adaptive_batching": 0
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": false,
                        "cpu_offload": false
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 4
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 749275136,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "inference_metrics": {
                    "raw_inference_metrics": {
                        "number_input_prompts": 128,
                        "total_input_tokens": 16384,
                        "total_generated_tokens": 16384
                    },
                    "inference_performance": {
                        "total_inference_time_sec": 30.884632632893045,
                        "average_latency_ms_per_batch": 3860.5790791116306,
                        "throughput_queries_per_sec": 4.144455966870599,
                        "throughput_tokens_per_sec": 530.4903637594367
                    }
                },
                "compute_metrics": {
                    "flops": 16949970993152,
                    "memory": {
                        "gpu_current_memory_allocated_bytes": 1609970688,
                        "gpu_max_memory_allocated_bytes": 1609970688,
                        "gpu_current_memory_reserved_bytes": 2128609280,
                        "gpu_max_memory_reserved_bytes": 2128609280
                    },
                    "compute_utilisation": {
                        "gpu_utilization_percent": [
                            0.0,
                            100.0,
                            100.0,
                            100.0
                        ],
                        "cpu_usage_percent": 3.5,
                        "cpu_memory_usage_bytes": 1971986432
                    }
                },
                "global_energy_metrics": {
                    "experiment_id": "0474",
                    "local_process_results": {
                        "cpu_power": {
                            "process_1": 112.5,
                            "process_0": 112.5,
                            "process_2": 112.5,
                            "process_3": 112.5
                        },
                        "gpu_power": {
                            "process_1": 56.33185209865598,
                            "process_0": 57.2855369384544,
                            "process_2": 58.73932524953022,
                            "process_3": 461.3189990631514
                        },
                        "ram_power": {
                            "process_1": 0.7230033874511719,
                            "process_0": 0.6884250640869141,
                            "process_2": 0.7257485389709473,
                            "process_3": 0.7210006713867188
                        },
                        "cpu_energy": {
                            "process_1": 0.0009386419188522266,
                            "process_0": 0.0009857602685569872,
                            "process_2": 0.0009880314423189702,
                            "process_3": 0.0009057407684031206
                        },
                        "gpu_energy": {
                            "process_1": 0.0038945845045521565,
                            "process_0": 0.003979705683761381,
                            "process_2": 0.003978323182655785,
                            "process_3": 0.0038080983242529953
                        },
                        "ram_energy": {
                            "process_1": 5.611386302963144e-06,
                            "process_0": 4.900715109001318e-06,
                            "process_2": 5.197444947025445e-06,
                            "process_3": 5.0267608694561765e-06
                        },
                        "total_energy_kwh": {
                            "process_1": 0.004838837809707346,
                            "process_0": 0.004970366667427369,
                            "process_2": 0.00497155206992178,
                            "process_3": 0.0047188658535255714
                        },
                        "total_energy_joules": {
                            "process_1": 17419.816114946443,
                            "process_0": 17893.32000273853,
                            "process_2": 17897.58745171841,
                            "process_3": 16987.917072692057
                        }
                    },
                    "global_experiment_results": {
                        "cpu_power_avg": 112.5,
                        "gpu_power_avg": 158.418928337448,
                        "ram_power_avg": 0.714544415473938,
                        "cpu_energy_total": 0.0038181743981313045,
                        "gpu_energy_total": 0.015660711695222318,
                        "ram_energy_total": 2.0736307228446083e-05,
                        "total_energy_kwh": 0.01949962240058207,
                        "total_energy_joules": 70198.64064209544
                    },
                    "global_derived_quantities": {
                        "tokens_per_joule": 0.23339483286482818,
                        "joules_per_token": 4.284585000127896,
                        "flops_per_joule": 241457253.8458494,
                        "joules_per_flop": 4.141519809706847e-09
                    },
                    "per-process_emissions": [
                        0.0018433552636080135,
                        0.0018934611819564563,
                        0.0018939127610367022,
                        0.0017976519469005666
                    ]
                },
                "local_energy_results": {}
            }
        }
    }
]