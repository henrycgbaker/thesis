{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "from accelerate import notebook_launcher\n",
    "from config.experiment_config import ExperimentConfig\n",
    "from experiment.experiment_runner import ExperimentRunner\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Create an ExperimentConfig (could also load from YAML)\n",
    "experiment_config = ExperimentConfig(\n",
    "    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    is_encoder_decoder=\"decoder_only\",\n",
    "    task_type=\"text_generation\",\n",
    "    inference_type=\"purely_generative\",\n",
    "    max_input_tokens=512,\n",
    "    max_output_tokens=50,\n",
    "    gpu_list=[0,1,2,3],\n",
    "    num_processes=4,\n",
    "    batching_options={\"adaptive_batching\": True, \"max_batch_size\": 8},\n",
    "    sharding_config={\"fsdp_config\": {}},\n",
    "    query_rate=1,\n",
    "    decoder_temperature=1,\n",
    "    fp_precision=\"float16\",\n",
    "    quantisation=True,\n",
    "    backend=\"pytorch\"\n",
    ")\n",
    "\n",
    "# Load prompts (example using a dataset)\n",
    "ds = load_dataset(\"lighteval/pile_helm\", \"arxiv\")[\"test\"]\n",
    "ds = ds.select(range(5))\n",
    "prompts = [sample[\"text\"] for sample in ds]\n",
    "\n",
    "notebook_launcher(\n",
    "    lambda: ExperimentRunner(experiment_config, prompts, inference_fn=text_gen_runinf, use_optimum=False).run(),\n",
    "    num_processes=experiment_config.num_processes\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
