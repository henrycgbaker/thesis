{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last left off\n",
    "- Change experimental_vars list:\n",
    "    - move the GPU list up to the experiment_setup JSON object (below counts of GPUs)... also get it to validate against which GPUs it actually ran on\n",
    "    - experiment_setup: move the GPU count and type above the CPU count and type\n",
    "- ASK O3 TO CONTINUE\n",
    "- CHECK QUERY RATE AND DECODER TEMP AND OTHER FUNCTIONALITY JUST ADDED\n",
    "    - GPU list: doesn't run on different GPUs (somewhere it's hard coded to 01)\n",
    "    - decoder temp: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
    "\n",
    "# Next\n",
    "- [ ] make some changes between ExperimentConfig and ExperimentRunner\n",
    "    - [ ] take the \"inference_fn\" of the notebook_launcher and instead have it determined in the run_experiment flow, by the parameter \"purely_generative\" of the experiment_config object.\n",
    "    - [ ] take the `backend` parameter from the ExperimentRunner and (i) have it determined in the experiment_config class, and (ii) outputed in the hardware setup output of the JSON\n",
    "- [ ] validate `tracking_mode=\"machine\"` in `EmissionsTracker`\n",
    "- [ ] move some of the experiment utils to metrics,py \n",
    "    - rename metrics_results.py\n",
    "    - rename function `aggregate_experiments` to `aggregate_experiment_results`\n",
    "- [ ] set up AMD CPU logic (currently only INTEL) -> confirm CPU usage metric\n",
    "- [ ] try CodeCarbon's save_to_logger \n",
    "- [ ] distributed env issues\n",
    "    - error logs:\n",
    "        - Launching training on 2 GPUs.\n",
    "        - Using device: cuda:0 (Local Rank: 0)\n",
    "        - Model is on cuda:1\n",
    "        - [codecarbon WARNING @ 18:23:07] Multiple instances of codecarbon are allowed to run at the same time.\n",
    "        - Using 2 GPUs: [0, 1]\n",
    "        - Model is on cuda:0\n",
    "        - [codecarbon WARNING @ 18:23:07] Multiple instances of codecarbon are allowed to run at the same time.\n",
    "        - Aggregated benchmark results saved to benchmark_results/text_generation_results.json\n",
    "        - [2025-03-05 18:23:17,500] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 3902871 via signal SIGTERM\n",
    "    - solution\n",
    "        - It looks like there's some inconsistency in how the GPUs are being assigned. Based on the logs:The training is being launched on 2 GPUs ([0, 1]). The device at Local Rank: 0 is cuda:0, but at a later point, the model is reported on cuda:1 and then cuda:0 again. codecarbon is running multiple instances, which might indicate multiple runs being initiated in parallel.The training process (PID 3902871) was closed via SIGTERM, which could be due to an error, manual interruption, or a resource issue.\n",
    "        - Check GPU Device Assignment: Ensure that your training script correctly assigns GPUs without switching unexpectedly. If using torch.nn.DataParallel, it should handle this automatically.\n",
    "        - If using torch.nn.parallel.DistributedDataParallel (DDP), check that each process is assigned the correct GPU using:\n",
    "            \"\"\"python\n",
    "            Copy\n",
    "            Edit\n",
    "            local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "            torch.cuda.set_device(local_rank)\n",
    "            If using manual device placement, explicitly check model.to(device) to avoid conflicts.\n",
    "            \"\"\"\n",
    "- refactor: (i) helper functions; (ii) Object oriented\n",
    "- from run_gen_inf() remove line `do_sample = True if inference_type == \"reasoning\" else False`\n",
    "- validate if the ExperimentConfigs actually change anything\n",
    "\n",
    "## Review Metrics:\n",
    "- [ ] review compute metrics:\n",
    "    - remove cpu vendor \n",
    "    - remove cuda \n",
    "    - sort out memory allocation metrics (they repeat, are they ever different?)\n",
    "    - sort out the gpu_utilisation_percent <- currently it shows overall utilisation by other programmes, but I think that is right? check?\n",
    "- [ ] work out what the GPU and CPU power metrics coming out of CodeCarbon are referring to (avg power draw?)\n",
    "- [ ] GPU & CPU power = 'avg across processors' <- add this in (ram power constant)\n",
    "- [ ] GPU & CPU energy = sum across processort\n",
    "- [ ] NB the RAM energy seems to be wrong (I am summing, but RAM is shared ????)\n",
    "    - NB THIS IS A MAJOR POINT: CODE CARBON GETS IT WRON BY THE PROCESS BY NOT CONSIDERING SHARED RESOURCES...??? \n",
    "- [ ] FLOPs monitoring\n",
    "- [ ] to experimental_variables: add avg_input_tokens and avg_output_tokens between and within batches / across the overall experiment??\n",
    "- is it possible to record power everysec as a file --> allow for plotting over time to see how this changes with (?)\n",
    "    \n",
    "- [ ] if staying on Hertie server: build in AMD tracking tools for the CPU (currently it can't measure it using CC)\n",
    "- [ ] log metrics both at (1) the individual per prompt, (2) the per-batch level, and (3) the experiment level (e.g. number of input/output tokens, energy etc etc)\n",
    "\n",
    "# Blockers\n",
    "- [ ] vLLM -> CUDA runs out of memory\n",
    "- [ ] Optimum benchmarking library -> CUDA runs out of memory\n",
    "\n",
    "# Low priority\n",
    "- [ ] add Carbon Tracker\n",
    "- [ ] add logic for task-specific metrics in the reporting\n",
    "- [ ] how is final GPU / CPU power calculated by CodeCarbon? is it an avg?\n",
    "- [ ] sort out the print statements in experiments to supress (i) the code carbon output, (ii) the final JSON print\n",
    "\n",
    "# Done\n",
    "- [x] change codeCarbon log_level to warning \n",
    "- [x] Code Carbon turn off multiple runs in allow_multiple_runs\n",
    "- [x] calculate computation metrics\n",
    "- [x] put the text_gen_inference() into the helper functionn (as it is pretty generic)\n",
    "- [x] us Optimum library \n",
    "- [x] sort out distributed inference using accelerate / vLLM. At the moment it's not working on either\n",
    "- [x] aggregate process metrics \n",
    "    - once it goes to multi-GPU it gets weird - i think this is why half the runs don't work, the other half do\n",
    "    - including assinging the unique ID\n",
    "    - decide which metrics to sum, which to avg\n",
    "- [x] check why the CodeCarbon CSV metrics and the JSON metrics don't align - Answer: because they are per process -> allows me to validate (also check that CodeCarbon don't offer their own aggregation)\n",
    "    - CC can do by machine or by process\n",
    "        - parameter: tracking_mode -> machine measure the power consumptions of the entire machine (defaults) / process try and isolate the tracked processes in isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
