{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next\n",
    "\n",
    "= [ ] aggregate process metrics IM NOT SURE IF IT'S DONE IT CORRECTLY\n",
    "- [ ] log metrics both at (1) the individual per prompt, (2) the per-batch level, and (3) the experiment level (e.g. number of input/output tokens, energy etc etc)\n",
    "- [ ] add Carbon Tracker\n",
    "\n",
    "# Low priority\n",
    "- [ ] add logic for task-specific metrics in the reporting\n",
    "- [ ] how is GPU / CPU power calculated by CodeCarbon?\n",
    "- [ ] sort out the print statements in experiments to supress (i) the code carbon output, (ii) the final JSON print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blockers\n",
    "\n",
    "- [ ] vLLM -> CUDA runs out of memory\n",
    "- [ ] Optimum benchmarking library -> CUDA runs out of memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done\n",
    "- [x] calculate computation metrics\n",
    "- [x] put the text_gen_inference() into the helper functionn (as it is pretty generic)\n",
    "- [x] us Optimum library \n",
    "- [x] sort out distributed inference using accelerate / vLLM. At the moment it's not working on either\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) need to not count each run (4*5 = 20, when there's just 5)\n",
    "2) need to append and not overwrite"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
