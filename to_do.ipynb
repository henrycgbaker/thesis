{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next\n",
    "- [ ] decoder temp: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
    "    - change this: so that there's the option in the experimental_configs to chose gredy decoding, beam search, random sampling, top-k sampling, top-p sampling\n",
    "        - params: do_sample; temperature, top_p, top_k\n",
    "        - Greedy deocing?\n",
    "        - Beam Search\n",
    "        - random sampling (basic): outputs = model.generate(input_ids, do_sample=True)\n",
    "        - Top-k Sampling (Controlled Randomness): outputs = model.generate(input_ids, do_sample=True, top_k=50)\n",
    "        -  Top-p (Nucleus) Sampling (Dynamic Control): outputs = model.generate(input_ids, do_sample=True, top_p=0.9)\n",
    "- [ ] from run_gen_inf() remove line `do_sample = True if inference_type == \"reasoning\" else False`\n",
    "- [ ] make some changes between ExperimentConfig and ExperimentRunner\n",
    "    - [ ] take the \"inference_fn\" of the notebook_launcher and instead have it determined in the run_experiment flow, by the parameter \"purely_generative\" of the experiment_config object.\n",
    "- set up option for repeated reasoning\n",
    "- [ ] set up AMD CPU logic (currently only INTEL) -> confirm CPU usage metric\n",
    "- [ ] currently seems that quantisation only possible on CPUs --> is it also possible on GPUs?\n",
    "\n",
    "## Review Metrics:\n",
    "- [ ] review compute metrics:\n",
    "    - [ ] sort out memory allocation metrics (they repeat, are they ever different?)\n",
    "    - [ ] sort out the gpu_utilisation_percent <- currently it shows overall utilisation by other programmes, but I think that is right? check? [now it seems to be set to always 0... whereas when Silke was running training it was much more variable, did I break something or is notone using the GPUs currently?]\n",
    "- [ ] FLOPs monitoring\n",
    "- [ ] to experimental_variables: add avg_input_tokens and avg_output_tokens between and within batches / across the overall experiment??\n",
    "- is it possible to record power everysec as a file --> allow for plotting over time to see how this changes with (?)\n",
    "    \n",
    "- [ ] if staying on Hertie server: build in AMD tracking tools for the CPU (currently it can't measure it using CC)\n",
    "- [ ] log metrics both at (1) the individual per prompt, (2) the per-batch level, and (3) the experiment level (e.g. number of input/output tokens, energy etc etc)\n",
    "- [ ] Add in warm up period before code carbon measurement+ inference run... Also at experiment level: interleave \n",
    "\n",
    "# Blockers\n",
    "- [ ] vLLM -> CUDA runs out of memory\n",
    "- [ ] Optimum benchmarking library -> CUDA runs out of memory\n",
    "- [ ] IT DOESN'T WORK ON MORE THAN 2 GPUS\n",
    "\n",
    "# Low priority\n",
    "- [ ] add Carbon Tracker\n",
    "- [ ] add power over time to the metrics\n",
    "- [ ] Measure L2 cash + stalls while system is waiting \n",
    "- [ ] add logic for task-specific metrics in the reporting\n",
    "- [ ] how is final GPU / CPU power calculated by CodeCarbon? is it an avg?\n",
    "- [ ] make experimental configs enterable as a YAML file rather than directly\n",
    "\n",
    "\n",
    "# Done\n",
    "- [x] change codeCarbon log_level to warning \n",
    "- [x] Code Carbon turn off multiple runs in allow_multiple_runs\n",
    "- [x] calculate computation metrics\n",
    "- [x] put the text_gen_inference() into the helper functionn (as it is pretty generic)\n",
    "- [x] us Optimum library \n",
    "- [x] sort out distributed inference using accelerate / vLLM. At the moment it's not working on either\n",
    "- [x] aggregate process metrics \n",
    "    - once it goes to multi-GPU it gets weird - i think this is why half the runs don't work, the other half do\n",
    "    - including assinging the unique ID\n",
    "    - decide which metrics to sum, which to avg\n",
    "- [x] check why the CodeCarbon CSV metrics and the JSON metrics don't align - Answer: because they are per process -> allows me to validate (also check that CodeCarbon don't offer their own aggregation)\n",
    "    - CC can do by machine or by process\n",
    "        - parameter: tracking_mode -> machine measure the power consumptions of the entire machine (defaults) / process try and isolate the tracked processes in isolation\n",
    "- [x] refactor: (i) helper functions; (ii) Object oriented\n",
    "- [x] Change experimental_vars list:\n",
    "    - [x] move the GPU list up to the experiment_setup JSON object (below counts of GPUs)... also get it to validate against which GPUs it actually ran on\n",
    "    - [x] experiment_setup: move the GPU count and type above the CPU count and type\n",
    "- [x] + also add if model in encoder-decoder Vs decoder only model, to see if these behave differently under different optimisations  (save that under experiment set up)\n",
    "- [x] move some of the experiment utils to metrics,py \n",
    "    - [x] rename metrics_results.py\n",
    "    - [x] rename function `aggregate_experiments` to `aggregate_experiment_results`\n",
    "- [x] work out what the GPU and CPU power metrics coming out of CodeCarbon are referring to (avg power draw?)\n",
    "- [x] GPU & CPU power = 'avg across processors' <- add this in (ram power constant)\n",
    "- [x] GPU & CPU energy = sum across processort\n",
    "- [x] NB the RAM energy seems to be wrong (I am summing, but RAM is shared ????)\n",
    "- [x] GPU list: doesn't run on different GPUs (somewhere it's hard coded to 01)\n",
    "- [x] review compute metrics:\n",
    "    - [x] remove cpu vendor \n",
    "    - [x] remove cuda \n",
    "- [x] sort out the print statements in experiments to supress (i) the code carbon output, (ii) the final JSON print\n",
    "- [x] distributed env issues\n",
    "- [x] Add in tunable optimisations into experimental variables: floating points, quantisation, batching, sharding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
