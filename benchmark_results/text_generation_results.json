[
    {
        "EXPERIMENT #0001": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 04, 2025 at 07:11:46 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "mecklenburg-vorpommern"
            },
            "experiment_variables": {
                "input_tokens": 2560,
                "output_tokens": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 2.9046354780439287,
                    "average_latency_ms_per_batch": 2904.6354780439287,
                    "average_ttft_ms": 0.0,
                    "throughput_queries_per_sec": 1.721420551464691,
                    "throughput_tokens_per_sec": 86.07102757323455
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 0.0,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.17437181807253316,
                    "cpu_energy": 0.00019891567426384428,
                    "gpu_energy": 0.000264675211752774,
                    "ram_energy": 0.00033291941715275514,
                    "total_energy_consumed_kwh": 0.0007965103031693735,
                    "total_energy_consumed_joules": 2867.4370914097444,
                    "final_emissions": 0.00030343059999237283
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2867941376.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0002": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 04, 2025 at 07:35:00 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "mecklenburg-vorpommern"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 3.069109580013901,
                    "average_latency_ms_per_batch": 3069.109580013901,
                    "average_ttft_ms": 0.0,
                    "throughput_queries_per_sec": 1.6291373232080473,
                    "throughput_tokens_per_sec": 81.45686616040237
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 235.83204577308427,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.15470288405248556,
                    "cpu_energy": 0.0002534404672478558,
                    "gpu_energy": 0.00021988628699887158,
                    "ram_energy": 0.00042445479029182227,
                    "total_energy_consumed_kwh": 0.0008977815445385496,
                    "total_energy_consumed_joules": 3232.013560338779,
                    "final_emissions": 0.0003420098793919605
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2872322048.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0003": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 04, 2025 at 07:38:29 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "mecklenburg-vorpommern"
            },
            "experiment_variables": {
                "total_token_inputted": 25600,
                "total_tokens_outputted": 2500,
                "number_runs": 50
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 9.075287469662726,
                    "average_latency_ms_per_batch": 1296.4696385232464,
                    "average_ttft_ms": 0.0,
                    "throughput_queries_per_sec": 5.509471823871618,
                    "throughput_tokens_per_sec": 275.4735911935809
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 196.4996529529407,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.4935331359579749,
                    "cpu_energy": 0.0006421420079132077,
                    "gpu_energy": 0.0010969730997913985,
                    "ram_energy": 0.0010750623629472243,
                    "total_energy_consumed_kwh": 0.00281417747065183,
                    "total_energy_consumed_joules": 10131.038894346588,
                    "final_emissions": 0.0010720609074448147
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.5,
                    "cpu_memory_usage_bytes": 2894456832.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0004": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 04, 2025 at 07:55:37 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "mecklenburg-vorpommern"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 2.9997439426369965,
                    "average_latency_ms_per_batch": 2999.7439426369965,
                    "average_ttft_ms": 0.0,
                    "throughput_queries_per_sec": 1.666872669142737,
                    "throughput_tokens_per_sec": 83.34363345713683
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 555.0654377502071,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.22743924192892567,
                    "cpu_energy": 0.00013839303403801752,
                    "gpu_energy": 0.00024096435945608619,
                    "ram_energy": 0.00023131384432084997,
                    "total_energy_consumed_kwh": 0.0006106712378149536,
                    "total_energy_consumed_joules": 2198.416456133833,
                    "final_emissions": 0.0002326352080456066
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2869719040.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0005": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 04, 2025 at 08:02:08 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "mecklenburg-vorpommern"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 2.7191423317417502,
                    "average_latency_ms_per_batch": 2719.1423317417502,
                    "throughput_queries_per_sec": 1.838846575118427,
                    "throughput_tokens_per_sec": 91.94232875592135
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 213.0940023505005,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.2395927850907166,
                    "cpu_energy": 0.00013870108078117485,
                    "gpu_energy": 0.00020884322260883437,
                    "ram_energy": 0.0002321461152097701,
                    "total_energy_consumed_kwh": 0.0005796904185997793,
                    "total_energy_consumed_joules": 2086.8855069592055,
                    "final_emissions": 0.00022083306496558595
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2875822080.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0006": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 04, 2025 at 08:07:31 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "mecklenburg-vorpommern"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 2.7960597195196897,
                    "average_latency_ms_per_batch": 2796.0597195196897,
                    "throughput_queries_per_sec": 1.7882384353254623,
                    "throughput_tokens_per_sec": 89.41192176627312
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 224.70957974853113,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.23969306028875886,
                    "cpu_energy": 0.00013523755414644257,
                    "gpu_energy": 0.0002179215632196474,
                    "ram_energy": 0.00022629229255812113,
                    "total_energy_consumed_kwh": 0.000579451409924211,
                    "total_energy_consumed_joules": 2086.0250757271597,
                    "final_emissions": 0.0002207420146106282
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2870278144.0,
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0007": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 04, 2025 at 08:20:19 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "mecklenburg-vorpommern"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 2.704952041618526,
                    "average_latency_ms_per_batch": 2704.952041618526,
                    "throughput_queries_per_sec": 1.848461609557605,
                    "throughput_tokens_per_sec": 92.42308047788023
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 233.760442839909,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.2369246547161872,
                    "cpu_energy": 0.00013856290474359412,
                    "gpu_energy": 0.00021579628376855453,
                    "ram_energy": 0.00023185624590005158,
                    "total_energy_consumed_kwh": 0.0005862154344122003,
                    "total_energy_consumed_joules": 2110.375563883921,
                    "final_emissions": 0.0002233187697393277
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2870751232.0,
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0008": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 04, 2025 at 08:21:05 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "mecklenburg-vorpommern"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 2.793602622114122,
                    "average_latency_ms_per_batch": 2793.602622114122,
                    "throughput_queries_per_sec": 1.789815053666063,
                    "throughput_tokens_per_sec": 89.49075268330316
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 255.26291678050518,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.2340923220997962,
                    "cpu_energy": 0.00013597321054839996,
                    "gpu_energy": 0.0002298785172456519,
                    "ram_energy": 0.00022745646208425369,
                    "total_energy_consumed_kwh": 0.0005933081898783055,
                    "total_energy_consumed_joules": 2135.9094835619,
                    "final_emissions": 0.0002260207549341405
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2871678976.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        55.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0009": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 04, 2025 at 11:55:15 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "mecklenburg-vorpommern"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.986807068111375,
                    "average_latency_ms_per_batch": 1986.807068111375,
                    "throughput_queries_per_sec": 2.51661070851567,
                    "throughput_tokens_per_sec": 125.83053542578351
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 502.8708709741203,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.15596191286041672,
                    "cpu_energy": 0.00013075814796320627,
                    "gpu_energy": 0.0005410354328461153,
                    "ram_energy": 0.00021873723186499898,
                    "total_energy_consumed_kwh": 0.0008905308126743207,
                    "total_energy_consumed_joules": 3205.910925627554,
                    "final_emissions": 0.0003392477130882824
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2870417408.0,
                    "gpu_utilization_percent": [
                        100.0,
                        100.0,
                        100.0,
                        100.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0010": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 12:24:41 AM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "mecklenburg-vorpommern"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.5723746016155928,
                    "average_latency_ms_per_batch": 1572.3746016155928,
                    "throughput_queries_per_sec": 3.1799208211494445,
                    "throughput_tokens_per_sec": 158.9960410574722
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 199.8140015806576,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.28316376801069204,
                    "cpu_energy": 0.00010661004384746775,
                    "gpu_energy": 0.00020545377546454802,
                    "ram_energy": 0.0001784967024653698,
                    "total_energy_consumed_kwh": 0.0004905605217773855,
                    "total_energy_consumed_joules": 1766.017878398588,
                    "final_emissions": 0.00018687903077109502
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2879932416.0,
                    "gpu_utilization_percent": [
                        32.0,
                        28.5,
                        1.0,
                        100.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0011": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 12:30:29 AM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "mecklenburg-vorpommern"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.6368350298143923,
                    "average_latency_ms_per_batch": 1636.8350298143923,
                    "throughput_queries_per_sec": 3.0548034648276503,
                    "throughput_tokens_per_sec": 152.7401732413825
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 165.20222764533247,
                    "ram_power": 2.2502517700195312,
                    "energy_efficiency_tokens_per_joule": 0.5720718752591025,
                    "cpu_energy": 0.00011490836001757998,
                    "gpu_energy": 0.00012589398957629783,
                    "ram_energy": 2.0122285016936767e-06,
                    "total_energy_consumed_kwh": 0.0002428145780955715,
                    "total_energy_consumed_joules": 874.1324811440574,
                    "final_emissions": 9.250021352550796e-05
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2878892032.0,
                    "gpu_utilization_percent": [
                        13.0,
                        20.0,
                        100.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0012": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 12:32:11 AM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "mecklenburg-vorpommern"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.5809939389582723,
                    "average_latency_ms_per_batch": 1580.9939389582723,
                    "throughput_queries_per_sec": 3.1625887450195975,
                    "throughput_tokens_per_sec": 158.1294372509799
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 189.88546301955205,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.334706456263862,
                    "cpu_energy": 0.00010619419757858849,
                    "gpu_energy": 0.00013063010449343437,
                    "ram_energy": 0.00017814167439401382,
                    "total_energy_consumed_kwh": 0.0004149659764660367,
                    "total_energy_consumed_joules": 1493.877515277732,
                    "final_emissions": 0.00015808128873473666
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2884755456.0,
                    "gpu_utilization_percent": [
                        28.0,
                        41.0,
                        100.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0013": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 05:59:03 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.6034757257439196,
                    "average_latency_ms_per_batch": 1603.4757257439196,
                    "throughput_queries_per_sec": 3.1186658121729556,
                    "throughput_tokens_per_sec": 155.93329060864778
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 191.10863977362257,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.31979950055518386,
                    "cpu_energy": 0.00011061446912935936,
                    "gpu_energy": 0.000138283166180031,
                    "ram_energy": 0.00018556175715056755,
                    "total_energy_consumed_kwh": 0.0004344593924599579,
                    "total_energy_consumed_joules": 1564.0538128558485,
                    "final_emissions": 0.00016550730555762097
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2933344256.0,
                    "gpu_utilization_percent": [
                        41.0,
                        44.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0014": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 06:13:59 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.9807776021771133,
                    "average_latency_ms_per_batch": 1980.7776021771133,
                    "throughput_queries_per_sec": 2.524662767545661,
                    "throughput_tokens_per_sec": 126.23313837728304
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 683.3713222026317,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.1579118543093489,
                    "cpu_energy": 0.00013278989843092858,
                    "gpu_energy": 0.0005256759761067542,
                    "ram_energy": 0.00022274418633284613,
                    "total_energy_consumed_kwh": 0.0008812100608705289,
                    "total_energy_consumed_joules": 3172.3562191339042,
                    "final_emissions": 0.000335696972688628
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2942124032.0,
                    "gpu_utilization_percent": [
                        63.0,
                        81.0,
                        52.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0015": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 06:15:45 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.5498218704015017,
                    "average_latency_ms_per_batch": 1549.8218704015017,
                    "throughput_queries_per_sec": 3.2267031135022597,
                    "throughput_tokens_per_sec": 161.335155675113
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 205.69385072356806,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.33176744596411345,
                    "cpu_energy": 0.00010468894024961628,
                    "gpu_energy": 0.00013838733292459438,
                    "ram_energy": 0.00017561424196023263,
                    "total_energy_consumed_kwh": 0.0004186905151344433,
                    "total_energy_consumed_joules": 1507.2858544839958,
                    "final_emissions": 0.00015950015174046617
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2931130368.0,
                    "gpu_utilization_percent": [
                        47.0,
                        31.0,
                        0.0,
                        100.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0016": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 06:16:56 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.534587804460898,
                    "average_latency_ms_per_batch": 1534.587804460898,
                    "throughput_queries_per_sec": 3.2582242107822292,
                    "throughput_tokens_per_sec": 162.91121053911144
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 204.01190972193598,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.33827956028332634,
                    "cpu_energy": 0.00010413418059761171,
                    "gpu_energy": 0.00013176010540405514,
                    "ram_energy": 0.00017468228379316644,
                    "total_energy_consumed_kwh": 0.0004105765697948333,
                    "total_energy_consumed_joules": 1478.0756512613998,
                    "final_emissions": 0.00015640914426334176
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2945007616.0,
                    "gpu_utilization_percent": [
                        35.0,
                        47.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0017": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 06:23:16 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.5405514417216182,
                    "average_latency_ms_per_batch": 1540.5514417216182,
                    "throughput_queries_per_sec": 3.245732009238509,
                    "throughput_tokens_per_sec": 162.28660046192545
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 240.18342311340905,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.3276895717978593,
                    "cpu_energy": 0.00010446326088276692,
                    "gpu_energy": 0.00014417594866955596,
                    "ram_energy": 0.0001752417778531554,
                    "total_energy_consumed_kwh": 0.00042388098740547825,
                    "total_energy_consumed_joules": 1525.9715546597217,
                    "final_emissions": 0.00016147746215211696
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2937456640.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0018": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 06:41:39 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.5356565781403333,
                    "average_latency_ms_per_batch": 1535.6565781403333,
                    "throughput_queries_per_sec": 3.2561267005111585,
                    "throughput_tokens_per_sec": 162.80633502555793
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 197.99871201986875,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.33945601669795,
                    "cpu_energy": 0.00010444665217073634,
                    "gpu_energy": 0.00012951177026820915,
                    "ram_energy": 0.0001752111848798046,
                    "total_energy_consumed_kwh": 0.0004091696073187501,
                    "total_energy_consumed_joules": 1473.0105863475005,
                    "final_emissions": 0.00015587316190807788
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2936752128.0,
                    "gpu_utilization_percent": [
                        47.0,
                        38.0,
                        100.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0019": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 06:52:19 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.5853348837699741,
                    "average_latency_ms_per_batch": 1585.334883769974,
                    "throughput_queries_per_sec": 3.1539130051882824,
                    "throughput_tokens_per_sec": 157.69565025941412
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 202.9825055940948,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.3307372368898579,
                    "cpu_energy": 0.00010639232091489249,
                    "gpu_energy": 0.00013506677474595108,
                    "ram_energy": 0.00017847900784752953,
                    "total_energy_consumed_kwh": 0.00041993810350837306,
                    "total_energy_consumed_joules": 1511.777172630143,
                    "final_emissions": 0.00015997542053151472
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.5,
                    "cpu_memory_usage_bytes": 2933616640.0,
                    "gpu_utilization_percent": [
                        33.5,
                        34.0,
                        0.0,
                        91.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0020": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 07:12:34 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 2.7936179656535387,
                    "average_latency_ms_per_batch": 2793.6179656535387,
                    "throughput_queries_per_sec": 1.7898678572477535,
                    "throughput_tokens_per_sec": 89.49339286238768
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 197.72232811090606,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.25113005161137814,
                    "cpu_energy": 0.00013262356867198833,
                    "gpu_energy": 0.00019818599189846964,
                    "ram_energy": 0.00022245389276966517,
                    "total_energy_consumed_kwh": 0.0005532634533401231,
                    "total_energy_consumed_joules": 1991.7484320244432,
                    "final_emissions": 0.0002107657125499199
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2927912960.0,
                    "gpu_utilization_percent": [
                        32.0,
                        47.0,
                        0.0,
                        100.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0021": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 07:15:50 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.7830163554754108,
                    "average_latency_ms_per_batch": 1783.0163554754108,
                    "throughput_queries_per_sec": 2.8412979064258312,
                    "throughput_tokens_per_sec": 142.06489532129157
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 181.79108624616168,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.283763896559011,
                    "cpu_energy": 0.00011905919946730137,
                    "gpu_energy": 0.00017590847407689125,
                    "ram_energy": 0.0001996985604443615,
                    "total_energy_consumed_kwh": 0.0004946662339885542,
                    "total_energy_consumed_joules": 1780.7984423587948,
                    "final_emissions": 0.0001884431018379397
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2930403328.0,
                    "gpu_utilization_percent": [
                        22.0,
                        100.0,
                        100.0,
                        100.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0022": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 08:49:13 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.5769661308731884,
                    "average_latency_ms_per_batch": 1576.9661308731884,
                    "throughput_queries_per_sec": 3.171545941178989,
                    "throughput_tokens_per_sec": 158.57729705894945
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 104.94575837775582,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.21648605357603376,
                    "cpu_energy": 0.0001678856836660998,
                    "gpu_energy": 0.00019215848706721772,
                    "ram_energy": 0.00028161486978163416,
                    "total_energy_consumed_kwh": 0.0006416590405149516,
                    "total_energy_consumed_joules": 2309.972545853826,
                    "final_emissions": 0.00024444001148417083
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2950612992.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        100.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0023": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 08:57:35 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5,
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "batch_size": 8,
                "gpu_list": [
                    0,
                    1
                ],
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "fp_precision": "float16",
                "inference_type": "purely_generative",
                "quantisation": false,
                "batching_options": {},
                "sharding_config": {},
                "is_encoder_decoder": "na"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.5317176724784076,
                    "average_latency_ms_per_batch": 1531.7176724784076,
                    "throughput_queries_per_sec": 3.2643098890419715,
                    "throughput_tokens_per_sec": 163.2154944520986
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 113.66237254155376,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.21789325807439647,
                    "cpu_energy": 0.00016621336690150203,
                    "gpu_energy": 0.00019239682060856467,
                    "ram_energy": 0.0002788082870204614,
                    "total_energy_consumed_kwh": 0.0006374184745305281,
                    "total_energy_consumed_joules": 2294.706508309901,
                    "final_emissions": 0.0002428245678724047
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2925209600.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        100.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0024": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 09:04:22 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "batch_size": 8,
                "gpu_list": [
                    0,
                    1
                ],
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "fp_precision": "float16",
                "inference_type": "purely_generative",
                "quantisation": false,
                "batching_options": {},
                "sharding_config": {},
                "is_encoder_decoder": "na"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.537309342296794,
                    "average_latency_ms_per_batch": 1537.309342296794,
                    "throughput_queries_per_sec": 3.2527143337053697,
                    "throughput_tokens_per_sec": 162.6357166852685
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 102.57981315294741,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.21907936501838463,
                    "cpu_energy": 0.00016611575611750597,
                    "gpu_energy": 0.00018922404028387518,
                    "ram_energy": 0.00027864262383480354,
                    "total_energy_consumed_kwh": 0.0006339824202361848,
                    "total_energy_consumed_joules": 2282.336712850265,
                    "final_emissions": 0.00024151560298897457
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2933473280.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        100.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0025": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 09:08:30 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "batch_size": 8,
                "gpu_list": [
                    0,
                    1
                ],
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "fp_precision": "float16",
                "inference_type": "purely_generative",
                "quantisation": false,
                "batching_options": {},
                "sharding_config": {},
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.5747146089561284,
                    "average_latency_ms_per_batch": 1574.7146089561284,
                    "throughput_queries_per_sec": 3.1762513142970183,
                    "throughput_tokens_per_sec": 158.8125657148509
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 105.49291292472772,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.21604217804380937,
                    "cpu_energy": 0.00016770636997534896,
                    "gpu_energy": 0.00019389571068018085,
                    "ram_energy": 0.00028131791082286397,
                    "total_energy_consumed_kwh": 0.0006429199914783938,
                    "total_energy_consumed_joules": 2314.5119693222177,
                    "final_emissions": 0.0002449203707536941
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2957121536.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        100.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0026": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 09:25:42 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "batch_size": 8,
                "gpu_list": [
                    1,
                    2
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "float16",
                "inference_type": "purely_generative",
                "quantisation": false,
                "batching_options": {},
                "sharding_config": {},
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.5409394048620015,
                    "average_latency_ms_per_batch": 1540.9394048620015,
                    "throughput_queries_per_sec": 3.244793204313903,
                    "throughput_tokens_per_sec": 162.23966021569515
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 106.78343785293058,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.21861667242717478,
                    "cpu_energy": 0.0001656302709307056,
                    "gpu_energy": 0.0001918593201537533,
                    "ram_energy": 0.0002778331897013143,
                    "total_energy_consumed_kwh": 0.0006353227807857731,
                    "total_energy_consumed_joules": 2287.1620108287834,
                    "final_emissions": 0.00024202621334034027
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2939918336.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        82.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0027": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 09:39:26 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "batch_size": 8,
                "gpu_list": [
                    1,
                    2
                ],
                "decoder_temperature": 1,
                "query_rate": 2,
                "fp_precision": "float16",
                "inference_type": "purely_generative",
                "quantisation": false,
                "batching_options": {},
                "sharding_config": {},
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.5337991253472865,
                    "average_latency_ms_per_batch": 1533.7991253472865,
                    "throughput_queries_per_sec": 3.2601511927309867,
                    "throughput_tokens_per_sec": 163.00755963654933
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 139.48710052862862,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.2661146425850737,
                    "cpu_energy": 0.00013360211643157526,
                    "gpu_energy": 0.00016426568694782873,
                    "ram_energy": 0.00022408070085348585,
                    "total_energy_consumed_kwh": 0.0005219485042328898,
                    "total_energy_consumed_joules": 1879.0146152384032,
                    "final_emissions": 0.00019883628268751937
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.5,
                    "cpu_memory_usage_bytes": 2948493312.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        43.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0028": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 09:40:42 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "batch_size": 8,
                "gpu_list": [
                    1,
                    2
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "float32",
                "inference_type": "purely_generative",
                "quantisation": false,
                "batching_options": {},
                "sharding_config": {},
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.7336179555859417,
                    "average_latency_ms_per_batch": 1733.6179555859417,
                    "throughput_queries_per_sec": 2.8848434975199315,
                    "throughput_tokens_per_sec": 144.24217487599657
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 105.57195293352498,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.17452692681777804,
                    "cpu_energy": 0.00017728814492875243,
                    "gpu_energy": 0.0003211491458046112,
                    "ram_energy": 0.0002974083251066391,
                    "total_energy_consumed_kwh": 0.0007958456158400028,
                    "total_energy_consumed_joules": 2865.04421702401,
                    "final_emissions": 0.00030317738735424906
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 1847830528.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        100.0
                    ],
                    "current_memory_allocated_bytes": 8808907264.0,
                    "max_memory_allocated_bytes": 8808907264.0,
                    "current_memory_reserved_bytes": 12972982272.0,
                    "max_memory_reserved_bytes": 12972982272.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0029": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 11, 2025 at 03:56:12 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "batch_size": 8,
                "gpu_list": [
                    1,
                    2
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "float16",
                "inference_type": "purely_generative",
                "quantisation": false,
                "batching_options": {},
                "sharding_config": {},
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 3.061804956989363,
                    "average_latency_ms_per_batch": 3061.804956989363,
                    "throughput_queries_per_sec": 1.6335062361384378,
                    "throughput_tokens_per_sec": 81.6753118069219
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 2853.378562805508,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.151836428193442,
                    "cpu_energy": 0.0002424484434450278,
                    "gpu_energy": 0.0002852577282084212,
                    "ram_energy": 0.00040675174932283627,
                    "total_energy_consumed_kwh": 0.0009344579209762853,
                    "total_energy_consumed_joules": 3364.048515514627,
                    "final_emissions": 0.0003559817449959159
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2939246592.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0030": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 11, 2025 at 04:05:45 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "batch_size": 8,
                "gpu_list": [
                    1,
                    2
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "float16",
                "inference_type": "purely_generative",
                "quantisation": true,
                "batching_options": {},
                "sharding_config": {},
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 3.0782570028677583,
                    "average_latency_ms_per_batch": 3078.2570028677583,
                    "throughput_queries_per_sec": 1.6243680170469974,
                    "throughput_tokens_per_sec": 81.21840085234987
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 29.346061592167633,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.13707519020517586,
                    "cpu_energy": 0.0002682457180926576,
                    "gpu_energy": 0.00029493356930743175,
                    "ram_energy": 0.0004500547852406731,
                    "total_energy_consumed_kwh": 0.0010132340726407623,
                    "total_energy_consumed_joules": 3647.642661506745,
                    "final_emissions": 0.00038599151997249846
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2943318016.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0031": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 11, 2025 at 04:36:02 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "batch_size": 8,
                "gpu_list": [
                    1,
                    2
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "float16",
                "inference_type": "purely_generative",
                "quantisation": true,
                "batching_options": {
                    "adaptive_batching": true,
                    "max_batch_size": 16
                },
                "sharding_config": {
                    "fsdp_config": {
                        "reshard_after_forward": true
                    }
                },
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.2287663819734007,
                    "average_latency_ms_per_batch": 1228.7663819734007,
                    "throughput_queries_per_sec": 4.069190337511063,
                    "throughput_tokens_per_sec": 203.4595168755532
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 88.13416062025284,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.2620584195593782,
                    "cpu_energy": 0.0001480148242408177,
                    "gpu_energy": 0.0001339945516605212,
                    "ram_energy": 0.0002479849150232768,
                    "total_energy_consumed_kwh": 0.0005299942909246158,
                    "total_energy_consumed_joules": 1907.9794473286165,
                    "final_emissions": 0.00020190132512773237
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2929643520.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 7712172544.0,
                    "max_memory_allocated_bytes": 7712172544.0,
                    "current_memory_reserved_bytes": 12220104704.0,
                    "max_memory_reserved_bytes": 12220104704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0032": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 11, 2025 at 04:37:08 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "batch_size": 6,
                "gpu_list": [
                    1,
                    2
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "float16",
                "inference_type": "purely_generative",
                "quantisation": true,
                "batching_options": {
                    "adaptive_batching": true,
                    "max_batch_size": 6
                },
                "sharding_config": {
                    "fsdp_config": {
                        "reshard_after_forward": true
                    }
                },
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.2619782800320536,
                    "average_latency_ms_per_batch": 1261.9782800320536,
                    "throughput_queries_per_sec": 3.9621748585232988,
                    "throughput_tokens_per_sec": 198.10874292616495
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 86.90245876582026,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.25635676262777396,
                    "cpu_energy": 0.00014973414056294133,
                    "gpu_energy": 0.00014110177953341463,
                    "ram_energy": 0.00025094387648776973,
                    "total_energy_consumed_kwh": 0.0005417797965841257,
                    "total_energy_consumed_joules": 1950.4072677028526,
                    "final_emissions": 0.00020639101350872267
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2929293312.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 7712172544.0,
                    "max_memory_allocated_bytes": 7712172544.0,
                    "current_memory_reserved_bytes": 12220104704.0,
                    "max_memory_reserved_bytes": 12220104704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0033": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 11, 2025 at 04:41:55 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "batch_size": 6,
                "gpu_list": [
                    1,
                    2
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "float32",
                "inference_type": "purely_generative",
                "quantisation": true,
                "batching_options": {
                    "adaptive_batching": true,
                    "max_batch_size": 6
                },
                "sharding_config": {
                    "fsdp_config": {
                        "reshard_after_forward": true
                    }
                },
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.5308671619277447,
                    "average_latency_ms_per_batch": 1530.8671619277447,
                    "throughput_queries_per_sec": 3.2662309051251204,
                    "throughput_tokens_per_sec": 163.311545256256
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 90.97767618697573,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.21293113238015168,
                    "cpu_energy": 0.00016628999781096354,
                    "gpu_energy": 0.00020699349892083774,
                    "ram_energy": 0.00027899009758110776,
                    "total_energy_consumed_kwh": 0.0006522735943129091,
                    "total_energy_consumed_joules": 2348.1849395264726,
                    "final_emissions": 0.0002484836257535027
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 1841715200.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 15409717248.0,
                    "max_memory_allocated_bytes": 15409717248.0,
                    "current_memory_reserved_bytes": 23978835968.0,
                    "max_memory_reserved_bytes": 23978835968.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0034": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 11, 2025 at 04:43:09 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "batch_size": 6,
                "gpu_list": [
                    1,
                    2
                ],
                "decoder_temperature": 2,
                "query_rate": 1,
                "fp_precision": "float32",
                "inference_type": "purely_generative",
                "quantisation": true,
                "batching_options": {
                    "adaptive_batching": true,
                    "max_batch_size": 6
                },
                "sharding_config": {
                    "fsdp_config": {
                        "reshard_after_forward": true
                    }
                },
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.5248855410609394,
                    "average_latency_ms_per_batch": 1524.8855410609394,
                    "throughput_queries_per_sec": 3.279146354077803,
                    "throughput_tokens_per_sec": 163.95731770389014
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 91.29378183852977,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.21228238585749748,
                    "cpu_energy": 0.0001662277755240211,
                    "gpu_energy": 0.00020934572302166998,
                    "ram_energy": 0.00027869201016186146,
                    "total_energy_consumed_kwh": 0.0006542655087075524,
                    "total_energy_consumed_joules": 2355.355831347189,
                    "final_emissions": 0.00024924244554214213
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 1847697408.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 15409717248.0,
                    "max_memory_allocated_bytes": 15409717248.0,
                    "current_memory_reserved_bytes": 23978835968.0,
                    "max_memory_reserved_bytes": 23978835968.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0035": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 11, 2025 at 04:58:13 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "batch_size": 6,
                "gpu_list": [
                    1,
                    2
                ],
                "decoder_temperature": 2,
                "query_rate": 1,
                "fp_precision": "float16",
                "inference_type": "purely_generative",
                "quantisation": true,
                "batching_options": {
                    "adaptive_batching": true,
                    "max_batch_size": 6
                },
                "sharding_config": {
                    "fsdp_config": {
                        "reshard_after_forward": true
                    }
                },
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.3076690023299307,
                    "average_latency_ms_per_batch": 1307.6690023299307,
                    "throughput_queries_per_sec": 3.82490466862283,
                    "throughput_tokens_per_sec": 191.2452334311415
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 98.25236642119026,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.24947590460464963,
                    "cpu_energy": 0.00015235163853503762,
                    "gpu_energy": 0.000148832063509019,
                    "ram_energy": 0.00025558036808987416,
                    "total_energy_consumed_kwh": 0.000556764070133931,
                    "total_energy_consumed_joules": 2004.3506524821512,
                    "final_emissions": 0.000212099272517521
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2925481984.0,
                    "gpu_utilization_percent": [
                        11.0,
                        12.0,
                        5.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 7712172544.0,
                    "max_memory_allocated_bytes": 7712172544.0,
                    "current_memory_reserved_bytes": 12220104704.0,
                    "max_memory_reserved_bytes": 12220104704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0036": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 11, 2025 at 05:01:03 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "batch_size": 6,
                "gpu_list": [
                    1,
                    2
                ],
                "decoder_temperature": 2,
                "query_rate": 1,
                "fp_precision": "float16",
                "inference_type": "purely_generative",
                "quantisation": true,
                "batching_options": {
                    "adaptive_batching": true,
                    "max_batch_size": 6
                },
                "sharding_config": {
                    "fsdp_config": {
                        "reshard_after_forward": true
                    }
                },
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.298543034819886,
                    "average_latency_ms_per_batch": 1298.543034819886,
                    "throughput_queries_per_sec": 3.8505132511891667,
                    "throughput_tokens_per_sec": 192.52566255945834
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 91.9672153788457,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.2559461597924463,
                    "cpu_energy": 0.0001516987476061331,
                    "gpu_energy": 0.0001364562202752495,
                    "ram_energy": 0.00025449485688981173,
                    "total_energy_consumed_kwh": 0.0005426498247711943,
                    "total_energy_consumed_joules": 1953.5393691762997,
                    "final_emissions": 0.0002067224507465865
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2925787136.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 7712172544.0,
                    "max_memory_allocated_bytes": 7712172544.0,
                    "current_memory_reserved_bytes": 12220104704.0,
                    "max_memory_reserved_bytes": 12220104704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0037": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 11, 2025 at 05:34:19 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "batch_size": 6,
                "gpu_list": [
                    1,
                    2
                ],
                "decoder_temperature": 2,
                "query_rate": 1,
                "fp_precision": "float16",
                "inference_type": "purely_generative",
                "quantisation": true,
                "batching_options": {
                    "adaptive_batching": true,
                    "max_batch_size": 6
                },
                "sharding_config": {
                    "fsdp_config": {}
                },
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.439717702101916,
                    "average_latency_ms_per_batch": 1439.717702101916,
                    "throughput_queries_per_sec": 3.4734558953746997,
                    "throughput_tokens_per_sec": 173.672794768735
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 159.31977932910516,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.20938456939223538,
                    "cpu_energy": 0.0001613171518984018,
                    "gpu_energy": 0.00023154657412050028,
                    "ram_energy": 0.0002704730787498607,
                    "total_energy_consumed_kwh": 0.0006633368047687627,
                    "total_energy_consumed_joules": 2388.012497167546,
                    "final_emissions": 0.0002526981557766602
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.5,
                    "cpu_memory_usage_bytes": 2929352704.0,
                    "gpu_utilization_percent": [
                        15.0,
                        19.0,
                        29.0,
                        20.0
                    ],
                    "current_memory_allocated_bytes": 7712172544.0,
                    "max_memory_allocated_bytes": 7712172544.0,
                    "current_memory_reserved_bytes": 12220104704.0,
                    "max_memory_reserved_bytes": 12220104704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0038": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 11, 2025 at 05:57:10 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "gpu_list": [
                    1,
                    2
                ],
                "decoder_temperature": 2,
                "query_rate": 1,
                "fp_precision": "float16",
                "inference_type": "purely_generative",
                "quantisation": true,
                "batching_options": {
                    "adaptive_batching": true,
                    "max_batch_size": 6
                },
                "sharding_config": {
                    "fsdp_config": {}
                },
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.453078655526042,
                    "average_latency_ms_per_batch": 1453.078655526042,
                    "throughput_queries_per_sec": 3.4409700651111264,
                    "throughput_tokens_per_sec": 172.04850325555634
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 150.1596457706235,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.21122578485279464,
                    "cpu_energy": 0.00016148229815007652,
                    "gpu_energy": 0.0002257451806002564,
                    "ram_energy": 0.00027031055596795544,
                    "total_energy_consumed_kwh": 0.0006575380347182884,
                    "total_energy_consumed_joules": 2367.136924985838,
                    "final_emissions": 0.000250489114325932
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.5,
                    "cpu_memory_usage_bytes": 2932451328.0,
                    "gpu_utilization_percent": [
                        14.0,
                        19.0,
                        29.0,
                        21.0
                    ],
                    "current_memory_allocated_bytes": 7712172544.0,
                    "max_memory_allocated_bytes": 7712172544.0,
                    "current_memory_reserved_bytes": 12220104704.0,
                    "max_memory_reserved_bytes": 12220104704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0039": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 11, 2025 at 06:00:14 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "effective_batch_size": 6,
                "used_gpu": "cuda:0",
                "decoder_temperature": 2,
                "query_rate": 1,
                "effective_fp_precision": "torch.float16",
                "inference_type": "purely_generative",
                "quantisation": true,
                "batching_options": {
                    "adaptive_batching": true,
                    "max_batch_size": 6
                },
                "sharding_config": {
                    "fsdp_config": {}
                },
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.4672492477111518,
                    "average_latency_ms_per_batch": 1467.2492477111518,
                    "throughput_queries_per_sec": 3.4082595508002473,
                    "throughput_tokens_per_sec": 170.41297754001235
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 144.22895411212664,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.20791078479616962,
                    "cpu_energy": 0.0001630137464235304,
                    "gpu_energy": 0.00023157824081465606,
                    "ram_energy": 0.0002734932826304856,
                    "total_energy_consumed_kwh": 0.000668085269868672,
                    "total_energy_consumed_joules": 2405.1069715272197,
                    "final_emissions": 0.0002545070835564706
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2920908800.0,
                    "gpu_utilization_percent": [
                        15.0,
                        19.0,
                        29.0,
                        21.0
                    ],
                    "current_memory_allocated_bytes": 7712172544.0,
                    "max_memory_allocated_bytes": 7712172544.0,
                    "current_memory_reserved_bytes": 12220104704.0,
                    "max_memory_reserved_bytes": 12220104704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0043": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 11, 2025 at 06:59:50 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "effective_batch_size": 8,
                "used_gpu": [
                    "cuda:1",
                    "cuda:0"
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "inference_type": "purely_generative",
                "quantisation": true,
                "batching_options": {
                    "adaptive_batching": true,
                    "max_batch_size": 8
                },
                "sharding_config": {
                    "reshard_after_forward": null,
                    "cpu_offload": "CPUOffload(offload_params=False)",
                    "backward_prefetch": "BackwardPrefetch.BACKWARD_PRE"
                },
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.2472055335529149,
                    "average_latency_ms_per_batch": 1247.2055335529149,
                    "throughput_queries_per_sec": 4.008974684546207,
                    "throughput_tokens_per_sec": 200.44873422731035
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 174.7404524549766,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.2078365729364513,
                    "cpu_energy": 0.00015010391737450845,
                    "gpu_energy": 0.00026633493529715224,
                    "ram_energy": 0.0002518216278607747,
                    "total_energy_consumed_kwh": 0.0006682604805324354,
                    "total_energy_consumed_joules": 2405.7377299167674,
                    "final_emissions": 0.00025457383005883126
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2915487744.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 7712172544.0,
                    "max_memory_allocated_bytes": 7712172544.0,
                    "current_memory_reserved_bytes": 12220104704.0,
                    "max_memory_reserved_bytes": 12220104704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0044": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 11, 2025 at 07:04:01 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "effective_batch_size": 8,
                "used_gpu": [
                    "cuda:1",
                    "cuda:0"
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "inference_type": "purely_generative",
                "quantisation": true,
                "batching_options": {
                    "adaptive_batching": true,
                    "max_batch_size": 8
                },
                "sharding_config": {
                    "reshard_after_forward": null,
                    "cpu_offload": "CPUOffload(offload_params=False)",
                    "backward_prefetch": "BackwardPrefetch.BACKWARD_PRE"
                },
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.2850856601726264,
                    "average_latency_ms_per_batch": 1285.0856601726264,
                    "throughput_queries_per_sec": 3.890926664238033,
                    "throughput_tokens_per_sec": 194.54633321190164
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 173.95865950391186,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.2042255148659446,
                    "cpu_energy": 0.00015276022619218564,
                    "gpu_energy": 0.00027104466127525484,
                    "ram_energy": 0.00025627143316259935,
                    "total_energy_consumed_kwh": 0.0006800763206300398,
                    "total_energy_consumed_joules": 2448.274754268143,
                    "final_emissions": 0.0002590750743440137
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2915252224.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 7712172544.0,
                    "max_memory_allocated_bytes": 7712172544.0,
                    "current_memory_reserved_bytes": 12220104704.0,
                    "max_memory_reserved_bytes": 12220104704.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0045": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 11, 2025 at 07:07:33 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "effective_batch_size": 8,
                "used_gpu": [
                    "cuda:0",
                    "cuda:1"
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "inference_type": "purely_generative",
                "quantisation": true,
                "batching_options": {
                    "adaptive_batching": true,
                    "max_batch_size": 8
                },
                "sharding_config": {
                    "reshard_after_forward": null,
                    "cpu_offload": "CPUOffload(offload_params=False)",
                    "backward_prefetch": "BackwardPrefetch.BACKWARD_PRE"
                },
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.2630905155092478,
                    "average_latency_ms_per_batch": 1263.0905155092478,
                    "throughput_queries_per_sec": 3.958791574168515,
                    "throughput_tokens_per_sec": 197.93957870842576
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 172.70940217293736,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.20429646717490307,
                    "cpu_energy": 0.00015157095110043884,
                    "gpu_energy": 0.0002746196641467691,
                    "ram_energy": 0.0002536512838349191,
                    "total_energy_consumed_kwh": 0.000679841899082127,
                    "total_energy_consumed_joules": 2447.4308366956575,
                    "final_emissions": 0.0002589857714553363
                },
                "compute_performance": {
                    "gpu": "N/A",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2915561472.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 7712172544.0,
                    "max_memory_allocated_bytes": 7712172544.0,
                    "current_memory_reserved_bytes": 12220104704.0,
                    "max_memory_reserved_bytes": 12220104704.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0046": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 11, 2025 at 07:14:12 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 4,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "effective_batch_size": 8,
                "used_gpu": [
                    "cuda:0",
                    "cuda:1"
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "inference_type": "purely_generative",
                "quantisation": true,
                "batching_options": {
                    "adaptive_batching": true,
                    "max_batch_size": 8
                },
                "sharding_config": {
                    "reshard_after_forward": null,
                    "cpu_offload": "CPUOffload(offload_params=False)",
                    "backward_prefetch": "BackwardPrefetch.BACKWARD_PRE"
                },
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.361996426479891,
                    "average_latency_ms_per_batch": 1361.996426479891,
                    "throughput_queries_per_sec": 3.6710840991867077,
                    "throughput_tokens_per_sec": 183.55420495933538
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 159.4735205864071,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.19951962570638482,
                    "cpu_energy": 0.00016037111791956704,
                    "gpu_energy": 0.00026673632452656193,
                    "ram_energy": 0.0002690176394623673,
                    "total_energy_consumed_kwh": 0.0006961250819084964,
                    "total_energy_consumed_joules": 2506.050294870587,
                    "final_emissions": 0.00026518884995304173
                },
                "compute_performance": {
                    "gpu": "N/A",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2923595776.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 7712172544.0,
                    "max_memory_allocated_bytes": 7712172544.0,
                    "current_memory_reserved_bytes": 12220104704.0,
                    "max_memory_reserved_bytes": 12220104704.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0047": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 11, 2025 at 07:15:40 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 3,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "effective_batch_size": 8,
                "used_gpu": [
                    "cuda:0",
                    "cuda:1"
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "inference_type": "purely_generative",
                "quantisation": true,
                "batching_options": {
                    "adaptive_batching": true,
                    "max_batch_size": 8
                },
                "sharding_config": {
                    "reshard_after_forward": null,
                    "cpu_offload": "CPUOffload(offload_params=False)",
                    "backward_prefetch": "BackwardPrefetch.BACKWARD_PRE"
                },
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.2636698517017066,
                    "average_latency_ms_per_batch": 1263.6698517017066,
                    "throughput_queries_per_sec": 3.9567334266264407,
                    "throughput_tokens_per_sec": 197.83667133132204
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 141.5389215844162,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.22099669907651176,
                    "cpu_energy": 0.00015128465268935543,
                    "gpu_energy": 0.00022337684535500557,
                    "ram_energy": 0.00025380485985136426,
                    "total_energy_consumed_kwh": 0.0006284663578957253,
                    "total_energy_consumed_joules": 2262.478888424611,
                    "final_emissions": 0.00023941425904037657
                },
                "compute_performance": {
                    "gpu": "N/A",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2925293568.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 7712172544.0,
                    "max_memory_allocated_bytes": 7712172544.0,
                    "current_memory_reserved_bytes": 12220104704.0,
                    "max_memory_reserved_bytes": 12220104704.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0048": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 11, 2025 at 07:18:26 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 3,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 3
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "effective_batch_size": 8,
                "used_gpu": [
                    "cuda:0",
                    "cuda:2",
                    "cuda:1"
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "inference_type": "purely_generative",
                "quantisation": true,
                "batching_options": {
                    "adaptive_batching": true,
                    "max_batch_size": 8
                },
                "sharding_config": {
                    "reshard_after_forward": null,
                    "cpu_offload": "CPUOffload(offload_params=False)",
                    "backward_prefetch": "BackwardPrefetch.BACKWARD_PRE"
                },
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.2466681694301467,
                    "average_latency_ms_per_batch": 1246.6681694301467,
                    "throughput_queries_per_sec": 4.0107112243279985,
                    "throughput_tokens_per_sec": 200.53556121639994
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 158.1159315767113,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.2046309838096131,
                    "cpu_energy": 0.00022624578791146632,
                    "gpu_energy": 0.0004122953298448806,
                    "ram_energy": 0.0003795526951533008,
                    "total_energy_consumed_kwh": 0.0010180938129096479,
                    "total_energy_consumed_joules": 3665.137726474732,
                    "final_emissions": 0.00038784283802793037
                },
                "compute_performance": {
                    "gpu": "N/A",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2916358826.6666665,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 7345171456.0,
                    "max_memory_allocated_bytes": 7345171456.0,
                    "current_memory_reserved_bytes": 12097770837.333334,
                    "max_memory_reserved_bytes": 12097770837.333334
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0049": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 11, 2025 at 07:59:17 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 3,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "north rhine-westphalia",
                "is_encoder_decoder": "unknown"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "effective_batch_size": 8,
                "used_gpu": [
                    "cuda:1",
                    "cuda:0",
                    "cuda:2"
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "inference_type": "purely_generative",
                "quantisation": true,
                "batching_options": {
                    "adaptive_batching": true,
                    "max_batch_size": 8
                },
                "sharding_config": {
                    "reshard_after_forward": "None",
                    "cpu_offload": "CPUOffload(offload_params=False)",
                    "backward_prefetch": "BackwardPrefetch.BACKWARD_PRE"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 3,
                    "local_process_index": 0
                }
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.9479079769613843,
                    "average_latency_ms_per_batch": 1947.9079769613843,
                    "throughput_queries_per_sec": 2.56687165324136,
                    "throughput_tokens_per_sec": 128.34358266206797
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 117.87053540445338,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.1750347570844107,
                    "cpu_energy": 0.0003010012186568929,
                    "gpu_energy": 0.00038476780782303877,
                    "ram_energy": 0.0005044761934968562,
                    "total_energy_consumed_kwh": 0.0011902452199767878,
                    "total_energy_consumed_joules": 4284.882791916436,
                    "final_emissions": 0.00045342391655015733
                },
                "compute_performance": {
                    "gpu": "N/A",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2936845653.3333335,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 7345171456.0,
                    "max_memory_allocated_bytes": 7345171456.0,
                    "current_memory_reserved_bytes": 12097770837.333334,
                    "max_memory_reserved_bytes": 12097770837.333334
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0050": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": "unknown",
                "task_type": "text_generation",
                "date": "March 11, 2025 at 11:13:08 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "effective_batch_size": 8,
                "used_gpu": [
                    "cuda:0",
                    "cuda:1"
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": false,
                "batching_options": {
                    "adaptive_batching": true,
                    "max_batch_size": 8
                },
                "sharding_config": {
                    "reshard_after_forward": "None",
                    "cpu_offload": "CPUOffload(offload_params=False)",
                    "backward_prefetch": "BackwardPrefetch.BACKWARD_PRE"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 0
                },
                "inference_type": "purely_generative",
                "backend": "pytorch"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.12339484738186,
                    "average_latency_ms_per_batch": 1123.39484738186,
                    "throughput_queries_per_sec": 4.450797062458349,
                    "throughput_tokens_per_sec": 222.53985312291744
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 113.10882244812822,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.2528193065161625,
                    "cpu_energy": 0.00013921193928399588,
                    "gpu_energy": 0.00017663903018672045,
                    "ram_energy": 0.00023351049810259544,
                    "total_energy_consumed_kwh": 0.0005493614675733118,
                    "total_energy_consumed_joules": 1977.7012832639225,
                    "final_emissions": 0.0002092792510720531
                },
                "compute_performance": {
                    "gpu": "N/A",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2916710400.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 7712172544.0,
                    "max_memory_allocated_bytes": 7712172544.0,
                    "current_memory_reserved_bytes": 12220104704.0,
                    "max_memory_reserved_bytes": 12220104704.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0051": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": "unknown",
                "task_type": "text_generation",
                "date": "March 11, 2025 at 11:23:44 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "effective_batch_size": 8,
                "used_gpu": [
                    "cuda:0",
                    "cuda:1"
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": false,
                "batching_options": {
                    "adaptive_batching": true,
                    "max_batch_size": 8
                },
                "sharding_config": {
                    "reshard_after_forward": "None",
                    "cpu_offload": "CPUOffload(offload_params=False)",
                    "backward_prefetch": "BackwardPrefetch.BACKWARD_PRE"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 0
                },
                "inference_type": "purely_generative",
                "backend": "pytorch"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.162611410021782,
                    "average_latency_ms_per_batch": 1162.611410021782,
                    "throughput_queries_per_sec": 4.300664036702923,
                    "throughput_tokens_per_sec": 215.03320183514614
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 78.51173864401312,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.2504805318681688,
                    "cpu_energy": 0.00014115034222777467,
                    "gpu_energy": 0.0001765429190214718,
                    "ram_energy": 0.00023679729257628738,
                    "total_energy_consumed_kwh": 0.0005544905538255338,
                    "total_energy_consumed_joules": 1996.1659937719219,
                    "final_emissions": 0.0002112331764798371
                },
                "compute_performance": {
                    "gpu": "N/A",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2919751680.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 7712172544.0,
                    "max_memory_allocated_bytes": 7712172544.0,
                    "current_memory_reserved_bytes": 12220104704.0,
                    "max_memory_reserved_bytes": 12220104704.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0052": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": "unknown",
                "task_type": "text_generation",
                "date": "March 11, 2025 at 11:34:20 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "effective_batch_size": 8,
                "used_gpu": [
                    "cuda:0",
                    "cuda:1"
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": false,
                "batching_options": {
                    "adaptive_batching": true,
                    "max_batch_size": 8
                },
                "sharding_config": {
                    "reshard_after_forward": "None",
                    "cpu_offload": "CPUOffload(offload_params=False)",
                    "backward_prefetch": "BackwardPrefetch.BACKWARD_PRE"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 0
                },
                "inference_type": "purely_generative",
                "backend": "pytorch"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.1474910366814584,
                    "average_latency_ms_per_batch": 1147.4910366814584,
                    "throughput_queries_per_sec": 4.357333468438684,
                    "throughput_tokens_per_sec": 217.8666734219342
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 82.94679712199904,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.2518871880080786,
                    "cpu_energy": 0.00014058524697611576,
                    "gpu_energy": 0.00017495347330509503,
                    "ram_energy": 0.0002358585587858901,
                    "total_energy_consumed_kwh": 0.0005513972790671009,
                    "total_energy_consumed_joules": 1985.0302046415632,
                    "final_emissions": 0.0002100547934606121
                },
                "compute_performance": {
                    "gpu": "N/A",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2919835648.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 7712172544.0,
                    "max_memory_allocated_bytes": 7712172544.0,
                    "current_memory_reserved_bytes": 12220104704.0,
                    "max_memory_reserved_bytes": 12220104704.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0053": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": "decoder_only",
                "task_type": "text_generation",
                "date": "March 11, 2025 at 11:37:43 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "effective_batch_size": 8,
                "used_gpu": [
                    "cuda:0",
                    "cuda:1"
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": false,
                "batching_options": {
                    "adaptive_batching": true,
                    "max_batch_size": 8
                },
                "sharding_config": {
                    "reshard_after_forward": "None",
                    "cpu_offload": "CPUOffload(offload_params=False)",
                    "backward_prefetch": "BackwardPrefetch.BACKWARD_PRE"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 0
                },
                "inference_type": "purely_generative",
                "backend": "pytorch"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.1488070995546877,
                    "average_latency_ms_per_batch": 1148.8070995546877,
                    "throughput_queries_per_sec": 4.353101021083553,
                    "throughput_tokens_per_sec": 217.65505105417765
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 88.90371246549464,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.2502157345752699,
                    "cpu_energy": 0.00014149416195868982,
                    "gpu_energy": 0.00017622402986461339,
                    "ram_energy": 0.00023736331737962961,
                    "total_energy_consumed_kwh": 0.0005550815092029328,
                    "total_energy_consumed_joules": 1998.2934331305578,
                    "final_emissions": 0.00021145830093085725
                },
                "compute_performance": {
                    "gpu": "N/A",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2915225600.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 7712172544.0,
                    "max_memory_allocated_bytes": 7712172544.0,
                    "current_memory_reserved_bytes": 12220104704.0,
                    "max_memory_reserved_bytes": 12220104704.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0054": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": "decoder_only",
                "task_type": "text_generation",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "north rhine-westphalia",
                "date": "March 11, 2025 at 11:47:12 PM"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "effective_batch_size": 8,
                "used_gpu": [
                    "cuda:1",
                    "cuda:0"
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float32",
                "quantisation": false,
                "batching_options": {
                    "adaptive_batching": true,
                    "max_batch_size": 8
                },
                "sharding_config": {
                    "reshard_after_forward": "None",
                    "cpu_offload": "CPUOffload(offload_params=False)",
                    "backward_prefetch": "BackwardPrefetch.BACKWARD_PRE"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 0
                },
                "inference_type": "purely_generative",
                "backend": "pytorch"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.403931210981682,
                    "average_latency_ms_per_batch": 1403.931210981682,
                    "throughput_queries_per_sec": 3.561855044810822,
                    "throughput_tokens_per_sec": 178.09275224054107
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 106.07633232307207,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.1898829802606946,
                    "cpu_energy": 0.00015723607856489254,
                    "gpu_energy": 0.0003104252483510095,
                    "ram_energy": 0.0002637980771536435,
                    "total_energy_consumed_kwh": 0.0007314594040695455,
                    "total_energy_consumed_joules": 2633.253854650364,
                    "final_emissions": 0.0002786494599802934
                },
                "compute_performance": {
                    "gpu": "N/A",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 1842374656.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 15409717248.0,
                    "max_memory_allocated_bytes": 15409717248.0,
                    "current_memory_reserved_bytes": 23978835968.0,
                    "max_memory_reserved_bytes": 23978835968.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0055": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": "decoder_only",
                "task_type": "text_generation",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "north rhine-westphalia",
                "date": "March 11, 2025 at 11:53:09 PM"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "effective_batch_size": 8,
                "used_gpu": [
                    "cuda:0",
                    "cuda:1"
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": false,
                "batching_options": {
                    "adaptive_batching": true,
                    "max_batch_size": 8
                },
                "sharding_config": {
                    "reshard_after_forward": "None",
                    "cpu_offload": "CPUOffload(offload_params=False)",
                    "backward_prefetch": "BackwardPrefetch.BACKWARD_PRE"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 0
                },
                "inference_type": "purely_generative",
                "backend": "pytorch"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.1502813291735947,
                    "average_latency_ms_per_batch": 1150.2813291735947,
                    "throughput_queries_per_sec": 4.346773932654656,
                    "throughput_tokens_per_sec": 217.33869663273282
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 104.8592310391905,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.25077868405551834,
                    "cpu_energy": 0.00014076754382404035,
                    "gpu_energy": 0.00017690986373963824,
                    "ram_energy": 0.00023615501765314375,
                    "total_energy_consumed_kwh": 0.0005538324252168223,
                    "total_energy_consumed_joules": 1993.7967307805604,
                    "final_emissions": 0.00021098246238634847
                },
                "compute_performance": {
                    "gpu": "N/A",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2926428160.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 7712172544.0,
                    "max_memory_allocated_bytes": 7712172544.0,
                    "current_memory_reserved_bytes": 12220104704.0,
                    "max_memory_reserved_bytes": 12220104704.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0056": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": "decoder_only",
                "task_type": "text_generation",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "north rhine-westphalia",
                "date": "March 11, 2025 at 11:53:42 PM"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "effective_batch_size": 8,
                "used_gpu": [
                    "cuda:0",
                    "cuda:1"
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float32",
                "quantisation": false,
                "batching_options": {
                    "adaptive_batching": true,
                    "max_batch_size": 8
                },
                "sharding_config": {
                    "reshard_after_forward": "None",
                    "cpu_offload": "CPUOffload(offload_params=False)",
                    "backward_prefetch": "BackwardPrefetch.BACKWARD_PRE"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 0
                },
                "inference_type": "purely_generative",
                "backend": "pytorch"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.4040189103689045,
                    "average_latency_ms_per_batch": 1404.0189103689045,
                    "throughput_queries_per_sec": 3.561362279415264,
                    "throughput_tokens_per_sec": 178.06811397076322
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 111.57544877287037,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.1902977184581841,
                    "cpu_energy": 0.00015720185251848307,
                    "gpu_energy": 0.00030891330269611217,
                    "ram_energy": 0.00026374424979409795,
                    "total_energy_consumed_kwh": 0.0007298594050086933,
                    "total_energy_consumed_joules": 2627.4938580312955,
                    "final_emissions": 0.0002780399403380617
                },
                "compute_performance": {
                    "gpu": "N/A",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 1846958080.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 15409717248.0,
                    "max_memory_allocated_bytes": 15409717248.0,
                    "current_memory_reserved_bytes": 23978835968.0,
                    "max_memory_reserved_bytes": 23978835968.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0057": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": "decoder_only",
                "task_type": "text_generation",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "north rhine-westphalia",
                "date": "March 11, 2025 at 11:54:17 PM"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 20,
                "total_token_inputted": 10240,
                "total_tokens_outputted": 1000,
                "effective_batch_size": 8,
                "used_gpu": [
                    "cuda:0",
                    "cuda:1"
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float32",
                "quantisation": false,
                "batching_options": {
                    "adaptive_batching": true,
                    "max_batch_size": 8
                },
                "sharding_config": {
                    "reshard_after_forward": "None",
                    "cpu_offload": "CPUOffload(offload_params=False)",
                    "backward_prefetch": "BackwardPrefetch.BACKWARD_PRE"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 0
                },
                "inference_type": "purely_generative",
                "backend": "pytorch"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 4.2286352042574435,
                    "average_latency_ms_per_batch": 1409.5450680858144,
                    "throughput_queries_per_sec": 4.729666942329156,
                    "throughput_tokens_per_sec": 236.4833471164578
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 112.09020162133308,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.2351448370238357,
                    "cpu_energy": 0.0004705353715253296,
                    "gpu_energy": 0.001102626159877218,
                    "ram_energy": 0.000789455016267768,
                    "total_energy_consumed_kwh": 0.0023626165476703155,
                    "total_energy_consumed_joules": 8505.419571613136,
                    "final_emissions": 0.0009000387738350067
                },
                "compute_performance": {
                    "gpu": "N/A",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 1861488640.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 15409717248.0,
                    "max_memory_allocated_bytes": 15409717248.0,
                    "current_memory_reserved_bytes": 23980933120.0,
                    "max_memory_reserved_bytes": 23980933120.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0058": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": "decoder_only",
                "task_type": "text_generation",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "north rhine-westphalia",
                "date": "March 11, 2025 at 11:54:42 PM"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 20,
                "total_token_inputted": 10240,
                "total_tokens_outputted": 1000,
                "effective_batch_size": 8,
                "used_gpu": [
                    "cuda:0",
                    "cuda:1"
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float32",
                "quantisation": false,
                "batching_options": {
                    "adaptive_batching": true,
                    "max_batch_size": 8
                },
                "sharding_config": {
                    "reshard_after_forward": "None",
                    "cpu_offload": "CPUOffload(offload_params=False)",
                    "backward_prefetch": "BackwardPrefetch.BACKWARD_PRE"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 0
                },
                "inference_type": "purely_generative",
                "backend": "pytorch"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 4.207525264238939,
                    "average_latency_ms_per_batch": 1402.5084214129797,
                    "throughput_queries_per_sec": 4.753407896570189,
                    "throughput_tokens_per_sec": 237.67039482850944
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 108.15242779246942,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.23536610492458515,
                    "cpu_energy": 0.0004697031174291624,
                    "gpu_energy": 0.0011026342154565327,
                    "ram_energy": 0.0007880524651935976,
                    "total_energy_consumed_kwh": 0.002360389798079293,
                    "total_energy_consumed_joules": 8497.403273085454,
                    "final_emissions": 0.0008991904935783065
                },
                "compute_performance": {
                    "gpu": "N/A",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 1862432768.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 15409717248.0,
                    "max_memory_allocated_bytes": 15409717248.0,
                    "current_memory_reserved_bytes": 23980933120.0,
                    "max_memory_reserved_bytes": 23980933120.0
                },
                "task-specific_performance": {}
            }
        }
    }
]