[
    {
        "EXPERIMENT_0008": {
            "setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony",
                "date": "March 21, 2025 at 06:22:05 PM"
            },
            "variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_input_prompts": null,
                "used_gpu": "cuda:1",
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "adaptive_batching": false,
                    "max_batch_size": 8,
                    "adaptive_max_tokens": 256
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": null
        }
    },
    {
        "EXPERIMENT_0009": {
            "setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony",
                "date": "March 21, 2025 at 06:22:05 PM"
            },
            "variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_input_prompts": null,
                "used_gpu": "cuda:0",
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "adaptive_batching": false,
                    "max_batch_size": 8,
                    "adaptive_max_tokens": 256
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 0
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "experiment_results": {
                    "inference_performance": {
                        "total_inference_time_sec": 0.0,
                        "average_latency_ms_per_batch": 0.0,
                        "throughput_queries_per_sec": 0.0,
                        "throughput_tokens_per_sec": 0.0
                    },
                    "energy_performance": {
                        "cpu_power": 112.5,
                        "gpu_power": 207.81855314974774,
                        "ram_power": 188.82433032989502,
                        "energy_efficiency_tokens_per_joule": 0.43361681288057624,
                        "cpu_energy": 7.734670931677101e-05,
                        "gpu_energy": 0.00011322870169205856,
                        "ram_energy": 0.00012974964678614268,
                        "total_energy_kwh": 0.00032032505779497225,
                        "total_energy_joules": 1153.1702080619002,
                        "final_emissions": [
                            6.0510870410126185e-05,
                            6.15169603568685e-05
                        ]
                    },
                    "compute_performance": {
                        "FLOPs": 0,
                        "cpu_usage_percent": 0,
                        "gpu_utilization_percent": 0,
                        "current_memory_allocated_bytes": 0,
                        "max_memory_allocated_bytes": 0,
                        "current_memory_reserved_bytes": 0,
                        "max_memory_reserved_bytes": 0
                    },
                    "task_specific_performance": {}
                }
            }
        }
    },
    {
        "EXPERIMENT_0010": {
            "setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony",
                "date": "March 21, 2025 at 06:30:13 PM"
            },
            "variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_input_prompts": null,
                "used_gpu": "cuda:0",
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "adaptive_batching": false,
                    "max_batch_size": 1,
                    "adaptive_max_tokens": 256
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 0
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "experiment_results": {
                    "inference_performance": {
                        "total_inference_time_sec": 0.0,
                        "average_latency_ms_per_batch": 0.0,
                        "throughput_queries_per_sec": 0.0,
                        "throughput_tokens_per_sec": 0.0
                    },
                    "energy_performance": {
                        "cpu_power": 112.5,
                        "gpu_power": 99.11410355734836,
                        "ram_power": 188.82433032989502,
                        "energy_efficiency_tokens_per_joule": 0.09761774457485342,
                        "cpu_energy": 0.0004002226387729024,
                        "gpu_energy": 0.00035258722651354546,
                        "ram_energy": 0.0006700793958463762,
                        "total_energy_kwh": 0.0014228892611328242,
                        "total_energy_joules": 5122.401340078168,
                        "final_emissions": [
                            0.0002733645424196538,
                            0.0002686851216088956
                        ]
                    },
                    "compute_performance": {
                        "FLOPs": 0,
                        "cpu_usage_percent": 0,
                        "gpu_utilization_percent": 0,
                        "current_memory_allocated_bytes": 0,
                        "max_memory_allocated_bytes": 0,
                        "current_memory_reserved_bytes": 0,
                        "max_memory_reserved_bytes": 0
                    },
                    "task_specific_performance": {}
                }
            }
        }
    },
    {
        "EXPERIMENT_0011": {
            "setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony",
                "date": "March 21, 2025 at 06:30:13 PM"
            },
            "variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_input_prompts": null,
                "used_gpu": "cuda:1",
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "adaptive_batching": false,
                    "max_batch_size": 1,
                    "adaptive_max_tokens": 256
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": null
        }
    },
    {
        "EXPERIMENT_0012": {
            "setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony",
                "date": "March 21, 2025 at 06:31:46 PM"
            },
            "variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_input_prompts": null,
                "used_gpu": "cuda:0",
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "adaptive_batching": false,
                    "fixed_batch_size": 2,
                    "adaptive_max_tokens": 256
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 0
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "experiment_results": {
                    "inference_performance": {
                        "total_inference_time_sec": 0.0,
                        "average_latency_ms_per_batch": 0.0,
                        "throughput_queries_per_sec": 0.0,
                        "throughput_tokens_per_sec": 0.0
                    },
                    "energy_performance": {
                        "cpu_power": 112.5,
                        "gpu_power": 198.90999331310394,
                        "ram_power": 188.82433032989502,
                        "energy_efficiency_tokens_per_joule": 0.43791105065879055,
                        "cpu_energy": 7.659026771762002e-05,
                        "gpu_energy": 0.0001121106452446341,
                        "ram_energy": 0.0001284657835839562,
                        "total_energy_kwh": 0.0003171666965462103,
                        "total_energy_joules": 1141.8001075663572,
                        "final_emissions": [
                            6.0636498620547554e-05,
                            6.018815442873127e-05
                        ]
                    },
                    "compute_performance": {
                        "FLOPs": 0,
                        "cpu_usage_percent": 0,
                        "gpu_utilization_percent": 0,
                        "current_memory_allocated_bytes": 0,
                        "max_memory_allocated_bytes": 0,
                        "current_memory_reserved_bytes": 0,
                        "max_memory_reserved_bytes": 0
                    },
                    "task_specific_performance": {}
                }
            }
        }
    },
    {
        "EXPERIMENT_0013": {
            "setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony",
                "date": "March 21, 2025 at 06:31:46 PM"
            },
            "variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_input_prompts": null,
                "used_gpu": "cuda:1",
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "adaptive_batching": false,
                    "fixed_batch_size": 2,
                    "adaptive_max_tokens": 256
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": null
        }
    },
    {
        "EXPERIMENT_0014": {
            "setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony",
                "date": "March 21, 2025 at 06:36:28 PM"
            },
            "variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_input_prompts": null,
                "used_gpu": "cuda:1",
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 256
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": null
        }
    },
    {
        "EXPERIMENT_0015": {
            "setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony",
                "date": "March 21, 2025 at 06:36:28 PM"
            },
            "variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_input_prompts": null,
                "used_gpu": "cuda:0",
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 1,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 256
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 0
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "experiment_results": {
                    "inference_performance": {
                        "total_inference_time_sec": 0.0,
                        "average_latency_ms_per_batch": 0.0,
                        "throughput_queries_per_sec": 0.0,
                        "throughput_tokens_per_sec": 0.0
                    },
                    "energy_performance": {
                        "cpu_power": 112.5,
                        "gpu_power": 164.487536856085,
                        "ram_power": 188.82433032989502,
                        "energy_efficiency_tokens_per_joule": 0.11811196524625148,
                        "cpu_energy": 0.00031513370049833614,
                        "gpu_energy": 0.0003321822101902683,
                        "ram_energy": 0.0005286160173555818,
                        "total_energy_kwh": 0.0011759319280441863,
                        "total_energy_joules": 4233.3549409590705,
                        "final_emissions": [
                            0.0002249808698287441,
                            0.00022299039815968868
                        ]
                    },
                    "compute_performance": {
                        "FLOPs": 0,
                        "cpu_usage_percent": 0,
                        "gpu_utilization_percent": 0,
                        "current_memory_allocated_bytes": 0,
                        "max_memory_allocated_bytes": 0,
                        "current_memory_reserved_bytes": 0,
                        "max_memory_reserved_bytes": 0
                    },
                    "task_specific_performance": {}
                }
            }
        }
    },
    {
        "EXPERIMENT_0016": {
            "setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony",
                "date": "March 21, 2025 at 06:36:55 PM"
            },
            "variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_input_prompts": null,
                "used_gpu": "cuda:0",
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 2,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 256
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 0
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "experiment_results": {
                    "inference_performance": {
                        "total_inference_time_sec": 0.0,
                        "average_latency_ms_per_batch": 0.0,
                        "throughput_queries_per_sec": 0.0,
                        "throughput_tokens_per_sec": 0.0
                    },
                    "energy_performance": {
                        "cpu_power": 112.5,
                        "gpu_power": 166.318558263544,
                        "ram_power": 188.82433032989502,
                        "energy_efficiency_tokens_per_joule": 0.17057731697064146,
                        "cpu_energy": 0.00020306575534232254,
                        "gpu_energy": 0.0002705491053269782,
                        "ram_energy": 0.0003406160779516269,
                        "total_energy_kwh": 0.0008142309386209276,
                        "total_energy_joules": 2931.231379035339,
                        "final_emissions": [
                            0.0001548145896822053,
                            0.00015536668638543708
                        ]
                    },
                    "compute_performance": {
                        "FLOPs": 0,
                        "cpu_usage_percent": 0,
                        "gpu_utilization_percent": 0,
                        "current_memory_allocated_bytes": 0,
                        "max_memory_allocated_bytes": 0,
                        "current_memory_reserved_bytes": 0,
                        "max_memory_reserved_bytes": 0
                    },
                    "task_specific_performance": {}
                }
            }
        }
    },
    {
        "EXPERIMENT_0017": {
            "setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony",
                "date": "March 21, 2025 at 06:36:55 PM"
            },
            "variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_input_prompts": null,
                "used_gpu": "cuda:1",
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 2,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 256
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": null
        }
    },
    {
        "EXPERIMENT_0018": {
            "setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony",
                "date": "March 21, 2025 at 06:39:08 PM"
            },
            "variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_input_prompts": null,
                "used_gpu": "cuda:0",
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 2,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 256
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 0
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "experiment_results": {
                    "inference_performance": {
                        "total_inference_time_sec": 0.0,
                        "average_latency_ms_per_batch": 0.0,
                        "throughput_queries_per_sec": 0.0,
                        "throughput_tokens_per_sec": 0.0
                    },
                    "energy_performance": {
                        "cpu_power": 112.5,
                        "gpu_power": 114.43012839858753,
                        "ram_power": 188.82433032989502,
                        "energy_efficiency_tokens_per_joule": 0.13758968782228523,
                        "cpu_energy": 0.00027696064765223125,
                        "gpu_energy": 0.0002678563253959432,
                        "ram_energy": 0.0004646367971834934,
                        "total_energy_kwh": 0.001009453770231668,
                        "total_energy_joules": 3634.033572834004,
                        "final_emissions": [
                            0.00019291600291379137,
                            0.0001916354108559625
                        ]
                    },
                    "compute_performance": {
                        "FLOPs": 2648432967680.0,
                        "cpu_usage_percent": 0,
                        "gpu_utilization_percent": 0,
                        "current_memory_allocated_bytes": 0,
                        "max_memory_allocated_bytes": 0,
                        "current_memory_reserved_bytes": 0,
                        "max_memory_reserved_bytes": 0
                    },
                    "task_specific_performance": {}
                }
            }
        }
    },
    {
        "EXPERIMENT_0019": {
            "setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony",
                "date": "March 21, 2025 at 06:39:08 PM"
            },
            "variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_input_prompts": null,
                "used_gpu": "cuda:1",
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 2,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 256
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": null
        }
    },
    {
        "EXPERIMENT_0020": {
            "setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony",
                "date": "March 21, 2025 at 06:44:45 PM"
            },
            "variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_input_prompts": null,
                "used_gpu": "cuda:0",
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 2,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 256
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 0
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "experiment_results": {
                    "inference_performance": {
                        "total_inference_time_sec": 0.0,
                        "average_latency_ms_per_batch": 0.0,
                        "throughput_queries_per_sec": 0.0,
                        "throughput_tokens_per_sec": 0.0
                    },
                    "energy_performance": {
                        "cpu_power": 112.5,
                        "gpu_power": 175.87905795866828,
                        "ram_power": 188.82433032989502,
                        "energy_efficiency_tokens_per_joule": 0.1736828882117199,
                        "cpu_energy": 0.00019920012256261546,
                        "gpu_energy": 0.00026639632422842396,
                        "ram_energy": 0.0003341591785755288,
                        "total_energy_kwh": 0.0007997556253665681,
                        "total_energy_joules": 2879.120251319645,
                        "final_emissions": [
                            0.0001539153011503731,
                            0.00015075160433302104
                        ]
                    },
                    "compute_performance": {
                        "FLOPs": 0,
                        "cpu_usage_percent": 0,
                        "gpu_utilization_percent": 0,
                        "current_memory_allocated_bytes": 0,
                        "max_memory_allocated_bytes": 0,
                        "current_memory_reserved_bytes": 0,
                        "max_memory_reserved_bytes": 0
                    },
                    "task_specific_performance": {}
                }
            }
        }
    },
    {
        "EXPERIMENT_0021": {
            "setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony",
                "date": "March 21, 2025 at 06:44:45 PM"
            },
            "variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_input_prompts": null,
                "used_gpu": "cuda:1",
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 2,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 256
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": null
        }
    },
    {
        "EXPERIMENT_0022": {
            "setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony",
                "date": "March 21, 2025 at 06:54:31 PM"
            },
            "variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_input_prompts": null,
                "used_gpu": "cuda:0",
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 2,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 256
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 0
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "experiment_results": {
                    "inference_performance": {
                        "total_inference_time_sec": 0.0,
                        "average_latency_ms_per_batch": 0.0,
                        "throughput_queries_per_sec": 0.0,
                        "throughput_tokens_per_sec": 0.0
                    },
                    "energy_performance": {
                        "cpu_power": 112.5,
                        "gpu_power": 156.75918444039874,
                        "ram_power": 188.82433032989502,
                        "energy_efficiency_tokens_per_joule": 0.17517918053022324,
                        "cpu_energy": 0.00020052442031374086,
                        "gpu_energy": 0.00025596881588674236,
                        "ram_energy": 0.0003363802935782798,
                        "total_energy_kwh": 0.0007928735297787631,
                        "total_energy_joules": 2854.344707203547,
                        "final_emissions": [
                            0.00015201900992444317,
                            0.00015002616124477664
                        ]
                    },
                    "compute_performance": {
                        "FLOPs": 2648432967680.0,
                        "cpu_usage_percent": 0,
                        "gpu_utilization_percent": 0,
                        "current_memory_allocated_bytes": 0,
                        "max_memory_allocated_bytes": 0,
                        "current_memory_reserved_bytes": 0,
                        "max_memory_reserved_bytes": 0
                    },
                    "task_specific_performance": {}
                }
            }
        }
    },
    {
        "EXPERIMENT_0023": {
            "setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony",
                "date": "March 21, 2025 at 06:54:31 PM"
            },
            "variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_input_prompts": null,
                "used_gpu": "cuda:1",
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 2,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 256
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": null
        }
    },
    {
        "EXPERIMENT_0024": {
            "setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony",
                "date": "March 21, 2025 at 06:57:49 PM"
            },
            "variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_input_prompts": null,
                "used_gpu": "cuda:0",
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 2,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 256
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 0
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "experiment_results": {
                    "inference_performance": {
                        "total_inference_time_sec": 0.0,
                        "average_latency_ms_per_batch": 0.0,
                        "throughput_queries_per_sec": 0.0,
                        "throughput_tokens_per_sec": 0.0
                    },
                    "energy_performance": {
                        "cpu_power": 112.5,
                        "gpu_power": 122.77041227689168,
                        "ram_power": 188.82433032989502,
                        "energy_efficiency_tokens_per_joule": 0.15317715107300472,
                        "cpu_energy": 0.000243540600342385,
                        "gpu_energy": 0.0002546402037109097,
                        "ram_energy": 0.0004086120595568737,
                        "total_energy_kwh": 0.0009067928636101684,
                        "total_energy_joules": 3264.454308996606,
                        "final_emissions": [
                            0.00017426251946944002,
                            0.00017118022192285363
                        ]
                    },
                    "compute_performance": {
                        "FLOPs": 2648432967680.0,
                        "cpu_usage_percent": 0,
                        "gpu_utilization_percent": 0,
                        "current_memory_allocated_bytes": 0,
                        "max_memory_allocated_bytes": 0,
                        "current_memory_reserved_bytes": 0,
                        "max_memory_reserved_bytes": 0
                    },
                    "task_specific_performance": {}
                }
            }
        }
    },
    {
        "EXPERIMENT_0025": {
            "setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony",
                "date": "March 21, 2025 at 06:57:49 PM"
            },
            "variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_input_prompts": null,
                "used_gpu": "cuda:1",
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 2,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 256
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": null
        }
    },
    {
        "EXPERIMENT_0026": {
            "setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony",
                "date": "March 21, 2025 at 07:05:06 PM"
            },
            "variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_input_prompts": null,
                "used_gpu": "cuda:0",
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 2,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 256
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 0
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": {
                "experiment_results": {
                    "inference_performance": {
                        "total_inference_time_sec": 0.0,
                        "average_latency_ms_per_batch": 0.0,
                        "throughput_queries_per_sec": 0.0,
                        "throughput_tokens_per_sec": 0.0
                    },
                    "energy_performance": {
                        "cpu_power": 112.5,
                        "gpu_power": 153.78414889730158,
                        "ram_power": 188.82433032989502,
                        "energy_efficiency_tokens_per_joule": 0.17403997384840372,
                        "cpu_energy": 0.00020094281984347616,
                        "gpu_energy": 0.0002600260413556654,
                        "ram_energy": 0.00033707961748900535,
                        "total_energy_kwh": 0.0007980484786881469,
                        "total_energy_joules": 2872.9745232773294,
                        "final_emissions": [
                            0.0001527656023157481,
                            0.0001512509656405015
                        ]
                    },
                    "compute_performance": {
                        "FLOPs": 2648432967680.0,
                        "cpu_usage_percent": 0,
                        "gpu_utilization_percent": 0,
                        "current_memory_allocated_bytes": 0,
                        "max_memory_allocated_bytes": 0,
                        "current_memory_reserved_bytes": 0,
                        "max_memory_reserved_bytes": 0
                    },
                    "task_specific_performance": {}
                }
            }
        }
    },
    {
        "EXPERIMENT_0027": {
            "setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "is_encoder_decoder": false,
                "task_type": "text_generation",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "country": "Germany",
                "region": "saxony",
                "date": "March 21, 2025 at 07:05:06 PM"
            },
            "variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_input_prompts": null,
                "used_gpu": "cuda:1",
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "torch.float16",
                "quantisation": null,
                "batching_options": {
                    "fixed_max_batch_size": 2,
                    "adaptive_batching": false,
                    "adaptive_max_tokens": 256
                },
                "sharding_config": {
                    "fsdp_config": {
                        "use_orig_params": true,
                        "cpu_offload": true
                    },
                    "sharding_strategy": "NO_SHARD"
                },
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2,
                    "local_process_index": 1
                },
                "inference_type": "pure_generative",
                "backend": "pytorch"
            },
            "model_architecture": {
                "total_params": 1100048384,
                "architecture": "Unknown (no config attribute)"
            },
            "results": null
        }
    }
]