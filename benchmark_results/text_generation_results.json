[
    {
        "EXPERIMENT #0001": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 04, 2025 at 07:11:46 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "mecklenburg-vorpommern"
            },
            "experiment_variables": {
                "input_tokens": 2560,
                "output_tokens": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 2.9046354780439287,
                    "average_latency_ms_per_batch": 2904.6354780439287,
                    "average_ttft_ms": 0.0,
                    "throughput_queries_per_sec": 1.721420551464691,
                    "throughput_tokens_per_sec": 86.07102757323455
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 0.0,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.17437181807253316,
                    "cpu_energy": 0.00019891567426384428,
                    "gpu_energy": 0.000264675211752774,
                    "ram_energy": 0.00033291941715275514,
                    "total_energy_consumed_kwh": 0.0007965103031693735,
                    "total_energy_consumed_joules": 2867.4370914097444,
                    "final_emissions": 0.00030343059999237283
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2867941376.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0002": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 04, 2025 at 07:35:00 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "mecklenburg-vorpommern"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 3.069109580013901,
                    "average_latency_ms_per_batch": 3069.109580013901,
                    "average_ttft_ms": 0.0,
                    "throughput_queries_per_sec": 1.6291373232080473,
                    "throughput_tokens_per_sec": 81.45686616040237
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 235.83204577308427,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.15470288405248556,
                    "cpu_energy": 0.0002534404672478558,
                    "gpu_energy": 0.00021988628699887158,
                    "ram_energy": 0.00042445479029182227,
                    "total_energy_consumed_kwh": 0.0008977815445385496,
                    "total_energy_consumed_joules": 3232.013560338779,
                    "final_emissions": 0.0003420098793919605
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2872322048.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0003": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 04, 2025 at 07:38:29 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "mecklenburg-vorpommern"
            },
            "experiment_variables": {
                "total_token_inputted": 25600,
                "total_tokens_outputted": 2500,
                "number_runs": 50
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 9.075287469662726,
                    "average_latency_ms_per_batch": 1296.4696385232464,
                    "average_ttft_ms": 0.0,
                    "throughput_queries_per_sec": 5.509471823871618,
                    "throughput_tokens_per_sec": 275.4735911935809
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 196.4996529529407,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.4935331359579749,
                    "cpu_energy": 0.0006421420079132077,
                    "gpu_energy": 0.0010969730997913985,
                    "ram_energy": 0.0010750623629472243,
                    "total_energy_consumed_kwh": 0.00281417747065183,
                    "total_energy_consumed_joules": 10131.038894346588,
                    "final_emissions": 0.0010720609074448147
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.5,
                    "cpu_memory_usage_bytes": 2894456832.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0004": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 04, 2025 at 07:55:37 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "mecklenburg-vorpommern"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 2.9997439426369965,
                    "average_latency_ms_per_batch": 2999.7439426369965,
                    "average_ttft_ms": 0.0,
                    "throughput_queries_per_sec": 1.666872669142737,
                    "throughput_tokens_per_sec": 83.34363345713683
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 555.0654377502071,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.22743924192892567,
                    "cpu_energy": 0.00013839303403801752,
                    "gpu_energy": 0.00024096435945608619,
                    "ram_energy": 0.00023131384432084997,
                    "total_energy_consumed_kwh": 0.0006106712378149536,
                    "total_energy_consumed_joules": 2198.416456133833,
                    "final_emissions": 0.0002326352080456066
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2869719040.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0005": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 04, 2025 at 08:02:08 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "mecklenburg-vorpommern"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 2.7191423317417502,
                    "average_latency_ms_per_batch": 2719.1423317417502,
                    "throughput_queries_per_sec": 1.838846575118427,
                    "throughput_tokens_per_sec": 91.94232875592135
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 213.0940023505005,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.2395927850907166,
                    "cpu_energy": 0.00013870108078117485,
                    "gpu_energy": 0.00020884322260883437,
                    "ram_energy": 0.0002321461152097701,
                    "total_energy_consumed_kwh": 0.0005796904185997793,
                    "total_energy_consumed_joules": 2086.8855069592055,
                    "final_emissions": 0.00022083306496558595
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2875822080.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0006": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 04, 2025 at 08:07:31 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "mecklenburg-vorpommern"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 2.7960597195196897,
                    "average_latency_ms_per_batch": 2796.0597195196897,
                    "throughput_queries_per_sec": 1.7882384353254623,
                    "throughput_tokens_per_sec": 89.41192176627312
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 224.70957974853113,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.23969306028875886,
                    "cpu_energy": 0.00013523755414644257,
                    "gpu_energy": 0.0002179215632196474,
                    "ram_energy": 0.00022629229255812113,
                    "total_energy_consumed_kwh": 0.000579451409924211,
                    "total_energy_consumed_joules": 2086.0250757271597,
                    "final_emissions": 0.0002207420146106282
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2870278144.0,
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0007": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 04, 2025 at 08:20:19 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "mecklenburg-vorpommern"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 2.704952041618526,
                    "average_latency_ms_per_batch": 2704.952041618526,
                    "throughput_queries_per_sec": 1.848461609557605,
                    "throughput_tokens_per_sec": 92.42308047788023
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 233.760442839909,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.2369246547161872,
                    "cpu_energy": 0.00013856290474359412,
                    "gpu_energy": 0.00021579628376855453,
                    "ram_energy": 0.00023185624590005158,
                    "total_energy_consumed_kwh": 0.0005862154344122003,
                    "total_energy_consumed_joules": 2110.375563883921,
                    "final_emissions": 0.0002233187697393277
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2870751232.0,
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0008": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 04, 2025 at 08:21:05 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "mecklenburg-vorpommern"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 2.793602622114122,
                    "average_latency_ms_per_batch": 2793.602622114122,
                    "throughput_queries_per_sec": 1.789815053666063,
                    "throughput_tokens_per_sec": 89.49075268330316
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 255.26291678050518,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.2340923220997962,
                    "cpu_energy": 0.00013597321054839996,
                    "gpu_energy": 0.0002298785172456519,
                    "ram_energy": 0.00022745646208425369,
                    "total_energy_consumed_kwh": 0.0005933081898783055,
                    "total_energy_consumed_joules": 2135.9094835619,
                    "final_emissions": 0.0002260207549341405
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2871678976.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        55.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0009": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 04, 2025 at 11:55:15 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "mecklenburg-vorpommern"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.986807068111375,
                    "average_latency_ms_per_batch": 1986.807068111375,
                    "throughput_queries_per_sec": 2.51661070851567,
                    "throughput_tokens_per_sec": 125.83053542578351
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 502.8708709741203,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.15596191286041672,
                    "cpu_energy": 0.00013075814796320627,
                    "gpu_energy": 0.0005410354328461153,
                    "ram_energy": 0.00021873723186499898,
                    "total_energy_consumed_kwh": 0.0008905308126743207,
                    "total_energy_consumed_joules": 3205.910925627554,
                    "final_emissions": 0.0003392477130882824
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2870417408.0,
                    "gpu_utilization_percent": [
                        100.0,
                        100.0,
                        100.0,
                        100.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0010": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 12:24:41 AM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "mecklenburg-vorpommern"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.5723746016155928,
                    "average_latency_ms_per_batch": 1572.3746016155928,
                    "throughput_queries_per_sec": 3.1799208211494445,
                    "throughput_tokens_per_sec": 158.9960410574722
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 199.8140015806576,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.28316376801069204,
                    "cpu_energy": 0.00010661004384746775,
                    "gpu_energy": 0.00020545377546454802,
                    "ram_energy": 0.0001784967024653698,
                    "total_energy_consumed_kwh": 0.0004905605217773855,
                    "total_energy_consumed_joules": 1766.017878398588,
                    "final_emissions": 0.00018687903077109502
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2879932416.0,
                    "gpu_utilization_percent": [
                        32.0,
                        28.5,
                        1.0,
                        100.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0011": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 12:30:29 AM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "mecklenburg-vorpommern"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.6368350298143923,
                    "average_latency_ms_per_batch": 1636.8350298143923,
                    "throughput_queries_per_sec": 3.0548034648276503,
                    "throughput_tokens_per_sec": 152.7401732413825
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 165.20222764533247,
                    "ram_power": 2.2502517700195312,
                    "energy_efficiency_tokens_per_joule": 0.5720718752591025,
                    "cpu_energy": 0.00011490836001757998,
                    "gpu_energy": 0.00012589398957629783,
                    "ram_energy": 2.0122285016936767e-06,
                    "total_energy_consumed_kwh": 0.0002428145780955715,
                    "total_energy_consumed_joules": 874.1324811440574,
                    "final_emissions": 9.250021352550796e-05
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2878892032.0,
                    "gpu_utilization_percent": [
                        13.0,
                        20.0,
                        100.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0012": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 12:32:11 AM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "mecklenburg-vorpommern"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.5809939389582723,
                    "average_latency_ms_per_batch": 1580.9939389582723,
                    "throughput_queries_per_sec": 3.1625887450195975,
                    "throughput_tokens_per_sec": 158.1294372509799
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 189.88546301955205,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.334706456263862,
                    "cpu_energy": 0.00010619419757858849,
                    "gpu_energy": 0.00013063010449343437,
                    "ram_energy": 0.00017814167439401382,
                    "total_energy_consumed_kwh": 0.0004149659764660367,
                    "total_energy_consumed_joules": 1493.877515277732,
                    "final_emissions": 0.00015808128873473666
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2884755456.0,
                    "gpu_utilization_percent": [
                        28.0,
                        41.0,
                        100.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0013": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 05:59:03 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.6034757257439196,
                    "average_latency_ms_per_batch": 1603.4757257439196,
                    "throughput_queries_per_sec": 3.1186658121729556,
                    "throughput_tokens_per_sec": 155.93329060864778
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 191.10863977362257,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.31979950055518386,
                    "cpu_energy": 0.00011061446912935936,
                    "gpu_energy": 0.000138283166180031,
                    "ram_energy": 0.00018556175715056755,
                    "total_energy_consumed_kwh": 0.0004344593924599579,
                    "total_energy_consumed_joules": 1564.0538128558485,
                    "final_emissions": 0.00016550730555762097
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2933344256.0,
                    "gpu_utilization_percent": [
                        41.0,
                        44.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0014": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 06:13:59 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.9807776021771133,
                    "average_latency_ms_per_batch": 1980.7776021771133,
                    "throughput_queries_per_sec": 2.524662767545661,
                    "throughput_tokens_per_sec": 126.23313837728304
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 683.3713222026317,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.1579118543093489,
                    "cpu_energy": 0.00013278989843092858,
                    "gpu_energy": 0.0005256759761067542,
                    "ram_energy": 0.00022274418633284613,
                    "total_energy_consumed_kwh": 0.0008812100608705289,
                    "total_energy_consumed_joules": 3172.3562191339042,
                    "final_emissions": 0.000335696972688628
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2942124032.0,
                    "gpu_utilization_percent": [
                        63.0,
                        81.0,
                        52.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0015": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 06:15:45 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.5498218704015017,
                    "average_latency_ms_per_batch": 1549.8218704015017,
                    "throughput_queries_per_sec": 3.2267031135022597,
                    "throughput_tokens_per_sec": 161.335155675113
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 205.69385072356806,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.33176744596411345,
                    "cpu_energy": 0.00010468894024961628,
                    "gpu_energy": 0.00013838733292459438,
                    "ram_energy": 0.00017561424196023263,
                    "total_energy_consumed_kwh": 0.0004186905151344433,
                    "total_energy_consumed_joules": 1507.2858544839958,
                    "final_emissions": 0.00015950015174046617
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2931130368.0,
                    "gpu_utilization_percent": [
                        47.0,
                        31.0,
                        0.0,
                        100.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0016": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 06:16:56 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.534587804460898,
                    "average_latency_ms_per_batch": 1534.587804460898,
                    "throughput_queries_per_sec": 3.2582242107822292,
                    "throughput_tokens_per_sec": 162.91121053911144
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 204.01190972193598,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.33827956028332634,
                    "cpu_energy": 0.00010413418059761171,
                    "gpu_energy": 0.00013176010540405514,
                    "ram_energy": 0.00017468228379316644,
                    "total_energy_consumed_kwh": 0.0004105765697948333,
                    "total_energy_consumed_joules": 1478.0756512613998,
                    "final_emissions": 0.00015640914426334176
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2945007616.0,
                    "gpu_utilization_percent": [
                        35.0,
                        47.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0017": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 06:23:16 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.5405514417216182,
                    "average_latency_ms_per_batch": 1540.5514417216182,
                    "throughput_queries_per_sec": 3.245732009238509,
                    "throughput_tokens_per_sec": 162.28660046192545
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 240.18342311340905,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.3276895717978593,
                    "cpu_energy": 0.00010446326088276692,
                    "gpu_energy": 0.00014417594866955596,
                    "ram_energy": 0.0001752417778531554,
                    "total_energy_consumed_kwh": 0.00042388098740547825,
                    "total_energy_consumed_joules": 1525.9715546597217,
                    "final_emissions": 0.00016147746215211696
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2937456640.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0018": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 06:41:39 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.5356565781403333,
                    "average_latency_ms_per_batch": 1535.6565781403333,
                    "throughput_queries_per_sec": 3.2561267005111585,
                    "throughput_tokens_per_sec": 162.80633502555793
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 197.99871201986875,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.33945601669795,
                    "cpu_energy": 0.00010444665217073634,
                    "gpu_energy": 0.00012951177026820915,
                    "ram_energy": 0.0001752111848798046,
                    "total_energy_consumed_kwh": 0.0004091696073187501,
                    "total_energy_consumed_joules": 1473.0105863475005,
                    "final_emissions": 0.00015587316190807788
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2936752128.0,
                    "gpu_utilization_percent": [
                        47.0,
                        38.0,
                        100.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0019": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 06:52:19 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.5853348837699741,
                    "average_latency_ms_per_batch": 1585.334883769974,
                    "throughput_queries_per_sec": 3.1539130051882824,
                    "throughput_tokens_per_sec": 157.69565025941412
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 202.9825055940948,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.3307372368898579,
                    "cpu_energy": 0.00010639232091489249,
                    "gpu_energy": 0.00013506677474595108,
                    "ram_energy": 0.00017847900784752953,
                    "total_energy_consumed_kwh": 0.00041993810350837306,
                    "total_energy_consumed_joules": 1511.777172630143,
                    "final_emissions": 0.00015997542053151472
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.5,
                    "cpu_memory_usage_bytes": 2933616640.0,
                    "gpu_utilization_percent": [
                        33.5,
                        34.0,
                        0.0,
                        91.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0020": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 07:12:34 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 2.7936179656535387,
                    "average_latency_ms_per_batch": 2793.6179656535387,
                    "throughput_queries_per_sec": 1.7898678572477535,
                    "throughput_tokens_per_sec": 89.49339286238768
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 197.72232811090606,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.25113005161137814,
                    "cpu_energy": 0.00013262356867198833,
                    "gpu_energy": 0.00019818599189846964,
                    "ram_energy": 0.00022245389276966517,
                    "total_energy_consumed_kwh": 0.0005532634533401231,
                    "total_energy_consumed_joules": 1991.7484320244432,
                    "final_emissions": 0.0002107657125499199
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2927912960.0,
                    "gpu_utilization_percent": [
                        32.0,
                        47.0,
                        0.0,
                        100.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0021": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 07:15:50 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.7830163554754108,
                    "average_latency_ms_per_batch": 1783.0163554754108,
                    "throughput_queries_per_sec": 2.8412979064258312,
                    "throughput_tokens_per_sec": 142.06489532129157
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 181.79108624616168,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.283763896559011,
                    "cpu_energy": 0.00011905919946730137,
                    "gpu_energy": 0.00017590847407689125,
                    "ram_energy": 0.0001996985604443615,
                    "total_energy_consumed_kwh": 0.0004946662339885542,
                    "total_energy_consumed_joules": 1780.7984423587948,
                    "final_emissions": 0.0001884431018379397
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2930403328.0,
                    "gpu_utilization_percent": [
                        22.0,
                        100.0,
                        100.0,
                        100.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0022": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 08:49:13 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.5769661308731884,
                    "average_latency_ms_per_batch": 1576.9661308731884,
                    "throughput_queries_per_sec": 3.171545941178989,
                    "throughput_tokens_per_sec": 158.57729705894945
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 104.94575837775582,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.21648605357603376,
                    "cpu_energy": 0.0001678856836660998,
                    "gpu_energy": 0.00019215848706721772,
                    "ram_energy": 0.00028161486978163416,
                    "total_energy_consumed_kwh": 0.0006416590405149516,
                    "total_energy_consumed_joules": 2309.972545853826,
                    "final_emissions": 0.00024444001148417083
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2950612992.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        100.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0023": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 08:57:35 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "number_runs": 5,
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "batch_size": 8,
                "gpu_list": [
                    0,
                    1
                ],
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "fp_precision": "float16",
                "inference_type": "purely_generative",
                "quantisation": false,
                "batching_options": {},
                "sharding_config": {},
                "is_encoder_decoder": "na"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.5317176724784076,
                    "average_latency_ms_per_batch": 1531.7176724784076,
                    "throughput_queries_per_sec": 3.2643098890419715,
                    "throughput_tokens_per_sec": 163.2154944520986
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 113.66237254155376,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.21789325807439647,
                    "cpu_energy": 0.00016621336690150203,
                    "gpu_energy": 0.00019239682060856467,
                    "ram_energy": 0.0002788082870204614,
                    "total_energy_consumed_kwh": 0.0006374184745305281,
                    "total_energy_consumed_joules": 2294.706508309901,
                    "final_emissions": 0.0002428245678724047
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2925209600.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        100.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0024": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 09:04:22 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "batch_size": 8,
                "gpu_list": [
                    0,
                    1
                ],
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "fp_precision": "float16",
                "inference_type": "purely_generative",
                "quantisation": false,
                "batching_options": {},
                "sharding_config": {},
                "is_encoder_decoder": "na"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.537309342296794,
                    "average_latency_ms_per_batch": 1537.309342296794,
                    "throughput_queries_per_sec": 3.2527143337053697,
                    "throughput_tokens_per_sec": 162.6357166852685
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 102.57981315294741,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.21907936501838463,
                    "cpu_energy": 0.00016611575611750597,
                    "gpu_energy": 0.00018922404028387518,
                    "ram_energy": 0.00027864262383480354,
                    "total_energy_consumed_kwh": 0.0006339824202361848,
                    "total_energy_consumed_joules": 2282.336712850265,
                    "final_emissions": 0.00024151560298897457
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2933473280.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        100.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0025": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 09:08:30 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "batch_size": 8,
                "gpu_list": [
                    0,
                    1
                ],
                "decoder_temperature": 1.0,
                "query_rate": 1.0,
                "fp_precision": "float16",
                "inference_type": "purely_generative",
                "quantisation": false,
                "batching_options": {},
                "sharding_config": {},
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.5747146089561284,
                    "average_latency_ms_per_batch": 1574.7146089561284,
                    "throughput_queries_per_sec": 3.1762513142970183,
                    "throughput_tokens_per_sec": 158.8125657148509
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 105.49291292472772,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.21604217804380937,
                    "cpu_energy": 0.00016770636997534896,
                    "gpu_energy": 0.00019389571068018085,
                    "ram_energy": 0.00028131791082286397,
                    "total_energy_consumed_kwh": 0.0006429199914783938,
                    "total_energy_consumed_joules": 2314.5119693222177,
                    "final_emissions": 0.0002449203707536941
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2957121536.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        100.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0026": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 09:25:42 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "batch_size": 8,
                "gpu_list": [
                    1,
                    2
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "float16",
                "inference_type": "purely_generative",
                "quantisation": false,
                "batching_options": {},
                "sharding_config": {},
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.5409394048620015,
                    "average_latency_ms_per_batch": 1540.9394048620015,
                    "throughput_queries_per_sec": 3.244793204313903,
                    "throughput_tokens_per_sec": 162.23966021569515
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 106.78343785293058,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.21861667242717478,
                    "cpu_energy": 0.0001656302709307056,
                    "gpu_energy": 0.0001918593201537533,
                    "ram_energy": 0.0002778331897013143,
                    "total_energy_consumed_kwh": 0.0006353227807857731,
                    "total_energy_consumed_joules": 2287.1620108287834,
                    "final_emissions": 0.00024202621334034027
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2939918336.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        82.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0027": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 09:39:26 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "batch_size": 8,
                "gpu_list": [
                    1,
                    2
                ],
                "decoder_temperature": 1,
                "query_rate": 2,
                "fp_precision": "float16",
                "inference_type": "purely_generative",
                "quantisation": false,
                "batching_options": {},
                "sharding_config": {},
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.5337991253472865,
                    "average_latency_ms_per_batch": 1533.7991253472865,
                    "throughput_queries_per_sec": 3.2601511927309867,
                    "throughput_tokens_per_sec": 163.00755963654933
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 139.48710052862862,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.2661146425850737,
                    "cpu_energy": 0.00013360211643157526,
                    "gpu_energy": 0.00016426568694782873,
                    "ram_energy": 0.00022408070085348585,
                    "total_energy_consumed_kwh": 0.0005219485042328898,
                    "total_energy_consumed_joules": 1879.0146152384032,
                    "final_emissions": 0.00019883628268751937
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.5,
                    "cpu_memory_usage_bytes": 2948493312.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        43.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0028": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 05, 2025 at 09:40:42 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "batch_size": 8,
                "gpu_list": [
                    1,
                    2
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "float32",
                "inference_type": "purely_generative",
                "quantisation": false,
                "batching_options": {},
                "sharding_config": {},
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 1.7336179555859417,
                    "average_latency_ms_per_batch": 1733.6179555859417,
                    "throughput_queries_per_sec": 2.8848434975199315,
                    "throughput_tokens_per_sec": 144.24217487599657
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 105.57195293352498,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.17452692681777804,
                    "cpu_energy": 0.00017728814492875243,
                    "gpu_energy": 0.0003211491458046112,
                    "ram_energy": 0.0002974083251066391,
                    "total_energy_consumed_kwh": 0.0007958456158400028,
                    "total_energy_consumed_joules": 2865.04421702401,
                    "final_emissions": 0.00030317738735424906
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: DynamicCache",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 1847830528.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        100.0
                    ],
                    "current_memory_allocated_bytes": 8808907264.0,
                    "max_memory_allocated_bytes": 8808907264.0,
                    "current_memory_reserved_bytes": 12972982272.0,
                    "max_memory_reserved_bytes": 12972982272.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    },
    {
        "EXPERIMENT #0029": {
            "experiment_setup": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "task_type": "text_generation",
                "date": "March 11, 2025 at 03:56:12 PM",
                "cpu_count": 128,
                "cpu_model": "AMD EPYC 7742 64-Core Processor",
                "gpu_count": 2,
                "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
                "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
                "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
                "accelerate_config": {
                    "distributed_type": "DistributedType.MULTI_GPU",
                    "num_processes": 2
                },
                "country": "Germany",
                "region": "north rhine-westphalia"
            },
            "experiment_variables": {
                "max_input_tokens": 512,
                "max_output_tokens": 50,
                "number_runs": 5,
                "total_token_inputted": 2560,
                "total_tokens_outputted": 250,
                "batch_size": 8,
                "gpu_list": [
                    1,
                    2
                ],
                "decoder_temperature": 1,
                "query_rate": 1,
                "fp_precision": "float16",
                "inference_type": "purely_generative",
                "quantisation": false,
                "batching_options": {},
                "sharding_config": {},
                "is_encoder_decoder": "decoder_only"
            },
            "experiment_results": {
                "inference_performance": {
                    "total_inference_time_sec": 3.061804956989363,
                    "average_latency_ms_per_batch": 3061.804956989363,
                    "throughput_queries_per_sec": 1.6335062361384378,
                    "throughput_tokens_per_sec": 81.6753118069219
                },
                "energy_performance": {
                    "cpu_power": 112.5,
                    "gpu_power": 2853.378562805508,
                    "ram_power": 188.8243260383606,
                    "energy_efficiency_tokens_per_joule": 0.151836428193442,
                    "cpu_energy": 0.0002424484434450278,
                    "gpu_energy": 0.0002852577282084212,
                    "ram_energy": 0.00040675174932283627,
                    "total_energy_consumed_kwh": 0.0009344579209762853,
                    "total_energy_consumed_joules": 3364.048515514627,
                    "final_emissions": 0.0003559817449959159
                },
                "compute_performance": {
                    "gpu": "cuda:0",
                    "flops_forward_pass": "Error: name 'ModelWrapper' is not defined",
                    "cpu_usage_percent": 0.0,
                    "cpu_memory_usage_bytes": 2939246592.0,
                    "gpu_utilization_percent": [
                        0.0,
                        0.0,
                        0.0,
                        0.0
                    ],
                    "current_memory_allocated_bytes": 4410810880.0,
                    "max_memory_allocated_bytes": 4410810880.0,
                    "current_memory_reserved_bytes": 6715080704.0,
                    "max_memory_reserved_bytes": 6715080704.0,
                    "cpu_vendor": "AMD"
                },
                "task-specific_performance": {}
            }
        }
    }
]