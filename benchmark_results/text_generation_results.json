[
    {
        "experiment_setup": {
            "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "task_type": "text_generation",
            "cpu_count": 128,
            "cpu_model": "AMD EPYC 7742 64-Core Processor",
            "gpu_count": 4,
            "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
            "total_runs": 1,
            "date": "2025-03-03"
        },
        "experiment_results": {
            "total_inference_time_sec": 3.2583537199534476,
            "average_latency_ms_per_batch": 3258.3537199534476,
            "average_ttft_ms": 30.811988282948732,
            "throughput_queries_per_sec": 0.306903450621772,
            "throughput_tokens_per_sec": 15.3451725310886,
            "total_tokens_generated": 50,
            "energy_consumed_kwh": 0.0005607713338283634,
            "energy_consumed_joules": 2018.7768017821084,
            "energy_efficiency_tokens_per_joule": 0.024767473034097518
        }
    },
    {
        "experiment_setup": {
            "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "task_type": "text_generation",
            "cpu_count": 128,
            "cpu_model": "AMD EPYC 7742 64-Core Processor",
            "gpu_count": 4,
            "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
            "total_runs": 2,
            "date": "2025-03-03"
        },
        "experiment_results": {
            "total_inference_time_sec": 2.9895807951688766,
            "average_latency_ms_per_batch": 2989.5807951688766,
            "average_ttft_ms": 31.013576313853264,
            "throughput_queries_per_sec": 0.6689901150127716,
            "throughput_tokens_per_sec": 33.44950575063858,
            "total_tokens_generated": 100,
            "energy_consumed_kwh": 0.0005031272693872277,
            "energy_consumed_joules": 1811.2581697940198,
            "energy_efficiency_tokens_per_joule": 0.055210240962707276
        }
    },
    {
        "experiment_setup": {
            "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "task_type": "text_generation",
            "total_runs": 3,
            "cpu_count": 128,
            "cpu_model": "AMD EPYC 7742 64-Core Processor",
            "gpu_count": 4,
            "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
            "date": "2025-03-03"
        },
        "experiment_results": {
            "total_inference_time_sec": 4.09140178700909,
            "average_latency_ms_per_batch": 4091.40178700909,
            "average_ttft_ms": 104.6776189468801,
            "throughput_queries_per_sec": 0.7332450236311476,
            "throughput_tokens_per_sec": 36.66225118155738,
            "total_tokens_generated": 150,
            "energy_consumed_kwh": 0.0008540026943333823,
            "energy_consumed_joules": 3074.4096996001763,
            "energy_efficiency_tokens_per_joule": 0.04878985387650427
        }
    },
    {
        "experiment_setup": {
            "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "task_type": "text_generation",
            "date": "2025-03-03",
            "cpu_count": 128,
            "cpu_model": "AMD EPYC 7742 64-Core Processor",
            "gpu_count": 4,
            "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
            "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
            "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
            "accelerate_config": {
                "distributed_type": "DistributedType.NO",
                "num_processes": 1,
                "local_process_index": 0
            },
            "country": "Germany",
            "region": "mecklenburg-vorpommern"
        },
        "experiment_variables": {
            "input_tokens": null,
            "output_tokens": 150
        },
        "experiment_results": {
            "performance": {
                "total_inference_time_sec": 5.807851883582771,
                "average_latency_ms_per_batch": 5807.851883582771,
                "average_ttft_ms": 47.99634125083685,
                "throughput_queries_per_sec": 0.5165420985476903,
                "throughput_tokens_per_sec": 25.82710492738451,
                "total_tokens_generated": 150,
                "num_runs": 3
            },
            "energy": {
                "cpu_power": 112.5,
                "gpu_power": 668.6884726542255,
                "ram_power": 188.8243260383606,
                "cpu_energy": 0.00018183166041853838,
                "gpu_energy": 0.0010694230777588132,
                "ram_energy": 0.00030468873722414974,
                "total_energy_consumed_kwh": 0.001555943475401501,
                "total_energy_consumed_joules": 5601.396511445404,
                "energy_efficiency_tokens_per_joule": 0.02677903620882812,
                "final_emissions": 0.0005927366669542019
            }
        }
    },
    {
        "experiment_setup": {
            "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "task_type": "text_generation",
            "date": "2025-03-03",
            "cpu_count": 128,
            "cpu_model": "AMD EPYC 7742 64-Core Processor",
            "gpu_count": 4,
            "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
            "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
            "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
            "accelerate_config": {
                "distributed_type": "DistributedType.NO",
                "num_processes": 1,
                "local_process_index": 0
            },
            "country": "Germany",
            "region": "mecklenburg-vorpommern"
        },
        "experiment_variables": {
            "input_tokens": null,
            "output_tokens": 150
        },
        "experiment_results": {
            "performance": {
                "total_inference_time_sec": 3.690832010935992,
                "average_latency_ms_per_batch": 3690.832010935992,
                "average_ttft_ms": 46.87858512625098,
                "throughput_queries_per_sec": 0.8128248565935686,
                "throughput_tokens_per_sec": 40.641242829678426,
                "total_tokens_generated": 150,
                "num_runs": 3
            },
            "energy": {
                "cpu_power": 112.5,
                "gpu_power": 558.0228417622959,
                "ram_power": 188.8243260383606,
                "cpu_energy": 0.00011694990037358367,
                "gpu_energy": 0.00048769205682930306,
                "ram_energy": 0.00019592496704536663,
                "total_energy_consumed_kwh": 0.0008005669242482534,
                "total_energy_consumed_joules": 2882.0409272937122,
                "energy_efficiency_tokens_per_joule": 0.05204645033991681,
                "final_emissions": 0.00030497596979237214
            }
        }
    },
    {
        "experiment_setup": {
            "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "task_type": "text_generation",
            "date": "2025-03-03",
            "cpu_count": 128,
            "cpu_model": "AMD EPYC 7742 64-Core Processor",
            "gpu_count": 4,
            "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
            "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
            "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
            "accelerate_config": {
                "distributed_type": "DistributedType.NO",
                "num_processes": 1,
                "local_process_index": 0
            },
            "country": "Germany",
            "region": "mecklenburg-vorpommern"
        },
        "experiment_variables": {
            "input_tokens": null,
            "output_tokens": 150
        },
        "experiment_results": {
            "performance": {
                "total_inference_time_sec": 2.7755715209059417,
                "average_latency_ms_per_batch": 2775.5715209059417,
                "average_ttft_ms": 106.01719096302986,
                "throughput_queries_per_sec": 1.080858474517279,
                "throughput_tokens_per_sec": 54.04292372586395,
                "total_tokens_generated": 150,
                "num_runs": 3
            },
            "energy": {
                "cpu_power": 112.5,
                "gpu_power": 361.3660819318235,
                "ram_power": 188.8243260383606,
                "cpu_energy": 9.137811111577322e-05,
                "gpu_energy": 0.0003476483336868341,
                "ram_energy": 0.0001530362434789017,
                "total_energy_consumed_kwh": 0.000592062688281509,
                "total_energy_consumed_joules": 2131.4256778134322,
                "energy_efficiency_tokens_per_joule": 0.07037543066192233,
                "final_emissions": 0.00022554628110084087
            }
        }
    },
    {
        "experiment_setup": {
            "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "task_type": "text_generation",
            "date": "2025-03-04",
            "cpu_count": 128,
            "cpu_model": "AMD EPYC 7742 64-Core Processor",
            "gpu_count": 4,
            "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
            "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
            "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
            "accelerate_config": {
                "distributed_type": "DistributedType.NO",
                "num_processes": 1,
                "local_process_index": 0
            },
            "country": "Germany",
            "region": "mecklenburg-vorpommern"
        },
        "experiment_variables": {
            "input_tokens": 1536,
            "output_tokens": 150
        },
        "experiment_results": {
            "performance": {
                "total_inference_time_sec": 3.829248495399952,
                "average_latency_ms_per_batch": 3829.248495399952,
                "average_ttft_ms": 0.0,
                "throughput_queries_per_sec": 0.7834435408419897,
                "throughput_tokens_per_sec": 39.172177042099484,
                "total_tokens_generated": 150,
                "num_runs": 3
            },
            "energy": {
                "cpu_power": 112.5,
                "gpu_power": 385.8889433525045,
                "ram_power": 188.8243260383606,
                "cpu_energy": 0.00012065025488846005,
                "gpu_energy": 0.00048775539020340375,
                "ram_energy": 0.00020212489598916896,
                "total_energy_consumed_kwh": 0.0008105305410810327,
                "total_energy_consumed_joules": 2917.909947891718,
                "energy_efficiency_tokens_per_joule": 0.05140665842288236,
                "final_emissions": 0.00030877160962481943
            }
        }
    },
    {
        "experiment_setup": {
            "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "task_type": "text_generation",
            "date": "2025-03-04",
            "cpu_count": 128,
            "cpu_model": "AMD EPYC 7742 64-Core Processor",
            "gpu_count": 4,
            "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
            "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
            "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
            "accelerate_config": {
                "distributed_type": "DistributedType.NO",
                "num_processes": 1,
                "local_process_index": 0
            },
            "country": "Germany",
            "region": "mecklenburg-vorpommern"
        },
        "experiment_variables": {
            "input_tokens": 1536,
            "output_tokens": 150
        },
        "experiment_results": {
            "performance": {
                "total_inference_time_sec": 5.020318424794823,
                "average_latency_ms_per_batch": 5020.318424794823,
                "average_ttft_ms": 0.0,
                "throughput_queries_per_sec": 0.5975716570453612,
                "throughput_tokens_per_sec": 29.87858285226806,
                "total_tokens_generated": 150,
                "num_runs": 3
            },
            "energy": {
                "cpu_power": 112.5,
                "gpu_power": 79.66679341501025,
                "ram_power": 188.8243260383606,
                "cpu_energy": 0.00015547567908652125,
                "gpu_energy": 0.000698870836870924,
                "ram_energy": 0.00026043882954315296,
                "total_energy_consumed_kwh": 0.0011147853455005983,
                "total_energy_consumed_joules": 4013.2272438021537,
                "energy_efficiency_tokens_per_joule": 0.03737640330027491,
                "final_emissions": 0.0004246774773684529
            }
        }
    },
    {
        "experiment_setup": {
            "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "task_type": "text_generation",
            "date": "2025-03-04",
            "cpu_count": 128,
            "cpu_model": "AMD EPYC 7742 64-Core Processor",
            "gpu_count": 4,
            "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
            "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
            "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
            "accelerate_config": {
                "distributed_type": "DistributedType.NO",
                "num_processes": 1,
                "local_process_index": 0
            },
            "country": "Germany",
            "region": "mecklenburg-vorpommern"
        },
        "experiment_variables": {
            "input_tokens": 1536,
            "output_tokens": 150
        },
        "experiment_results": {
            "performance": {
                "total_inference_time_sec": 4.405871281400323,
                "average_latency_ms_per_batch": 4405.871281400323,
                "average_ttft_ms": 0.0,
                "throughput_queries_per_sec": 0.6809095882271229,
                "throughput_tokens_per_sec": 34.04547941135614,
                "total_tokens_generated": 150,
                "num_runs": 3
            },
            "energy": {
                "cpu_power": 112.5,
                "gpu_power": 285.69904483545696,
                "ram_power": 188.8243260383606,
                "cpu_energy": 0.00013808324276760685,
                "gpu_energy": 0.0005983296453280218,
                "ram_energy": 0.00023123652108647606,
                "total_energy_consumed_kwh": 0.0009676494091821048,
                "total_energy_consumed_joules": 3483.537873055577,
                "energy_efficiency_tokens_per_joule": 0.043059672512883534,
                "final_emissions": 0.0003686260424279228
            }
        }
    },
    {
        "experiment_setup": {
            "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "task_type": "text_generation",
            "date": "2025-03-04",
            "cpu_count": 128,
            "cpu_model": "AMD EPYC 7742 64-Core Processor",
            "gpu_count": 4,
            "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
            "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
            "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
            "accelerate_config": {
                "distributed_type": "DistributedType.NO",
                "num_processes": 1,
                "local_process_index": 0
            },
            "country": "Germany",
            "region": "mecklenburg-vorpommern"
        },
        "experiment_variables": {
            "input_tokens": 5120,
            "output_tokens": 500
        },
        "experiment_results": {
            "inference performance": {
                "total_inference_time_sec": 7.624095981940627,
                "average_latency_ms_per_batch": 3812.0479909703135,
                "average_ttft_ms": 0.0,
                "throughput_queries_per_sec": 1.3116309164637003,
                "throughput_tokens_per_sec": 65.58154582318501,
                "total_tokens_generated": 500,
                "num_runs": 10
            },
            "energy performance": {
                "cpu_power": 112.5,
                "gpu_power": 289.4098363220737,
                "ram_power": 188.8243260383606,
                "cpu_energy": 0.0002417430438217707,
                "gpu_energy": 0.0009724424446062585,
                "ram_energy": 0.00040493145420702483,
                "total_energy_consumed_kwh": 0.001619116942635054,
                "total_energy_consumed_joules": 5828.820993486194,
                "energy_efficiency_tokens_per_joule": 0.08578064081205418,
                "final_emissions": 0.0006168025992968239
            },
            "task-specific performance": {}
        }
    },
    {
        "experiment_setup": {
            "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "task_type": "text_generation",
            "date": "2025-03-04",
            "cpu_count": 128,
            "cpu_model": "AMD EPYC 7742 64-Core Processor",
            "gpu_count": 4,
            "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
            "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
            "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
            "accelerate_config": {
                "distributed_type": "DistributedType.NO",
                "num_processes": 1,
                "local_process_index": 0
            },
            "country": "Germany",
            "region": "mecklenburg-vorpommern"
        },
        "experiment_variables": {
            "input_tokens": 5120,
            "output_tokens": 500
        },
        "experiment_results": {
            "inference_performance": {
                "total_inference_time_sec": 8.15219265408814,
                "average_latency_ms_per_batch": 4076.0963270440698,
                "average_ttft_ms": 0.0,
                "throughput_queries_per_sec": 1.2266638466873359,
                "throughput_tokens_per_sec": 61.3331923343668,
                "total_tokens_generated": 500,
                "num_runs": 10
            },
            "energy_performance": {
                "cpu_power": 112.5,
                "gpu_power": 354.3198812821852,
                "ram_power": 188.8243260383606,
                "cpu_energy": 0.00026088926970260215,
                "gpu_energy": 0.001044465835569497,
                "ram_energy": 0.00043583109710925534,
                "total_energy_consumed_kwh": 0.0017411862023813543,
                "total_energy_consumed_joules": 6268.270328572876,
                "energy_efficiency_tokens_per_joule": 0.07976682143410958,
                "final_emissions": 0.0006633048837971769
            },
            "task-specific_performance": {}
        }
    },
    {
        "experiment_setup": {
            "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "task_type": "text_generation",
            "date": "2025-03-04",
            "cpu_count": 128,
            "cpu_model": "AMD EPYC 7742 64-Core Processor",
            "gpu_count": 4,
            "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
            "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
            "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
            "accelerate_config": {
                "distributed_type": "DistributedType.NO",
                "num_processes": 1,
                "local_process_index": 0
            },
            "country": "Germany",
            "region": "mecklenburg-vorpommern"
        },
        "experiment_variables": {
            "input_tokens": 5120,
            "output_tokens": 500
        },
        "experiment_results": {
            "inference_performance": {
                "total_inference_time_sec": 8.155038401018828,
                "average_latency_ms_per_batch": 4077.519200509414,
                "average_ttft_ms": 0.0,
                "throughput_queries_per_sec": 1.2262357953765952,
                "throughput_tokens_per_sec": 61.31178976882976,
                "total_tokens_generated": 500,
                "num_runs": 10
            },
            "energy_performance": {
                "cpu_power": 112.5,
                "gpu_power": 371.7806942617151,
                "ram_power": 188.8243260383606,
                "cpu_energy": 0.0002603725245426176,
                "gpu_energy": 0.0010486991722800099,
                "ram_energy": 0.00043334235302526985,
                "total_energy_consumed_kwh": 0.0017424140498478974,
                "total_energy_consumed_joules": 6272.690579452431,
                "energy_efficiency_tokens_per_joule": 0.07971061120691324,
                "final_emissions": 0.0006637726322895565
            },
            "task-specific_performance": {}
        }
    },
    {
        "experiment_setup": {
            "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "task_type": "text_generation",
            "date": "2025-03-04",
            "cpu_count": 128,
            "cpu_model": "AMD EPYC 7742 64-Core Processor",
            "gpu_count": 4,
            "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
            "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
            "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
            "accelerate_config": {
                "distributed_type": "DistributedType.NO",
                "num_processes": 1,
                "local_process_index": 0
            },
            "country": "Germany",
            "region": "mecklenburg-vorpommern"
        },
        "experiment_variables": {
            "input_tokens": 5120,
            "output_tokens": 500
        },
        "experiment_results": {
            "inference_performance": {
                "total_inference_time_sec": 8.235722966957837,
                "average_latency_ms_per_batch": 4117.861483478919,
                "average_ttft_ms": 0.0,
                "throughput_queries_per_sec": 1.214222484185121,
                "throughput_tokens_per_sec": 60.71112420925605,
                "total_tokens_generated": 500,
                "num_runs": 10
            },
            "energy_performance": {
                "cpu_power": 112.5,
                "gpu_power": 362.7514906195493,
                "ram_power": 188.8243260383606,
                "cpu_energy": 0.00026200044872530276,
                "gpu_energy": 0.0010490605614705828,
                "ram_energy": 0.000436041728255414,
                "total_energy_consumed_kwh": 0.0017471027384512998,
                "total_energy_consumed_joules": 6289.569858424679,
                "energy_efficiency_tokens_per_joule": 0.07949669234220617,
                "final_emissions": 0.0006655587882130226
            },
            "task-specific_performance": {}
        }
    },
    {
        "experiment_setup": {
            "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "task_type": "text_generation",
            "date": "2025-03-04",
            "cpu_count": 128,
            "cpu_model": "AMD EPYC 7742 64-Core Processor",
            "gpu_count": 4,
            "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
            "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
            "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
            "accelerate_config": {
                "distributed_type": "DistributedType.NO",
                "num_processes": 1,
                "local_process_index": 0
            },
            "country": "Germany",
            "region": "mecklenburg-vorpommern"
        },
        "experiment_variables": {
            "input_tokens": 5120,
            "output_tokens": 500
        },
        "experiment_results": {
            "inference_performance": {
                "total_inference_time_sec": 10.103197081014514,
                "average_latency_ms_per_batch": 5051.598540507257,
                "average_ttft_ms": 0.0,
                "throughput_queries_per_sec": 0.9897857004879735,
                "throughput_tokens_per_sec": 49.489285024398676,
                "total_tokens_generated": 500,
                "num_runs": 10
            },
            "energy_performance": {
                "cpu_power": 112.5,
                "gpu_power": 603.0694346966926,
                "ram_power": 188.8243260383606,
                "cpu_energy": 0.00031733668019296606,
                "gpu_energy": 0.0018836767847290048,
                "ram_energy": 0.0005316183382532099,
                "total_energy_consumed_kwh": 0.00273263180317518,
                "total_energy_consumed_joules": 9837.474491430648,
                "energy_efficiency_tokens_per_joule": 0.050826053011425475,
                "final_emissions": 0.0010409960854195848
            },
            "task-specific_performance": {}
        }
    },
    {
        "experiment_setup": {
            "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "task_type": "text_generation",
            "date": "2025-03-04",
            "cpu_count": 128,
            "cpu_model": "AMD EPYC 7742 64-Core Processor",
            "gpu_count": 4,
            "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
            "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
            "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
            "accelerate_config": {
                "distributed_type": "DistributedType.NO",
                "num_processes": 1,
                "local_process_index": 0
            },
            "country": "Germany",
            "region": "mecklenburg-vorpommern"
        },
        "experiment_variables": {
            "input_tokens": 5120,
            "output_tokens": 500
        },
        "experiment_results": {
            "inference_performance": {
                "total_inference_time_sec": 10.230211943853647,
                "average_latency_ms_per_batch": 5115.105971926823,
                "average_ttft_ms": 0.0,
                "throughput_queries_per_sec": 0.9774968548924385,
                "throughput_tokens_per_sec": 48.87484274462193,
                "total_tokens_generated": 500,
                "num_runs": 10
            },
            "energy_performance": {
                "cpu_power": 112.5,
                "gpu_power": 589.8162551984462,
                "ram_power": 188.8243260383606,
                "cpu_energy": 0.00031913396042364186,
                "gpu_energy": 0.0019027534666520296,
                "ram_energy": 0.0005346544814609824,
                "total_energy_consumed_kwh": 0.002756541908536654,
                "total_energy_consumed_joules": 9923.550870731953,
                "energy_efficiency_tokens_per_joule": 0.0503851903933577,
                "final_emissions": 0.0010501046400570382
            },
            "task-specific_performance": {}
        }
    },
    {
        "experiment_setup": {
            "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "task_type": "text_generation",
            "date": "2025-03-04",
            "cpu_count": 128,
            "cpu_model": "AMD EPYC 7742 64-Core Processor",
            "gpu_count": 4,
            "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
            "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
            "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
            "accelerate_config": {
                "distributed_type": "DistributedType.NO",
                "num_processes": 1,
                "local_process_index": 0
            },
            "country": "Germany",
            "region": "mecklenburg-vorpommern"
        },
        "experiment_variables": {
            "input_tokens": 5120,
            "output_tokens": 500
        },
        "experiment_results": {
            "inference_performance": {
                "total_inference_time_sec": 9.609458426013589,
                "average_latency_ms_per_batch": 4804.729213006794,
                "average_ttft_ms": 0.0,
                "throughput_queries_per_sec": 1.0406413719350909,
                "throughput_tokens_per_sec": 52.03206859675454,
                "total_tokens_generated": 500,
                "num_runs": 10
            },
            "energy_performance": {
                "cpu_power": 112.5,
                "gpu_power": 445.2114011423819,
                "ram_power": 188.8243260383606,
                "cpu_energy": 0.0003038457370712422,
                "gpu_energy": 0.0017807775357425726,
                "ram_energy": 0.0005088604282156486,
                "total_energy_consumed_kwh": 0.0025934837010294634,
                "total_energy_consumed_joules": 9336.541323706067,
                "energy_efficiency_tokens_per_joule": 0.053553021688070766,
                "final_emissions": 0.000987987615907174
            },
            "task-specific_performance": {}
        }
    },
    {
        "experiment_setup": {
            "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "task_type": "text_generation",
            "date": "2025-03-04",
            "cpu_count": 128,
            "cpu_model": "AMD EPYC 7742 64-Core Processor",
            "gpu_count": 4,
            "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
            "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
            "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
            "accelerate_config": {
                "distributed_type": "DistributedType.NO",
                "num_processes": 1,
                "local_process_index": 0
            },
            "country": "Germany",
            "region": "mecklenburg-vorpommern"
        },
        "experiment_variables": {
            "input_tokens": 5120,
            "output_tokens": 500
        },
        "experiment_results": {
            "inference_performance": {
                "total_inference_time_sec": 9.98050385992974,
                "average_latency_ms_per_batch": 4990.25192996487,
                "average_ttft_ms": 0.0,
                "throughput_queries_per_sec": 1.0019534224267508,
                "throughput_tokens_per_sec": 50.09767112133754,
                "total_tokens_generated": 500,
                "num_runs": 10
            },
            "energy_performance": {
                "cpu_power": 112.5,
                "gpu_power": 715.5292116126603,
                "ram_power": 188.8243260383606,
                "cpu_energy": 0.0003129972505266778,
                "gpu_energy": 0.001865626770282347,
                "ram_energy": 0.0005242497198840961,
                "total_energy_consumed_kwh": 0.002702873740693121,
                "total_energy_consumed_joules": 9730.345466495237,
                "energy_efficiency_tokens_per_joule": 0.05138563699733618,
                "final_emissions": 0.0010296597515170446
            },
            "task-specific_performance": {}
        }
    },
    {
        "experiment_setup": {
            "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "task_type": "text_generation",
            "date": "2025-03-04",
            "cpu_count": 128,
            "cpu_model": "AMD EPYC 7742 64-Core Processor",
            "gpu_count": 4,
            "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
            "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
            "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
            "accelerate_config": {
                "distributed_type": "DistributedType.NO",
                "num_processes": 1,
                "local_process_index": 0
            },
            "country": "Germany",
            "region": "mecklenburg-vorpommern"
        },
        "experiment_variables": {
            "input_tokens": 5120,
            "output_tokens": 500
        },
        "experiment_results": {
            "inference_performance": {
                "total_inference_time_sec": 9.516558612696826,
                "average_latency_ms_per_batch": 4758.279306348413,
                "average_ttft_ms": 0.0,
                "throughput_queries_per_sec": 1.050800022043491,
                "throughput_tokens_per_sec": 52.54000110217455,
                "total_tokens_generated": 500,
                "num_runs": 10
            },
            "energy_performance": {
                "cpu_power": 112.5,
                "gpu_power": 635.3290238620494,
                "ram_power": 188.8243260383606,
                "cpu_energy": 0.00029986698781431193,
                "gpu_energy": 0.0017491250104058054,
                "ram_energy": 0.0005023431127653182,
                "total_energy_consumed_kwh": 0.0025513351109854353,
                "total_energy_consumed_joules": 9184.806399547568,
                "energy_efficiency_tokens_per_joule": 0.054437728815343284,
                "final_emissions": 0.0009719311105299016
            },
            "task-specific_performance": {}
        }
    },
    {
        "experiment_setup": {
            "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "task_type": "text_generation",
            "date": "2025-03-04",
            "cpu_count": 128,
            "cpu_model": "AMD EPYC 7742 64-Core Processor",
            "gpu_count": 4,
            "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
            "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
            "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
            "accelerate_config": {
                "distributed_type": "DistributedType.NO",
                "num_processes": 1,
                "local_process_index": 0
            },
            "country": "Germany",
            "region": "mecklenburg-vorpommern"
        },
        "experiment_variables": {
            "input_tokens": 5120,
            "output_tokens": 500
        },
        "experiment_results": {
            "inference_performance": {
                "total_inference_time_sec": 8.909039220772684,
                "average_latency_ms_per_batch": 4454.519610386342,
                "average_ttft_ms": 0.0,
                "throughput_queries_per_sec": 1.1224554917979919,
                "throughput_tokens_per_sec": 56.12277458989959,
                "total_tokens_generated": 500,
                "num_runs": 10
            },
            "energy_performance": {
                "cpu_power": 112.5,
                "gpu_power": 778.8182806478335,
                "ram_power": 188.8243260383606,
                "cpu_energy": 0.0002807138858770486,
                "gpu_energy": 0.0016325474171381416,
                "ram_energy": 0.0004702775552068521,
                "total_energy_consumed_kwh": 0.002383538858222042,
                "total_energy_consumed_joules": 8580.739889599352,
                "energy_efficiency_tokens_per_joule": 0.058270033404234305,
                "final_emissions": 0.000908009128039687
            },
            "task-specific_performance": {}
        }
    },
    {
        "experiment_setup": {
            "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "task_type": "text_generation",
            "date": "2025-03-04",
            "cpu_count": 128,
            "cpu_model": "AMD EPYC 7742 64-Core Processor",
            "gpu_count": 4,
            "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
            "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
            "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
            "accelerate_config": {
                "distributed_type": "DistributedType.MULTI_GPU",
                "num_processes": 4,
                "local_process_index": 0
            },
            "country": "Germany",
            "region": "mecklenburg-vorpommern"
        },
        "experiment_variables": {
            "input_tokens": 5120,
            "output_tokens": 500
        },
        "experiment_results": {
            "inference_performance": {
                "total_inference_time_sec": 2.635169151239097,
                "average_latency_ms_per_batch": 1317.5845756195486,
                "average_ttft_ms": 0.0,
                "throughput_queries_per_sec": 3.7948228087361473,
                "throughput_tokens_per_sec": 189.74114043680737,
                "total_tokens_generated": 500,
                "num_runs": 10
            },
            "energy_performance": {
                "cpu_power": 112.5,
                "gpu_power": 698.0864273696374,
                "ram_power": 188.8243260383606,
                "cpu_energy": 9.070010569121222e-05,
                "gpu_energy": 0.0005859335242988095,
                "ram_energy": 0.0001518640137964447,
                "total_energy_consumed_kwh": 0.0008284976437864665,
                "total_energy_consumed_joules": 2982.591517631279,
                "energy_efficiency_tokens_per_joule": 0.1676394494667815,
                "final_emissions": 0.0003156161774004544
            },
            "task-specific_performance": {}
        }
    },
    {
        "experiment_setup": {
            "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "task_type": "text_generation",
            "date": "2025-03-04",
            "cpu_count": 128,
            "cpu_model": "AMD EPYC 7742 64-Core Processor",
            "gpu_count": 4,
            "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
            "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
            "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
            "accelerate_config": {
                "distributed_type": "DistributedType.MULTI_GPU",
                "num_processes": 4,
                "local_process_index": 3
            },
            "country": "Germany",
            "region": "mecklenburg-vorpommern"
        },
        "experiment_variables": {
            "input_tokens": 5120,
            "output_tokens": 500
        },
        "experiment_results": {
            "inference_performance": {
                "total_inference_time_sec": 3.246740653179586,
                "average_latency_ms_per_batch": 1623.370326589793,
                "average_ttft_ms": 0.0,
                "throughput_queries_per_sec": 3.080011946814057,
                "throughput_tokens_per_sec": 154.00059734070285,
                "total_tokens_generated": 500,
                "num_runs": 10
            },
            "energy_performance": {
                "cpu_power": 112.5,
                "gpu_power": 750.0642018177916,
                "ram_power": 188.8243260383606,
                "cpu_energy": 0.00010821487262728624,
                "gpu_energy": 0.000703449173862225,
                "ram_energy": 0.00018113642741445396,
                "total_energy_consumed_kwh": 0.0009928004739039654,
                "total_energy_consumed_joules": 3574.0817060542754,
                "energy_efficiency_tokens_per_joule": 0.13989607432673704,
                "final_emissions": 0.0003782073405337156
            },
            "task-specific_performance": {}
        }
    },
    {
        "experiment_setup": {
            "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "task_type": "text_generation",
            "date": "2025-03-04",
            "cpu_count": 128,
            "cpu_model": "AMD EPYC 7742 64-Core Processor",
            "gpu_count": 4,
            "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
            "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
            "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
            "accelerate_config": {
                "distributed_type": "DistributedType.MULTI_GPU",
                "num_processes": 4,
                "local_process_index": 1
            },
            "country": "Germany",
            "region": "mecklenburg-vorpommern"
        },
        "experiment_variables": {
            "input_tokens": 5120,
            "output_tokens": 500
        },
        "experiment_results": {
            "inference_performance": {
                "total_inference_time_sec": 3.4434093511663377,
                "average_latency_ms_per_batch": 1721.7046755831689,
                "average_ttft_ms": 0.0,
                "throughput_queries_per_sec": 2.9040985198616713,
                "throughput_tokens_per_sec": 145.20492599308358,
                "total_tokens_generated": 500,
                "num_runs": 10
            },
            "energy_performance": {
                "cpu_power": 112.5,
                "gpu_power": 755.8817175114579,
                "ram_power": 188.8243260383606,
                "cpu_energy": 0.00011429962785041425,
                "gpu_energy": 0.0007476347647781267,
                "ram_energy": 0.00019131300056627178,
                "total_energy_consumed_kwh": 0.0010532473931948129,
                "total_energy_consumed_joules": 3791.690615501326,
                "energy_efficiency_tokens_per_joule": 0.13186729897104,
                "final_emissions": 0.000401234594437564
            },
            "task-specific_performance": {}
        }
    },
    {
        "experiment_setup": {
            "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "task_type": "text_generation",
            "date": "2025-03-04",
            "cpu_count": 128,
            "cpu_model": "AMD EPYC 7742 64-Core Processor",
            "gpu_count": 4,
            "gpu_model": "4 x NVIDIA A100-PCIE-40GB",
            "os": "Linux-5.15.0-113-generic-x86_64-with-glibc2.31",
            "python_version": "3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]",
            "accelerate_config": {
                "distributed_type": "DistributedType.MULTI_GPU",
                "num_processes": 4,
                "local_process_index": 2
            },
            "country": "Germany",
            "region": "mecklenburg-vorpommern"
        },
        "experiment_variables": {
            "input_tokens": 5120,
            "output_tokens": 500
        },
        "experiment_results": {
            "inference_performance": {
                "total_inference_time_sec": 3.457033898681402,
                "average_latency_ms_per_batch": 1728.516949340701,
                "average_ttft_ms": 0.0,
                "throughput_queries_per_sec": 2.8926531509610727,
                "throughput_tokens_per_sec": 144.63265754805363,
                "total_tokens_generated": 500,
                "num_runs": 10
            },
            "energy_performance": {
                "cpu_power": 112.5,
                "gpu_power": 755.4807256456219,
                "ram_power": 188.8243260383606,
                "cpu_energy": 0.00011523479368770496,
                "gpu_energy": 0.0007489180991342437,
                "ram_energy": 0.00019291064778252832,
                "total_energy_consumed_kwh": 0.001057063540604477,
                "total_energy_consumed_joules": 3805.4287461761173,
                "energy_efficiency_tokens_per_joule": 0.13139123955544424,
                "final_emissions": 0.00040268835579327554
            },
            "task-specific_performance": {}
        }
    }
]